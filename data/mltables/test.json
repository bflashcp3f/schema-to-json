{"2210.00044v1_table0": {"table_code": "\\begin{table}[t]\n    \\centering\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{cccccc} \\hline\n    {\\bf Setting} & {\\bf Task} & {\\bf Train} & {\\bf Val} & {\\bf Test} & {\\bf Classes} \\\\ \\hline\n    \\multirow{5}{1em}{\\rotatebox[origin=c]{90}{Diverse}}  & Group 1 & 44254 & 11148 & 28315 & 2205 \\\\\n    & Group 2 & 39867 & 10202 & 22713  & 1874 \\\\\n    & Group 3 & 37477 & 9386 & 23095 & 1849 \\\\\n    & Group 4 & 35264 & 8871 & 22157 & 2119 \\\\\n    & Group 5 & 24454 & 6028 & 14490 & 1777 \\\\ \\hline\n    \\multirow{5}{1em}{\\rotatebox[origin=c]{90}{Taxonomy}} \n    & Animals & 37270 & 9237 & 22588 & 1331 \\\\\n    & Food & 26191 & 6612 & 15967 & 1365 \\\\\n    & Interior & 43576 & 11038 & 26594 & 2096 \\\\\n    & Sports & 32885 & 8468 & 19205 & 1471 \\\\\n    & Transport & 41394 &10280 & 25416 & 1954 \\\\\\hline\n    \\multirow{5}{1em}{\\rotatebox[origin=c]{90}{Question}} \n    & Action & 18730 & 4700 & 11008 & 233 \\\\\n    & Color & 34588 & 8578 & 21559 & 92 \\\\\n    & Count & 38857 & 9649 & 23261 & 42 \\\\\n    & Scene & 25850 & 6417 & 14847 & 170 \\\\\n    & Subcategory & 22324 & 8578 & 21559 & 659 \\\\\\hline\n    \\end{tabular}}\n    \\caption{Statistics per task within each setting.} \n    \\label{tab:stats}\n\\end{table}", "table_label": "{tab:stats}", "table_numeric_cells": [["44254", "44254", 266, 271, 266, 271], ["11148", "11148", 274, 279, 274, 279], ["28315", "28315", 282, 287, 282, 287], ["2205", "2205", 290, 294, 290, 294], ["39867", "39867", 314, 319, 314, 319], ["10202", "10202", 322, 327, 322, 327], ["22713", "22713", 330, 335, 330, 335], ["1874", "1874", 339, 343, 339, 343], ["37477", "37477", 363, 368, 363, 368], ["9386", "9386", 371, 375, 371, 375], ["23095", "23095", 378, 383, 378, 383], ["1849", "1849", 386, 390, 386, 390], ["35264", "35264", 410, 415, 410, 415], ["8871", "8871", 418, 422, 418, 422], ["22157", "22157", 425, 430, 425, 430], ["2119", "2119", 433, 437, 433, 437], ["24454", "24454", 457, 462, 457, 462], ["6028", "6028", 465, 469, 465, 469], ["14490", "14490", 472, 477, 472, 477], ["1777", "1777", 480, 484, 480, 484], ["37270", "37270", 570, 575, 570, 575], ["9237", "9237", 578, 582, 578, 582], ["22588", "22588", 585, 590, 585, 590], ["1331", "1331", 593, 597, 593, 597], ["26191", "26191", 614, 619, 614, 619], ["6612", "6612", 622, 626, 622, 626], ["15967", "15967", 629, 634, 629, 634], ["1365", "1365", 637, 641, 637, 641], ["43576", "43576", 662, 667, 662, 667], ["11038", "11038", 670, 675, 670, 675], ["26594", "26594", 678, 683, 678, 683], ["2096", "2096", 686, 690, 686, 690], ["32885", "32885", 709, 714, 709, 714], ["8468", "8468", 717, 721, 717, 721], ["19205", "19205", 724, 729, 724, 729], ["1471", "1471", 732, 736, 732, 736], ["41394", "41394", 758, 763, 758, 763], ["10280", "10280", 765, 770, 765, 770], ["25416", "25416", 773, 778, 773, 778], ["1954", "1954", 781, 785, 781, 785], ["18730", "18730", 869, 874, 869, 874], ["4700", "4700", 877, 881, 877, 881], ["11008", "11008", 884, 889, 884, 889], ["233", "233", 892, 895, 892, 895], ["34588", "34588", 913, 918, 913, 918], ["8578", "8578", 921, 925, 921, 925], ["21559", "21559", 928, 933, 928, 933], ["92", "92", 936, 938, 936, 938], ["38857", "38857", 956, 961, 956, 961], ["9649", "9649", 964, 968, 964, 968], ["23261", "23261", 971, 976, 971, 976], ["42", "42", 979, 981, 979, 981], ["25850", "25850", 999, 1004, 999, 1004], ["6417", "6417", 1007, 1011, 1007, 1011], ["14847", "14847", 1014, 1019, 1014, 1019], ["170", "170", 1022, 1025, 1022, 1025], ["22324", "22324", 1049, 1054, 1049, 1054], ["8578", "8578", 1057, 1061, 1057, 1061], ["21559", "21559", 1064, 1069, 1064, 1069], ["659", "659", 1072, 1075, 1072, 1075]], "text_chunk_selected": "\\section{Settings for Continual VQA} \\label{sec:settings}\n\\subsection{Problem formulation}\nIn continual learning, model parameters $\\bm{\\theta}$ are incrementally updated as new data become available.\nWe assume that samples from tasks $t=1\\dots T$ arrive sequentially as $D_t=\\{\\bm{x}_i, \\bm{y}_i\\}_{i=1}^{N_t}$, where $N_t$ is the number of data for task $t$.\nFollowing previous work, VQA is formulated as a multi-label classification problem with soft targets $\\bm{y}_i$~\\cite{Anderson2017up-down}.\nStarting from parameters $\\bm{\\theta}_{t-1}$ of the previous model, the updated parameters $\\bm{\\theta}_{t}$ are obtained by training on the new data $D_t$. \nSome approaches also use a memory $M_t$ containing a subset of samples from previous tasks, e.g. $D_{1}, \\dots, D_{t-1}$.\nIn our setup, all tasks share a common output head which is extended with new classes from each task.\nThis allows inference to be task-agnostic but creates a more challenging setting than multi-head learning where separate heads are learned for each task~\\cite{hussain2021towards}.\nAt the end of the training sequence, the objective is to achieve strong performance across all tasks observed so far. \nThis objective encloses two challenges: 1) minimizing catastrophic forgetting of tasks seen earlier in training, 2) facilitating positive transfer to improve performance on new tasks~\\cite{HADSELL20201028embracing}. \n\n\\subsection{Task settings}\nWe define three continual learning settings for VQA based on different task definitions $D_t$, as summarized in Table~\\ref{tab:stats}.\nTwo of these settings are based on visual object categories, see Subsection \\ref{sec:vis_splits} and one setting is motivated by language capabilities, see  Subsection \\ref{sec:q_splits}.\nConcurrent work~\\cite{lei2022symbolic} has followed a similar definition of continual learning settings for VQA. However, our work focuses on understanding how differences in task definitions affect the difficulty of the continual learning problem.\nWe study this problem from the point of view of both the downstream performance as well as the quality of the learned representations. \nThis is in line with work on holistic evaluation frameworks for grounded language learning~\\cite{suglia-etal-2020-compguesswhat}.\n\n\\subsubsection{Visual Settings} \\label{sec:vis_splits}\nWe design two settings based on visual object categories, which correspond to expanding the domain on which the VQA system is applied to. We take advantage of the fact that images in the VQA-v2 dataset\noriginate from the COCO dataset~\\cite{lin2014coco} which provides object-level image annotations.\nFollowing previous work in image captioning~\\cite{del2020ratt}, we organize $50$ object categories into five groups.\nImages with objects from multiple groups are discarded in order to create clean task splits $D_t$ --\nresulting in a total of 181K train, 45K validation, and 110K test samples. \n\n\\subsubsection{Language Setting}\\label{sec:q_splits}\nWe create a third setting {\\em Question Types}, where each task corresponds to learning to answer a different category of questions.\nWe use a classification scheme developed by~\\citet{whitehead2021separating}\nto form a sequence of five tasks: \nCount, Color, Scene-level, Subcategory, and Action recognition. The splits for Count, Color, and Subcategory questions are obtained from~\\citet{whitehead2021separating}. We create two additional tasks from the remaining questions.\nIn particular, we cluster question embeddings from Sentence-BERT~\\cite{reimers-2019-sentence-bert} \\footnote{We use the `all-MiniLM-L6-v2' model and Fast Clustering algorithm from the sentence-transformers package (\\url{https://www.sbert.net/}).} so that each cluster has at least 15 questions and a minimum cosine similarity of 0.8 between all embeddings.\nWe annotate clusters as `scene', `action' or `irrelevant' question types.\nBased on a seed of 10K annotated questions, we retrieve all other questions with similarity above 0.8 and label them using the K-nearest neighbor algorithm ($K=5$).  \nQuestion Types have a total of 140K train, 35K validation and 84K test samples (cf.\\ Table~\\ref{tab:stats}). \nCommon question words and answers per task are presented in the Appendix (Figure~\\ref{fig:top_ans_q}). \n\nWe also experiment with a baseline Pseudo-Replay method for the Question Types setting.\nInstead of storing raw data from previous tasks, we use a data augmentation method, inspired by~\\cite{kafle2017data,kil2021discovering}. \nWhen training on task $t$, we augment the data $D_t$ by retrieving past questions based on their shared detected objects classes. For example, if an elephant is detected on the current picture, we retrieve a past question about an elephant. We then use the previous model $f_{\\theta_{t-1}}$ to generate a distribution $\\tilde{ \\bm{y}} = f_{\\theta_{t-1}}(\\tilde{\\bm{x}})$ which serves as soft targets for the new sample $\\tilde{\\bm{x}}$.\nBy not storing the original answers, we address privacy and efficiency concerns of replay approaches~\\cite{van2018generative,Delangesurvey}.\n\n\\paragraph{Experimental Setup.} \nWe first look into pairwise task relationships following studies in transfer~\\cite{zamir2018taskonomy} and multitask learning ~\\cite{pmlr-v119-standley20a, lu202012}.\nIn particular, we measure the extent to which each task is forgotten after training on a second task. \nWe finetune a pretrained model on Task $T_{1}$ and compute the accuracy $A_{11}$ on its test set. Then, we finetune this model on another Task $T_{2}$ and compute the new accuracy $A_{12}$ on the test set of $T_{1}$.\nForgetting is measured as the relative accuracy drop: $(A_{12} - A_{11}) / A_{11}$.\nGiven the varying dataset sizes, we finetune on $T_{2}$ for a fixed number of 400 steps using a batch size of 512 and learning rate 5e-5.\n\nNext, we compute the Spearman correlation between the relative accuracy drops and different factors of task dissimilarity.\nHere, we consider the answer distributions , as well as average embeddings of the image, question and the joint pair.\nConsider $P$, $Q$  the answer distributions of Tasks $T_1,T_2$ respectively.\nSince some answers of $T_1$ do not appear in $T_2$, we measure the skew divergence~\\cite{pmlr-vR3-lee01a} between $P$ and $Q$ as the KL divergence between $P$ and a mixture distribution \\mbox{$(1-\\alpha) P + \\alpha Q$} with $\\alpha=0.99$~\\cite{ruder-plank-2017-learning}.\nFor the input embeddings, we measure the cosine distance between the average task representation. \nAs image representations, we utilize Faster R-CNN features from~\\cite{Anderson2017up-down}, while questions are embedded using Sentence-BERT.\nJoint embeddings for image-question pairs are obtained using the final layer representation of the [CLS] token of UNITER~\\footnote{[CLS] is the first token of the input sequence which aggregates multimodal information. Its representation from the final encoder layer is passed to the classifier to predict an answer.}.\nThe detailed similarity measures are shown in the Appendix Figure~\\ref{fig:dissimilarity}.\n\n\\paragraph{Results.} \nTable~\\ref{tab:Tpairs} shows the relative accuracy drop for all task pairs. \nOverall, we observe that each setting has a distinct pattern. \nQuestion Types is evidently a more challenging setting, where several task combinations show more than $90\\%$ drop.\nWhen comparing the visual settings, forgetting in Diverse Domains fluctuates less depending on the task pairing. \nThis suggests that the task relationships in Taxonomy Domains might play a more important factor.\nAlthough some relations make sense based on the expected similarity of the visual scenes, e.g., low forgetting between Food and Interior, others are less intuitive, e.g., low forgetting between Transport and Interior.\nMoreover, certain second tasks seem to consistently affect the amount of forgetting after finetuning on them. \nBased on the total number of classes per task as shown in Table ~\\ref{tab:stats}, we notice that the model is more robust against forgetting when Task $T_2$ has a wide range of possible answers (e.g., Interior); while $T_2$ with a narrow answer set (e.g., Food, Color, Count) lead to maximum forgetting.", "table_source": "\\begin{table}[t]\n    \\centering\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{cccccc} \\hline\n    {\\bf Setting} & {\\bf Task} & {\\bf Train} & {\\bf Val} & {\\bf Test} & {\\bf Classes} \\\\ \\hline\n    \\multirow{5}{1em}{\\rotatebox[origin=c]{90}{Diverse}}  & Group 1 & 44254 & 11148 & 28315 & 2205 \\\\\n    & Group 2 & 39867 & 10202 & 22713  & 1874 \\\\\n    & Group 3 & 37477 & 9386 & 23095 & 1849 \\\\\n    & Group 4 & 35264 & 8871 & 22157 & 2119 \\\\\n    & Group 5 & 24454 & 6028 & 14490 & 1777 \\\\ \\hline\n    \\multirow{5}{1em}{\\rotatebox[origin=c]{90}{Taxonomy}} \n    & Animals & 37270 & 9237 & 22588 & 1331 \\\\\n    & Food & 26191 & 6612 & 15967 & 1365 \\\\\n    & Interior & 43576 & 11038 & 26594 & 2096 \\\\\n    & Sports & 32885 & 8468 & 19205 & 1471 \\\\\n    & Transport & 41394 &10280 & 25416 & 1954 \\\\\\hline\n    \\multirow{5}{1em}{\\rotatebox[origin=c]{90}{Question}} \n    & Action & 18730 & 4700 & 11008 & 233 \\\\\n    & Color & 34588 & 8578 & 21559 & 92 \\\\\n    & Count & 38857 & 9649 & 23261 & 42 \\\\\n    & Scene & 25850 & 6417 & 14847 & 170 \\\\\n    & Subcategory & 22324 & 8578 & 21559 & 659 \\\\\\hline\n    \\end{tabular}}\n    \\caption{Statistics per task within each setting.} \n    \\label{tab:stats}\n\\end{table}", "cell_list_gold": [{"value": "44254", "char_index": [266, 271], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 1 Train", "dataset features": {"xx": "yy"}}, {"value": "11148", "char_index": [274, 279], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 1 Val", "dataset features": {"xx": "yy"}}, {"value": "28315", "char_index": [282, 287], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 1 Test", "dataset features": {"xx": "yy"}}, {"value": "2205", "char_index": [290, 294], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of classes", "sub-set/group name": "Group 1", "dataset features": {"xx": "yy"}}, {"value": "39867", "char_index": [314, 319], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 2 Train", "dataset features": {"xx": "yy"}}, {"value": "10202", "char_index": [322, 327], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 2 Val", "dataset features": {"xx": "yy"}}, {"value": "22713", "char_index": [330, 335], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 2 Test", "dataset features": {"xx": "yy"}}, {"value": "1874", "char_index": [339, 343], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of classes", "sub-set/group name": "Group 2", "dataset features": {"xx": "yy"}}, {"value": "37477", "char_index": [363, 368], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 3 Train", "dataset features": {"xx": "yy"}}, {"value": "9386", "char_index": [371, 375], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 3 Val", "dataset features": {"xx": "yy"}}, {"value": "23095", "char_index": [378, 383], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 3 Test", "dataset features": {"xx": "yy"}}, {"value": "1849", "char_index": [386, 390], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of classes", "sub-set/group name": "Group 3", "dataset features": {"xx": "yy"}}, {"value": "35264", "char_index": [410, 415], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 4 Train", "dataset features": {"xx": "yy"}}, {"value": "8871", "char_index": [418, 422], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 4 Val", "dataset features": {"xx": "yy"}}, {"value": "22157", "char_index": [425, 430], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 4 Test", "dataset features": {"xx": "yy"}}, {"value": "2119", "char_index": [433, 437], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of classes", "sub-set/group name": "Group 4", "dataset features": {"xx": "yy"}}, {"value": "24454", "char_index": [457, 462], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 5 Train", "dataset features": {"xx": "yy"}}, {"value": "6028", "char_index": [465, 469], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 5 Val", "dataset features": {"xx": "yy"}}, {"value": "14490", "char_index": [472, 477], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of samples", "sub-set/group name": "Group 5 Test", "dataset features": {"xx": "yy"}}, {"value": "1777", "char_index": [480, 484], "type": "Data Stat.", "dataset": "VQA-v2 Diverse", "attribute name": "number of classes", "sub-set/group name": "Group 5", "dataset features": {"xx": "yy"}}, {"value": "37270", "char_index": [570, 575], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Animals Train", "dataset features": {"xx": "yy"}}, {"value": "9237", "char_index": [578, 582], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Animals Val", "dataset features": {"xx": "yy"}}, {"value": "22588", "char_index": [585, 590], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Animals Test", "dataset features": {"xx": "yy"}}, {"value": "1331", "char_index": [593, 597], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of classes", "sub-set/group name": "Animals", "dataset features": {"xx": "yy"}}, {"value": "26191", "char_index": [614, 619], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Food Train", "dataset features": {"xx": "yy"}}, {"value": "6612", "char_index": [622, 626], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Food Val", "dataset features": {"xx": "yy"}}, {"value": "15967", "char_index": [629, 634], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Food Test", "dataset features": {"xx": "yy"}}, {"value": "1365", "char_index": [637, 641], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of classes", "sub-set/group name": "Food", "dataset features": {"xx": "yy"}}, {"value": "43576", "char_index": [662, 667], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Interior Train", "dataset features": {"xx": "yy"}}, {"value": "11038", "char_index": [670, 675], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Interior Val", "dataset features": {"xx": "yy"}}, {"value": "26594", "char_index": [678, 683], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Interior Test", "dataset features": {"xx": "yy"}}, {"value": "2096", "char_index": [686, 690], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of classes", "sub-set/group name": "Interior", "dataset features": {"xx": "yy"}}, {"value": "32885", "char_index": [709, 714], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Sports Train", "dataset features": {"xx": "yy"}}, {"value": "8468", "char_index": [717, 721], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Sports Val", "dataset features": {"xx": "yy"}}, {"value": "19205", "char_index": [724, 729], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Sports Test", "dataset features": {"xx": "yy"}}, {"value": "1471", "char_index": [732, 736], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of classes", "sub-set/group name": "Sports", "dataset features": {"xx": "yy"}}, {"value": "41394", "char_index": [758, 763], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Transport Train", "dataset features": {"xx": "yy"}}, {"value": "10280", "char_index": [765, 770], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Transport Val", "dataset features": {"xx": "yy"}}, {"value": "25416", "char_index": [773, 778], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of samples", "sub-set/group name": "Transport Test", "dataset features": {"xx": "yy"}}, {"value": "1954", "char_index": [781, 785], "type": "Data Stat.", "dataset": "VQA-v2 Taxonomy", "attribute name": "number of classes", "sub-set/group name": "Transport", "dataset features": {"xx": "yy"}}, {"value": "18730", "char_index": [869, 874], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Action Train", "dataset features": {"xx": "yy"}}, {"value": "4700", "char_index": [877, 881], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Action Val", "dataset features": {"xx": "yy"}}, {"value": "11008", "char_index": [884, 889], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Action Test", "dataset features": {"xx": "yy"}}, {"value": "233", "char_index": [892, 895], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of classes", "sub-set/group name": "Action", "dataset features": {"xx": "yy"}}, {"value": "34588", "char_index": [913, 918], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Color Train", "dataset features": {"xx": "yy"}}, {"value": "8578", "char_index": [921, 925], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Color Val", "dataset features": {"xx": "yy"}}, {"value": "21559", "char_index": [928, 933], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Color Test", "dataset features": {"xx": "yy"}}, {"value": "92", "char_index": [936, 938], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of classes", "sub-set/group name": "Color", "dataset features": {"xx": "yy"}}, {"value": "38857", "char_index": [956, 961], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Count Train", "dataset features": {"xx": "yy"}}, {"value": "9649", "char_index": [964, 968], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Count Val", "dataset features": {"xx": "yy"}}, {"value": "23261", "char_index": [971, 976], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Count Test", "dataset features": {"xx": "yy"}}, {"value": "42", "char_index": [979, 981], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of classes", "sub-set/group name": "Count", "dataset features": {"xx": "yy"}}, {"value": "25850", "char_index": [999, 1004], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Scene Train", "dataset features": {"xx": "yy"}}, {"value": "6417", "char_index": [1007, 1011], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Scene Val", "dataset features": {"xx": "yy"}}, {"value": "14847", "char_index": [1014, 1019], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Scene Test", "dataset features": {"xx": "yy"}}, {"value": "170", "char_index": [1022, 1025], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of classes", "sub-set/group name": "Scene", "dataset features": {"xx": "yy"}}, {"value": "22324", "char_index": [1049, 1054], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Subcategory Train", "dataset features": {"xx": "yy"}}, {"value": "8578", "char_index": [1057, 1061], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Subcategory Val", "dataset features": {"xx": "yy"}}, {"value": "21559", "char_index": [1064, 1069], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of samples", "sub-set/group name": "Subcategory Test", "dataset features": {"xx": "yy"}}, {"value": "659", "char_index": [1072, 1075], "type": "Data Stat.", "dataset": "VQA-v2 Question", "attribute name": "number of classes", "sub-set/group name": "Subcategory", "dataset features": {"xx": "yy"}}]}, "2210.00044v1_table2": {"table_code": "\\begin{table}[h]\n    \\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{l|ccccc}\n\\multicolumn{6}{l}{{\\bf Diverse Domains}} \\\\\\hline\n\\diagbox[width=6em]{Task 1}{Task 2}\n& Group 1 & Group 2 & Group 3 & Group 4 & Group 5 \\\\\\hline\nGroup 1 & \\cellcolor{gray!60} - & \\cellcolor{red!5.47985781990521}  -6.58 & \\cellcolor{red!4.344391785150075}  -5.21 & \\cellcolor{SoftGreen!3.139810426540407}  -4.84 & \\cellcolor{red!5.91182859399684}  -7.09 \\\\\nGroup 2 & \\cellcolor{SoftGreen!9.010600706713689}  -4.55 & \\cellcolor{gray!60} - & \\cellcolor{red!4.674617196702005}  -5.61 & \\cellcolor{SoftGreen!9.893992932862119}  -4.51 & \\cellcolor{SoftGreen!0.17667844522966902}  -4.99 \\\\\nGroup 3 & \\cellcolor{SoftGreen!7.1015106593253705}  -4.64 & \\cellcolor{red!6.988564167725539}  -8.39 & \\cellcolor{gray!60} - & \\cellcolor{red!6.141465480728503}  -7.37 & \\cellcolor{red!9.718104381382652}  -11.66 \\\\\nGroup 4 & \\cellcolor{SoftGreen!6.197139781639095}  -4.69 & \\cellcolor{red!5.9203444564047425}  -7.10 & \\cellcolor{red!6.163821825823982}  -7.40 & \\cellcolor{gray!60} - & \\cellcolor{red!8.02193859244451}  -9.63 \\\\\nGroup 5 & \\cellcolor{SoftGreen!14.294750158127727}  -4.29 & \\cellcolor{red!4.849251528568416}  -5.82 & \\cellcolor{red!5.073265865485981}  -6.09 & \\cellcolor{SoftGreen!24.09867172675527}  -3.80 & \\cellcolor{gray!60} - \\\\\\hline\n\\multicolumn{6}{l}{}\\\\[2pt]\n\n\\multicolumn{6}{l}{{\\bf Taxonomy Domains}} \\\\\\hline\n\\diagbox[width=6em]{Task 1}{Task 2} \n& Animals & Food & Interior & Sports & Transport \\\\\\hline\nAnimals & \\cellcolor{gray!60} - & \\cellcolor{red!6.719879929048997}  -8.06 & \\cellcolor{SoftGreen!27.41165234001882}  -3.63 & \\cellcolor{red!4.866512029835813}  -5.84 & \\cellcolor{SoftGreen!12.948560513030117}  -4.35 \\\\\nFood & \\cellcolor{red!13.65079365079365}  -16.38 & \\cellcolor{gray!60} - & \\cellcolor{SoftGreen!14.285714285714192}  -4.29 & \\cellcolor{red!14.232804232804229}  -17.08 & \\cellcolor{red!9.947089947089951}  -11.94 \\\\\nInterior & \\cellcolor{red!4.788538155072028}  -5.75 & \\cellcolor{red!4.328838492185116}  -5.19 & \\cellcolor{gray!60} - & \\cellcolor{red!6.3591786699356465}  -7.63 & \\cellcolor{SoftGreen!43.30370824394703}  -2.83 \\\\\nSports & \\cellcolor{red!9.689658306070521}  -11.63 & \\cellcolor{red!15.164940021810253}  -18.20 & \\cellcolor{red!7.997091966557623}  -9.60 & \\cellcolor{gray!60} - & \\cellcolor{red!7.894856415848786}  -9.47 \\\\\nTransport & \\cellcolor{SoftGreen!16.27906976744177}  -4.19 & \\cellcolor{red!7.067183462532299}  -8.48 & \\cellcolor{SoftGreen!47.596899224806265}  -2.62 & \\cellcolor{SoftGreen!26.511627906976827}  -3.67 & \\cellcolor{gray!60} - \\\\\\hline\n\\multicolumn{6}{l}{}\\\\[2pt]\n    \n\\multicolumn{6}{l}{{\\bf Question Types}} \\\\\\hline\n\\diagbox[width=6em]{Task 1}{Task 2}\n& Action & Color & Count & Scene & Subcat. \\\\\\hline\nAction & \\cellcolor{gray!60} - & \\cellcolor{red!57.00123915737299}  -68.40 & \\cellcolor{red!75.37495192923984}  -90.45 & \\cellcolor{red!16.32269367175149}  -19.59 & \\cellcolor{red!10.479425714651969}  -12.58 \\\\\nColor & \\cellcolor{red!74.07521705139284}  -88.89 & \\cellcolor{gray!60} - & \\cellcolor{red!83.04530304900631}  -99.65 & \\cellcolor{red!23.12471711311361}  -27.75 & \\cellcolor{red!52.05118709624326}  -62.46 \\\\\nCount & \\cellcolor{red!82.64429312581063}  -99.17 & \\cellcolor{red!83.063121487246}  -99.68 & \\cellcolor{gray!60} - & \\cellcolor{red!81.26621271076525}  -97.52 & \\cellcolor{red!72.4978383052313}  -87.00 \\\\\nScene & \\cellcolor{red!9.091433848995615}  -10.91 & \\cellcolor{red!28.66928346032479}  -34.40 & \\cellcolor{red!64.7752636034788}  -77.73 & \\cellcolor{gray!60} - & \\cellcolor{red!12.679904563996002}  -15.22 \\\\\nSubcat. & \\cellcolor{red!26.44189628615437}  -31.73 & \\cellcolor{red!71.21056534885048}  -85.45 & \\cellcolor{red!80.12436533744082}  -96.15 & \\cellcolor{red!25.457812767414}  -30.55 & \\cellcolor{gray!60} - \\\\\\hline\n\\multicolumn{6}{l}{}\\\\[2pt]\n\\end{tabular}}\n    \\caption{Task difficulty measured by forgetting in pairwise tasks.\n Non-diagonal elements show relative accuracy drop (\\%) after finetuning on Task 2.}\n    \\label{tab:Tpairs}\n\\end{table}", "table_label": "{tab:Tpairs}", "table_numeric_cells": [["-6.58", "\\cellcolor{red!5.47985781990521}  -6.58", 300, 305, 266, 305], ["-5.21", "\\cellcolor{red!4.344391785150075}  -5.21", 343, 348, 308, 348], ["-4.84", "\\cellcolor{SoftGreen!3.139810426540407}  -4.84", 392, 397, 351, 397], ["-7.09", "\\cellcolor{red!5.91182859399684}  -7.09", 434, 439, 400, 439], ["-4.55", "\\cellcolor{SoftGreen!9.010600706713689}  -4.55", 494, 499, 453, 499], ["-5.61", "\\cellcolor{red!4.674617196702005}  -5.61", 561, 566, 526, 566], ["-4.51", "\\cellcolor{SoftGreen!9.893992932862119}  -4.51", 610, 615, 569, 615], ["-4.99", "\\cellcolor{SoftGreen!0.17667844522966902}  -4.99", 661, 666, 618, 666], ["-4.64", "\\cellcolor{SoftGreen!7.1015106593253705}  -4.64", 722, 727, 680, 727], ["-8.39", "\\cellcolor{red!6.988564167725539}  -8.39", 765, 770, 730, 770], ["-7.37", "\\cellcolor{red!6.141465480728503}  -7.37", 832, 837, 797, 837], ["-11.66", "\\cellcolor{red!9.718104381382652}  -11.66", 875, 881, 840, 881], ["-4.69", "\\cellcolor{SoftGreen!6.197139781639095}  -4.69", 936, 941, 895, 941], ["-7.10", "\\cellcolor{red!5.9203444564047425}  -7.10", 980, 985, 944, 985], ["-7.40", "\\cellcolor{red!6.163821825823982}  -7.40", 1023, 1028, 988, 1028], ["-9.63", "\\cellcolor{red!8.02193859244451}  -9.63", 1089, 1094, 1055, 1094], ["-4.29", "\\cellcolor{SoftGreen!14.294750158127727}  -4.29", 1150, 1155, 1108, 1155], ["-5.82", "\\cellcolor{red!4.849251528568416}  -5.82", 1193, 1198, 1158, 1198], ["-6.09", "\\cellcolor{red!5.073265865485981}  -6.09", 1236, 1241, 1201, 1241], ["-3.80", "\\cellcolor{SoftGreen!24.09867172675527}  -3.80", 1285, 1290, 1244, 1290], ["-8.06", "\\cellcolor{red!6.719879929048997}  -8.06", 1569, 1574, 1534, 1574], ["-3.63", "\\cellcolor{SoftGreen!27.41165234001882}  -3.63", 1618, 1623, 1577, 1623], ["-5.84", "\\cellcolor{red!4.866512029835813}  -5.84", 1661, 1666, 1626, 1666], ["-4.35", "\\cellcolor{SoftGreen!12.948560513030117}  -4.35", 1711, 1716, 1669, 1716], ["-16.38", "\\cellcolor{red!13.65079365079365}  -16.38", 1762, 1768, 1727, 1768], ["-4.29", "\\cellcolor{SoftGreen!14.285714285714192}  -4.29", 1837, 1842, 1795, 1842], ["-17.08", "\\cellcolor{red!14.232804232804229}  -17.08", 1881, 1887, 1845, 1887], ["-11.94", "\\cellcolor{red!9.947089947089951}  -11.94", 1925, 1931, 1890, 1931], ["-5.75", "\\cellcolor{red!4.788538155072028}  -5.75", 1981, 1986, 1946, 1986], ["-5.19", "\\cellcolor{red!4.328838492185116}  -5.19", 2024, 2029, 1989, 2029], ["-7.63", "\\cellcolor{red!6.3591786699356465}  -7.63", 2092, 2097, 2056, 2097], ["-2.83", "\\cellcolor{SoftGreen!43.30370824394703}  -2.83", 2141, 2146, 2100, 2146], ["-11.63", "\\cellcolor{red!9.689658306070521}  -11.63", 2194, 2200, 2159, 2200], ["-18.20", "\\cellcolor{red!15.164940021810253}  -18.20", 2239, 2245, 2203, 2245], ["-9.60", "\\cellcolor{red!7.997091966557623}  -9.60", 2283, 2288, 2248, 2288], ["-9.47", "\\cellcolor{red!7.894856415848786}  -9.47", 2350, 2355, 2315, 2355], ["-4.19", "\\cellcolor{SoftGreen!16.27906976744177}  -4.19", 2412, 2417, 2371, 2417], ["-8.48", "\\cellcolor{red!7.067183462532299}  -8.48", 2455, 2460, 2420, 2460], ["-2.62", "\\cellcolor{SoftGreen!47.596899224806265}  -2.62", 2505, 2510, 2463, 2510], ["-3.67", "\\cellcolor{SoftGreen!26.511627906976827}  -3.67", 2555, 2560, 2513, 2560], ["-68.40", "\\cellcolor{red!57.00123915737299}  -68.40", 2833, 2839, 2798, 2839], ["-90.45", "\\cellcolor{red!75.37495192923984}  -90.45", 2877, 2883, 2842, 2883], ["-19.59", "\\cellcolor{red!16.32269367175149}  -19.59", 2921, 2927, 2886, 2927], ["-12.58", "\\cellcolor{red!10.479425714651969}  -12.58", 2966, 2972, 2930, 2972], ["-88.89", "\\cellcolor{red!74.07521705139284}  -88.89", 3019, 3025, 2984, 3025], ["-99.65", "\\cellcolor{red!83.04530304900631}  -99.65", 3087, 3093, 3052, 3093], ["-27.75", "\\cellcolor{red!23.12471711311361}  -27.75", 3131, 3137, 3096, 3137], ["-62.46", "\\cellcolor{red!52.05118709624326}  -62.46", 3175, 3181, 3140, 3181], ["-99.17", "\\cellcolor{red!82.64429312581063}  -99.17", 3228, 3234, 3193, 3234], ["-99.68", "\\cellcolor{red!83.063121487246}  -99.68", 3270, 3276, 3237, 3276], ["-97.52", "\\cellcolor{red!81.26621271076525}  -97.52", 3338, 3344, 3303, 3344], ["-87.00", "\\cellcolor{red!72.4978383052313}  -87.00", 3381, 3387, 3347, 3387], ["-10.91", "\\cellcolor{red!9.091433848995615}  -10.91", 3434, 3440, 3399, 3440], ["-34.40", "\\cellcolor{red!28.66928346032479}  -34.40", 3478, 3484, 3443, 3484], ["-77.73", "\\cellcolor{red!64.7752636034788}  -77.73", 3521, 3527, 3487, 3527], ["-15.22", "\\cellcolor{red!12.679904563996002}  -15.22", 3590, 3596, 3554, 3596], ["-31.73", "\\cellcolor{red!26.44189628615437}  -31.73", 3645, 3651, 3610, 3651], ["-85.45", "\\cellcolor{red!71.21056534885048}  -85.45", 3689, 3695, 3654, 3695], ["-96.15", "\\cellcolor{red!80.12436533744082}  -96.15", 3733, 3739, 3698, 3739], ["-30.55", "\\cellcolor{red!25.457812767414}  -30.55", 3775, 3781, 3742, 3781]], "text_chunk_selected": "\\section{Settings for Continual VQA} \\label{sec:settings}\n\\subsection{Problem formulation}\nIn continual learning, model parameters $\\bm{\\theta}$ are incrementally updated as new data become available.\nWe assume that samples from tasks $t=1\\dots T$ arrive sequentially as $D_t=\\{\\bm{x}_i, \\bm{y}_i\\}_{i=1}^{N_t}$, where $N_t$ is the number of data for task $t$.\nFollowing previous work, VQA is formulated as a multi-label classification problem with soft targets $\\bm{y}_i$~\\cite{Anderson2017up-down}.\nStarting from parameters $\\bm{\\theta}_{t-1}$ of the previous model, the updated parameters $\\bm{\\theta}_{t}$ are obtained by training on the new data $D_t$. \nSome approaches also use a memory $M_t$ containing a subset of samples from previous tasks, e.g. $D_{1}, \\dots, D_{t-1}$.\nIn our setup, all tasks share a common output head which is extended with new classes from each task.\nThis allows inference to be task-agnostic but creates a more challenging setting than multi-head learning where separate heads are learned for each task~\\cite{hussain2021towards}.\nAt the end of the training sequence, the objective is to achieve strong performance across all tasks observed so far. \nThis objective encloses two challenges: 1) minimizing catastrophic forgetting of tasks seen earlier in training, 2) facilitating positive transfer to improve performance on new tasks~\\cite{HADSELL20201028embracing}. \n\n\\subsubsection{Language Setting}\\label{sec:q_splits}\nWe create a third setting {\\em Question Types}, where each task corresponds to learning to answer a different category of questions.\nWe use a classification scheme developed by~\\citet{whitehead2021separating}\nto form a sequence of five tasks: \nCount, Color, Scene-level, Subcategory, and Action recognition. The splits for Count, Color, and Subcategory questions are obtained from~\\citet{whitehead2021separating}. We create two additional tasks from the remaining questions.\nIn particular, we cluster question embeddings from Sentence-BERT~\\cite{reimers-2019-sentence-bert} \\footnote{We use the `all-MiniLM-L6-v2' model and Fast Clustering algorithm from the sentence-transformers package (\\url{https://www.sbert.net/}).} so that each cluster has at least 15 questions and a minimum cosine similarity of 0.8 between all embeddings.\nWe annotate clusters as `scene', `action' or `irrelevant' question types.\nBased on a seed of 10K annotated questions, we retrieve all other questions with similarity above 0.8 and label them using the K-nearest neighbor algorithm ($K=5$).  \nQuestion Types have a total of 140K train, 35K validation and 84K test samples (cf.\\ Table~\\ref{tab:stats}). \nCommon question words and answers per task are presented in the Appendix (Figure~\\ref{fig:top_ans_q}). \n\nWe also experiment with a baseline Pseudo-Replay method for the Question Types setting.\nInstead of storing raw data from previous tasks, we use a data augmentation method, inspired by~\\cite{kafle2017data,kil2021discovering}. \nWhen training on task $t$, we augment the data $D_t$ by retrieving past questions based on their shared detected objects classes. For example, if an elephant is detected on the current picture, we retrieve a past question about an elephant. We then use the previous model $f_{\\theta_{t-1}}$ to generate a distribution $\\tilde{ \\bm{y}} = f_{\\theta_{t-1}}(\\tilde{\\bm{x}})$ which serves as soft targets for the new sample $\\tilde{\\bm{x}}$.\nBy not storing the original answers, we address privacy and efficiency concerns of replay approaches~\\cite{van2018generative,Delangesurvey}.\n\n\\subsection{Evaluation Metrics}\nAfter training on task $t$, we compute the VQA accuracy $A_{t,i}$ on data from the previous task $i$. \nWe report the macro-average accuracy at the end of the training sequence: $\\mathrm{A} = \\frac{1}{T} \\sum_{i=1}^T A_{T,i}$.\nFollowing~\\citet{riemer2018learning}, we report the learned accuracy $\\mathrm{LA} = \\frac{1}{T} \\sum_{i=1}^T A_{i,i}$, which measures the ability to learn the new task $i$.\nWe also compute backward transfer $\\mathrm{BWT} = \\frac{1}{T-1} \\sum_{i=1}^{T-1} A_{T,i} - A_{i,i}$~\\cite{lopez2017GEM}, that captures the impact of catastrophic forgetting.\n\n\\begin{equation}\n   S_{T,i} = \\frac{1}{N} \\sum_{j=1}^{N} (1-\\mathrm{cos}(\\bm{e_{Tj}}, \\bm{e_{ij}}))\\cdot \\Delta^{Ti}_j\n\\end{equation}\n\n\\section{Results}\n\\subsection{Main Results}\n\\paragraph{Task Settings.}\nTable~\\ref{tab:results} summarizes the results averaged over five task orders using the UNITER backbone.\nThe results show an increasing difficulty for the three incremental learning task definitions, i.e. Diversity Domains $<$ Taxonomy Domains $<$ Question Types, which we will further investigate in Section \\ref{sec:task_relationships}. \n\n\\paragraph{Experimental Setup.} \nWe first look into pairwise task relationships following studies in transfer~\\cite{zamir2018taskonomy} and multitask learning ~\\cite{pmlr-v119-standley20a, lu202012}.\nIn particular, we measure the extent to which each task is forgotten after training on a second task. \nWe finetune a pretrained model on Task $T_{1}$ and compute the accuracy $A_{11}$ on its test set. Then, we finetune this model on another Task $T_{2}$ and compute the new accuracy $A_{12}$ on the test set of $T_{1}$.\nForgetting is measured as the relative accuracy drop: $(A_{12} - A_{11}) / A_{11}$.\nGiven the varying dataset sizes, we finetune on $T_{2}$ for a fixed number of 400 steps using a batch size of 512 and learning rate 5e-5.\n\n\\paragraph{Results.} \nTable~\\ref{tab:Tpairs} shows the relative accuracy drop for all task pairs. \nOverall, we observe that each setting has a distinct pattern. \nQuestion Types is evidently a more challenging setting, where several task combinations show more than $90\\%$ drop.\nWhen comparing the visual settings, forgetting in Diverse Domains fluctuates less depending on the task pairing. \nThis suggests that the task relationships in Taxonomy Domains might play a more important factor.\nAlthough some relations make sense based on the expected similarity of the visual scenes, e.g., low forgetting between Food and Interior, others are less intuitive, e.g., low forgetting between Transport and Interior.\nMoreover, certain second tasks seem to consistently affect the amount of forgetting after finetuning on them. \nBased on the total number of classes per task as shown in Table ~\\ref{tab:stats}, we notice that the model is more robust against forgetting when Task $T_2$ has a wide range of possible answers (e.g., Interior); while $T_2$ with a narrow answer set (e.g., Food, Color, Count) lead to maximum forgetting.", "table_source": "\\begin{table}[h]\n    \\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{l|ccccc}\n\\multicolumn{6}{l}{{\\bf Diverse Domains}} \\\\\\hline\n\\diagbox[width=6em]{Task 1}{Task 2}\n& Group 1 & Group 2 & Group 3 & Group 4 & Group 5 \\\\\\hline\nGroup 1 & \\cellcolor{gray!60} - & \\cellcolor{red!5.47985781990521}  -6.58 & \\cellcolor{red!4.344391785150075}  -5.21 & \\cellcolor{SoftGreen!3.139810426540407}  -4.84 & \\cellcolor{red!5.91182859399684}  -7.09 \\\\\nGroup 2 & \\cellcolor{SoftGreen!9.010600706713689}  -4.55 & \\cellcolor{gray!60} - & \\cellcolor{red!4.674617196702005}  -5.61 & \\cellcolor{SoftGreen!9.893992932862119}  -4.51 & \\cellcolor{SoftGreen!0.17667844522966902}  -4.99 \\\\\nGroup 3 & \\cellcolor{SoftGreen!7.1015106593253705}  -4.64 & \\cellcolor{red!6.988564167725539}  -8.39 & \\cellcolor{gray!60} - & \\cellcolor{red!6.141465480728503}  -7.37 & \\cellcolor{red!9.718104381382652}  -11.66 \\\\\nGroup 4 & \\cellcolor{SoftGreen!6.197139781639095}  -4.69 & \\cellcolor{red!5.9203444564047425}  -7.10 & \\cellcolor{red!6.163821825823982}  -7.40 & \\cellcolor{gray!60} - & \\cellcolor{red!8.02193859244451}  -9.63 \\\\\nGroup 5 & \\cellcolor{SoftGreen!14.294750158127727}  -4.29 & \\cellcolor{red!4.849251528568416}  -5.82 & \\cellcolor{red!5.073265865485981}  -6.09 & \\cellcolor{SoftGreen!24.09867172675527}  -3.80 & \\cellcolor{gray!60} - \\\\\\hline\n\\multicolumn{6}{l}{}\\\\[2pt]\n\n\\multicolumn{6}{l}{{\\bf Taxonomy Domains}} \\\\\\hline\n\\diagbox[width=6em]{Task 1}{Task 2} \n& Animals & Food & Interior & Sports & Transport \\\\\\hline\nAnimals & \\cellcolor{gray!60} - & \\cellcolor{red!6.719879929048997}  -8.06 & \\cellcolor{SoftGreen!27.41165234001882}  -3.63 & \\cellcolor{red!4.866512029835813}  -5.84 & \\cellcolor{SoftGreen!12.948560513030117}  -4.35 \\\\\nFood & \\cellcolor{red!13.65079365079365}  -16.38 & \\cellcolor{gray!60} - & \\cellcolor{SoftGreen!14.285714285714192}  -4.29 & \\cellcolor{red!14.232804232804229}  -17.08 & \\cellcolor{red!9.947089947089951}  -11.94 \\\\\nInterior & \\cellcolor{red!4.788538155072028}  -5.75 & \\cellcolor{red!4.328838492185116}  -5.19 & \\cellcolor{gray!60} - & \\cellcolor{red!6.3591786699356465}  -7.63 & \\cellcolor{SoftGreen!43.30370824394703}  -2.83 \\\\\nSports & \\cellcolor{red!9.689658306070521}  -11.63 & \\cellcolor{red!15.164940021810253}  -18.20 & \\cellcolor{red!7.997091966557623}  -9.60 & \\cellcolor{gray!60} - & \\cellcolor{red!7.894856415848786}  -9.47 \\\\\nTransport & \\cellcolor{SoftGreen!16.27906976744177}  -4.19 & \\cellcolor{red!7.067183462532299}  -8.48 & \\cellcolor{SoftGreen!47.596899224806265}  -2.62 & \\cellcolor{SoftGreen!26.511627906976827}  -3.67 & \\cellcolor{gray!60} - \\\\\\hline\n\\multicolumn{6}{l}{}\\\\[2pt]\n    \n\\multicolumn{6}{l}{{\\bf Question Types}} \\\\\\hline\n\\diagbox[width=6em]{Task 1}{Task 2}\n& Action & Color & Count & Scene & Subcat. \\\\\\hline\nAction & \\cellcolor{gray!60} - & \\cellcolor{red!57.00123915737299}  -68.40 & \\cellcolor{red!75.37495192923984}  -90.45 & \\cellcolor{red!16.32269367175149}  -19.59 & \\cellcolor{red!10.479425714651969}  -12.58 \\\\\nColor & \\cellcolor{red!74.07521705139284}  -88.89 & \\cellcolor{gray!60} - & \\cellcolor{red!83.04530304900631}  -99.65 & \\cellcolor{red!23.12471711311361}  -27.75 & \\cellcolor{red!52.05118709624326}  -62.46 \\\\\nCount & \\cellcolor{red!82.64429312581063}  -99.17 & \\cellcolor{red!83.063121487246}  -99.68 & \\cellcolor{gray!60} - & \\cellcolor{red!81.26621271076525}  -97.52 & \\cellcolor{red!72.4978383052313}  -87.00 \\\\\nScene & \\cellcolor{red!9.091433848995615}  -10.91 & \\cellcolor{red!28.66928346032479}  -34.40 & \\cellcolor{red!64.7752636034788}  -77.73 & \\cellcolor{gray!60} - & \\cellcolor{red!12.679904563996002}  -15.22 \\\\\nSubcat. & \\cellcolor{red!26.44189628615437}  -31.73 & \\cellcolor{red!71.21056534885048}  -85.45 & \\cellcolor{red!80.12436533744082}  -96.15 & \\cellcolor{red!25.457812767414}  -30.55 & \\cellcolor{gray!60} - \\\\\\hline\n\\multicolumn{6}{l}{}\\\\[2pt]\n\\end{tabular}}\n    \\caption{Task difficulty measured by forgetting in pairwise tasks.\n Non-diagonal elements show relative accuracy drop (\\%) after finetuning on Task 2.}\n    \\label{tab:Tpairs}\n\\end{table}", "cell_list_gold": [{"value": "-6.58", "char_index": [300, 305], "type": "Other"}, {"value": "-5.21", "char_index": [343, 348], "type": "Other"}, {"value": "-4.84", "char_index": [392, 397], "type": "Other"}, {"value": "-7.09", "char_index": [434, 439], "type": "Other"}, {"value": "-4.55", "char_index": [494, 499], "type": "Other"}, {"value": "-5.61", "char_index": [561, 566], "type": "Other"}, {"value": "-4.51", "char_index": [610, 615], "type": "Other"}, {"value": "-4.99", "char_index": [661, 666], "type": "Other"}, {"value": "-4.64", "char_index": [722, 727], "type": "Other"}, {"value": "-8.39", "char_index": [765, 770], "type": "Other"}, {"value": "-7.37", "char_index": [832, 837], "type": "Other"}, {"value": "-11.66", "char_index": [875, 881], "type": "Other"}, {"value": "-4.69", "char_index": [936, 941], "type": "Other"}, {"value": "-7.10", "char_index": [980, 985], "type": "Other"}, {"value": "-7.40", "char_index": [1023, 1028], "type": "Other"}, {"value": "-9.63", "char_index": [1089, 1094], "type": "Other"}, {"value": "-4.29", "char_index": [1150, 1155], "type": "Other"}, {"value": "-5.82", "char_index": [1193, 1198], "type": "Other"}, {"value": "-6.09", "char_index": [1236, 1241], "type": "Other"}, {"value": "-3.80", "char_index": [1285, 1290], "type": "Other"}, {"value": "-8.06", "char_index": [1569, 1574], "type": "Other"}, {"value": "-3.63", "char_index": [1618, 1623], "type": "Other"}, {"value": "-5.84", "char_index": [1661, 1666], "type": "Other"}, {"value": "-4.35", "char_index": [1711, 1716], "type": "Other"}, {"value": "-16.38", "char_index": [1762, 1768], "type": "Other"}, {"value": "-4.29", "char_index": [1837, 1842], "type": "Other"}, {"value": "-17.08", "char_index": [1881, 1887], "type": "Other"}, {"value": "-11.94", "char_index": [1925, 1931], "type": "Other"}, {"value": "-5.75", "char_index": [1981, 1986], "type": "Other"}, {"value": "-5.19", "char_index": [2024, 2029], "type": "Other"}, {"value": "-7.63", "char_index": [2092, 2097], "type": "Other"}, {"value": "-2.83", "char_index": [2141, 2146], "type": "Other"}, {"value": "-11.63", "char_index": [2194, 2200], "type": "Other"}, {"value": "-18.20", "char_index": [2239, 2245], "type": "Other"}, {"value": "-9.60", "char_index": [2283, 2288], "type": "Other"}, {"value": "-9.47", "char_index": [2350, 2355], "type": "Other"}, {"value": "-4.19", "char_index": [2412, 2417], "type": "Other"}, {"value": "-8.48", "char_index": [2455, 2460], "type": "Other"}, {"value": "-2.62", "char_index": [2505, 2510], "type": "Other"}, {"value": "-3.67", "char_index": [2555, 2560], "type": "Other"}, {"value": "-68.40", "char_index": [2833, 2839], "type": "Other"}, {"value": "-90.45", "char_index": [2877, 2883], "type": "Other"}, {"value": "-19.59", "char_index": [2921, 2927], "type": "Other"}, {"value": "-12.58", "char_index": [2966, 2972], "type": "Other"}, {"value": "-88.89", "char_index": [3019, 3025], "type": "Other"}, {"value": "-99.65", "char_index": [3087, 3093], "type": "Other"}, {"value": "-27.75", "char_index": [3131, 3137], "type": "Other"}, {"value": "-62.46", "char_index": [3175, 3181], "type": "Other"}, {"value": "-99.17", "char_index": [3228, 3234], "type": "Other"}, {"value": "-99.68", "char_index": [3270, 3276], "type": "Other"}, {"value": "-97.52", "char_index": [3338, 3344], "type": "Other"}, {"value": "-87.00", "char_index": [3381, 3387], "type": "Other"}, {"value": "-10.91", "char_index": [3434, 3440], "type": "Other"}, {"value": "-34.40", "char_index": [3478, 3484], "type": "Other"}, {"value": "-77.73", "char_index": [3521, 3527], "type": "Other"}, {"value": "-15.22", "char_index": [3590, 3596], "type": "Other"}, {"value": "-31.73", "char_index": [3645, 3651], "type": "Other"}, {"value": "-85.45", "char_index": [3689, 3695], "type": "Other"}, {"value": "-96.15", "char_index": [3733, 3739], "type": "Other"}, {"value": "-30.55", "char_index": [3775, 3781], "type": "Other"}]}, "2210.00044v1_table4": {"table_code": "\\begin{table}\n    \\centering\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{lccc}\\hline\n    \\multicolumn{4}{c}{{\\bf w/o Pretraining}}  \\\\\\hline\n        \\textbf{Method}  & \\textbf{What animal} & \\textbf{What room} & \\textbf{What sport} \\\\\\hline\n        Finetuning &  33.09 \\small{\\textcolor{black}{$\\pm$ 13.38}} & 54.38 \\small{\\textcolor{black}{$\\pm$ 32.42}}  &  25.14 \\small{\\textcolor{black}{$\\pm$ 32.11}} \\\\\n        EWC & 48.18 \\small{\\textcolor{black}{$\\pm$ 15.67}} & 83.48 \\small{\\textcolor{black}{$\\pm$ 7.61}}  & 62.81 \\small{\\textcolor{black}{$\\pm$ 13.67}} \\\\\n        ER & 73.11 \\small{\\textcolor{black}{$\\pm$ 0.70}} & 89.04 \\small{\\textcolor{black}{$\\pm$ 2.80}}  & 87.20 \\small{\\textcolor{black}{$\\pm$ 1.84}} \\\\\\hline\n    \\multicolumn{4}{c}{{\\bf w/ Pretraining}}  \\\\\\hline\n        \\textbf{Method}  & \\textbf{What animal} & \\textbf{What room} & \\textbf{What sport} \\\\\\hline\n        Finetuning &   75.07 \\small{\\textcolor{black}{$\\pm$ 3.54}} & 83.26 \\small{\\textcolor{black}{$\\pm$ 12.47}}  &  69.92 \\small{\\textcolor{black}{$\\pm$ 14.14}}\\\\\n        EWC & 81.75 \\small{\\textcolor{black}{$\\pm$ 1.42}} & 94.32 \\small{\\textcolor{black}{$\\pm$ 0.88}}  & 90.82 \\small{\\textcolor{black}{$\\pm$ 1.36}}  \\\\\n        ER & 80.73 \\small{\\textcolor{black}{$\\pm$ 0.37}} & 94.10 \\small{\\textcolor{black}{$\\pm$ 1.39}}  &  90.92 \\small{\\textcolor{black}{$\\pm$ 0.71}}  \\\\\\hline\n    \\end{tabular}}\n    \\caption{Accuracy and standard deviation of the best performing models on different sub-questions in Taxonomy Domains.}\n    \\label{tab:tax_task_order}\n\\end{table}", "table_label": "{tab:tax_task_order}", "table_numeric_cells": [["33.09", "33.09 \\small{\\textcolor{black}{$\\pm$ 13.38}}", 272, 277, 272, 316], ["54.38", "54.38 \\small{\\textcolor{black}{$\\pm$ 32.42}}", 319, 324, 319, 363], ["25.14", "25.14 \\small{\\textcolor{black}{$\\pm$ 32.11}}", 368, 373, 368, 412], ["48.18", "48.18 \\small{\\textcolor{black}{$\\pm$ 15.67}}", 430, 435, 430, 474], ["83.48", "83.48 \\small{\\textcolor{black}{$\\pm$ 7.61}}", 477, 482, 477, 520], ["62.81", "62.81 \\small{\\textcolor{black}{$\\pm$ 13.67}}", 524, 529, 524, 568], ["73.11", "73.11 \\small{\\textcolor{black}{$\\pm$ 0.70}}", 585, 590, 585, 628], ["89.04", "89.04 \\small{\\textcolor{black}{$\\pm$ 2.80}}", 631, 636, 631, 674], ["87.20", "87.20 \\small{\\textcolor{black}{$\\pm$ 1.84}}", 678, 683, 678, 721], ["75.07", "75.07 \\small{\\textcolor{black}{$\\pm$ 3.54}}", 909, 914, 909, 952], ["83.26", "83.26 \\small{\\textcolor{black}{$\\pm$ 12.47}}", 955, 960, 955, 999], ["69.92", "69.92 \\small{\\textcolor{black}{$\\pm$ 14.14}}", 1004, 1009, 1004, 1048], ["81.75", "81.75 \\small{\\textcolor{black}{$\\pm$ 1.42}}", 1065, 1070, 1065, 1108], ["94.32", "94.32 \\small{\\textcolor{black}{$\\pm$ 0.88}}", 1111, 1116, 1111, 1154], ["90.82", "90.82 \\small{\\textcolor{black}{$\\pm$ 1.36}}", 1158, 1163, 1158, 1201], ["80.73", "80.73 \\small{\\textcolor{black}{$\\pm$ 0.37}}", 1219, 1224, 1219, 1262], ["94.10", "94.10 \\small{\\textcolor{black}{$\\pm$ 1.39}}", 1265, 1270, 1265, 1308], ["90.92", "90.92 \\small{\\textcolor{black}{$\\pm$ 0.71}}", 1313, 1318, 1313, 1356]], "text_chunk_selected": "\\section{Settings for Continual VQA} \\label{sec:settings}\n\\subsection{Problem formulation}\nIn continual learning, model parameters $\\bm{\\theta}$ are incrementally updated as new data become available.\nWe assume that samples from tasks $t=1\\dots T$ arrive sequentially as $D_t=\\{\\bm{x}_i, \\bm{y}_i\\}_{i=1}^{N_t}$, where $N_t$ is the number of data for task $t$.\nFollowing previous work, VQA is formulated as a multi-label classification problem with soft targets $\\bm{y}_i$~\\cite{Anderson2017up-down}.\nStarting from parameters $\\bm{\\theta}_{t-1}$ of the previous model, the updated parameters $\\bm{\\theta}_{t}$ are obtained by training on the new data $D_t$. \nSome approaches also use a memory $M_t$ containing a subset of samples from previous tasks, e.g. $D_{1}, \\dots, D_{t-1}$.\nIn our setup, all tasks share a common output head which is extended with new classes from each task.\nThis allows inference to be task-agnostic but creates a more challenging setting than multi-head learning where separate heads are learned for each task~\\cite{hussain2021towards}.\nAt the end of the training sequence, the objective is to achieve strong performance across all tasks observed so far. \nThis objective encloses two challenges: 1) minimizing catastrophic forgetting of tasks seen earlier in training, 2) facilitating positive transfer to improve performance on new tasks~\\cite{HADSELL20201028embracing}. \n\n\\subsubsection{Language Setting}\\label{sec:q_splits}\nWe create a third setting {\\em Question Types}, where each task corresponds to learning to answer a different category of questions.\nWe use a classification scheme developed by~\\citet{whitehead2021separating}\nto form a sequence of five tasks: \nCount, Color, Scene-level, Subcategory, and Action recognition. The splits for Count, Color, and Subcategory questions are obtained from~\\citet{whitehead2021separating}. We create two additional tasks from the remaining questions.\nIn particular, we cluster question embeddings from Sentence-BERT~\\cite{reimers-2019-sentence-bert} \\footnote{We use the `all-MiniLM-L6-v2' model and Fast Clustering algorithm from the sentence-transformers package (\\url{https://www.sbert.net/}).} so that each cluster has at least 15 questions and a minimum cosine similarity of 0.8 between all embeddings.\nWe annotate clusters as `scene', `action' or `irrelevant' question types.\nBased on a seed of 10K annotated questions, we retrieve all other questions with similarity above 0.8 and label them using the K-nearest neighbor algorithm ($K=5$).  \nQuestion Types have a total of 140K train, 35K validation and 84K test samples (cf.\\ Table~\\ref{tab:stats}). \nCommon question words and answers per task are presented in the Appendix (Figure~\\ref{fig:top_ans_q}). \n\nWe also experiment with a baseline Pseudo-Replay method for the Question Types setting.\nInstead of storing raw data from previous tasks, we use a data augmentation method, inspired by~\\cite{kafle2017data,kil2021discovering}. \nWhen training on task $t$, we augment the data $D_t$ by retrieving past questions based on their shared detected objects classes. For example, if an elephant is detected on the current picture, we retrieve a past question about an elephant. We then use the previous model $f_{\\theta_{t-1}}$ to generate a distribution $\\tilde{ \\bm{y}} = f_{\\theta_{t-1}}(\\tilde{\\bm{x}})$ which serves as soft targets for the new sample $\\tilde{\\bm{x}}$.\nBy not storing the original answers, we address privacy and efficiency concerns of replay approaches~\\cite{van2018generative,Delangesurvey}.\n\n\\subsection{Evaluation Metrics}\nAfter training on task $t$, we compute the VQA accuracy $A_{t,i}$ on data from the previous task $i$. \nWe report the macro-average accuracy at the end of the training sequence: $\\mathrm{A} = \\frac{1}{T} \\sum_{i=1}^T A_{T,i}$.\nFollowing~\\citet{riemer2018learning}, we report the learned accuracy $\\mathrm{LA} = \\frac{1}{T} \\sum_{i=1}^T A_{i,i}$, which measures the ability to learn the new task $i$.\nWe also compute backward transfer $\\mathrm{BWT} = \\frac{1}{T-1} \\sum_{i=1}^{T-1} A_{T,i} - A_{i,i}$~\\cite{lopez2017GEM}, that captures the impact of catastrophic forgetting.\n\n\\begin{equation}\n   S_{T,i} = \\frac{1}{N} \\sum_{j=1}^{N} (1-\\mathrm{cos}(\\bm{e_{Tj}}, \\bm{e_{ij}}))\\cdot \\Delta^{Ti}_j\n\\end{equation}\n\n\\section{Results}\n\\subsection{Main Results}\n\\paragraph{Task Settings.}\nTable~\\ref{tab:results} summarizes the results averaged over five task orders using the UNITER backbone.\nThe results show an increasing difficulty for the three incremental learning task definitions, i.e. Diversity Domains $<$ Taxonomy Domains $<$ Question Types, which we will further investigate in Section \\ref{sec:task_relationships}. \n\nPrevious work on task-incremental learning for image classification~\\cite{Yoon2020Scalable} has discussed the impact of task order to final performance, especially when tasks are dissimilar.\nSimilarly, we observe a high standard deviation in the Question Types results of  Table~\\ref{tab:results}. \nIn order to investiagte this further, we plot the final accuracy of a pretrained model for five training sequences in Figure~\\ref{fig:qtask_order}, each ending with a different task. \nOur results show that task order can lead to Finetuning accuracy that varies more than 15$\\%$. Although EWC improves the average accuracy, there is still a 10$\\%$ fluctuation depending on the order. However, replay-based methods are able to improve performance and mitigate the sensitivity to task order.\n\nWhile Table~\\ref{tab:results} shows low variance in Taxonomy Domains, we find high variance when examining the performance on specific questions. In particular, we find that certain question types, such as Animals, Interior, and Sports, have high variance. Table~\\ref{tab:tax_task_order} reveals a standard deviation which is up to 30 times higher compared to the average results in Table~\\ref{tab:results}.\nHigh standard deviation across randomized task orders is problematic since models can have different behavior in practice despite similar (aggregated) performance.\nIn other words, the current task performance will highly depend on the previous task order, even though the overall  accuracy from the randomized trials appears similar.", "table_source": "\\begin{table}\n    \\centering\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{lccc}\\hline\n    \\multicolumn{4}{c}{{\\bf w/o Pretraining}}  \\\\\\hline\n        \\textbf{Method}  & \\textbf{What animal} & \\textbf{What room} & \\textbf{What sport} \\\\\\hline\n        Finetuning &  33.09 \\small{\\textcolor{black}{$\\pm$ 13.38}} & 54.38 \\small{\\textcolor{black}{$\\pm$ 32.42}}  &  25.14 \\small{\\textcolor{black}{$\\pm$ 32.11}} \\\\\n        EWC & 48.18 \\small{\\textcolor{black}{$\\pm$ 15.67}} & 83.48 \\small{\\textcolor{black}{$\\pm$ 7.61}}  & 62.81 \\small{\\textcolor{black}{$\\pm$ 13.67}} \\\\\n        ER & 73.11 \\small{\\textcolor{black}{$\\pm$ 0.70}} & 89.04 \\small{\\textcolor{black}{$\\pm$ 2.80}}  & 87.20 \\small{\\textcolor{black}{$\\pm$ 1.84}} \\\\\\hline\n    \\multicolumn{4}{c}{{\\bf w/ Pretraining}}  \\\\\\hline\n        \\textbf{Method}  & \\textbf{What animal} & \\textbf{What room} & \\textbf{What sport} \\\\\\hline\n        Finetuning &   75.07 \\small{\\textcolor{black}{$\\pm$ 3.54}} & 83.26 \\small{\\textcolor{black}{$\\pm$ 12.47}}  &  69.92 \\small{\\textcolor{black}{$\\pm$ 14.14}}\\\\\n        EWC & 81.75 \\small{\\textcolor{black}{$\\pm$ 1.42}} & 94.32 \\small{\\textcolor{black}{$\\pm$ 0.88}}  & 90.82 \\small{\\textcolor{black}{$\\pm$ 1.36}}  \\\\\n        ER & 80.73 \\small{\\textcolor{black}{$\\pm$ 0.37}} & 94.10 \\small{\\textcolor{black}{$\\pm$ 1.39}}  &  90.92 \\small{\\textcolor{black}{$\\pm$ 0.71}}  \\\\\\hline\n    \\end{tabular}}\n    \\caption{Accuracy and standard deviation of the best performing models on different sub-questions in Taxonomy Domains.}\n    \\label{tab:tax_task_order}\n\\end{table}", "cell_list_gold": [{"value": "33.09", "char_index": [272, 277], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What animal", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "Finetuning"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "54.38", "char_index": [319, 324], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What room", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "Finetuning"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "25.14", "char_index": [368, 373], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What sport", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "Finetuning"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "48.18", "char_index": [430, 435], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What animal", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "EWC"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "83.48", "char_index": [477, 482], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What room", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "EWC"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "62.81", "char_index": [524, 529], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What sport", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "EWC"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "73.11", "char_index": [585, 590], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What animal", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "ER"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "89.04", "char_index": [631, 636], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What room", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "ER"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "87.20", "char_index": [678, 683], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What sport", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "ER"}, "model": "UNITER", "model settings": {"w/o Pretraining": "true"}}, {"value": "75.07", "char_index": [909, 914], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What animal", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "Finetuning"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}, {"value": "83.26", "char_index": [955, 960], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What room", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "Finetuning"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}, {"value": "69.92", "char_index": [1004, 1009], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What sport", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "Finetuning"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}, {"value": "81.75", "char_index": [1065, 1070], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What animal", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "EWC"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}, {"value": "94.32", "char_index": [1111, 1116], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What room", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "EWC"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}, {"value": "90.82", "char_index": [1158, 1163], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What sport", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "EWC"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}, {"value": "80.73", "char_index": [1219, 1224], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What animal", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "ER"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}, {"value": "94.10", "char_index": [1265, 1270], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What room", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "ER"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}, {"value": "90.92", "char_index": [1313, 1318], "type": "Result", "training data/set": "VQA-v2", "test data/set": "VQA-v2 What sport", "task": ["VQA", "visual question answering"], "metric": "Accuracy", "experimental settings": {"Method": "ER"}, "model": "UNITER", "model settings": {"w/ Pretraining": "true"}}]}, "2210.00044v1_table5": {"table_code": "\\begin{table}[]\n    \\centering\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{lcccccc}\\hline\n     \\textbf{Dissimilarity} & \\textbf{Diverse} & \\textbf{Taxonomy} & \\textbf{Questions}  \\\\\\hline\n        Answers  & \\underline{0.567} (0.009) & \\underline{0.791} (0.000) & \\underline{0.795} (0.000) \\\\\n        Image embed. &  0.248 (0.293) & \\underline{0.492} (0.028) & \\underline{-0.640} (0.002)) \\\\\n        Question embed. & 0.184 (0.437) & \\underline{0.531} (0.016) & \\underline{0.631} (0.003) \\\\\n        Joint embed. & 0.220 (0.350) &   \\underline{0.622} (0.003) & -0.223 (0.344) \\\\\\hline\n    \\end{tabular}}\n    \\caption{Spearman correlation of pairwise performance drop and and different dissimilarity heuristics. In addition to the results in Table~\\ref{tab:spearman}, we show in parentheses the corresponding p-values. We underline statistically significant results (p < 0.05).}\n    \\label{tab:spearman_appendix}\n\\end{table}", "table_label": "{tab:spearman_appendix}", "table_numeric_cells": [["0.567", "\\underline{0.567} (0.009)", 227, 232, 216, 241], ["0.791", "\\underline{0.791} (0.000)", 255, 260, 244, 269], ["0.795", "\\underline{0.795} (0.000)", 283, 288, 272, 297], ["0.248", "0.248 (0.293)", 325, 330, 325, 338], ["0.492", "\\underline{0.492} (0.028)", 352, 357, 341, 366], ["-0.640", "\\underline{-0.640} (0.002))", 380, 386, 369, 396], ["0.184", "0.184 (0.437)", 426, 431, 426, 439], ["0.531", "\\underline{0.531} (0.016)", 453, 458, 442, 467], ["0.631", "\\underline{0.631} (0.003)", 481, 486, 470, 495], ["0.220", "0.220 (0.350)", 522, 527, 522, 535], ["0.622", "\\underline{0.622} (0.003)", 551, 556, 540, 565], ["-0.223", "-0.223 (0.344)", 568, 574, 568, 582]], "text_chunk_selected": "\\section{Introduction}\\label{sec:intro}\nThe current paradigm to approach Vision+Language (V+L) tasks is to pretrain large-scale models, which are then finetuned and evaluated on independent and identically distributed (i.i.d.) data.\nIn practice, the i.i.d. assumption does not hold:\nNew data becomes available over time, which often results in a shift in data distribution.\nOne solution is to continuously adapt an existing model via finetuning. However, this will lead to catastrophic forgetting, i.e.\\ significant performance degradation on previous data~\\cite{mccloskey1989catastrophic,ratcliff1990connectionist}.\nContinual learning provides a counterpart to i.i.d. learning: It defines a class of algorithms aiming at incremental learning with minimal forgetting.\nThis line of work becomes increasingly relevant given the financial and environmental costs of (re-)training large models~\\cite{strubell-etal-2019-energy,bender2021dangers}, and the limited generalization of static models~\\cite{lazaridou2021pitfalls}.\n\n\\subsubsection{Language Setting}\\label{sec:q_splits}\nWe create a third setting {\\em Question Types}, where each task corresponds to learning to answer a different category of questions.\nWe use a classification scheme developed by~\\citet{whitehead2021separating}\nto form a sequence of five tasks: \nCount, Color, Scene-level, Subcategory, and Action recognition. The splits for Count, Color, and Subcategory questions are obtained from~\\citet{whitehead2021separating}. We create two additional tasks from the remaining questions.\nIn particular, we cluster question embeddings from Sentence-BERT~\\cite{reimers-2019-sentence-bert} \\footnote{We use the `all-MiniLM-L6-v2' model and Fast Clustering algorithm from the sentence-transformers package (\\url{https://www.sbert.net/}).} so that each cluster has at least 15 questions and a minimum cosine similarity of 0.8 between all embeddings.\nWe annotate clusters as `scene', `action' or `irrelevant' question types.\nBased on a seed of 10K annotated questions, we retrieve all other questions with similarity above 0.8 and label them using the K-nearest neighbor algorithm ($K=5$).  \nQuestion Types have a total of 140K train, 35K validation and 84K test samples (cf.\\ Table~\\ref{tab:stats}). \nCommon question words and answers per task are presented in the Appendix (Figure~\\ref{fig:top_ans_q}). \n\n\\subsection{Continual Learning Methods}\nWe benchmark common continual learning algorithms, including regularization- and replay-based approaches. \nWe investigate two regularization-based approaches:\n{\\em Learning without Forgetting} (LwF)~\\cite{li2017lwf}, which uses knowledge distillation~\\cite{hinton2015distilling} in order to retain knowledge from previous tasks, and  \n{\\em Elastic Weight Consolidation} (EWC)~\\cite{kirkpatrick2017elastic}. The EWC regularization term discourages big changes of parameters that were important for previous tasks, where importance is approximated using the Fisher information matrix.\n\nWe also experiment with a baseline Pseudo-Replay method for the Question Types setting.\nInstead of storing raw data from previous tasks, we use a data augmentation method, inspired by~\\cite{kafle2017data,kil2021discovering}. \nWhen training on task $t$, we augment the data $D_t$ by retrieving past questions based on their shared detected objects classes. For example, if an elephant is detected on the current picture, we retrieve a past question about an elephant. We then use the previous model $f_{\\theta_{t-1}}$ to generate a distribution $\\tilde{ \\bm{y}} = f_{\\theta_{t-1}}(\\tilde{\\bm{x}})$ which serves as soft targets for the new sample $\\tilde{\\bm{x}}$.\nBy not storing the original answers, we address privacy and efficiency concerns of replay approaches~\\cite{van2018generative,Delangesurvey}.\n\n\\begin{equation}\n   S_{T,i} = \\frac{1}{N} \\sum_{j=1}^{N} (1-\\mathrm{cos}(\\bm{e_{Tj}}, \\bm{e_{ij}}))\\cdot \\Delta^{Ti}_j\n\\end{equation}\n\nNext, we compute the Spearman correlation between the relative accuracy drops and different factors of task dissimilarity.\nHere, we consider the answer distributions , as well as average embeddings of the image, question and the joint pair.\nConsider $P$, $Q$  the answer distributions of Tasks $T_1,T_2$ respectively.\nSince some answers of $T_1$ do not appear in $T_2$, we measure the skew divergence~\\cite{pmlr-vR3-lee01a} between $P$ and $Q$ as the KL divergence between $P$ and a mixture distribution \\mbox{$(1-\\alpha) P + \\alpha Q$} with $\\alpha=0.99$~\\cite{ruder-plank-2017-learning}.\nFor the input embeddings, we measure the cosine distance between the average task representation. \nAs image representations, we utilize Faster R-CNN features from~\\cite{Anderson2017up-down}, while questions are embedded using Sentence-BERT.\nJoint embeddings for image-question pairs are obtained using the final layer representation of the [CLS] token of UNITER~\\footnote{[CLS] is the first token of the input sequence which aggregates multimodal information. Its representation from the final encoder layer is passed to the classifier to predict an answer.}.\nThe detailed similarity measures are shown in the Appendix Figure~\\ref{fig:dissimilarity}.\n\n\\section{Implementation Details} \\label{sec:implementation_details}\nOur implementation is based on the publicly available PyTorch codebase of UNITER (\\url{https://github.com/ChenRocks/UNITER}).\nFor the continual learning experiments, we train a UNITER-base model (86M parameters) on a cluster of NVIDIA V100 GPUs using a single node with 4 GPUs.\nTraining on a sequence of 5 tasks requires on average $\\sim$ 5 GPU hours. The main experiments (Table~\\ref{tab:results}) require approximately a total of 200 GPU hours.\n\n\\begin{itemize}[leftmargin=*]\n \\setlength\\itemsep{1pt}\n    \\item \\textbf{Diverse Domains}\n    \\item group 5, group 3, group 2, group 4, group 1\n    \\item group 1, group 2, group 5, group 3, group 4\n    \\item group 4, group 3, group 5, group 1, group 2\n    \\item group 3, group 1, group 4, group 2, group 5\n    \\item group 2, group 5, group 1, group 4, group 3\n    \\item \\textbf{Taxonomy Domains}\n    \\item food, animals, sports, interior, transport\n    \\item transport, sports, food, animals, interior\n    \\item interior, animals, food, transport, sports\n    \\item animals, food, interior, sports, transport\n    \\item sports, interior, transport, animals, food\n    \\item \\textbf{Question types}\n    \\item action, count, subcategory, scene, color\n    \\item color, subcategory, action, count, scene\n    \\item scene, count, action, color, subcategory\n    \\item subcategory, color, scene, action, count\n    \\item count, scene, color, subcategory, action\n\\end{itemize}", "table_source": "\\begin{table}[]\n    \\centering\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{lcccccc}\\hline\n     \\textbf{Dissimilarity} & \\textbf{Diverse} & \\textbf{Taxonomy} & \\textbf{Questions}  \\\\\\hline\n        Answers  & \\underline{0.567} (0.009) & \\underline{0.791} (0.000) & \\underline{0.795} (0.000) \\\\\n        Image embed. &  0.248 (0.293) & \\underline{0.492} (0.028) & \\underline{-0.640} (0.002)) \\\\\n        Question embed. & 0.184 (0.437) & \\underline{0.531} (0.016) & \\underline{0.631} (0.003) \\\\\n        Joint embed. & 0.220 (0.350) &   \\underline{0.622} (0.003) & -0.223 (0.344) \\\\\\hline\n    \\end{tabular}}\n    \\caption{Spearman correlation of pairwise performance drop and and different dissimilarity heuristics. In addition to the results in Table~\\ref{tab:spearman}, we show in parentheses the corresponding p-values. We underline statistically significant results (p < 0.05).}\n    \\label{tab:spearman_appendix}\n\\end{table}", "cell_list_gold": [{"value": "0.567", "char_index": [227, 232], "type": "Other"}, {"value": "0.791", "char_index": [255, 260], "type": "Other"}, {"value": "0.795", "char_index": [283, 288], "type": "Other"}, {"value": "0.248", "char_index": [325, 330], "type": "Other"}, {"value": "0.492", "char_index": [352, 357], "type": "Other"}, {"value": "-0.640", "char_index": [380, 386], "type": "Other"}, {"value": "0.184", "char_index": [426, 431], "type": "Other"}, {"value": "0.531", "char_index": [453, 458], "type": "Other"}, {"value": "0.631", "char_index": [481, 486], "type": "Other"}, {"value": "0.220", "char_index": [522, 527], "type": "Other"}, {"value": "0.622", "char_index": [551, 556], "type": "Other"}, {"value": "-0.223", "char_index": [568, 574], "type": "Other"}]}, "2210.00044v1_table9": {"table_code": "\\begin{table*}[]\n    \\centering\n\\resizebox{0.8\\textwidth}{!}{\n    \\begin{tabular}{lccccccc}\n\\hline\n\\multicolumn{2}{c}{\\textbf{Reference}} & \\multicolumn{3}{c}{\\textbf{Compared Answer 1}} & \\multicolumn{3}{c}{\\textbf{Compared Answer 2}} \\\\\n\\textbf{Answer}\t& \\textbf{Acc}\t& \\textbf{Answer} &\t\\textbf{Acc} & \\textbf{SBWT} & \\textbf{Answer} & \\textbf{Acc} & \\textbf{SBWT} \\\\\\hline\nskateboarding & 1 & skateboard & 0 & -0.164 & black & 0 & -0.836 \\\\\nsnowboarding  & 1 & skiing & 0 & -0.134 & winter & 0 & -0.529 \\\\\nbreakfast & 1 & sandwich  & 0 & -0.340 & one & 0 & -0.855 \\\\\nfood & 1 & meat & 0 & -0.320 & toothbrush & 0 & -0.832 \\\\\nskateboarding & 1 & skateboard  & 0.3 & -0.115 & skateboard & 0 & -0.164 \\\\\ncarrots & 1 & carrot & 0.3 & -0.093 & three & 0 & -0.818 \\\\\nsheep & 1 & goat & 0.3 & -0.197 & white & 0 & -0.676 \\\\\ncloudy & 1 & overcast & 0.3 & -0.151 & gray & 0 & -0.577 \\\\\nblack & 0 & black and white & 1 & 0.136 & brown & 1 & 0.269 \\\\\n\\hline\n    \\end{tabular}}\n    \\caption{Comparison of the SBWT metric of two answers with respect to the same reference answer. We verify that semantically more similar answers have higher SBWT.}\n    \\label{tab:sbwt_examples}\n\\end{table*}", "table_label": "{tab:sbwt_examples}", "table_numeric_cells": [["1", "1", 393, 394, 393, 394], ["0", "0", 410, 411, 410, 411], ["-0.164", "-0.164", 414, 420, 414, 420], ["0", "0", 431, 432, 431, 432], ["-0.836", "-0.836", 435, 441, 435, 441], ["1", "1", 461, 462, 461, 462], ["0", "0", 474, 475, 474, 475], ["-0.134", "-0.134", 478, 484, 478, 484], ["0", "0", 496, 497, 496, 497], ["-0.529", "-0.529", 500, 506, 500, 506], ["1", "1", 522, 523, 522, 523], ["0", "0", 538, 539, 538, 539], ["-0.340", "-0.340", 542, 548, 542, 548], ["0", "0", 557, 558, 557, 558], ["-0.855", "-0.855", 561, 567, 561, 567], ["1", "1", 578, 579, 578, 579], ["0", "0", 589, 590, 589, 590], ["-0.320", "-0.320", 593, 599, 593, 599], ["0", "0", 615, 616, 615, 616], ["-0.832", "-0.832", 619, 625, 619, 625], ["1", "1", 645, 646, 645, 646], ["0.3", "0.3", 663, 666, 663, 666], ["-0.115", "-0.115", 669, 675, 669, 675], ["0", "0", 691, 692, 691, 692], ["-0.164", "-0.164", 695, 701, 695, 701], ["1", "1", 715, 716, 715, 716], ["0.3", "0.3", 728, 731, 728, 731], ["-0.093", "-0.093", 734, 740, 734, 740], ["0", "0", 751, 752, 751, 752], ["-0.818", "-0.818", 755, 761, 755, 761], ["1", "1", 773, 774, 773, 774], ["0.3", "0.3", 784, 787, 784, 787], ["-0.197", "-0.197", 790, 796, 790, 796], ["0", "0", 807, 808, 807, 808], ["-0.676", "-0.676", 811, 817, 811, 817], ["1", "1", 830, 831, 830, 831], ["0.3", "0.3", 845, 848, 845, 848], ["-0.151", "-0.151", 851, 857, 851, 857], ["0", "0", 867, 868, 867, 868], ["-0.577", "-0.577", 871, 877, 871, 877], ["0", "0", 889, 890, 889, 890], ["1", "1", 911, 912, 911, 912], ["0.136", "0.136", 915, 920, 915, 920], ["1", "1", 931, 932, 931, 932], ["0.269", "0.269", 935, 940, 935, 940]], "text_chunk_selected": "\\subsubsection{Language Setting}\\label{sec:q_splits}\nWe create a third setting {\\em Question Types}, where each task corresponds to learning to answer a different category of questions.\nWe use a classification scheme developed by~\\citet{whitehead2021separating}\nto form a sequence of five tasks: \nCount, Color, Scene-level, Subcategory, and Action recognition. The splits for Count, Color, and Subcategory questions are obtained from~\\citet{whitehead2021separating}. We create two additional tasks from the remaining questions.\nIn particular, we cluster question embeddings from Sentence-BERT~\\cite{reimers-2019-sentence-bert} \\footnote{We use the `all-MiniLM-L6-v2' model and Fast Clustering algorithm from the sentence-transformers package (\\url{https://www.sbert.net/}).} so that each cluster has at least 15 questions and a minimum cosine similarity of 0.8 between all embeddings.\nWe annotate clusters as `scene', `action' or `irrelevant' question types.\nBased on a seed of 10K annotated questions, we retrieve all other questions with similarity above 0.8 and label them using the K-nearest neighbor algorithm ($K=5$).  \nQuestion Types have a total of 140K train, 35K validation and 84K test samples (cf.\\ Table~\\ref{tab:stats}). \nCommon question words and answers per task are presented in the Appendix (Figure~\\ref{fig:top_ans_q}). \n\n\\subsection{Evaluation Metrics}\nAfter training on task $t$, we compute the VQA accuracy $A_{t,i}$ on data from the previous task $i$. \nWe report the macro-average accuracy at the end of the training sequence: $\\mathrm{A} = \\frac{1}{T} \\sum_{i=1}^T A_{T,i}$.\nFollowing~\\citet{riemer2018learning}, we report the learned accuracy $\\mathrm{LA} = \\frac{1}{T} \\sum_{i=1}^T A_{i,i}$, which measures the ability to learn the new task $i$.\nWe also compute backward transfer $\\mathrm{BWT} = \\frac{1}{T-1} \\sum_{i=1}^{T-1} A_{T,i} - A_{i,i}$~\\cite{lopez2017GEM}, that captures the impact of catastrophic forgetting.\n\nIn addition, we introduce a new metric, we term {\\em semantic backward transfer} (SBWT), that weights backward transfer with the cosine distance of the predicted answer embeddings.\nThe motivation for this metric is simply that some incorrect answers are worse than others.\nConsider the example in Figure~\\ref{fig:exmaple_intro}, where the ground truth is `duck'. After training on subsequent tasks, the sample gets misclassified as `seagull' which might have a milder impact on the downstream application than completely unsuited answers such as `black and white' or `one'.\nMore detailed examples are provided in the Appendix Table~\\ref{tab:sbwt_examples}.\n\nFor each sample $j=1\\dots, N$ of task $i$, we measure the accuracy difference $\\Delta^{Ti}_j$ of the answers predicted by the $T$-th and $i$-th models and\nweigh it by cosine distance of the two answer embeddings $\\bm{e_{Tj}}$ and $\\bm{e_{ij}}$.\nThe final SBWT is computed as  :\n\nAppendix \\ref{tab:sbwt_examples} provides a qualitative analysis  with examples from the validation set.\nIn addition, Table  \\ref{tab:sbwt_examples} in the Appendix provides an example-based analysis of our suggested metric, showing that semantically similar answers have higher SBWT scores.\n\nNext, we compute the Spearman correlation between the relative accuracy drops and different factors of task dissimilarity.\nHere, we consider the answer distributions , as well as average embeddings of the image, question and the joint pair.\nConsider $P$, $Q$  the answer distributions of Tasks $T_1,T_2$ respectively.\nSince some answers of $T_1$ do not appear in $T_2$, we measure the skew divergence~\\cite{pmlr-vR3-lee01a} between $P$ and $Q$ as the KL divergence between $P$ and a mixture distribution \\mbox{$(1-\\alpha) P + \\alpha Q$} with $\\alpha=0.99$~\\cite{ruder-plank-2017-learning}.\nFor the input embeddings, we measure the cosine distance between the average task representation. \nAs image representations, we utilize Faster R-CNN features from~\\cite{Anderson2017up-down}, while questions are embedded using Sentence-BERT.\nJoint embeddings for image-question pairs are obtained using the final layer representation of the [CLS] token of UNITER~\\footnote{[CLS] is the first token of the input sequence which aggregates multimodal information. Its representation from the final encoder layer is passed to the classifier to predict an answer.}.\nThe detailed similarity measures are shown in the Appendix Figure~\\ref{fig:dissimilarity}.\n\n\\section{Qualitative Results}\\label{appendix:qualitative}\nTable~\\ref{tab:examples} shows examples of predicted answers with different approaches. The two top examples are from two different task orders in Question Types, and the two bottom examples are from Taxonomy Domains. \nThe model trained from scratch (column w/o PT) fails to retain knowledge from the corresponding training task.\nThe pretrained model (column PT) is more resistant to forgetting and we observe that for the first and third images, it even manages to recover the correct answer during the training sequence.  \nHowever, relying only on pretraining is insufficient, as the model still tends to change the predicted answer based on the most recent training task. \nBoth EWC and ER combined with pretraining successfully retain previous knowledge.\n\nTable~\\ref{tab:sbwt_examples} presents examples of the SBWT metric. Specifically, it compares SBWT for two pairs of predicted answers with the same initial reference answer. When the initial prediction (reference answer) is correct, and both compared answers are wrong, we observe that SBWT penalizes similar answers less than unrelated ones (see the first four rows of  Table~\\ref{tab:sbwt_examples}). \nSimilarly, when one of the compared answers is partially correct (rows 5-8) according to the VQA accuracy metric, SBWT is less punishing compared to BWT, which in our examples would be $-0.7$.\nFinally, the last row shows an example of corrected compared answers, where the accuracy improvement is weighted with the semantic distance of reference and compared answers.", "table_source": "\\begin{table*}[]\n    \\centering\n\\resizebox{0.8\\textwidth}{!}{\n    \\begin{tabular}{lccccccc}\n\\hline\n\\multicolumn{2}{c}{\\textbf{Reference}} & \\multicolumn{3}{c}{\\textbf{Compared Answer 1}} & \\multicolumn{3}{c}{\\textbf{Compared Answer 2}} \\\\\n\\textbf{Answer}\t& \\textbf{Acc}\t& \\textbf{Answer} &\t\\textbf{Acc} & \\textbf{SBWT} & \\textbf{Answer} & \\textbf{Acc} & \\textbf{SBWT} \\\\\\hline\nskateboarding & 1 & skateboard & 0 & -0.164 & black & 0 & -0.836 \\\\\nsnowboarding  & 1 & skiing & 0 & -0.134 & winter & 0 & -0.529 \\\\\nbreakfast & 1 & sandwich  & 0 & -0.340 & one & 0 & -0.855 \\\\\nfood & 1 & meat & 0 & -0.320 & toothbrush & 0 & -0.832 \\\\\nskateboarding & 1 & skateboard  & 0.3 & -0.115 & skateboard & 0 & -0.164 \\\\\ncarrots & 1 & carrot & 0.3 & -0.093 & three & 0 & -0.818 \\\\\nsheep & 1 & goat & 0.3 & -0.197 & white & 0 & -0.676 \\\\\ncloudy & 1 & overcast & 0.3 & -0.151 & gray & 0 & -0.577 \\\\\nblack & 0 & black and white & 1 & 0.136 & brown & 1 & 0.269 \\\\\n\\hline\n    \\end{tabular}}\n    \\caption{Comparison of the SBWT metric of two answers with respect to the same reference answer. We verify that semantically more similar answers have higher SBWT.}\n    \\label{tab:sbwt_examples}\n\\end{table*}", "cell_list_gold": [{"value": "1", "char_index": [393, 394], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "skateboarding"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [410, 411], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "skateboard"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.164", "char_index": [414, 420], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "skateboard"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [431, 432], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "black"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.836", "char_index": [435, 441], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "black"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [461, 462], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "snowboarding"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [474, 475], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "skiing"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.134", "char_index": [478, 484], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "skiing"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [496, 497], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "winter"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.529", "char_index": [500, 506], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "winter"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [522, 523], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "breakfast"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [538, 539], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "sandwich"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.340", "char_index": [542, 548], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "sandwich"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [557, 558], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "one"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.855", "char_index": [561, 567], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "one"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [578, 579], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "food"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [589, 590], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "meat"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.320", "char_index": [593, 599], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "meat"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [615, 616], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "toothbrush"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.832", "char_index": [619, 625], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "toothbrush"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [645, 646], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "skateboarding"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [663, 666], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "skateboard"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.115", "char_index": [669, 675], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "skateboard"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [691, 692], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "skateboard"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.164", "char_index": [695, 701], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "skateboard"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [715, 716], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "carrots"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [728, 731], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "carrot"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.093", "char_index": [734, 740], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "carrot"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [751, 752], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "three"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.818", "char_index": [755, 761], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "three"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [773, 774], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "sheep"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [784, 787], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "goat"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.197", "char_index": [790, 796], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "goat"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [807, 808], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "white"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.676", "char_index": [811, 817], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "white"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [830, 831], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "cloudy"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [845, 848], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "overcast"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.151", "char_index": [851, 857], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "overcast"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [867, 868], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "gray"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "-0.577", "char_index": [871, 877], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "gray"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [889, 890], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Reference Answer": "black"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [911, 912], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 1": "black and white"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0.136", "char_index": [915, 920], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 1": "black and white"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [931, 932], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["Accuracy", "Acc"], "experimental settings": {"Compared Answer 2": "brown"}, "model": "UNITER", "model settings": {"xx": "yy"}}, {"value": "0.269", "char_index": [935, 940], "type": "Result", "training data/set": "VQA-v2", "test data/set": "xx", "task": ["VQA", "visual question answering"], "metric": ["semantic backward transfer", "SBWT"], "experimental settings": {"Compared Answer 2": "brown"}, "model": "UNITER", "model settings": {"xx": "yy"}}]}, "2210.00055v1_table2": {"table_code": "\\begin{table}[ht!]\n\\centering\n\\caption{Results from the Background Challenge on ImageNet-9 using ResNet-50. Our method outperforms the baselines on both Mixed-same and Only FG test sets.}\n\\begin{tabular}{lcccccc}\nMethod & Original   & Mixed-same  & Mixed-rand & Only-FG   \\\\ \\midrule\n\nBaseline~\\citep{xiao2020noise} & 96.3   & 89.8  & 75.6 & 85.6 \\\\ \nCIM~\\citep{taghanaki2021robust}  & \\textbf{97.7}   & 89.8 & \\textbf{81.1} & - \\\\%& 8.8  \\\\\n\nSIN~\\citep{sauer2021counterfactual} & 89.2   & 73.1  & 63.7 & - \\\\ \nINSIN~\\citep{sauer2021counterfactual} & 94.7  & 85.9  & 78.5 & - \\\\ \n\nINCGN~\\citep{sauer2021counterfactual} & 94.2   & 83.4  & 80.1  & - \\\\\n \n\\textit{MaskTune} (Ours) & 95.6   & \\textbf{91.1}  & 78.6  & \\textbf{88.1} \\\\ \n\\end{tabular}\n\\label{tab:bg}\n\\end{table}", "table_label": "{tab:bg}", "table_numeric_cells": [["96.3", "96.3", 318, 322, 318, 322], ["89.8", "89.8", 327, 331, 327, 331], ["75.6", "75.6", 335, 339, 335, 339], ["85.6", "85.6", 342, 346, 342, 346], ["97.7", "\\textbf{97.7}", 394, 398, 386, 399], ["89.8", "89.8", 404, 408, 404, 408], ["81.1", "\\textbf{81.1}", 419, 423, 411, 424], ["8.8", "8.8", 434, 437, 434, 437], ["89.2", "89.2", 481, 485, 481, 485], ["73.1", "73.1", 490, 494, 490, 494], ["63.7", "63.7", 498, 502, 498, 502], ["94.7", "94.7", 551, 555, 551, 555], ["85.9", "85.9", 559, 563, 559, 563], ["78.5", "78.5", 567, 571, 567, 571], ["94.2", "94.2", 621, 625, 621, 625], ["83.4", "83.4", 630, 634, 630, 634], ["80.1", "80.1", 638, 642, 638, 642], ["95.6", "95.6", 680, 684, 680, 684], ["91.1", "\\textbf{91.1}", 697, 701, 689, 702], ["78.6", "78.6", 706, 710, 706, 710], ["88.1", "\\textbf{88.1}", 722, 726, 714, 727]], "text_chunk_selected": "\\begin{abstract}\nA fundamental challenge of over-parameterized deep learning models is learning meaningful data representations that yield good performance on a downstream task without over-fitting spurious input features. This work proposes \\textit{MaskTune}, a masking strategy that prevents over-reliance on spurious (or a limited number of) features. \\textit{MaskTune} forces the trained model to explore new features during a single epoch finetuning by masking previously discovered features. \\textit{MaskTune}, unlike earlier approaches for mitigating shortcut learning, does not require any supervision, such as annotating spurious features or labels for subgroup samples in a dataset. Our empirical results on biased MNIST, CelebA, Waterbirds, and ImagenNet-9L datasets show that \\textit{MaskTune} is effective on tasks that often suffer from the existence of spurious correlations. Finally, we show that \\textit{MaskTune} outperforms or achieves similar performance to the competing methods when applied to the selective classification (classification with rejection option) task. Code for \\textit{MaskTune} is available at \\url{https://github.com/aliasgharkhani/Masktune}.\n\\end{abstract}\n\nWe apply \\textit{MaskTune} to two main tasks: a) robustness to spurious correlations, and b) selective classification. We cover four different datasets under (a) including MNIST with synthetic spurious features, CelebA and Waterbirds with spurious features in different subgroups~\\citep{sagawa2019distributionally}, and the Background Challenge~\\citep{xiao2020noise} which is a dataset for measuring the reliance of methods on background information for prediction. Under (b) we test \\textit{MaskTune} on CIFAR-10~\\citep{krizhevsky2009learning}, SVHN~\\citep{netzer2011reading}, and Cats vs. Dogs~\\citep{geifman2019selectivenet} datasets. On both tasks, we outperform or perform similarly to the previous complex methods using our simple technique.\n\n\\subsection{Adapting \\textit{MaskTune} for Selective Classification}\nHere we show how to use \\textit{MaskTune} for the selective classification problem. In order to make a more reliable prediction, we ensemble the original model (\\ensuremath{m^\\text{initial}_\\theta}) and \\textit{MaskTune} (\\ensuremath{m^\\text{final}_\\theta}) and only predict if both models agree.\nAs a result, if there exist two sets of features that can predict the label, our method only predicts if both agree on the label (e.g., grass and cow in Figure~\\ref{fig:cow}).\n\n\\subsection{Classification with Spurious Features} \\label{sec:spurious}\n\\textbf{MNIST with Colored Squares on the Corners:} As a warm-up, we test the ability of our method to distinguish between two MNIST digit groups (0-4 and 5-9) in the presence of spurious features. We construct a dataset such that a classifier would achieve poor accuracy by relying on spurious input features. To this end, we group digits labeled 0 to 4 into class 0 and those labeled 5 to 9 into class 1. Next, in the training set, we place 99\\% and 1\\% of the new class 0 and new class 1 data on a background with a small blue square on the top left corner, respectively, and keep the remaining data intact. We use two test sets during testing: the original raw MNIST test set and a biased test set. To create the biased test set we place all of the digits from the group ``5-9'' on a background with a small blue square (representing spurious features) on the top left corner and keep all of the digits from the group ``0-4'' unchanged (i.e., we don't add any squares to their background and use the original images). Figure~\\ref{fig:mnist} demonstrates the performance of the ERM, RandMask, and our \\textit{MaskTune} on both original and biased test sets. The RandMask method is similar to \\textit{MaskTune}, but masks a \\textit{randomly} chosen area of the input image. As shown in Figure~\\ref{fig:mnist}, \\textit{MaskTune} outperforms other methods by a large margin on both test sets. As demonstrated in Figure~\\ref{fig:mnist} (right), \\textit{MaskTune} forces the model to explore more features, as opposed to the ERM, which only looks into the spurious feature (the blue square). We also ran an experiment with multiple spurious features (Appendix~\\ref{app:exps}) and reported results for iterative version of ~\\textit{MaskTune}.\n\n\\textbf{Classification with Spurious Features in Subgroups.} In this experiment, we leverage the CelebA~\\citep{liu2015deep} and the Waterbirds~\\citep{sagawa2019distributionally} datasets. In the CelebA dataset, there is a high correlation between features gender=\\{male, female\\} and hair\\_color=\\{blond, dark\\}, meaning that the feature gender might be used as a proxy to predict the hair\\_color. In other words, an ERM would assign the label dark to male images since the majority of male images have dark hair, and there are only 1,387 (0.85\\%) blond males in the training set of size 162,770. To balance the accuracy of predictions on different subgroups of data, the existing methods~\\citep{sagawa2019distributionally} use subgroup information, i.e., wrong predictions of an ERM on the worst group (blond males) can be penalized during training. Some other approaches~\\citep{levy2020large,nam2020learning,pezeshki2021gradient,taghanaki2021robust,liu2021just} use subgroup information during model selection (labeled validation set). However, it is impractical to recognize ``all'' subgroups in a dataset and label them. \nIn Table~\\ref{tab:celeba}, we show that \\textit{MaskTune} achieves comparable performance to both these groups of methods \\emph{without} using group information in training or in model selection. \nWe also show \\textit{MaskTune} significantly improves worst-group accuracy (78\\% vs. 55\\% classification accuracy) in comparison to methods that do not use subgroup information during training or model selection. In Figure~\\ref{fig:celeba} (left), we highlight the important input features for predicting hair color on the CelebA dataste. As shown, the ERM model leverages gender features while \\textit{MaskTune} forces to investigate other features as well.\n\nThe Waterbirds dataset~\\citep{sagawa2019distributionally} was proposed to assess the degree to which models pick up spurious correlations in the training set. We discovered and fixed two \\textit{issues} with the Waterbirds dataset: a) because the background images in the Places dataset~\\citep{zhou2017places} may already contain bird images, multiple birds may appear in an image after overlaying the segmented bird images from the Caltech-UCSD Birds-200-2011 (CUB) dataset~\\citep{wah2011caltech}. For example, the label of an image may be ``landbird'', but the image contains both land and water birds. We manually removed such images from the dataset. b) Because the names of the species are similar, some land birds have been mislabeled as waterbirds which we corrected. The \\textit{corrected} Waterbirds dataset can be found on \\textit{MaskTune}'s GitHub page. After addressing the two issues, the ERM model's worst-group accuracy increased from 60\\% which is reported in~\\citep{sagawa2019distributionally} to 80.8$\\pm$1.3\\%. We repeated the group-DRO method and ERM experiments on the corrected waterbirds dataset and reported the results in Table~\\ref{tab:waterbirds}. As demonstrated, our method (without any group supervision) achieves similar accuracy to the group-DRO that benefits from full supervision. In Figure~\\ref{fig:celeba} (right), we visualized feature importance before and after applying our ~\\textit{MaskTune} on the modified Waterbirds dataset.\n\n\\textbf{The Background Challenge.} As a further step, we evaluated \\textit{MaskTune} on the Background Challenge data~\\citep{xiao2020noise} to see if the positive observations from the MNIST experiment apply to a more realistic scenario. The Background Challenge is a publicly available dataset that consists of ImageNet-9~\\citep{deng2009imagenet} test sets with various levels of foreground and background signals. It is intended to assess how much deep classifiers rely on spurious features for image classification. We used two configurations to compare \\textit{MaskTune}'s performance: Only FG, in which the background is completely removed, Mixed-same, in which the foreground is placed on a different background from the same class, and Mixed-rand, where the foreground is overlaid onto a random background.\n\nAs shown in Table~\\ref{tab:bg}, \\textit{MaskTune} outperforms the baseline ResNet-50's performance by 2.5\\% on the Only-FG test set and 1.2\\% on Mixed-same, showing that \\textit{MaskTune} does not rely much on the background and uses both background and foreground for prediction. On the Mixed-same and Only-FG test sets, ~\\textit{MaskTune} outperforms other techniques because mixing/removing the background texture/info confuses other methods. These results show that our technique helps to learn task-relevant features without depending on nuisance signal sources.", "table_source": "\\begin{table}[ht!]\n\\centering\n\\caption{Results from the Background Challenge on ImageNet-9 using ResNet-50. Our method outperforms the baselines on both Mixed-same and Only FG test sets.}\n\\begin{tabular}{lcccccc}\nMethod & Original   & Mixed-same  & Mixed-rand & Only-FG   \\\\ \\midrule\n\nBaseline~\\citep{xiao2020noise} & 96.3   & 89.8  & 75.6 & 85.6 \\\\ \nCIM~\\citep{taghanaki2021robust}  & \\textbf{97.7}   & 89.8 & \\textbf{81.1} & - \\\\%& 8.8  \\\\\n\nSIN~\\citep{sauer2021counterfactual} & 89.2   & 73.1  & 63.7 & - \\\\ \nINSIN~\\citep{sauer2021counterfactual} & 94.7  & 85.9  & 78.5 & - \\\\ \n\nINCGN~\\citep{sauer2021counterfactual} & 94.2   & 83.4  & 80.1  & - \\\\\n \n\\textit{MaskTune} (Ours) & 95.6   & \\textbf{91.1}  & 78.6  & \\textbf{88.1} \\\\ \n\\end{tabular}\n\\label{tab:bg}\n\\end{table}", "cell_list_gold": [{"value": "96.3", "char_index": [318, 322], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Original", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["ResNet-50", "Baseline"], "model settings": {"xx": "yy"}}, {"value": "89.8", "char_index": [327, 331], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-same", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["ResNet-50", "Baseline"], "model settings": {"xx": "yy"}}, {"value": "75.6", "char_index": [335, 339], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-rand", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["ResNet-50", "Baseline"], "model settings": {"xx": "yy"}}, {"value": "85.6", "char_index": [342, 346], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Only-FG", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["ResNet-50", "Baseline"], "model settings": {"xx": "yy"}}, {"value": "97.7", "char_index": [394, 398], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Original", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "CIM", "model settings": {"xx": "yy"}}, {"value": "89.8", "char_index": [404, 408], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-same", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "CIM", "model settings": {"xx": "yy"}}, {"value": "81.1", "char_index": [419, 423], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-rand", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "CIM", "model settings": {"xx": "yy"}}, {"value": "89.2", "char_index": [481, 485], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Original", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "SIN", "model settings": {"xx": "yy"}}, {"value": "73.1", "char_index": [490, 494], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-same", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "SIN", "model settings": {"xx": "yy"}}, {"value": "63.7", "char_index": [498, 502], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-rand", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "SIN", "model settings": {"xx": "yy"}}, {"value": "94.7", "char_index": [551, 555], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Original", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "INSIN", "model settings": {"xx": "yy"}}, {"value": "85.9", "char_index": [559, 563], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-same", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "INSIN", "model settings": {"xx": "yy"}}, {"value": "78.5", "char_index": [567, 571], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-rand", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "INSIN", "model settings": {"xx": "yy"}}, {"value": "94.2", "char_index": [621, 625], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Original", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "INCGN", "model settings": {"xx": "yy"}}, {"value": "83.4", "char_index": [630, 634], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-same", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "INCGN", "model settings": {"xx": "yy"}}, {"value": "80.1", "char_index": [638, 642], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-rand", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "INCGN", "model settings": {"xx": "yy"}}, {"value": "95.6", "char_index": [680, 684], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Original", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "91.1", "char_index": [697, 701], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-same", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "78.6", "char_index": [706, 710], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Mixed-rand", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "88.1", "char_index": [722, 726], "type": "Result", "training data/set": "ImageNet-9", "test data/set": "ImageNet-9 Only-FG", "task": ["Background Challenge", "image classification"], "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "MaskTune", "model settings": {"xx": "yy"}}]}, "2210.00055v1_table3": {"table_code": "\\begin{table}[h!]\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\caption{Selective classification results on CIFAR-10, SVHN, and Cats vs. Dogs datasets for different coverage values.}\n\\begin{tabular}{c|c|cccccccccc}\n\\multirow{2}{*}{Dataset}      & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Target\\\\ Coverage\\end{tabular}} & \\multicolumn{2}{c}{SR} & \\multicolumn{2}{c}{SN} & \\multicolumn{2}{c}{DG} & \\multicolumn{2}{c}{OSP} & \\multicolumn{2}{c}{MaskTune} \\\\\n                              &                                                                            & Cov.       & Err.     & Cov.      & Err.      & Cov.      & Err.      & Cov.   & Err.          & Cov.      & Err.            \\\\ \\midrule\n\\multirow{3}{*}{Cifar-10}     & 100\\%                                                                      & 99.99      & 9.58      & 100       & 11.07      & 100       & 10.81      & 100    & 9.74           & 99.99$\\pm$0.02     & \\textbf{8.96}$\\pm$\\textbf{0.48}    \\\\\n                              & 95\\%                                                                       & 95.2       & 8.74      & 94.7      & 8.34       & 95.1      & 8.21       & 95.1   & 6.98           & 94.86$\\pm$0.18     & \\textbf{6.54}$\\pm$\\textbf{0.39}    \\\\\n                              & 90\\%                                                                       & 90.5       & 6.52      & 89.6      & 6.45       & 90.1      & 6.14       & 90.0   & \\textbf{4.67}           & 89.73$\\pm$0.22     & 4.74$\\pm$0.31    \\\\ \\midrule\n\\multirow{3}{*}{SVHN}         & 100\\%                                                                      & 99.97      & 3.86      & 100       & 4.27       & 100       & 4.03       & 100    & 4.27           & 100.0$\\pm$0.00       & \\textbf{3.68}$\\pm$\\textbf{0.16}    \\\\\n                              & 95\\%                                                                       & 95.1       & 1.86      & 95.1      & 2.53       & 95.0      & 2.05       & 95.1   & \\textbf{1.83}  & 95.19$\\pm$0.09     & 1.84$\\pm$0.23             \\\\\n                              & 90\\%                                                                       & 90.0       & 1.04      & 90.1      & 1.31       & 90.0      & 1.06       & 90.1   & 1.01           & 89.55$\\pm$0.26     & \\textbf{0.96}$\\pm$\\textbf{0.11}     \\\\ \\midrule\n\\multirow{3}{*}{Cats vs. Dogs} & 100\\%                                                                      & 100        & 5.72      & 100       & 7.36       & 100       & 6.16       & 100    & 5.93           & 99.98$\\pm$0.00     & \\textbf{4.83}$\\pm$\\textbf{0.17}    \\\\\n                              & 95\\%                                                                       & 95.0       & 3.46      & 95.2      & 5.1        & 95.1      & 4.28       & 95.1   & 2.97           & 95.01$\\pm$0.14     & \\textbf{2.96}$\\pm$\\textbf{0.15}    \\\\\n                              & 90\\%                                                                       & 90.0       & 2.28      & 90.2      & 3.3        & 90.0      & 2.5        & 90.0   & \\textbf{1.74}  & 90.78$\\pm$0.16   & 1.94$\\pm$0.18            \n\\end{tabular}\n\\label{tab:selective}\n\\end{table}", "table_label": "{tab:selective}", "table_numeric_cells": [["100", "100\\%", 729, 732, 729, 734], ["99.99", "99.99", 806, 811, 806, 811], ["9.58", "9.58", 819, 823, 819, 823], ["100", "100", 831, 834, 831, 834], ["11.07", "11.07", 843, 848, 843, 848], ["100", "100", 856, 859, 856, 859], ["10.81", "10.81", 868, 873, 868, 873], ["100", "100", 881, 884, 881, 884], ["9.74", "9.74", 890, 894, 890, 894], ["99.99", "99.99$\\pm$0.02", 907, 912, 907, 921], ["8.96", "\\textbf{8.96}$\\pm$\\textbf{0.48}", 936, 940, 928, 959], ["95", "95\\%", 998, 1000, 998, 1002], ["95.2", "95.2", 1075, 1079, 1075, 1079], ["8.74", "8.74", 1088, 1092, 1088, 1092], ["94.7", "94.7", 1100, 1104, 1100, 1104], ["8.34", "8.34", 1112, 1116, 1112, 1116], ["95.1", "95.1", 1125, 1129, 1125, 1129], ["8.21", "8.21", 1137, 1141, 1137, 1141], ["95.1", "95.1", 1150, 1154, 1150, 1154], ["6.98", "6.98", 1159, 1163, 1159, 1163], ["94.86", "94.86$\\pm$0.18", 1176, 1181, 1176, 1190], ["6.54", "\\textbf{6.54}$\\pm$\\textbf{0.39}", 1205, 1209, 1197, 1228], ["90", "90\\%", 1267, 1269, 1267, 1271], ["90.5", "90.5", 1344, 1348, 1344, 1348], ["6.52", "6.52", 1357, 1361, 1357, 1361], ["89.6", "89.6", 1369, 1373, 1369, 1373], ["6.45", "6.45", 1381, 1385, 1381, 1385], ["90.1", "90.1", 1394, 1398, 1394, 1398], ["6.14", "6.14", 1406, 1410, 1406, 1410], ["90.0", "90.0", 1419, 1423, 1419, 1423], ["4.67", "\\textbf{4.67}", 1436, 1440, 1428, 1441], ["89.73", "89.73$\\pm$0.22", 1454, 1459, 1454, 1468], ["4.74", "4.74$\\pm$0.31", 1475, 1479, 1475, 1488], ["100", "100\\%", 1536, 1539, 1536, 1541], ["99.97", "99.97", 1613, 1618, 1613, 1618], ["3.86", "3.86", 1626, 1630, 1626, 1630], ["100", "100", 1638, 1641, 1638, 1641], ["4.27", "4.27", 1650, 1654, 1650, 1654], ["100", "100", 1663, 1666, 1663, 1666], ["4.03", "4.03", 1675, 1679, 1675, 1679], ["100", "100", 1688, 1691, 1688, 1691], ["4.27", "4.27", 1697, 1701, 1697, 1701], ["100.0", "100.0$\\pm$0.00", 1714, 1719, 1714, 1728], ["3.68", "\\textbf{3.68}$\\pm$\\textbf{0.16}", 1745, 1749, 1737, 1768], ["95", "95\\%", 1807, 1809, 1807, 1811], ["95.1", "95.1", 1884, 1888, 1884, 1888], ["1.86", "1.86", 1897, 1901, 1897, 1901], ["95.1", "95.1", 1909, 1913, 1909, 1913], ["2.53", "2.53", 1921, 1925, 1921, 1925], ["95.0", "95.0", 1934, 1938, 1934, 1938], ["2.05", "2.05", 1946, 1950, 1946, 1950], ["95.1", "95.1", 1959, 1963, 1959, 1963], ["1.83", "\\textbf{1.83}", 1976, 1980, 1968, 1981], ["95.19", "95.19$\\pm$0.09", 1985, 1990, 1985, 1999], ["1.84", "1.84$\\pm$0.23", 2006, 2010, 2006, 2019], ["90", "90\\%", 2067, 2069, 2067, 2071], ["90.0", "90.0", 2144, 2148, 2144, 2148], ["1.04", "1.04", 2157, 2161, 2157, 2161], ["90.1", "90.1", 2169, 2173, 2169, 2173], ["1.31", "1.31", 2181, 2185, 2181, 2185], ["90.0", "90.0", 2194, 2198, 2194, 2198], ["1.06", "1.06", 2206, 2210, 2206, 2210], ["90.1", "90.1", 2219, 2223, 2219, 2223], ["1.01", "1.01", 2228, 2232, 2228, 2232], ["89.55", "89.55$\\pm$0.26", 2245, 2250, 2245, 2259], ["0.96", "\\textbf{0.96}$\\pm$\\textbf{0.11}", 2274, 2278, 2266, 2297], ["100", "100\\%", 2347, 2350, 2347, 2352], ["100", "100", 2424, 2427, 2424, 2427], ["5.72", "5.72", 2437, 2441, 2437, 2441], ["100", "100", 2449, 2452, 2449, 2452], ["7.36", "7.36", 2461, 2465, 2461, 2465], ["100", "100", 2474, 2477, 2474, 2477], ["6.16", "6.16", 2486, 2490, 2486, 2490], ["100", "100", 2499, 2502, 2499, 2502], ["5.93", "5.93", 2508, 2512, 2508, 2512], ["99.98", "99.98$\\pm$0.00", 2525, 2530, 2525, 2539], ["4.83", "\\textbf{4.83}$\\pm$\\textbf{0.17}", 2554, 2558, 2546, 2577], ["95", "95\\%", 2616, 2618, 2616, 2620], ["95.0", "95.0", 2693, 2697, 2693, 2697], ["3.46", "3.46", 2706, 2710, 2706, 2710], ["95.2", "95.2", 2718, 2722, 2718, 2722], ["5.1", "5.1", 2730, 2733, 2730, 2733], ["95.1", "95.1", 2743, 2747, 2743, 2747], ["4.28", "4.28", 2755, 2759, 2755, 2759], ["95.1", "95.1", 2768, 2772, 2768, 2772], ["2.97", "2.97", 2777, 2781, 2777, 2781], ["95.01", "95.01$\\pm$0.14", 2794, 2799, 2794, 2808], ["2.96", "\\textbf{2.96}$\\pm$\\textbf{0.15}", 2823, 2827, 2815, 2846], ["90", "90\\%", 2885, 2887, 2885, 2889], ["90.0", "90.0", 2962, 2966, 2962, 2966], ["2.28", "2.28", 2975, 2979, 2975, 2979], ["90.2", "90.2", 2987, 2991, 2987, 2991], ["3.3", "3.3", 2999, 3002, 2999, 3002], ["90.0", "90.0", 3012, 3016, 3012, 3016], ["2.5", "2.5", 3024, 3027, 3024, 3027], ["90.0", "90.0", 3037, 3041, 3037, 3041], ["1.74", "\\textbf{1.74}", 3054, 3058, 3046, 3059], ["90.78", "90.78$\\pm$0.16", 3063, 3068, 3063, 3077], ["1.94", "1.94$\\pm$0.18", 3082, 3086, 3082, 3095]], "text_chunk_selected": "\\section{Method}\n\\textbf{Setup.} \nWe consider the supervised learning setting with inputs $x \\in \\mathcal{X} \\subset \\mathbb{R}^d$ and corresponding labels $y \\in \\mathcal{Y} = \\{1, \\ldots, k\\}$. We assume having access to samples $\\mathcal{D}^0 = \\{(x_i, y_i)\\}_{i=1}^n$ drawn from an unknown underlying distribution $p_{data}(x,y)$.\n\nwhere $\\mathcal{T}$ refers to a thresholding function by the threshold factor $\\tau$ (i.e., $ \\mathcal{T}=\\mathbbm{1}_{\\mathcal{A}_{x_i} \\le \\tau }$), and $\\odot$ denotes element-wise multiplication. As the resolution of $\\mathcal{A}$ is typically coarser than that of the input data, $\\mathcal{T}(\\mathcal{A}_{x_i})$ is up-sampled to the size of the input.\n\nProcedurally, we first learn model $m_{\\theta}^{\\text{initial}}$ using original unmasked training data $\\mathcal{D}^{\\text{initial}}$. Then we use $m_{\\theta}^{\\text{initial}}$, $\\mathcal{G}$ and $\\mathcal{T}$ to create the masked set $\\mathcal{D}^{\\text{masked}}$. Finally, the fully trained predictor $m_{\\theta}^\\text{initial}$ is tuned using $\\mathcal{D}^{\\text{masked}}$ to obtain $m_{\\theta}^{\\text{final}}$. \nAs for the masking step, any explainability approach can be applied (note that some may have more computational complexity, such as ScoreCAM~\\citep{wang2020score}). We use xGradCAM~\\citep{selvaraju2017grad} as it is fast and produces relatively denser heat-maps than other methods~\\citep{srinivas2019full, selvaraju2017grad, wang2020score}. \n\n\\textbf{Classification with Spurious Features in Subgroups.} In this experiment, we leverage the CelebA~\\citep{liu2015deep} and the Waterbirds~\\citep{sagawa2019distributionally} datasets. In the CelebA dataset, there is a high correlation between features gender=\\{male, female\\} and hair\\_color=\\{blond, dark\\}, meaning that the feature gender might be used as a proxy to predict the hair\\_color. In other words, an ERM would assign the label dark to male images since the majority of male images have dark hair, and there are only 1,387 (0.85\\%) blond males in the training set of size 162,770. To balance the accuracy of predictions on different subgroups of data, the existing methods~\\citep{sagawa2019distributionally} use subgroup information, i.e., wrong predictions of an ERM on the worst group (blond males) can be penalized during training. Some other approaches~\\citep{levy2020large,nam2020learning,pezeshki2021gradient,taghanaki2021robust,liu2021just} use subgroup information during model selection (labeled validation set). However, it is impractical to recognize ``all'' subgroups in a dataset and label them. \nIn Table~\\ref{tab:celeba}, we show that \\textit{MaskTune} achieves comparable performance to both these groups of methods \\emph{without} using group information in training or in model selection. \nWe also show \\textit{MaskTune} significantly improves worst-group accuracy (78\\% vs. 55\\% classification accuracy) in comparison to methods that do not use subgroup information during training or model selection. In Figure~\\ref{fig:celeba} (left), we highlight the important input features for predicting hair color on the CelebA dataste. As shown, the ERM model leverages gender features while \\textit{MaskTune} forces to investigate other features as well.\n\nThe Waterbirds dataset~\\citep{sagawa2019distributionally} was proposed to assess the degree to which models pick up spurious correlations in the training set. We discovered and fixed two \\textit{issues} with the Waterbirds dataset: a) because the background images in the Places dataset~\\citep{zhou2017places} may already contain bird images, multiple birds may appear in an image after overlaying the segmented bird images from the Caltech-UCSD Birds-200-2011 (CUB) dataset~\\citep{wah2011caltech}. For example, the label of an image may be ``landbird'', but the image contains both land and water birds. We manually removed such images from the dataset. b) Because the names of the species are similar, some land birds have been mislabeled as waterbirds which we corrected. The \\textit{corrected} Waterbirds dataset can be found on \\textit{MaskTune}'s GitHub page. After addressing the two issues, the ERM model's worst-group accuracy increased from 60\\% which is reported in~\\citep{sagawa2019distributionally} to 80.8$\\pm$1.3\\%. We repeated the group-DRO method and ERM experiments on the corrected waterbirds dataset and reported the results in Table~\\ref{tab:waterbirds}. As demonstrated, our method (without any group supervision) achieves similar accuracy to the group-DRO that benefits from full supervision. In Figure~\\ref{fig:celeba} (right), we visualized feature importance before and after applying our ~\\textit{MaskTune} on the modified Waterbirds dataset.\n\n\\subsection{Selective Classification} \\label{sec:selective}\nFor selective classification task, we evaluated \\textit{MaskTune} on three datasets: CIFAR-10~\\citep{krizhevsky2009learning}, SVHN~\\citep{netzer2011reading}, and Cats vs. Dogs~\\citep{geifman2019selectivenet}, with coverage values of \\{90\\%, 95\\%, 100\\%\\}. For example 90\\% coverage means abstaining 10\\% of the samples.\n\nGiven input image $j$ and the original (\\ensuremath{m^\\text{initial}_\\theta}) and finetuned (\\ensuremath{m^\\text{final}_\\theta}) models, let their respective inference-time prediction probabilities for class $i$ be $P_{ij}^\\text{init} $ and $P_{ij}^\\text{final}$. Then \\textit{MaskTune} does not abstain and declares the class $i$ as the predicted class, iff $P_{ij}^\\text{init} \\cdot P_{ij}^\\text{final} > \\gamma $, where $\\gamma$ is a threshold that allows for controlling the target coverage. \nTo find the proper $\\gamma$ for achieving the targeted coverage, we iterate over the validation set and find a threshold that, if applied to the validation set, would give the desired coverage (i.e., if our target coverage is 90\\%, we seek for a threshold which if we apply for abstention on the validation set, we will abstain predicting 10\\% of validation data). Note that the probabilities of the two models are multiplied to ensure that as the value of either probability decreases, the possibility of abstention increases.\n\nWe compare our method to the other selective classification approaches such as Softmax Response (SR)~\\citep{geifman2019selectivenet}, SelectiveNet (SN)~\\citep{geifman2019selectivenet}, Deep Gamblers (DG)~\\citep{liu2019deep}, and One-sided Prediction (OSP)~\\citep{gangrade2021selective}. Like the OSP, we first split the training data into train and validation sets. After training, we search for the abstention threshold on the validation set, then use it at the test time. \nAs seen in Table~\\ref{tab:selective}, \\textit{MaskTune} outperforms all prior techniques on all three datasets on 95\\% and 100\\% coverage rates. For results on more coverage rates refer to Appendix~\\ref{app:exps}.", "table_source": "\\begin{table}[h!]\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\caption{Selective classification results on CIFAR-10, SVHN, and Cats vs. Dogs datasets for different coverage values.}\n\\begin{tabular}{c|c|cccccccccc}\n\\multirow{2}{*}{Dataset}      & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Target\\\\ Coverage\\end{tabular}} & \\multicolumn{2}{c}{SR} & \\multicolumn{2}{c}{SN} & \\multicolumn{2}{c}{DG} & \\multicolumn{2}{c}{OSP} & \\multicolumn{2}{c}{MaskTune} \\\\\n                              &                                                                            & Cov.       & Err.     & Cov.      & Err.      & Cov.      & Err.      & Cov.   & Err.          & Cov.      & Err.            \\\\ \\midrule\n\\multirow{3}{*}{Cifar-10}     & 100\\%                                                                      & 99.99      & 9.58      & 100       & 11.07      & 100       & 10.81      & 100    & 9.74           & 99.99$\\pm$0.02     & \\textbf{8.96}$\\pm$\\textbf{0.48}    \\\\\n                              & 95\\%                                                                       & 95.2       & 8.74      & 94.7      & 8.34       & 95.1      & 8.21       & 95.1   & 6.98           & 94.86$\\pm$0.18     & \\textbf{6.54}$\\pm$\\textbf{0.39}    \\\\\n                              & 90\\%                                                                       & 90.5       & 6.52      & 89.6      & 6.45       & 90.1      & 6.14       & 90.0   & \\textbf{4.67}           & 89.73$\\pm$0.22     & 4.74$\\pm$0.31    \\\\ \\midrule\n\\multirow{3}{*}{SVHN}         & 100\\%                                                                      & 99.97      & 3.86      & 100       & 4.27       & 100       & 4.03       & 100    & 4.27           & 100.0$\\pm$0.00       & \\textbf{3.68}$\\pm$\\textbf{0.16}    \\\\\n                              & 95\\%                                                                       & 95.1       & 1.86      & 95.1      & 2.53       & 95.0      & 2.05       & 95.1   & \\textbf{1.83}  & 95.19$\\pm$0.09     & 1.84$\\pm$0.23             \\\\\n                              & 90\\%                                                                       & 90.0       & 1.04      & 90.1      & 1.31       & 90.0      & 1.06       & 90.1   & 1.01           & 89.55$\\pm$0.26     & \\textbf{0.96}$\\pm$\\textbf{0.11}     \\\\ \\midrule\n\\multirow{3}{*}{Cats vs. Dogs} & 100\\%                                                                      & 100        & 5.72      & 100       & 7.36       & 100       & 6.16       & 100    & 5.93           & 99.98$\\pm$0.00     & \\textbf{4.83}$\\pm$\\textbf{0.17}    \\\\\n                              & 95\\%                                                                       & 95.0       & 3.46      & 95.2      & 5.1        & 95.1      & 4.28       & 95.1   & 2.97           & 95.01$\\pm$0.14     & \\textbf{2.96}$\\pm$\\textbf{0.15}    \\\\\n                              & 90\\%                                                                       & 90.0       & 2.28      & 90.2      & 3.3        & 90.0      & 2.5        & 90.0   & \\textbf{1.74}  & 90.78$\\pm$0.16   & 1.94$\\pm$0.18            \n\\end{tabular}\n\\label{tab:selective}\n\\end{table}", "cell_list_gold": [{"value": "100", "char_index": [729, 732], "type": "Other"}, {"value": "99.99", "char_index": [806, 811], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "9.58", "char_index": [819, 823], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [831, 834], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "11.07", "char_index": [843, 848], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [856, 859], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "10.81", "char_index": [868, 873], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [881, 884], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "9.74", "char_index": [890, 894], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "99.99", "char_index": [907, 912], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "8.96", "char_index": [936, 940], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "95", "char_index": [998, 1000], "type": "Other"}, {"value": "95.2", "char_index": [1075, 1079], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "8.74", "char_index": [1088, 1092], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "94.7", "char_index": [1100, 1104], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "8.34", "char_index": [1112, 1116], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "95.1", "char_index": [1125, 1129], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "8.21", "char_index": [1137, 1141], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "95.1", "char_index": [1150, 1154], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "6.98", "char_index": [1159, 1163], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "94.86", "char_index": [1176, 1181], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "6.54", "char_index": [1205, 1209], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "90", "char_index": [1267, 1269], "type": "Other"}, {"value": "90.5", "char_index": [1344, 1348], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "6.52", "char_index": [1357, 1361], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "89.6", "char_index": [1369, 1373], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "6.45", "char_index": [1381, 1385], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "90.1", "char_index": [1394, 1398], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "6.14", "char_index": [1406, 1410], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "90.0", "char_index": [1419, 1423], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "4.67", "char_index": [1436, 1440], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "89.73", "char_index": [1454, 1459], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "4.74", "char_index": [1475, 1479], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [1536, 1539], "type": "Other"}, {"value": "99.97", "char_index": [1613, 1618], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "3.86", "char_index": [1626, 1630], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [1638, 1641], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "4.27", "char_index": [1650, 1654], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [1663, 1666], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "4.03", "char_index": [1675, 1679], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [1688, 1691], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "4.27", "char_index": [1697, 1701], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "100.0", "char_index": [1714, 1719], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "3.68", "char_index": [1745, 1749], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "95", "char_index": [1807, 1809], "type": "Other"}, {"value": "95.1", "char_index": [1884, 1888], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "1.86", "char_index": [1897, 1901], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "95.1", "char_index": [1909, 1913], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "2.53", "char_index": [1921, 1925], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "95.0", "char_index": [1934, 1938], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "2.05", "char_index": [1946, 1950], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "95.1", "char_index": [1959, 1963], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "1.83", "char_index": [1976, 1980], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "95.19", "char_index": [1985, 1990], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "1.84", "char_index": [2006, 2010], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "90", "char_index": [2067, 2069], "type": "Other"}, {"value": "90.0", "char_index": [2144, 2148], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "1.04", "char_index": [2157, 2161], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "90.1", "char_index": [2169, 2173], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "1.31", "char_index": [2181, 2185], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "90.0", "char_index": [2194, 2198], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "1.06", "char_index": [2206, 2210], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "90.1", "char_index": [2219, 2223], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "1.01", "char_index": [2228, 2232], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "89.55", "char_index": [2245, 2250], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "0.96", "char_index": [2274, 2278], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [2347, 2350], "type": "Other"}, {"value": "100", "char_index": [2424, 2427], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "5.72", "char_index": [2437, 2441], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [2449, 2452], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "7.36", "char_index": [2461, 2465], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [2474, 2477], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "6.16", "char_index": [2486, 2490], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [2499, 2502], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "5.93", "char_index": [2508, 2512], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "99.98", "char_index": [2525, 2530], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "4.83", "char_index": [2554, 2558], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "95", "char_index": [2616, 2618], "type": "Other"}, {"value": "95.0", "char_index": [2693, 2697], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "3.46", "char_index": [2706, 2710], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "95.2", "char_index": [2718, 2722], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "5.1", "char_index": [2730, 2733], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "95.1", "char_index": [2743, 2747], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "4.28", "char_index": [2755, 2759], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "95.1", "char_index": [2768, 2772], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "2.97", "char_index": [2777, 2781], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "95.01", "char_index": [2794, 2799], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "2.96", "char_index": [2823, 2827], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "90", "char_index": [2885, 2887], "type": "Other"}, {"value": "90.0", "char_index": [2962, 2966], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "2.28", "char_index": [2975, 2979], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["SR", "Softmax Response"], "model settings": {"xx": "yy"}}, {"value": "90.2", "char_index": [2987, 2991], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "3.3", "char_index": [2999, 3002], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["SN", "SelectiveNet"], "model settings": {"xx": "yy"}}, {"value": "90.0", "char_index": [3012, 3016], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "2.5", "char_index": [3024, 3027], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["DG", "Deep Gamblers"], "model settings": {"xx": "yy"}}, {"value": "90.0", "char_index": [3037, 3041], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "1.74", "char_index": [3054, 3058], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": ["OSP", "One-sided Prediction"], "model settings": {"xx": "yy"}}, {"value": "90.78", "char_index": [3063, 3068], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "1.94", "char_index": [3082, 3086], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}]}, "2210.00055v1_table4": {"table_code": "\\begin{table}[h!]\n\\centering\n\\setlength{\\tabcolsep}{3pt}\n\\caption{Results on running \\textit{MaskTune} for different masking iterations with two colored patches as two different spurious correlations on the MNIST dataset.}\n\\begin{tabular}{c|cc|cc}\n\\multirow{2}{*}{\\# massking iteration} & \\multicolumn{2}{c|}{accumulative}                  & \\multicolumn{2}{c}{non-accumulative}               \\\\ \n                              & \\multicolumn{1}{c|}{biased mnist} & original mnist & \\multicolumn{1}{c|}{biased mnist} & original mnist \\\\ \\midrule\nERM-no masking                           & \\multicolumn{1}{l|}{18.59$\\pm$0.75}   & 60.78$\\pm$0.74     & \\multicolumn{1}{l|}{18.59$\\pm$0.75}   & 60.78$\\pm$0.74     \\\\\n1                             & \\multicolumn{1}{l|}{24.68$\\pm$2.73}   & 59.93$\\pm$5.25     & \\multicolumn{1}{l|}{28.08$\\pm$11.44}   & 64.38$\\pm$10.45     \\\\\n2                             & \\multicolumn{1}{l|}{97.01$\\pm$2.86}   & 98.61$\\pm$0.61     & \\multicolumn{1}{l|}{30.69$\\pm$5.69}   & 62.07$\\pm$4.03     \\\\\n3                             & \\multicolumn{1}{l|}{98.88$\\pm$0.60}   & 99.00$\\pm$0.14     & \\multicolumn{1}{l|}{24.91$\\pm$2.45}   & 58.76$\\pm$0.76    \n\\end{tabular}\n\\label{tab:iter}\n\\end{table}", "table_label": "{tab:iter}", "table_numeric_cells": [["18.59", "\\multicolumn{1}{l|}{18.59$\\pm$0.75}", 608, 613, 588, 623], ["60.78", "60.78$\\pm$0.74", 628, 633, 628, 642], ["18.59", "\\multicolumn{1}{l|}{18.59$\\pm$0.75}", 669, 674, 649, 684], ["60.78", "60.78$\\pm$0.74", 689, 694, 689, 703], ["1", "1", 711, 712, 711, 712], ["24.68", "\\multicolumn{1}{l|}{24.68$\\pm$2.73}", 763, 768, 743, 778], ["59.93", "59.93$\\pm$5.25", 783, 788, 783, 797], ["28.08", "\\multicolumn{1}{l|}{28.08$\\pm$11.44}", 824, 829, 804, 840], ["64.38", "64.38$\\pm$10.45", 845, 850, 845, 860], ["2", "2", 868, 869, 868, 869], ["97.01", "\\multicolumn{1}{l|}{97.01$\\pm$2.86}", 920, 925, 900, 935], ["98.61", "98.61$\\pm$0.61", 940, 945, 940, 954], ["30.69", "\\multicolumn{1}{l|}{30.69$\\pm$5.69}", 981, 986, 961, 996], ["62.07", "62.07$\\pm$4.03", 1001, 1006, 1001, 1015], ["3", "3", 1023, 1024, 1023, 1024], ["98.88", "\\multicolumn{1}{l|}{98.88$\\pm$0.60}", 1075, 1080, 1055, 1090], ["99.00", "99.00$\\pm$0.14", 1095, 1100, 1095, 1109], ["24.91", "\\multicolumn{1}{l|}{24.91$\\pm$2.45}", 1136, 1141, 1116, 1151], ["58.76", "58.76$\\pm$0.76", 1156, 1161, 1156, 1170]], "text_chunk_selected": "\\section{Method}\n\\textbf{Setup.} \nWe consider the supervised learning setting with inputs $x \\in \\mathcal{X} \\subset \\mathbb{R}^d$ and corresponding labels $y \\in \\mathcal{Y} = \\{1, \\ldots, k\\}$. We assume having access to samples $\\mathcal{D}^0 = \\{(x_i, y_i)\\}_{i=1}^n$ drawn from an unknown underlying distribution $p_{data}(x,y)$.\n\n\\textbf{Input Masking.}\nA key ingredient of our approach is a masking function $\\mathcal{G}$ that is applied offline (i.e., after full training). The goal here is to construct a new masked dataset by concealing the most discriminative features in the input discovered by a model after full training. This should encourage the model to investigate new features with the masked training set during finetuning. As for $\\mathcal{G}$, we adopt the xGradCAM~\\citep{selvaraju2017grad}, which was originally designed for a visual explanation of deep models by creating rough localization maps based on the gradient of the model loss w.r.t. the output of a desired model layer. Given an input image of size $H \\times W \\times C$, xGradCAM outputs a localization map $\\mathcal{A}$ of size $H \\times W \\times 1$, which shows the contribution of each pixel of the input image in predicting the most probable class, i.e., it calculates the loss by choosing the class with highest logit value (not the true label) as the target class. After acquiring the localization map, for each sample $(x_i, y_i)$, where $x_i \\in X$ and $y_i \\in Y$, we mask the locations with the most contribution as:\n\nwhere $\\mathcal{T}$ refers to a thresholding function by the threshold factor $\\tau$ (i.e., $ \\mathcal{T}=\\mathbbm{1}_{\\mathcal{A}_{x_i} \\le \\tau }$), and $\\odot$ denotes element-wise multiplication. As the resolution of $\\mathcal{A}$ is typically coarser than that of the input data, $\\mathcal{T}(\\mathcal{A}_{x_i})$ is up-sampled to the size of the input.\n\nProcedurally, we first learn model $m_{\\theta}^{\\text{initial}}$ using original unmasked training data $\\mathcal{D}^{\\text{initial}}$. Then we use $m_{\\theta}^{\\text{initial}}$, $\\mathcal{G}$ and $\\mathcal{T}$ to create the masked set $\\mathcal{D}^{\\text{masked}}$. Finally, the fully trained predictor $m_{\\theta}^\\text{initial}$ is tuned using $\\mathcal{D}^{\\text{masked}}$ to obtain $m_{\\theta}^{\\text{final}}$. \nAs for the masking step, any explainability approach can be applied (note that some may have more computational complexity, such as ScoreCAM~\\citep{wang2020score}). We use xGradCAM~\\citep{selvaraju2017grad} as it is fast and produces relatively denser heat-maps than other methods~\\citep{srinivas2019full, selvaraju2017grad, wang2020score}. \n\n\\subsection{Classification with Spurious Features} \\label{sec:spurious}\n\\textbf{MNIST with Colored Squares on the Corners:} As a warm-up, we test the ability of our method to distinguish between two MNIST digit groups (0-4 and 5-9) in the presence of spurious features. We construct a dataset such that a classifier would achieve poor accuracy by relying on spurious input features. To this end, we group digits labeled 0 to 4 into class 0 and those labeled 5 to 9 into class 1. Next, in the training set, we place 99\\% and 1\\% of the new class 0 and new class 1 data on a background with a small blue square on the top left corner, respectively, and keep the remaining data intact. We use two test sets during testing: the original raw MNIST test set and a biased test set. To create the biased test set we place all of the digits from the group ``5-9'' on a background with a small blue square (representing spurious features) on the top left corner and keep all of the digits from the group ``0-4'' unchanged (i.e., we don't add any squares to their background and use the original images). Figure~\\ref{fig:mnist} demonstrates the performance of the ERM, RandMask, and our \\textit{MaskTune} on both original and biased test sets. The RandMask method is similar to \\textit{MaskTune}, but masks a \\textit{randomly} chosen area of the input image. As shown in Figure~\\ref{fig:mnist}, \\textit{MaskTune} outperforms other methods by a large margin on both test sets. As demonstrated in Figure~\\ref{fig:mnist} (right), \\textit{MaskTune} forces the model to explore more features, as opposed to the ERM, which only looks into the spurious feature (the blue square). We also ran an experiment with multiple spurious features (Appendix~\\ref{app:exps}) and reported results for iterative version of ~\\textit{MaskTune}.\n\nThe Waterbirds dataset~\\citep{sagawa2019distributionally} was proposed to assess the degree to which models pick up spurious correlations in the training set. We discovered and fixed two \\textit{issues} with the Waterbirds dataset: a) because the background images in the Places dataset~\\citep{zhou2017places} may already contain bird images, multiple birds may appear in an image after overlaying the segmented bird images from the Caltech-UCSD Birds-200-2011 (CUB) dataset~\\citep{wah2011caltech}. For example, the label of an image may be ``landbird'', but the image contains both land and water birds. We manually removed such images from the dataset. b) Because the names of the species are similar, some land birds have been mislabeled as waterbirds which we corrected. The \\textit{corrected} Waterbirds dataset can be found on \\textit{MaskTune}'s GitHub page. After addressing the two issues, the ERM model's worst-group accuracy increased from 60\\% which is reported in~\\citep{sagawa2019distributionally} to 80.8$\\pm$1.3\\%. We repeated the group-DRO method and ERM experiments on the corrected waterbirds dataset and reported the results in Table~\\ref{tab:waterbirds}. As demonstrated, our method (without any group supervision) achieves similar accuracy to the group-DRO that benefits from full supervision. In Figure~\\ref{fig:celeba} (right), we visualized feature importance before and after applying our ~\\textit{MaskTune} on the modified Waterbirds dataset.\n\nGiven input image $j$ and the original (\\ensuremath{m^\\text{initial}_\\theta}) and finetuned (\\ensuremath{m^\\text{final}_\\theta}) models, let their respective inference-time prediction probabilities for class $i$ be $P_{ij}^\\text{init} $ and $P_{ij}^\\text{final}$. Then \\textit{MaskTune} does not abstain and declares the class $i$ as the predicted class, iff $P_{ij}^\\text{init} \\cdot P_{ij}^\\text{final} > \\gamma $, where $\\gamma$ is a threshold that allows for controlling the target coverage. \nTo find the proper $\\gamma$ for achieving the targeted coverage, we iterate over the validation set and find a threshold that, if applied to the validation set, would give the desired coverage (i.e., if our target coverage is 90\\%, we seek for a threshold which if we apply for abstention on the validation set, we will abstain predicting 10\\% of validation data). Note that the probabilities of the two models are multiplied to ensure that as the value of either probability decreases, the possibility of abstention increases.\n\n\\paragraph{Multiple Spurious Features Scenario.} \nRunning \\textit{MaskTune} for only one iteration on a dataset with \\textit{more} than one spurious feature still performs better than ERM. If we run \\textit{MaskTune} iteratively, the performance improves even more. We ran two iterative versions of \\textit{MaskTune}: accumulative and non-accumulative on the MNIST dataset (Table~\\ref{tab:iter}). We added two coloured patches to MNIST digits as two distinct spurious features. We ran 1, 2, and 3 iterations of masking. \\textit{MaskTune} works well when we do iterative accumulative masking (i.e., add new masks to the previously masked samples), but when we apply masks from each iteration to the raw input (non-accumulative), the results are not as good as the accumulative version. To reduce the running time, we only used one iteration of masking. One way to stop the accumulative masking is to monitor the training accuracy. If the model is not able to fit the data after a certain number of masking iterations (because there are no useful features left), the training can be stopped.", "table_source": "\\begin{table}[h!]\n\\centering\n\\setlength{\\tabcolsep}{3pt}\n\\caption{Results on running \\textit{MaskTune} for different masking iterations with two colored patches as two different spurious correlations on the MNIST dataset.}\n\\begin{tabular}{c|cc|cc}\n\\multirow{2}{*}{\\# massking iteration} & \\multicolumn{2}{c|}{accumulative}                  & \\multicolumn{2}{c}{non-accumulative}               \\\\ \n                              & \\multicolumn{1}{c|}{biased mnist} & original mnist & \\multicolumn{1}{c|}{biased mnist} & original mnist \\\\ \\midrule\nERM-no masking                           & \\multicolumn{1}{l|}{18.59$\\pm$0.75}   & 60.78$\\pm$0.74     & \\multicolumn{1}{l|}{18.59$\\pm$0.75}   & 60.78$\\pm$0.74     \\\\\n1                             & \\multicolumn{1}{l|}{24.68$\\pm$2.73}   & 59.93$\\pm$5.25     & \\multicolumn{1}{l|}{28.08$\\pm$11.44}   & 64.38$\\pm$10.45     \\\\\n2                             & \\multicolumn{1}{l|}{97.01$\\pm$2.86}   & 98.61$\\pm$0.61     & \\multicolumn{1}{l|}{30.69$\\pm$5.69}   & 62.07$\\pm$4.03     \\\\\n3                             & \\multicolumn{1}{l|}{98.88$\\pm$0.60}   & 99.00$\\pm$0.14     & \\multicolumn{1}{l|}{24.91$\\pm$2.45}   & 58.76$\\pm$0.76    \n\\end{tabular}\n\\label{tab:iter}\n\\end{table}", "cell_list_gold": [{"value": "18.59", "char_index": [608, 613], "type": "Result", "training data/set": "MNIST", "test data/set": "biased mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "ERM-no masking"}}, {"value": "60.78", "char_index": [628, 633], "type": "Result", "training data/set": "MNIST", "test data/set": "original mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "ERM-no masking"}}, {"value": "18.59", "char_index": [669, 674], "type": "Result", "training data/set": "MNIST", "test data/set": "biased mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"non-accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "ERM-no masking"}}, {"value": "60.78", "char_index": [689, 694], "type": "Result", "training data/set": "MNIST", "test data/set": "original mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"non-accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "ERM-no masking"}}, {"value": "1", "char_index": [711, 712], "type": "Hyper-parameter/Architecture", "model": "MaskTune", "parameter/architecture name": ["# massking iteration", "number of massking iteration"], "dataset": "MNIST"}, {"value": "24.68", "char_index": [763, 768], "type": "Result", "training data/set": "MNIST", "test data/set": "biased mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "1"}}, {"value": "59.93", "char_index": [783, 788], "type": "Result", "training data/set": "MNIST", "test data/set": "original mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "1"}}, {"value": "28.08", "char_index": [824, 829], "type": "Result", "training data/set": "MNIST", "test data/set": "biased mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"non-accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "1"}}, {"value": "64.38", "char_index": [845, 850], "type": "Result", "training data/set": "MNIST", "test data/set": "original mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"non-accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "1"}}, {"value": "2", "char_index": [868, 869], "type": "Hyper-parameter/Architecture", "model": "MaskTune", "parameter/architecture name": ["# massking iteration", "number of massking iteration"], "dataset": "MNIST"}, {"value": "97.01", "char_index": [920, 925], "type": "Result", "training data/set": "MNIST", "test data/set": "biased mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "2"}}, {"value": "98.61", "char_index": [940, 945], "type": "Result", "training data/set": "MNIST", "test data/set": "original mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "2"}}, {"value": "30.69", "char_index": [981, 986], "type": "Result", "training data/set": "MNIST", "test data/set": "biased mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"non-accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "2"}}, {"value": "62.07", "char_index": [1001, 1006], "type": "Result", "training data/set": "MNIST", "test data/set": "original mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"non-accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "2"}}, {"value": "3", "char_index": [1023, 1024], "type": "Hyper-parameter/Architecture", "model": "MaskTune", "parameter/architecture name": ["# massking iteration", "number of massking iteration"], "dataset": "MNIST"}, {"value": "98.88", "char_index": [1075, 1080], "type": "Result", "training data/set": "MNIST", "test data/set": "biased mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "3"}}, {"value": "99.00", "char_index": [1095, 1100], "type": "Result", "training data/set": "MNIST", "test data/set": "original mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "3"}}, {"value": "24.91", "char_index": [1136, 1141], "type": "Result", "training data/set": "MNIST", "test data/set": "biased mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"non-accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "3"}}, {"value": "58.76", "char_index": [1156, 1161], "type": "Result", "training data/set": "MNIST", "test data/set": "original mnist", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"non-accumulative": "true"}, "model": "MaskTune", "model settings": {"# massking iteration": "3"}}]}, "2210.00055v1_table5": {"table_code": "\\begin{table}[h!]\n\\centering\n\\setlength{\\tabcolsep}{3pt}\n\\caption{Compare \\textit{MaskTune} with random masking.}\n\\begin{tabular}{c|c|c}\nTest data type & Masking method & Accuracy     \\\\ \\midrule\nbiased         & random masking & 59.06$\\pm$09.25 \\\\\noriginal       & random masking & 91.02$\\pm$01.87 \\\\\nbiased         & MaskTune       & \\textbf{98.26$\\pm$00.27} \\\\\noriginal       & MaskTune       & \\textbf{98.46$\\pm$00.21}\n\\end{tabular}\n\\label{tab:randmask2}\n\\end{table}", "table_label": "{tab:randmask2}", "table_numeric_cells": [["59.06", "59.06$\\pm$09.25", 230, 235, 230, 245], ["91.02", "91.02$\\pm$01.87", 283, 288, 283, 298], ["98.26", "\\textbf{98.26$\\pm$00.27}", 344, 349, 336, 360], ["98.46", "\\textbf{98.46$\\pm$00.21}", 406, 411, 398, 422]], "text_chunk_selected": "\\section{Method}\n\\textbf{Setup.} \nWe consider the supervised learning setting with inputs $x \\in \\mathcal{X} \\subset \\mathbb{R}^d$ and corresponding labels $y \\in \\mathcal{Y} = \\{1, \\ldots, k\\}$. We assume having access to samples $\\mathcal{D}^0 = \\{(x_i, y_i)\\}_{i=1}^n$ drawn from an unknown underlying distribution $p_{data}(x,y)$.\n\n\\textbf{Input Masking.}\nA key ingredient of our approach is a masking function $\\mathcal{G}$ that is applied offline (i.e., after full training). The goal here is to construct a new masked dataset by concealing the most discriminative features in the input discovered by a model after full training. This should encourage the model to investigate new features with the masked training set during finetuning. As for $\\mathcal{G}$, we adopt the xGradCAM~\\citep{selvaraju2017grad}, which was originally designed for a visual explanation of deep models by creating rough localization maps based on the gradient of the model loss w.r.t. the output of a desired model layer. Given an input image of size $H \\times W \\times C$, xGradCAM outputs a localization map $\\mathcal{A}$ of size $H \\times W \\times 1$, which shows the contribution of each pixel of the input image in predicting the most probable class, i.e., it calculates the loss by choosing the class with highest logit value (not the true label) as the target class. After acquiring the localization map, for each sample $(x_i, y_i)$, where $x_i \\in X$ and $y_i \\in Y$, we mask the locations with the most contribution as:\n\nwhere $\\mathcal{T}$ refers to a thresholding function by the threshold factor $\\tau$ (i.e., $ \\mathcal{T}=\\mathbbm{1}_{\\mathcal{A}_{x_i} \\le \\tau }$), and $\\odot$ denotes element-wise multiplication. As the resolution of $\\mathcal{A}$ is typically coarser than that of the input data, $\\mathcal{T}(\\mathcal{A}_{x_i})$ is up-sampled to the size of the input.\n\nProcedurally, we first learn model $m_{\\theta}^{\\text{initial}}$ using original unmasked training data $\\mathcal{D}^{\\text{initial}}$. Then we use $m_{\\theta}^{\\text{initial}}$, $\\mathcal{G}$ and $\\mathcal{T}$ to create the masked set $\\mathcal{D}^{\\text{masked}}$. Finally, the fully trained predictor $m_{\\theta}^\\text{initial}$ is tuned using $\\mathcal{D}^{\\text{masked}}$ to obtain $m_{\\theta}^{\\text{final}}$. \nAs for the masking step, any explainability approach can be applied (note that some may have more computational complexity, such as ScoreCAM~\\citep{wang2020score}). We use xGradCAM~\\citep{selvaraju2017grad} as it is fast and produces relatively denser heat-maps than other methods~\\citep{srinivas2019full, selvaraju2017grad, wang2020score}. \n\n\\subsection{Classification with Spurious Features} \\label{sec:spurious}\n\\textbf{MNIST with Colored Squares on the Corners:} As a warm-up, we test the ability of our method to distinguish between two MNIST digit groups (0-4 and 5-9) in the presence of spurious features. We construct a dataset such that a classifier would achieve poor accuracy by relying on spurious input features. To this end, we group digits labeled 0 to 4 into class 0 and those labeled 5 to 9 into class 1. Next, in the training set, we place 99\\% and 1\\% of the new class 0 and new class 1 data on a background with a small blue square on the top left corner, respectively, and keep the remaining data intact. We use two test sets during testing: the original raw MNIST test set and a biased test set. To create the biased test set we place all of the digits from the group ``5-9'' on a background with a small blue square (representing spurious features) on the top left corner and keep all of the digits from the group ``0-4'' unchanged (i.e., we don't add any squares to their background and use the original images). Figure~\\ref{fig:mnist} demonstrates the performance of the ERM, RandMask, and our \\textit{MaskTune} on both original and biased test sets. The RandMask method is similar to \\textit{MaskTune}, but masks a \\textit{randomly} chosen area of the input image. As shown in Figure~\\ref{fig:mnist}, \\textit{MaskTune} outperforms other methods by a large margin on both test sets. As demonstrated in Figure~\\ref{fig:mnist} (right), \\textit{MaskTune} forces the model to explore more features, as opposed to the ERM, which only looks into the spurious feature (the blue square). We also ran an experiment with multiple spurious features (Appendix~\\ref{app:exps}) and reported results for iterative version of ~\\textit{MaskTune}.\n\nThe Waterbirds dataset~\\citep{sagawa2019distributionally} was proposed to assess the degree to which models pick up spurious correlations in the training set. We discovered and fixed two \\textit{issues} with the Waterbirds dataset: a) because the background images in the Places dataset~\\citep{zhou2017places} may already contain bird images, multiple birds may appear in an image after overlaying the segmented bird images from the Caltech-UCSD Birds-200-2011 (CUB) dataset~\\citep{wah2011caltech}. For example, the label of an image may be ``landbird'', but the image contains both land and water birds. We manually removed such images from the dataset. b) Because the names of the species are similar, some land birds have been mislabeled as waterbirds which we corrected. The \\textit{corrected} Waterbirds dataset can be found on \\textit{MaskTune}'s GitHub page. After addressing the two issues, the ERM model's worst-group accuracy increased from 60\\% which is reported in~\\citep{sagawa2019distributionally} to 80.8$\\pm$1.3\\%. We repeated the group-DRO method and ERM experiments on the corrected waterbirds dataset and reported the results in Table~\\ref{tab:waterbirds}. As demonstrated, our method (without any group supervision) achieves similar accuracy to the group-DRO that benefits from full supervision. In Figure~\\ref{fig:celeba} (right), we visualized feature importance before and after applying our ~\\textit{MaskTune} on the modified Waterbirds dataset.\n\nGiven input image $j$ and the original (\\ensuremath{m^\\text{initial}_\\theta}) and finetuned (\\ensuremath{m^\\text{final}_\\theta}) models, let their respective inference-time prediction probabilities for class $i$ be $P_{ij}^\\text{init} $ and $P_{ij}^\\text{final}$. Then \\textit{MaskTune} does not abstain and declares the class $i$ as the predicted class, iff $P_{ij}^\\text{init} \\cdot P_{ij}^\\text{final} > \\gamma $, where $\\gamma$ is a threshold that allows for controlling the target coverage. \nTo find the proper $\\gamma$ for achieving the targeted coverage, we iterate over the validation set and find a threshold that, if applied to the validation set, would give the desired coverage (i.e., if our target coverage is 90\\%, we seek for a threshold which if we apply for abstention on the validation set, we will abstain predicting 10\\% of validation data). Note that the probabilities of the two models are multiplied to ensure that as the value of either probability decreases, the possibility of abstention increases.\n\n\\paragraph{More Aggressive Random Masking as a Baseline.} We compare \\textit{MaskTune} to a another version of random masking with different window sizes on the MNIST dataset. For each image, we randomly selected a window of $\\{2\\times2, 3\\times3, 4\\times4, .... n \\times n\\}$ pixels where $n$ is the image size divided by $2$. As shown in Table \\ref{tab:randmask2}, \\textit{MaskTune} still outperforms the random masking approach.", "table_source": "\\begin{table}[h!]\n\\centering\n\\setlength{\\tabcolsep}{3pt}\n\\caption{Compare \\textit{MaskTune} with random masking.}\n\\begin{tabular}{c|c|c}\nTest data type & Masking method & Accuracy     \\\\ \\midrule\nbiased         & random masking & 59.06$\\pm$09.25 \\\\\noriginal       & random masking & 91.02$\\pm$01.87 \\\\\nbiased         & MaskTune       & \\textbf{98.26$\\pm$00.27} \\\\\noriginal       & MaskTune       & \\textbf{98.46$\\pm$00.21}\n\\end{tabular}\n\\label{tab:randmask2}\n\\end{table}", "cell_list_gold": [{"value": "59.06", "char_index": [230, 235], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"Test data type": "biased"}, "model": "random masking", "model settings": {"xx": "yy"}}, {"value": "91.02", "char_index": [283, 288], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"Test data type": "original"}, "model": "random masking", "model settings": {"xx": "yy"}}, {"value": "98.26", "char_index": [344, 349], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"Test data type": "biased"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "98.46", "char_index": [406, 411], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": ["Classification with Spurious Features", "image classification"], "metric": "Accuracy", "experimental settings": {"Test data type": "original"}, "model": "MaskTune", "model settings": {"xx": "yy"}}]}, "2210.00055v1_table6": {"table_code": "\\begin{table}[h!]\n\\centering\n\\setlength{\\tabcolsep}{3pt}\n\\caption{Selective clssification results on CIFAR-10, SVHN, and Cats vs. Dogs datasets for different coverage values.}\n\\begin{tabular}{c|c|cc}\n\\multirow{2}{*}{Dataset}      & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Target\\\\ coverage\\end{tabular}} & \\multicolumn{2}{c}{MaskTune} \\\\\n                              &                                                                            & Cov.          & Error        \\\\ \\midrule\n\\multirow{5}{*}{Cifar-10}     & 100\\%                                                                      & 99.99$\\pm$0.02    & 8.96$\\pm$0.48    \\\\\n                              & 95\\%                                                                       & 94.86$\\pm$0.18    & 6.54$\\pm$0.39    \\\\\n                              & 90\\%                                                                       & 89.73$\\pm$0.22    & 4.74$\\pm$0.31    \\\\\n                              & 85\\%                                                                       & 84.46$\\pm$0.07    & 3.23$\\pm$0.20    \\\\\n                              & 80\\%                                                                       & 79.03$\\pm$0.44    & 2.13$\\pm$0.11    \\\\ \\midrule\n\\multirow{5}{*}{SVHN}         & 100\\%                                                                      & 100.0$\\pm$0.00     & 3.68$\\pm$0.16    \\\\\n                              & 95\\%                                                                       & 95.19$\\pm$0.09    & 1.84$\\pm$0.23    \\\\\n                              & 90\\%                                                                       & 89.55$\\pm$0.26    & 0.96$\\pm$0.11    \\\\\n                              & 85\\%                                                                       & 83.72$\\pm$0.71    & 0.57$\\pm$0.04    \\\\\n                              & 80\\%                                                                       & 77.81$\\pm$0.75    & 0.45$\\pm$0.05    \\\\ \\midrule\n\\multirow{5}{*}{Cats vs. Dogs} & 100\\%                                                                      & 99.98$\\pm$0.0     & 4.83$\\pm$0.17    \\\\\n                              & 95\\%                                                                       & 95.01$\\pm$0.14    & 2.96$\\pm$0.15    \\\\\n                              & 90\\%                                                                       & 90.78$\\pm$0.16    & 1.94$\\pm$0.18    \\\\\n                              & 85\\%                                                                       & 86.33$\\pm$0.61    & 1.24$\\pm$0.21    \\\\\n                              & 80\\%                                                                       & 81.98$\\pm$0.47    & 0.89$\\pm$0.20    \n\\end{tabular}\n\\label{tab:complete_selective}\n\\end{table}", "table_label": "{tab:complete_selective}", "table_numeric_cells": [["100", "100\\%", 523, 526, 523, 528], ["99.99", "99.99$\\pm$0.02", 600, 605, 600, 614], ["8.96", "8.96$\\pm$0.48", 620, 624, 620, 633], ["95", "95\\%", 672, 674, 672, 676], ["94.86", "94.86$\\pm$0.18", 749, 754, 749, 763], ["6.54", "6.54$\\pm$0.39", 769, 773, 769, 782], ["90", "90\\%", 821, 823, 821, 825], ["89.73", "89.73$\\pm$0.22", 898, 903, 898, 912], ["4.74", "4.74$\\pm$0.31", 918, 922, 918, 931], ["85", "85\\%", 970, 972, 970, 974], ["84.46", "84.46$\\pm$0.07", 1047, 1052, 1047, 1061], ["3.23", "3.23$\\pm$0.20", 1067, 1071, 1067, 1080], ["80", "80\\%", 1119, 1121, 1119, 1123], ["79.03", "79.03$\\pm$0.44", 1196, 1201, 1196, 1210], ["2.13", "2.13$\\pm$0.11", 1216, 1220, 1216, 1229], ["100", "100\\%", 1277, 1280, 1277, 1282], ["100.0", "100.0$\\pm$0.00", 1354, 1359, 1354, 1368], ["3.68", "3.68$\\pm$0.16", 1375, 1379, 1375, 1388], ["95", "95\\%", 1427, 1429, 1427, 1431], ["95.19", "95.19$\\pm$0.09", 1504, 1509, 1504, 1518], ["1.84", "1.84$\\pm$0.23", 1524, 1528, 1524, 1537], ["90", "90\\%", 1576, 1578, 1576, 1580], ["89.55", "89.55$\\pm$0.26", 1653, 1658, 1653, 1667], ["0.96", "0.96$\\pm$0.11", 1673, 1677, 1673, 1686], ["85", "85\\%", 1725, 1727, 1725, 1729], ["83.72", "83.72$\\pm$0.71", 1802, 1807, 1802, 1816], ["0.57", "0.57$\\pm$0.04", 1822, 1826, 1822, 1835], ["80", "80\\%", 1874, 1876, 1874, 1878], ["77.81", "77.81$\\pm$0.75", 1951, 1956, 1951, 1965], ["0.45", "0.45$\\pm$0.05", 1971, 1975, 1971, 1984], ["100", "100\\%", 2033, 2036, 2033, 2038], ["99.98", "99.98$\\pm$0.0", 2110, 2115, 2110, 2123], ["4.83", "4.83$\\pm$0.17", 2130, 2134, 2130, 2143], ["95", "95\\%", 2182, 2184, 2182, 2186], ["95.01", "95.01$\\pm$0.14", 2259, 2264, 2259, 2273], ["2.96", "2.96$\\pm$0.15", 2279, 2283, 2279, 2292], ["90", "90\\%", 2331, 2333, 2331, 2335], ["90.78", "90.78$\\pm$0.16", 2408, 2413, 2408, 2422], ["1.94", "1.94$\\pm$0.18", 2428, 2432, 2428, 2441], ["85", "85\\%", 2480, 2482, 2480, 2484], ["86.33", "86.33$\\pm$0.61", 2557, 2562, 2557, 2571], ["1.24", "1.24$\\pm$0.21", 2577, 2581, 2577, 2590], ["80", "80\\%", 2629, 2631, 2629, 2633], ["81.98", "81.98$\\pm$0.47", 2706, 2711, 2706, 2720], ["0.89", "0.89$\\pm$0.20", 2726, 2730, 2726, 2739]], "text_chunk_selected": "\\section{Method}\n\\textbf{Setup.} \nWe consider the supervised learning setting with inputs $x \\in \\mathcal{X} \\subset \\mathbb{R}^d$ and corresponding labels $y \\in \\mathcal{Y} = \\{1, \\ldots, k\\}$. We assume having access to samples $\\mathcal{D}^0 = \\{(x_i, y_i)\\}_{i=1}^n$ drawn from an unknown underlying distribution $p_{data}(x,y)$.\n\nOur goal is to learn the parameters $\\theta \\in \\Theta$ of a prediction model $m_\\theta: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that obtains low classification error w.r.t some loss function (e.g., cross entropy) $\\ell: \\Theta \\times (\\mathcal{X} \\times \\mathcal{Y}) \\rightarrow \\mathbb{R}$. Specifically, we minimize:\\\\\n\nwhere $\\mathcal{T}$ refers to a thresholding function by the threshold factor $\\tau$ (i.e., $ \\mathcal{T}=\\mathbbm{1}_{\\mathcal{A}_{x_i} \\le \\tau }$), and $\\odot$ denotes element-wise multiplication. As the resolution of $\\mathcal{A}$ is typically coarser than that of the input data, $\\mathcal{T}(\\mathcal{A}_{x_i})$ is up-sampled to the size of the input.\n\nProcedurally, we first learn model $m_{\\theta}^{\\text{initial}}$ using original unmasked training data $\\mathcal{D}^{\\text{initial}}$. Then we use $m_{\\theta}^{\\text{initial}}$, $\\mathcal{G}$ and $\\mathcal{T}$ to create the masked set $\\mathcal{D}^{\\text{masked}}$. Finally, the fully trained predictor $m_{\\theta}^\\text{initial}$ is tuned using $\\mathcal{D}^{\\text{masked}}$ to obtain $m_{\\theta}^{\\text{final}}$. \nAs for the masking step, any explainability approach can be applied (note that some may have more computational complexity, such as ScoreCAM~\\citep{wang2020score}). We use xGradCAM~\\citep{selvaraju2017grad} as it is fast and produces relatively denser heat-maps than other methods~\\citep{srinivas2019full, selvaraju2017grad, wang2020score}. \n\nThe Waterbirds dataset~\\citep{sagawa2019distributionally} was proposed to assess the degree to which models pick up spurious correlations in the training set. We discovered and fixed two \\textit{issues} with the Waterbirds dataset: a) because the background images in the Places dataset~\\citep{zhou2017places} may already contain bird images, multiple birds may appear in an image after overlaying the segmented bird images from the Caltech-UCSD Birds-200-2011 (CUB) dataset~\\citep{wah2011caltech}. For example, the label of an image may be ``landbird'', but the image contains both land and water birds. We manually removed such images from the dataset. b) Because the names of the species are similar, some land birds have been mislabeled as waterbirds which we corrected. The \\textit{corrected} Waterbirds dataset can be found on \\textit{MaskTune}'s GitHub page. After addressing the two issues, the ERM model's worst-group accuracy increased from 60\\% which is reported in~\\citep{sagawa2019distributionally} to 80.8$\\pm$1.3\\%. We repeated the group-DRO method and ERM experiments on the corrected waterbirds dataset and reported the results in Table~\\ref{tab:waterbirds}. As demonstrated, our method (without any group supervision) achieves similar accuracy to the group-DRO that benefits from full supervision. In Figure~\\ref{fig:celeba} (right), we visualized feature importance before and after applying our ~\\textit{MaskTune} on the modified Waterbirds dataset.\n\n\\subsection{Selective Classification} \\label{sec:selective}\nFor selective classification task, we evaluated \\textit{MaskTune} on three datasets: CIFAR-10~\\citep{krizhevsky2009learning}, SVHN~\\citep{netzer2011reading}, and Cats vs. Dogs~\\citep{geifman2019selectivenet}, with coverage values of \\{90\\%, 95\\%, 100\\%\\}. For example 90\\% coverage means abstaining 10\\% of the samples.\n\nGiven input image $j$ and the original (\\ensuremath{m^\\text{initial}_\\theta}) and finetuned (\\ensuremath{m^\\text{final}_\\theta}) models, let their respective inference-time prediction probabilities for class $i$ be $P_{ij}^\\text{init} $ and $P_{ij}^\\text{final}$. Then \\textit{MaskTune} does not abstain and declares the class $i$ as the predicted class, iff $P_{ij}^\\text{init} \\cdot P_{ij}^\\text{final} > \\gamma $, where $\\gamma$ is a threshold that allows for controlling the target coverage. \nTo find the proper $\\gamma$ for achieving the targeted coverage, we iterate over the validation set and find a threshold that, if applied to the validation set, would give the desired coverage (i.e., if our target coverage is 90\\%, we seek for a threshold which if we apply for abstention on the validation set, we will abstain predicting 10\\% of validation data). Note that the probabilities of the two models are multiplied to ensure that as the value of either probability decreases, the possibility of abstention increases.\n\n\\section{Appendix} \\label{app:exps}\n\\subsection{More Experimental Results}\n\\paragraph{Selective Classification.} In table \\ref{tab:complete_selective} we show the results of the selective classification for the \\textit{MaskTune} on different coverage values. We have run every experiment three times and reported the mean and std for each.", "table_source": "\\begin{table}[h!]\n\\centering\n\\setlength{\\tabcolsep}{3pt}\n\\caption{Selective clssification results on CIFAR-10, SVHN, and Cats vs. Dogs datasets for different coverage values.}\n\\begin{tabular}{c|c|cc}\n\\multirow{2}{*}{Dataset}      & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Target\\\\ coverage\\end{tabular}} & \\multicolumn{2}{c}{MaskTune} \\\\\n                              &                                                                            & Cov.          & Error        \\\\ \\midrule\n\\multirow{5}{*}{Cifar-10}     & 100\\%                                                                      & 99.99$\\pm$0.02    & 8.96$\\pm$0.48    \\\\\n                              & 95\\%                                                                       & 94.86$\\pm$0.18    & 6.54$\\pm$0.39    \\\\\n                              & 90\\%                                                                       & 89.73$\\pm$0.22    & 4.74$\\pm$0.31    \\\\\n                              & 85\\%                                                                       & 84.46$\\pm$0.07    & 3.23$\\pm$0.20    \\\\\n                              & 80\\%                                                                       & 79.03$\\pm$0.44    & 2.13$\\pm$0.11    \\\\ \\midrule\n\\multirow{5}{*}{SVHN}         & 100\\%                                                                      & 100.0$\\pm$0.00     & 3.68$\\pm$0.16    \\\\\n                              & 95\\%                                                                       & 95.19$\\pm$0.09    & 1.84$\\pm$0.23    \\\\\n                              & 90\\%                                                                       & 89.55$\\pm$0.26    & 0.96$\\pm$0.11    \\\\\n                              & 85\\%                                                                       & 83.72$\\pm$0.71    & 0.57$\\pm$0.04    \\\\\n                              & 80\\%                                                                       & 77.81$\\pm$0.75    & 0.45$\\pm$0.05    \\\\ \\midrule\n\\multirow{5}{*}{Cats vs. Dogs} & 100\\%                                                                      & 99.98$\\pm$0.0     & 4.83$\\pm$0.17    \\\\\n                              & 95\\%                                                                       & 95.01$\\pm$0.14    & 2.96$\\pm$0.15    \\\\\n                              & 90\\%                                                                       & 90.78$\\pm$0.16    & 1.94$\\pm$0.18    \\\\\n                              & 85\\%                                                                       & 86.33$\\pm$0.61    & 1.24$\\pm$0.21    \\\\\n                              & 80\\%                                                                       & 81.98$\\pm$0.47    & 0.89$\\pm$0.20    \n\\end{tabular}\n\\label{tab:complete_selective}\n\\end{table}", "cell_list_gold": [{"value": "100", "char_index": [523, 526], "type": "Other"}, {"value": "99.99", "char_index": [600, 605], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "8.96", "char_index": [620, 624], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "95", "char_index": [672, 674], "type": "Other"}, {"value": "94.86", "char_index": [749, 754], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "6.54", "char_index": [769, 773], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "90", "char_index": [821, 823], "type": "Other"}, {"value": "89.73", "char_index": [898, 903], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "4.74", "char_index": [918, 922], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "85", "char_index": [970, 972], "type": "Other"}, {"value": "84.46", "char_index": [1047, 1052], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "85"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "3.23", "char_index": [1067, 1071], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "85"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "80", "char_index": [1119, 1121], "type": "Other"}, {"value": "79.03", "char_index": [1196, 1201], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "80"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "2.13", "char_index": [1216, 1220], "type": "Result", "training data/set": "CIFAR-10", "test data/set": "CIFAR-10", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "80"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [1277, 1280], "type": "Other"}, {"value": "100.0", "char_index": [1354, 1359], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "3.68", "char_index": [1375, 1379], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "95", "char_index": [1427, 1429], "type": "Other"}, {"value": "95.19", "char_index": [1504, 1509], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "1.84", "char_index": [1524, 1528], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "90", "char_index": [1576, 1578], "type": "Other"}, {"value": "89.55", "char_index": [1653, 1658], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "0.96", "char_index": [1673, 1677], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "85", "char_index": [1725, 1727], "type": "Other"}, {"value": "83.72", "char_index": [1802, 1807], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "85"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "0.57", "char_index": [1822, 1826], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "85"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "80", "char_index": [1874, 1876], "type": "Other"}, {"value": "77.81", "char_index": [1951, 1956], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "80"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "0.45", "char_index": [1971, 1975], "type": "Result", "training data/set": "SVHN", "test data/set": "SVHN", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "80"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "100", "char_index": [2033, 2036], "type": "Other"}, {"value": "99.98", "char_index": [2110, 2115], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "4.83", "char_index": [2130, 2134], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "100"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "95", "char_index": [2182, 2184], "type": "Other"}, {"value": "95.01", "char_index": [2259, 2264], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "2.96", "char_index": [2279, 2283], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "95"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "90", "char_index": [2331, 2333], "type": "Other"}, {"value": "90.78", "char_index": [2408, 2413], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "1.94", "char_index": [2428, 2432], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "90"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "85", "char_index": [2480, 2482], "type": "Other"}, {"value": "86.33", "char_index": [2557, 2562], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "85"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "1.24", "char_index": [2577, 2581], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "85"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "80", "char_index": [2629, 2631], "type": "Other"}, {"value": "81.98", "char_index": [2706, 2711], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Cov.", "Coverage"], "experimental settings": {"Target Coverage": "80"}, "model": "MaskTune", "model settings": {"xx": "yy"}}, {"value": "0.89", "char_index": [2726, 2730], "type": "Result", "training data/set": "Cats vs. Dogs", "test data/set": "Cats vs. Dogs", "task": ["Selective classification", "image classification"], "metric": ["Err.", "Error"], "experimental settings": {"Target Coverage": "80"}, "model": "MaskTune", "model settings": {"xx": "yy"}}]}, "2210.00084v1_table1": {"table_code": "\\begin{table*}[t]\n\\vspace{-0.1in}\n\\caption{Few-shot graph classification results. The best results are highlighted in bold while the best baseline results are underlined. -I and -T denote inductive and transductive settings of CGFL, respectively. Teacher indicates the CGFL model without knowledge distillation.}\n\\vspace{-0.1in}\n\\label{tab:graph-cls-1}\n\\centering\n\\resizebox{1\\textwidth}{!}{\n\\begin{tabular} {c|cc|cc|cc|cc}\n\\toprule\n\\multirow{2}{*}{Method} & \\multicolumn{2}{c|}{Letter-High} &\\multicolumn{2}{c|}{Triangles} &\\multicolumn{2}{c|}{Reddit-12K} &\\multicolumn{2}{c}{Enzymes}  \\\\ \n\\cmidrule{2-9}\n& 5-shot & 10-shot & 5-shot & 10-shot & 5-shot & 10-shot & 5-shot & 10-shot  \\\\\n\\midrule\nWL &65.27$\\pm$7.67&68.39$\\pm$4.69  &51.25$\\pm$4.02&53.26$\\pm$2.95 &40.26$\\pm$5.17 &42.57$\\pm$3.69 &55.78$\\pm$4.72 &58.47$\\pm$3.84  \\\\ \nGraphlet &33.76$\\pm$6.94&37.59$\\pm$4.60  &40.17$\\pm$3.18&43.76$\\pm$3.09 &33.76$\\pm$6.94 &37.59$\\pm$4.60 &53.17$\\pm$5.92 &55.30$\\pm$3.78  \\\\ \nAWE &40.60$\\pm$3.91&42.20$\\pm$2.87  &39.36$\\pm$3.85&42.58$\\pm$3.11 &30.24$\\pm$2.34 &33.44$\\pm$2.04 &43.75$\\pm$1.85 &45.58$\\pm$2.11\\\\ \nGraph2Vec  &66.12$\\pm$5.21&68.17$\\pm$4.26  &48.38$\\pm$3.85&50.16$\\pm$4.15 &27.85$\\pm$4.21 &29.97$\\pm$3.17 &55.88$\\pm$4.86 &58.22$\\pm$4.30  \\\\ \nDiffpool  &58.69$\\pm$6.39&61.59$\\pm$5.21 &64.17$\\pm$5.87&67.12$\\pm$4.29 &35.24$\\pm$5.69 &37.43$\\pm$3.94 &45.64$\\pm$4.56 &49.64$\\pm$4.23 \\\\ \nCapsGNN  &56.60$\\pm$7.86&60.67$\\pm$5.24 &65.40$\\pm$6.13&68.37$\\pm$3.67 &36.58$\\pm$4.28 &39.16$\\pm$3.73 &52.67$\\pm$5.51 &55.31$\\pm$4.23  \\\\ \nGIN &65.83$\\pm$7.17&69.16$\\pm$5.14 &63.80$\\pm$5.61&67.30$\\pm$4.35 &40.36$\\pm$4.69 &43.70$\\pm$3.98 &55.73$\\pm$5.80 &58.83$\\pm$5.32 \\\\ \nGIN-KNN &63.52$\\pm$7.27&65.66$\\pm$8.69 &58.34$\\pm$3.91&61.55$\\pm$3.19 &41.31$\\pm$2.84 &43.58$\\pm$2.80 &57.24$\\pm$7.06 &59.34$\\pm$5.24 \\\\ \nGSM-GCN &68.69$\\pm$6.50&72.80$\\pm$4.12 &69.37$\\pm$4.92&73.11$\\pm$3.94 &40.77$\\pm$4.32 &44.28$\\pm$3.86 &54.34$\\pm$5.64 &58.16$\\pm$4.39 \\\\ \nGSM-GAT &69.91$\\pm$5.90&\\underline{73.28$\\pm$3.46} &71.40$\\pm$4.34 &\\underline{75.60$\\pm$3.67} &41.59$\\pm$4.12 &\\underline{45.67$\\pm$3.68} &55.42$\\pm$5.74 &60.64$\\pm$3.84 \\\\ \nAS-MAML &\\underline{70.23$\\pm$1.53} &73.19$\\pm$1.17 &\\underline{71.56$\\pm$1.17} &75.56$\\pm$2.39 &\\underline{41.90$\\pm$1.65} &45.66$\\pm$1.11 &\\underline{56.03$\\pm$1.85} &\\underline{60.79$\\pm$2.74} \\\\ \n\\midrule\nTeacher-I &{71.43$\\pm$5.23} &{73.62$\\pm$2.93}  &{71.93$\\pm$3.51} &{76.21$\\pm$2.87} &{42.32$\\pm$4.48} &{46.31$\\pm$3.84}  &{57.53$\\pm$3.16} &{61.12$\\pm$4.80} \\\\\nCGFL-I &{72.12$\\pm$4.88} &{74.08$\\pm$3.30} &{72.34$\\pm$3.42} &{76.91$\\pm$2.98} &{42.85$\\pm$4.62} &{46.89$\\pm$4.96} &{57.94$\\pm$2.85} &{61.66$\\pm$4.61}\\\\\nTeacher-T &{75.20$\\pm$4.34} &{78.35$\\pm$1.95}   &{78.55$\\pm$3.75} &{81.03$\\pm$3.37} &{44.80$\\pm$4.85} &{48.95$\\pm$4.03}  &{59.85$\\pm$2.34} &{62.30$\\pm$3.29}  \\\\\nCGFL-T &\\textbf{75.97$\\pm$5.02} &\\textbf{79.10$\\pm$4.23}  &\\textbf{79.32$\\pm$4.05} &\\textbf{81.78$\\pm$3.30} &\\textbf{45.55$\\pm$3.67} &\\textbf{49.32$\\pm$4.21} &\\textbf{60.34$\\pm$4.04} &\\textbf{62.97$\\pm$2.92}  \\\\\n\\midrule\n\\end{tabular}}\n\\vspace{-0.2in}\n\\end{table*}", "table_label": "{tab:graph-cls-1}", "table_numeric_cells": [["65.27", "65.27$\\pm$7.67", 699, 704, 699, 713], ["68.39", "68.39$\\pm$4.69", 714, 719, 714, 728], ["51.25", "51.25$\\pm$4.02", 731, 736, 731, 745], ["53.26", "53.26$\\pm$2.95", 746, 751, 746, 760], ["40.26", "40.26$\\pm$5.17", 762, 767, 762, 776], ["42.57", "42.57$\\pm$3.69", 778, 783, 778, 792], ["55.78", "55.78$\\pm$4.72", 794, 799, 794, 808], ["58.47", "58.47$\\pm$3.84", 810, 815, 810, 824], ["33.76", "33.76$\\pm$6.94", 840, 845, 840, 854], ["37.59", "37.59$\\pm$4.60", 855, 860, 855, 869], ["40.17", "40.17$\\pm$3.18", 872, 877, 872, 886], ["43.76", "43.76$\\pm$3.09", 887, 892, 887, 901], ["33.76", "33.76$\\pm$6.94", 903, 908, 903, 917], ["37.59", "37.59$\\pm$4.60", 919, 924, 919, 933], ["53.17", "53.17$\\pm$5.92", 935, 940, 935, 949], ["55.30", "55.30$\\pm$3.78", 951, 956, 951, 965], ["40.60", "40.60$\\pm$3.91", 976, 981, 976, 990], ["42.20", "42.20$\\pm$2.87", 991, 996, 991, 1005], ["39.36", "39.36$\\pm$3.85", 1008, 1013, 1008, 1022], ["42.58", "42.58$\\pm$3.11", 1023, 1028, 1023, 1037], ["30.24", "30.24$\\pm$2.34", 1039, 1044, 1039, 1053], ["33.44", "33.44$\\pm$2.04", 1055, 1060, 1055, 1069], ["43.75", "43.75$\\pm$1.85", 1071, 1076, 1071, 1085], ["45.58", "45.58$\\pm$2.11", 1087, 1092, 1087, 1101], ["66.12", "66.12$\\pm$5.21", 1117, 1122, 1117, 1131], ["68.17", "68.17$\\pm$4.26", 1132, 1137, 1132, 1146], ["48.38", "48.38$\\pm$3.85", 1149, 1154, 1149, 1163], ["50.16", "50.16$\\pm$4.15", 1164, 1169, 1164, 1178], ["27.85", "27.85$\\pm$4.21", 1180, 1185, 1180, 1194], ["29.97", "29.97$\\pm$3.17", 1196, 1201, 1196, 1210], ["55.88", "55.88$\\pm$4.86", 1212, 1217, 1212, 1226], ["58.22", "58.22$\\pm$4.30", 1228, 1233, 1228, 1242], ["58.69", "58.69$\\pm$6.39", 1259, 1264, 1259, 1273], ["61.59", "61.59$\\pm$5.21", 1274, 1279, 1274, 1288], ["64.17", "64.17$\\pm$5.87", 1290, 1295, 1290, 1304], ["67.12", "67.12$\\pm$4.29", 1305, 1310, 1305, 1319], ["35.24", "35.24$\\pm$5.69", 1321, 1326, 1321, 1335], ["37.43", "37.43$\\pm$3.94", 1337, 1342, 1337, 1351], ["45.64", "45.64$\\pm$4.56", 1353, 1358, 1353, 1367], ["49.64", "49.64$\\pm$4.23", 1369, 1374, 1369, 1383], ["56.60", "56.60$\\pm$7.86", 1398, 1403, 1398, 1412], ["60.67", "60.67$\\pm$5.24", 1413, 1418, 1413, 1427], ["65.40", "65.40$\\pm$6.13", 1429, 1434, 1429, 1443], ["68.37", "68.37$\\pm$3.67", 1444, 1449, 1444, 1458], ["36.58", "36.58$\\pm$4.28", 1460, 1465, 1460, 1474], ["39.16", "39.16$\\pm$3.73", 1476, 1481, 1476, 1490], ["52.67", "52.67$\\pm$5.51", 1492, 1497, 1492, 1506], ["55.31", "55.31$\\pm$4.23", 1508, 1513, 1508, 1522], ["65.83", "65.83$\\pm$7.17", 1533, 1538, 1533, 1547], ["69.16", "69.16$\\pm$5.14", 1548, 1553, 1548, 1562], ["63.80", "63.80$\\pm$5.61", 1564, 1569, 1564, 1578], ["67.30", "67.30$\\pm$4.35", 1579, 1584, 1579, 1593], ["40.36", "40.36$\\pm$4.69", 1595, 1600, 1595, 1609], ["43.70", "43.70$\\pm$3.98", 1611, 1616, 1611, 1625], ["55.73", "55.73$\\pm$5.80", 1627, 1632, 1627, 1641], ["58.83", "58.83$\\pm$5.32", 1643, 1648, 1643, 1657], ["63.52", "63.52$\\pm$7.27", 1671, 1676, 1671, 1685], ["65.66", "65.66$\\pm$8.69", 1686, 1691, 1686, 1700], ["58.34", "58.34$\\pm$3.91", 1702, 1707, 1702, 1716], ["61.55", "61.55$\\pm$3.19", 1717, 1722, 1717, 1731], ["41.31", "41.31$\\pm$2.84", 1733, 1738, 1733, 1747], ["43.58", "43.58$\\pm$2.80", 1749, 1754, 1749, 1763], ["57.24", "57.24$\\pm$7.06", 1765, 1770, 1765, 1779], ["59.34", "59.34$\\pm$5.24", 1781, 1786, 1781, 1795], ["68.69", "68.69$\\pm$6.50", 1809, 1814, 1809, 1823], ["72.80", "72.80$\\pm$4.12", 1824, 1829, 1824, 1838], ["69.37", "69.37$\\pm$4.92", 1840, 1845, 1840, 1854], ["73.11", "73.11$\\pm$3.94", 1855, 1860, 1855, 1869], ["40.77", "40.77$\\pm$4.32", 1871, 1876, 1871, 1885], ["44.28", "44.28$\\pm$3.86", 1887, 1892, 1887, 1901], ["54.34", "54.34$\\pm$5.64", 1903, 1908, 1903, 1917], ["58.16", "58.16$\\pm$4.39", 1919, 1924, 1919, 1933], ["69.91", "69.91$\\pm$5.90", 1947, 1952, 1947, 1961], ["73.28", "\\underline{73.28$\\pm$3.46}", 1973, 1978, 1962, 1988], ["71.40", "71.40$\\pm$4.34", 1990, 1995, 1990, 2004], ["75.60", "\\underline{75.60$\\pm$3.67}", 2017, 2022, 2006, 2032], ["41.59", "41.59$\\pm$4.12", 2034, 2039, 2034, 2048], ["45.67", "\\underline{45.67$\\pm$3.68}", 2061, 2066, 2050, 2076], ["55.42", "55.42$\\pm$5.74", 2078, 2083, 2078, 2092], ["60.64", "60.64$\\pm$3.84", 2094, 2099, 2094, 2108], ["70.23", "\\underline{70.23$\\pm$1.53}", 2133, 2138, 2122, 2148], ["73.19", "73.19$\\pm$1.17", 2150, 2155, 2150, 2164], ["71.56", "\\underline{71.56$\\pm$1.17}", 2177, 2182, 2166, 2192], ["75.56", "75.56$\\pm$2.39", 2194, 2199, 2194, 2208], ["41.90", "\\underline{41.90$\\pm$1.65}", 2221, 2226, 2210, 2236], ["45.66", "45.66$\\pm$1.11", 2238, 2243, 2238, 2252], ["56.03", "\\underline{56.03$\\pm$1.85}", 2265, 2270, 2254, 2280], ["60.79", "\\underline{60.79$\\pm$2.74}", 2293, 2298, 2282, 2308], ["71.43", "{71.43$\\pm$5.23}", 2334, 2339, 2333, 2349], ["73.62", "{73.62$\\pm$2.93}", 2352, 2357, 2351, 2367], ["71.93", "{71.93$\\pm$3.51}", 2371, 2376, 2370, 2386], ["76.21", "{76.21$\\pm$2.87}", 2389, 2394, 2388, 2404], ["42.32", "{42.32$\\pm$4.48}", 2407, 2412, 2406, 2422], ["46.31", "{46.31$\\pm$3.84}", 2425, 2430, 2424, 2440], ["57.53", "{57.53$\\pm$3.16}", 2444, 2449, 2443, 2459], ["61.12", "{61.12$\\pm$4.80}", 2462, 2467, 2461, 2477], ["72.12", "{72.12$\\pm$4.88}", 2490, 2495, 2489, 2505], ["74.08", "{74.08$\\pm$3.30}", 2508, 2513, 2507, 2523], ["72.34", "{72.34$\\pm$3.42}", 2526, 2531, 2525, 2541], ["76.91", "{76.91$\\pm$2.98}", 2544, 2549, 2543, 2559], ["42.85", "{42.85$\\pm$4.62}", 2562, 2567, 2561, 2577], ["46.89", "{46.89$\\pm$4.96}", 2580, 2585, 2579, 2595], ["57.94", "{57.94$\\pm$2.85}", 2598, 2603, 2597, 2613], ["61.66", "{61.66$\\pm$4.61}", 2616, 2621, 2615, 2631], ["75.20", "{75.20$\\pm$4.34}", 2646, 2651, 2645, 2661], ["78.35", "{78.35$\\pm$1.95}", 2664, 2669, 2663, 2679], ["78.55", "{78.55$\\pm$3.75}", 2684, 2689, 2683, 2699], ["81.03", "{81.03$\\pm$3.37}", 2702, 2707, 2701, 2717], ["44.80", "{44.80$\\pm$4.85}", 2720, 2725, 2719, 2735], ["48.95", "{48.95$\\pm$4.03}", 2738, 2743, 2737, 2753], ["59.85", "{59.85$\\pm$2.34}", 2757, 2762, 2756, 2772], ["62.30", "{62.30$\\pm$3.29}", 2775, 2780, 2774, 2790], ["75.97", "\\textbf{75.97$\\pm$5.02}", 2811, 2816, 2803, 2826], ["79.10", "\\textbf{79.10$\\pm$4.23}", 2836, 2841, 2828, 2851], ["79.32", "\\textbf{79.32$\\pm$4.05}", 2862, 2867, 2854, 2877], ["81.78", "\\textbf{81.78$\\pm$3.30}", 2887, 2892, 2879, 2902], ["45.55", "\\textbf{45.55$\\pm$3.67}", 2912, 2917, 2904, 2927], ["49.32", "\\textbf{49.32$\\pm$4.21}", 2937, 2942, 2929, 2952], ["60.34", "\\textbf{60.34$\\pm$4.04}", 2962, 2967, 2954, 2977], ["62.97", "\\textbf{62.97$\\pm$2.92}", 2987, 2992, 2979, 3002]], "text_chunk_selected": "\\textbf{GFL Setting and Problem}. Let $\\mathcal{C}_{base}$ and $\\mathcal{C}_{novel}$ denote the base classes set and novel (new) classes set in training data $\\mathcal{T}_{train}$ and testing data $\\mathcal{T}_{test}$, respectively. \nSimilar to the general meta-learning problem~\\cite{finn2017model}, the purpose of graph few-shot learning (GFL) is to train a GNN encoder $f_\\theta(\\cdot)$ over $\\mathcal{C}_{base}$, such that the trained GNN encoder can be quickly adapted to $\\mathcal{C}_{novel}$ with few labels per class. Note that there is no overlapping between base classes and novel classes, i.e., $\\mathcal{C}_{base} \\cap \\mathcal{C}_{novel} = \\emptyset$. \nIn $K$-shot setting, during the meta-training phase,\na batch of classes (tasks) is randomly sampled from $\\mathcal{C}_{base}$, where $K$ labeled instances per class are sampled to form the support set $\\mathcal{S}$ for model training and the remaining instances are taken as the query set $\\mathcal{Q}$ for model evaluation. After sufficient training, the model is further transferred to the meta-testing phase to conduct $N$-way classification over $\\mathcal{C}_{novel}$ ($N$ is the number of novel classes), where each class is only with $K$ labeled instances. GFL applies to different graph mining problems, depending on the class meaning. Each class corresponds to a node label for the node classification problem or corresponds to a graph label for the graph classification problem. In this work, we will study both node classification and graph classification problems under few-shot setting, which are formally defined as follows:\n\n\\subsection{Self-Distilled Graph Contrastive Learning}\n\\textbf{GNN Contrastive Pre-training}. In the first phase, we firstly introduce contrastive learning to pre-train GNN. Inspired by the representation bootstrapping technique~\\cite{grill2020bootstrap}, our method learns node (or graph) representation by discriminating context instances. Specifically, two GNN encoders: an online GNN $f_\\theta(\\cdot)$ and a target GNN $f_\\xi(\\cdot)$, are introduced to encode two randomly augmented views of a given graph. The online GNN is supervised under the target GNN's output, while the target GNN is updated by the online GNN's exponential moving average. The contrastive pre-training step is shown in Figure~\\ref{fig:ssl}(a).\n\\\\\n\\textit{Graph Augmentation:} The given graph $G$ is processed with randomly data augmentations to generate a contrastive pair ($G^{\\prime}$, $G^{\\prime\\prime}$) as the input for two GNN branches (online branch and target branch) of the following GNN training. In this work, we apply a combination of stochastic node feature masking, edge removing, and node dropping with constant probabilities for graph augmentation.\n\\\\\n\\textit{GNN Update:} With the generated graph pair ($G^{\\prime}$, $G^{\\prime\\prime}$), the online GNN $f_\\theta(\\cdot)$ and the target GNN $f_\\xi(\\cdot)$ are respectively utilized to process $G^{\\prime}$ and $G^{\\prime\\prime}$ for node (or graph) embeddings generation. Both GNNs have the same architecture, while a two-layer FC (one-layer FC) is attached after online GNN (target GNN) to refine embedding. The reason that two branches have different FC layers is to prevent the prediction of the online model from being exactly the same as the output of the target model, thus avoiding the learned representation collapse. Later, to enforce online GNN's embeddings $z_\\theta$ approximate the target GNN's embeddings $h_\\xi$, the mean squared error between them is formulated as the objective function:\n\nwhere $\\tau \\in [0, 1]$ is the decay rate. Note that the target GNN stops the backpropagation from $\\mathcal{L}_{\\theta,\\xi}$, and it is only updated by EMA.\n\\textbf{Contrastive Distillation}.\nWith the pre-trained GNN $f_\\theta(\\cdot)$ obtained in the previous step, we introduce a self-distillation step to elevate $f_\\theta(\\cdot)$. This is inspired by the Born-again strategy~\\cite{furlanello2018born}, which implies a well-trained teacher can boost a random initialized identical student. The distillation step adopts a similar contrastive framework as the previous step, as shown in Figure~\\ref{fig:ssl}(a). Specifically, we load the pre-trained GNN $f_\\theta(\\cdot)$ and take it as the teacher model $f_{\\theta_{t}}(\\cdot)$. The teacher model is frozen and applied to distill a student model $f_{\\theta_{s}}(\\cdot)$.\nTwo augmented views ($G^{*}$, $G^{**}$) of a graph $G$ are generated and fed to $f_{\\theta_{t}}(\\cdot)$ and $f_{\\theta_{s}}(\\cdot)$, respectively. Later,\nthe student's normalized output is forced to approximate the teacher's normalized output as follows:\n\nwhere $\\alpha$ is the learning step size, $\\mathcal{L}^{\\mathcal{S}_{i}}_{\\mathcal{T}_i}$ denotes the downstream task loss over $\\mathcal{S}_{i}$. In this work, we employ prototypical loss~\\cite{snell2017prototypical} for node or graph classification, which uses embeddings extracted from the support set by a neural network as the class prototype, and the query set is classified according to the distance between its embeddings and prototypes. The task-specific parameter $\\theta_{{s, i}}'$ is further utilized to compute the loss over query set $\\mathcal{Q}_{i}$ of $T_i$: $\\mathcal{L}^{\\mathcal{Q}_{i}}_{\\mathcal{T}_i}(f_{\\theta_{{s, i}}'})$. Later, $\\mathcal{L}^{\\mathcal{Q}_{i}}_{\\mathcal{T}_i}(f_{\\theta_{{s, i}}'})$ of a batch of randomly sampled tasks are summed up to update the model parameters $\\theta_{s}$ (i.e., outer-loop):\n\nwhere $x_{i}$ denotes a random variable of $i$-th node attribute in the graph. \nThen, we introduce a  noise perturbation-based method to approximate $H(x_{i}|{\\bf {z}})$. Specifically,  let $\\widetilde{x_i}=x_i+{\\bf \\epsilon}_i$ (${\\bf \\epsilon}_i \\sim \\mathcal{N}(0, {\\bf \\Sigma}_i={\\bf \\sigma}^2_i{\\bf I})$) and we aim to optimize the following loss function:\n\nwhere $\\bm{\\sigma}$ = $[\\sigma_1, \\sigma_2, \\cdots, \\sigma_n]$ are learnable parameters; \n$\\beta$ is trade-off weight. In particular, the first term of the above objective minimizes difference between the encoded embedding of noisy input and the hidden state while the second term encourages a high conditional entropy according to the maximum entropy principle. In this way, we have $p(\\widetilde{x_i}|{\\bf z})=p({\\bf \\epsilon}_i)$ and $H(x_i|{\\bf z})$ is approximated by $H(\\widetilde{x}_i|{\\bf z})$:\n\nwhere $C = \\frac{1}{2}\\log (2\\pi e)$. By taking Eqn.~\\ref{eq:infoloss-7} into Eqn.~\\ref{eq:infoloss-6}, \nwe can optimize $\\mathcal{L}(\\bm{\\sigma})$ with Adam optimizer and obtain the optimal $\\bm{\\sigma}$ for computing the overall discarded information $H(G|Z)$. Furthermore, we can explain GFL model's capability by comparing discarded information of different models. Ideally, we expect the GFL model to encode valid node (or graph) embeddings as much as possible and therefore discard information as small as possible. \n\n\\vspace{-0.15in}\n\\subsection{Few-Shot Graph Classification}\n\\vspace{-0.1in}\n\\textbf{Overall Performance}.\nThe results of all models for 5/10-shot graph classification\nare reported in Table~\\ref{tab:graph-cls-1}. Similar to the findings obtained from Table~\\ref{tab:node-cls-1}, according to this table: (1) CGFL outperforms all baseline models, showing its superiority for few-shot graph classification;\n(2) The improvement of CGFL-I over baseline models ranges from 0.95\\% to 38.36\\% (5-shot) and from 0.8\\% to 34.33\\% (10-shot). Meanwhile, this value of CGFL-T ranges from 5.65\\% to 46.21\\% (5-shot) and from 2.18\\% to 41.51\\% (10-shot). This demonstrates the effect of contrastive pre-training and self-distillation in learning rich graph embedding from unlabeled data; (3) CGFL-T (or Teacher-T) outperforms CGFL-I (or Teacher-I), showing that unlabeled data in testing set improves model's generalization; {(4) CGFL is better than teacher model as contrastive distillation step further elevates the model.}", "table_source": "\\begin{table*}[t]\n\\vspace{-0.1in}\n\\caption{Few-shot graph classification results. The best results are highlighted in bold while the best baseline results are underlined. -I and -T denote inductive and transductive settings of CGFL, respectively. Teacher indicates the CGFL model without knowledge distillation.}\n\\vspace{-0.1in}\n\\label{tab:graph-cls-1}\n\\centering\n\\resizebox{1\\textwidth}{!}{\n\\begin{tabular} {c|cc|cc|cc|cc}\n\\toprule\n\\multirow{2}{*}{Method} & \\multicolumn{2}{c|}{Letter-High} &\\multicolumn{2}{c|}{Triangles} &\\multicolumn{2}{c|}{Reddit-12K} &\\multicolumn{2}{c}{Enzymes}  \\\\ \n\\cmidrule{2-9}\n& 5-shot & 10-shot & 5-shot & 10-shot & 5-shot & 10-shot & 5-shot & 10-shot  \\\\\n\\midrule\nWL &65.27$\\pm$7.67&68.39$\\pm$4.69  &51.25$\\pm$4.02&53.26$\\pm$2.95 &40.26$\\pm$5.17 &42.57$\\pm$3.69 &55.78$\\pm$4.72 &58.47$\\pm$3.84  \\\\ \nGraphlet &33.76$\\pm$6.94&37.59$\\pm$4.60  &40.17$\\pm$3.18&43.76$\\pm$3.09 &33.76$\\pm$6.94 &37.59$\\pm$4.60 &53.17$\\pm$5.92 &55.30$\\pm$3.78  \\\\ \nAWE &40.60$\\pm$3.91&42.20$\\pm$2.87  &39.36$\\pm$3.85&42.58$\\pm$3.11 &30.24$\\pm$2.34 &33.44$\\pm$2.04 &43.75$\\pm$1.85 &45.58$\\pm$2.11\\\\ \nGraph2Vec  &66.12$\\pm$5.21&68.17$\\pm$4.26  &48.38$\\pm$3.85&50.16$\\pm$4.15 &27.85$\\pm$4.21 &29.97$\\pm$3.17 &55.88$\\pm$4.86 &58.22$\\pm$4.30  \\\\ \nDiffpool  &58.69$\\pm$6.39&61.59$\\pm$5.21 &64.17$\\pm$5.87&67.12$\\pm$4.29 &35.24$\\pm$5.69 &37.43$\\pm$3.94 &45.64$\\pm$4.56 &49.64$\\pm$4.23 \\\\ \nCapsGNN  &56.60$\\pm$7.86&60.67$\\pm$5.24 &65.40$\\pm$6.13&68.37$\\pm$3.67 &36.58$\\pm$4.28 &39.16$\\pm$3.73 &52.67$\\pm$5.51 &55.31$\\pm$4.23  \\\\ \nGIN &65.83$\\pm$7.17&69.16$\\pm$5.14 &63.80$\\pm$5.61&67.30$\\pm$4.35 &40.36$\\pm$4.69 &43.70$\\pm$3.98 &55.73$\\pm$5.80 &58.83$\\pm$5.32 \\\\ \nGIN-KNN &63.52$\\pm$7.27&65.66$\\pm$8.69 &58.34$\\pm$3.91&61.55$\\pm$3.19 &41.31$\\pm$2.84 &43.58$\\pm$2.80 &57.24$\\pm$7.06 &59.34$\\pm$5.24 \\\\ \nGSM-GCN &68.69$\\pm$6.50&72.80$\\pm$4.12 &69.37$\\pm$4.92&73.11$\\pm$3.94 &40.77$\\pm$4.32 &44.28$\\pm$3.86 &54.34$\\pm$5.64 &58.16$\\pm$4.39 \\\\ \nGSM-GAT &69.91$\\pm$5.90&\\underline{73.28$\\pm$3.46} &71.40$\\pm$4.34 &\\underline{75.60$\\pm$3.67} &41.59$\\pm$4.12 &\\underline{45.67$\\pm$3.68} &55.42$\\pm$5.74 &60.64$\\pm$3.84 \\\\ \nAS-MAML &\\underline{70.23$\\pm$1.53} &73.19$\\pm$1.17 &\\underline{71.56$\\pm$1.17} &75.56$\\pm$2.39 &\\underline{41.90$\\pm$1.65} &45.66$\\pm$1.11 &\\underline{56.03$\\pm$1.85} &\\underline{60.79$\\pm$2.74} \\\\ \n\\midrule\nTeacher-I &{71.43$\\pm$5.23} &{73.62$\\pm$2.93}  &{71.93$\\pm$3.51} &{76.21$\\pm$2.87} &{42.32$\\pm$4.48} &{46.31$\\pm$3.84}  &{57.53$\\pm$3.16} &{61.12$\\pm$4.80} \\\\\nCGFL-I &{72.12$\\pm$4.88} &{74.08$\\pm$3.30} &{72.34$\\pm$3.42} &{76.91$\\pm$2.98} &{42.85$\\pm$4.62} &{46.89$\\pm$4.96} &{57.94$\\pm$2.85} &{61.66$\\pm$4.61}\\\\\nTeacher-T &{75.20$\\pm$4.34} &{78.35$\\pm$1.95}   &{78.55$\\pm$3.75} &{81.03$\\pm$3.37} &{44.80$\\pm$4.85} &{48.95$\\pm$4.03}  &{59.85$\\pm$2.34} &{62.30$\\pm$3.29}  \\\\\nCGFL-T &\\textbf{75.97$\\pm$5.02} &\\textbf{79.10$\\pm$4.23}  &\\textbf{79.32$\\pm$4.05} &\\textbf{81.78$\\pm$3.30} &\\textbf{45.55$\\pm$3.67} &\\textbf{49.32$\\pm$4.21} &\\textbf{60.34$\\pm$4.04} &\\textbf{62.97$\\pm$2.92}  \\\\\n\\midrule\n\\end{tabular}}\n\\vspace{-0.2in}\n\\end{table*}", "cell_list_gold": [{"value": "65.27", "char_index": [699, 704], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "WL", "model settings": {"xx": "yy"}}, {"value": "68.39", "char_index": [714, 719], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "WL", "model settings": {"xx": "yy"}}, {"value": "51.25", "char_index": [731, 736], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "WL", "model settings": {"xx": "yy"}}, {"value": "53.26", "char_index": [746, 751], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "WL", "model settings": {"xx": "yy"}}, {"value": "40.26", "char_index": [762, 767], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "WL", "model settings": {"xx": "yy"}}, {"value": "42.57", "char_index": [778, 783], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "WL", "model settings": {"xx": "yy"}}, {"value": "55.78", "char_index": [794, 799], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "WL", "model settings": {"xx": "yy"}}, {"value": "58.47", "char_index": [810, 815], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "WL", "model settings": {"xx": "yy"}}, {"value": "33.76", "char_index": [840, 845], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Graphlet", "model settings": {"xx": "yy"}}, {"value": "37.59", "char_index": [855, 860], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Graphlet", "model settings": {"xx": "yy"}}, {"value": "40.17", "char_index": [872, 877], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Graphlet", "model settings": {"xx": "yy"}}, {"value": "43.76", "char_index": [887, 892], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Graphlet", "model settings": {"xx": "yy"}}, {"value": "33.76", "char_index": [903, 908], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Graphlet", "model settings": {"xx": "yy"}}, {"value": "37.59", "char_index": [919, 924], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Graphlet", "model settings": {"xx": "yy"}}, {"value": "53.17", "char_index": [935, 940], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Graphlet", "model settings": {"xx": "yy"}}, {"value": "55.30", "char_index": [951, 956], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Graphlet", "model settings": {"xx": "yy"}}, {"value": "40.60", "char_index": [976, 981], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "AWE", "model settings": {"xx": "yy"}}, {"value": "42.20", "char_index": [991, 996], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "AWE", "model settings": {"xx": "yy"}}, {"value": "39.36", "char_index": [1008, 1013], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "AWE", "model settings": {"xx": "yy"}}, {"value": "42.58", "char_index": [1023, 1028], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "AWE", "model settings": {"xx": "yy"}}, {"value": "30.24", "char_index": [1039, 1044], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "AWE", "model settings": {"xx": "yy"}}, {"value": "33.44", "char_index": [1055, 1060], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "AWE", "model settings": {"xx": "yy"}}, {"value": "43.75", "char_index": [1071, 1076], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "AWE", "model settings": {"xx": "yy"}}, {"value": "45.58", "char_index": [1087, 1092], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "AWE", "model settings": {"xx": "yy"}}, {"value": "66.12", "char_index": [1117, 1122], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Graph2Vec", "model settings": {"xx": "yy"}}, {"value": "68.17", "char_index": [1132, 1137], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Graph2Vec", "model settings": {"xx": "yy"}}, {"value": "48.38", "char_index": [1149, 1154], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Graph2Vec", "model settings": {"xx": "yy"}}, {"value": "50.16", "char_index": [1164, 1169], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Graph2Vec", "model settings": {"xx": "yy"}}, {"value": "27.85", "char_index": [1180, 1185], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Graph2Vec", "model settings": {"xx": "yy"}}, {"value": "29.97", "char_index": [1196, 1201], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Graph2Vec", "model settings": {"xx": "yy"}}, {"value": "55.88", "char_index": [1212, 1217], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Graph2Vec", "model settings": {"xx": "yy"}}, {"value": "58.22", "char_index": [1228, 1233], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Graph2Vec", "model settings": {"xx": "yy"}}, {"value": "58.69", "char_index": [1259, 1264], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Diffpool", "model settings": {"xx": "yy"}}, {"value": "61.59", "char_index": [1274, 1279], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Diffpool", "model settings": {"xx": "yy"}}, {"value": "64.17", "char_index": [1290, 1295], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Diffpool", "model settings": {"xx": "yy"}}, {"value": "67.12", "char_index": [1305, 1310], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Diffpool", "model settings": {"xx": "yy"}}, {"value": "35.24", "char_index": [1321, 1326], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Diffpool", "model settings": {"xx": "yy"}}, {"value": "37.43", "char_index": [1337, 1342], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Diffpool", "model settings": {"xx": "yy"}}, {"value": "45.64", "char_index": [1353, 1358], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Diffpool", "model settings": {"xx": "yy"}}, {"value": "49.64", "char_index": [1369, 1374], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Diffpool", "model settings": {"xx": "yy"}}, {"value": "56.60", "char_index": [1398, 1403], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CapsGNN", "model settings": {"xx": "yy"}}, {"value": "60.67", "char_index": [1413, 1418], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CapsGNN", "model settings": {"xx": "yy"}}, {"value": "65.40", "char_index": [1429, 1434], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CapsGNN", "model settings": {"xx": "yy"}}, {"value": "68.37", "char_index": [1444, 1449], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CapsGNN", "model settings": {"xx": "yy"}}, {"value": "36.58", "char_index": [1460, 1465], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CapsGNN", "model settings": {"xx": "yy"}}, {"value": "39.16", "char_index": [1476, 1481], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CapsGNN", "model settings": {"xx": "yy"}}, {"value": "52.67", "char_index": [1492, 1497], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CapsGNN", "model settings": {"xx": "yy"}}, {"value": "55.31", "char_index": [1508, 1513], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CapsGNN", "model settings": {"xx": "yy"}}, {"value": "65.83", "char_index": [1533, 1538], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GIN", "model settings": {"xx": "yy"}}, {"value": "69.16", "char_index": [1548, 1553], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GIN", "model settings": {"xx": "yy"}}, {"value": "63.80", "char_index": [1564, 1569], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GIN", "model settings": {"xx": "yy"}}, {"value": "67.30", "char_index": [1579, 1584], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GIN", "model settings": {"xx": "yy"}}, {"value": "40.36", "char_index": [1595, 1600], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GIN", "model settings": {"xx": "yy"}}, {"value": "43.70", "char_index": [1611, 1616], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GIN", "model settings": {"xx": "yy"}}, {"value": "55.73", "char_index": [1627, 1632], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GIN", "model settings": {"xx": "yy"}}, {"value": "58.83", "char_index": [1643, 1648], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GIN", "model settings": {"xx": "yy"}}, {"value": "63.52", "char_index": [1671, 1676], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GIN-KNN", "model settings": {"xx": "yy"}}, {"value": "65.66", "char_index": [1686, 1691], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GIN-KNN", "model settings": {"xx": "yy"}}, {"value": "58.34", "char_index": [1702, 1707], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GIN-KNN", "model settings": {"xx": "yy"}}, {"value": "61.55", "char_index": [1717, 1722], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GIN-KNN", "model settings": {"xx": "yy"}}, {"value": "41.31", "char_index": [1733, 1738], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GIN-KNN", "model settings": {"xx": "yy"}}, {"value": "43.58", "char_index": [1749, 1754], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GIN-KNN", "model settings": {"xx": "yy"}}, {"value": "57.24", "char_index": [1765, 1770], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GIN-KNN", "model settings": {"xx": "yy"}}, {"value": "59.34", "char_index": [1781, 1786], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GIN-KNN", "model settings": {"xx": "yy"}}, {"value": "68.69", "char_index": [1809, 1814], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GSM-GCN", "model settings": {"xx": "yy"}}, {"value": "72.80", "char_index": [1824, 1829], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GSM-GCN", "model settings": {"xx": "yy"}}, {"value": "69.37", "char_index": [1840, 1845], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GSM-GCN", "model settings": {"xx": "yy"}}, {"value": "73.11", "char_index": [1855, 1860], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GSM-GCN", "model settings": {"xx": "yy"}}, {"value": "40.77", "char_index": [1871, 1876], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GSM-GCN", "model settings": {"xx": "yy"}}, {"value": "44.28", "char_index": [1887, 1892], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GSM-GCN", "model settings": {"xx": "yy"}}, {"value": "54.34", "char_index": [1903, 1908], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GSM-GCN", "model settings": {"xx": "yy"}}, {"value": "58.16", "char_index": [1919, 1924], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GSM-GCN", "model settings": {"xx": "yy"}}, {"value": "69.91", "char_index": [1947, 1952], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GSM-GAT", "model settings": {"xx": "yy"}}, {"value": "73.28", "char_index": [1973, 1978], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GSM-GAT", "model settings": {"xx": "yy"}}, {"value": "71.40", "char_index": [1990, 1995], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GSM-GAT", "model settings": {"xx": "yy"}}, {"value": "75.60", "char_index": [2017, 2022], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GSM-GAT", "model settings": {"xx": "yy"}}, {"value": "41.59", "char_index": [2034, 2039], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GSM-GAT", "model settings": {"xx": "yy"}}, {"value": "45.67", "char_index": [2061, 2066], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GSM-GAT", "model settings": {"xx": "yy"}}, {"value": "55.42", "char_index": [2078, 2083], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "GSM-GAT", "model settings": {"xx": "yy"}}, {"value": "60.64", "char_index": [2094, 2099], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "GSM-GAT", "model settings": {"xx": "yy"}}, {"value": "70.23", "char_index": [2133, 2138], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "AS-MAML", "model settings": {"xx": "yy"}}, {"value": "73.19", "char_index": [2150, 2155], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "AS-MAML", "model settings": {"xx": "yy"}}, {"value": "71.56", "char_index": [2177, 2182], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "AS-MAML", "model settings": {"xx": "yy"}}, {"value": "75.56", "char_index": [2194, 2199], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "AS-MAML", "model settings": {"xx": "yy"}}, {"value": "41.90", "char_index": [2221, 2226], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "AS-MAML", "model settings": {"xx": "yy"}}, {"value": "45.66", "char_index": [2238, 2243], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "AS-MAML", "model settings": {"xx": "yy"}}, {"value": "56.03", "char_index": [2265, 2270], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "AS-MAML", "model settings": {"xx": "yy"}}, {"value": "60.79", "char_index": [2293, 2298], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "AS-MAML", "model settings": {"xx": "yy"}}, {"value": "71.43", "char_index": [2334, 2339], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Teacher-I", "model settings": {"xx": "yy"}}, {"value": "73.62", "char_index": [2352, 2357], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Teacher-I", "model settings": {"xx": "yy"}}, {"value": "71.93", "char_index": [2371, 2376], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Teacher-I", "model settings": {"xx": "yy"}}, {"value": "76.21", "char_index": [2389, 2394], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Teacher-I", "model settings": {"xx": "yy"}}, {"value": "42.32", "char_index": [2407, 2412], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Teacher-I", "model settings": {"xx": "yy"}}, {"value": "46.31", "char_index": [2425, 2430], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Teacher-I", "model settings": {"xx": "yy"}}, {"value": "57.53", "char_index": [2444, 2449], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Teacher-I", "model settings": {"xx": "yy"}}, {"value": "61.12", "char_index": [2462, 2467], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Teacher-I", "model settings": {"xx": "yy"}}, {"value": "72.12", "char_index": [2490, 2495], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CGFL-I", "model settings": {"xx": "yy"}}, {"value": "74.08", "char_index": [2508, 2513], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CGFL-I", "model settings": {"xx": "yy"}}, {"value": "72.34", "char_index": [2526, 2531], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CGFL-I", "model settings": {"xx": "yy"}}, {"value": "76.91", "char_index": [2544, 2549], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CGFL-I", "model settings": {"xx": "yy"}}, {"value": "42.85", "char_index": [2562, 2567], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CGFL-I", "model settings": {"xx": "yy"}}, {"value": "46.89", "char_index": [2580, 2585], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CGFL-I", "model settings": {"xx": "yy"}}, {"value": "57.94", "char_index": [2598, 2603], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CGFL-I", "model settings": {"xx": "yy"}}, {"value": "61.66", "char_index": [2616, 2621], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CGFL-I", "model settings": {"xx": "yy"}}, {"value": "75.20", "char_index": [2646, 2651], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Teacher-T", "model settings": {"xx": "yy"}}, {"value": "78.35", "char_index": [2664, 2669], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Teacher-T", "model settings": {"xx": "yy"}}, {"value": "78.55", "char_index": [2684, 2689], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Teacher-T", "model settings": {"xx": "yy"}}, {"value": "81.03", "char_index": [2702, 2707], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Teacher-T", "model settings": {"xx": "yy"}}, {"value": "44.80", "char_index": [2720, 2725], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Teacher-T", "model settings": {"xx": "yy"}}, {"value": "48.95", "char_index": [2738, 2743], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Teacher-T", "model settings": {"xx": "yy"}}, {"value": "59.85", "char_index": [2757, 2762], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "Teacher-T", "model settings": {"xx": "yy"}}, {"value": "62.30", "char_index": [2775, 2780], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "Teacher-T", "model settings": {"xx": "yy"}}, {"value": "75.97", "char_index": [2811, 2816], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CGFL-T", "model settings": {"xx": "yy"}}, {"value": "79.10", "char_index": [2836, 2841], "type": "Result", "training data/set": "Letter-High", "test data/set": "Letter-High", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CGFL-T", "model settings": {"xx": "yy"}}, {"value": "79.32", "char_index": [2862, 2867], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CGFL-T", "model settings": {"xx": "yy"}}, {"value": "81.78", "char_index": [2887, 2892], "type": "Result", "training data/set": "Triangles", "test data/set": "Triangles", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CGFL-T", "model settings": {"xx": "yy"}}, {"value": "45.55", "char_index": [2912, 2917], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CGFL-T", "model settings": {"xx": "yy"}}, {"value": "49.32", "char_index": [2937, 2942], "type": "Result", "training data/set": "Reddit-12K", "test data/set": "Reddit-12K", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CGFL-T", "model settings": {"xx": "yy"}}, {"value": "60.34", "char_index": [2962, 2967], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "CGFL-T", "model settings": {"xx": "yy"}}, {"value": "62.97", "char_index": [2987, 2992], "type": "Result", "training data/set": "Enzymes", "test data/set": "Enzymes", "task": ["Few-shot graph classification", "graph classification"], "metric": "Accuracy", "experimental settings": {"10-shot": "true"}, "model": "CGFL-T", "model settings": {"xx": "yy"}}]}, "2210.00084v1_table2": {"table_code": "\\begin{table}[htbp]\n\\vspace{-0.1in}\n\\caption{Statistics of datasets used in the node classification task.}\n\\label{tab:node-data-stat}\n\\vspace{1mm}\n\\small\n\\renewcommand\\arraystretch{1}\n\\centering\n\\resizebox{0.67\\textwidth}{!}{\\begin{tabular*}{0.7\\textwidth}{c| @{\\extracolsep{\\fill}} |c|c|c|c|c}\n\\toprule\n{Dataset} &  \\# Graph & \\# Node & \\#  Edge &  \\# Feat. &  \\# Label  \\\\\n\\midrule\nogbn-arxiv       &1 &169,343 &1,166,243 &128 &40  \\\\ \nTissue-PPI       &24 &51,194 &1,350,412 &50 &10 \\\\ \nFold-PPI       &144 &274,606 &3,666,563 &512 &29 \\\\ \nCora       &1 &2,708 &10,556 &1,433 &7 \\\\ \nCiteseer       &1 &3,327 &9,228 &3,703 &6 \\\\ \n\\bottomrule\n\\end{tabular*}\n}\n\\end{table}", "table_label": "{tab:node-data-stat}", "table_numeric_cells": [["1", "1", 402, 403, 402, 403], ["169,343", "169,343", 405, 412, 405, 412], ["1,166,243", "1,166,243", 414, 423, 414, 423], ["128", "128", 425, 428, 425, 428], ["40", "40", 430, 432, 430, 432], ["24", "24", 456, 458, 456, 458], ["51,194", "51,194", 460, 466, 460, 466], ["1,350,412", "1,350,412", 468, 477, 468, 477], ["50", "50", 479, 481, 479, 481], ["10", "10", 483, 485, 483, 485], ["144", "144", 506, 509, 506, 509], ["274,606", "274,606", 511, 518, 511, 518], ["3,666,563", "3,666,563", 520, 529, 520, 529], ["512", "512", 531, 534, 531, 534], ["29", "29", 536, 538, 536, 538], ["1", "1", 555, 556, 555, 556], ["2,708", "2,708", 558, 563, 558, 563], ["10,556", "10,556", 565, 571, 565, 571], ["1,433", "1,433", 573, 578, 573, 578], ["7", "7", 580, 581, 580, 581], ["1", "1", 602, 603, 602, 603], ["3,327", "3,327", 605, 610, 605, 610], ["9,228", "9,228", 612, 617, 612, 617], ["3,703", "3,703", 619, 624, 619, 624], ["6", "6", 626, 627, 626, 627]], "text_chunk_selected": "\\section{Related Work}\n\\label{sec:related_work}\n\\textbf{Few-Shot Learning on Graphs}. \nMany GFL models~\\citep{zhang2022few} have been proposed to solve various graph mining problems in face of label sparsity issue, such as node classification~\\citep{yao2020graph,ding2020graph,huang2020graph,qian2021adapting,qian2021distilling,wang2022task,zhang2022few}, relation prediction~\\cite{xiong2018one,lv2019adapting,chen2019meta,zhang2020few,zhang2020few2}, and graph classification~\\cite{chauhan2020few,ma2020adaptive,guo2021few,wang2021property}. They are built on meta-learning (or few-shot learning) techniques that can be categorized into two major groups: (1) metric-based approaches~\\cite{vinyals2016matching,snell2017prototypical}; (2) optimization-based algorithms~\\cite{finn2017model}. For\nthe first group, they learn effective similarity\nmetric between few-shot support data and query data. For example, \nGPN~\\cite{ding2020graph} conducts node informativeness propagation to build weighted class prototypes for a distance-based node classifier. \nThe second group proposes to learn well-initialized GNN parameters that can be fast adapted to new graph tasks with few labeled data. For instance,\nG-Meta~\\cite{huang2020graph} builds local subgraphs to extract subgraph-specific information and optimizes GNN via MAML~\\cite{finn2017model}. Unlike prior efforts that rely on labeled data and have the task and data-specific design, we aim to build a novel framework that explores unlabeled data and has a generic design for a general graph mining purpose.\n\\\\\n\\textbf{Self-Supervised Learning on Graphs}. \nRecently, self-supervised graph learning (SGL) has attracted significant attention due to its effectiveness in pre-training GNN and competitive performance in various graph mining applications. \n{Previous SGL models can be categorized into two major groups: generative or contrastive, according to their learning tasks~\\cite{SSLSurveyGorC, sohn2020fixmatch}.} The generative models learn graph representation by recovering feature or structural information on the graph. The task can solely recover adjacency matrix alone~\\cite{GraphRNN} or along with the node features~\\cite{GPT-GNN}. \nAs for the contrastive methods, they firstly define the node context which can be node-level or graph-level instances. Then, they perform contrastive learning by either maximizing the mutual information between the node-context pairs~\\cite{MVGRL,DGI,InfoGraph} or by discriminating context instances~\\cite{GCC,GCA,zhao2021multi,yu2022sail}. {In addition to above strategy, recently random propagation applies graph augmentation~\\cite{Rong2020DropEdge:} for semi-supervised learning~\\cite{NEURIPS2020_fb4c835f}.} Motivated by the success of SGL, we propose to leverage it to boost GFL. \n\n\\textbf{GFL Setting and Problem}. Let $\\mathcal{C}_{base}$ and $\\mathcal{C}_{novel}$ denote the base classes set and novel (new) classes set in training data $\\mathcal{T}_{train}$ and testing data $\\mathcal{T}_{test}$, respectively. \nSimilar to the general meta-learning problem~\\cite{finn2017model}, the purpose of graph few-shot learning (GFL) is to train a GNN encoder $f_\\theta(\\cdot)$ over $\\mathcal{C}_{base}$, such that the trained GNN encoder can be quickly adapted to $\\mathcal{C}_{novel}$ with few labels per class. Note that there is no overlapping between base classes and novel classes, i.e., $\\mathcal{C}_{base} \\cap \\mathcal{C}_{novel} = \\emptyset$. \nIn $K$-shot setting, during the meta-training phase,\na batch of classes (tasks) is randomly sampled from $\\mathcal{C}_{base}$, where $K$ labeled instances per class are sampled to form the support set $\\mathcal{S}$ for model training and the remaining instances are taken as the query set $\\mathcal{Q}$ for model evaluation. After sufficient training, the model is further transferred to the meta-testing phase to conduct $N$-way classification over $\\mathcal{C}_{novel}$ ($N$ is the number of novel classes), where each class is only with $K$ labeled instances. GFL applies to different graph mining problems, depending on the class meaning. Each class corresponds to a node label for the node classification problem or corresponds to a graph label for the graph classification problem. In this work, we will study both node classification and graph classification problems under few-shot setting, which are formally defined as follows:\n\nwhere $C = \\frac{1}{2}\\log (2\\pi e)$. By taking Eqn.~\\ref{eq:infoloss-7} into Eqn.~\\ref{eq:infoloss-6}, \nwe can optimize $\\mathcal{L}(\\bm{\\sigma})$ with Adam optimizer and obtain the optimal $\\bm{\\sigma}$ for computing the overall discarded information $H(G|Z)$. Furthermore, we can explain GFL model's capability by comparing discarded information of different models. Ideally, we expect the GFL model to encode valid node (or graph) embeddings as much as possible and therefore discard information as small as possible. \n\n\\subsection{Experimental Setup}\n\\label{sec:exp-set}\n\\textbf{Datasets}.\nWe use multiple graph datasets to conduct experiments: for the node classification task, we use ogbn-arxiv~\\citep{hu2020open}, Tissue-PPI~\\citep{hamilton2017inductive}, Fold-PPI~\\citep{zitnik2017predicting}, Cora~\\citep{sen2008collective}, and Citeseer~\\citep{sen2008collective}; for the graph classification task, we use datasets in ~\\citep{chauhan2020few}, i.e., Letter-High, Triangles, Reddit-12K, and Enzymes. The detailed information of datasets is illustrated in Appendix~\\ref{app: dataset}. \n\\\\\n\\textbf{Baseline Methods}. \nWe employ a variety of baseline methods for model comparison in two tasks. For few-shot node classification, we use node2vec~\\citep{grover2016node2vec}, DeepWalk~\\citep{perozzi2014deepwalk}, Meta-GNN~\\citep{zhou2019meta}, FS-GIN~\\citep{xu2018powerful}, FS-SGC~\\citep{wu2019simplifying}, No-Finetune~\\citep{triantafillou2019meta},\nFinetune~\\citep{triantafillou2019meta},\nKNN~~\\citep{triantafillou2019meta},  ProtoNet~\\citep{snell2017prototypical}, MAML~\\citep{finn2017model}, G-Meta~\\citep{huang2020graph}, and TENT~\\citep{wang2022task}. For few-shot graph classification, we utilize WL~\\citep{shervashidze2011weisfeiler}, Graphlet~\\citep{shervashidze2009efficient}, AWE~\\citep{ivanov2018anonymous}, Graph2Vec~\\citep{narayanan2017graph2vec}, Diffpool~\\citep{lee2019self}, CapsGNN~\\citep{xinyi2018capsule}, GIN~\\citep{xu2018powerful}, GIN-KNN~\\citep{xu2018powerful}, GSM-GCN~\\citep{chauhan2020few}, GSM-GAT~\\citep{chauhan2020few}, and AS-MAML~\\citep{ma2020adaptive}. Details of baselines are illustrated in Appendix~\\ref{app: implementation}. \n\\\\\n\\textbf{Experimental Settings}.\nIn CGFL, we use GCN~\\citep{kipf2016semi} as the GNN backbone for the node classification task, including two-layer graph convolution and one-layer FC. The graph classification task adds an extra average pooling operation as a readout layer. \nIn the pretraining phase, we pre-train GNN on the unlabeled graph dataset with contrastive learning. \nFor graph data augmentation, the node drop rate is 15\\%, the edge removing rate is 15\\%; and the feature masking rate is 20\\%. The mini-batch size is set to 2,048, and the learning rate is set to 0.05 with a decay factor = 0.9. Meanwhile, the $\\tau$ in exponential moving average is 0.999. We consider both inductive and transductive settings. For the inductive setting (\\textbf{CGFL-I}), we only use unlabeled data in the training set; for the transductive setting (\\textbf{CGFL-T}), we use unlabeled data in both training data and testing data. Additionally, for pre-trained models without knowledge distillation elevation, we refer to them as \\textbf{Teacher-I} and \\textbf{Teacher-T} for two settings, respectively. \nIn GFL phase, we employ MAML to fine-tune the model.\nWe implement CGFL by PyTorch and train it on NVIDIA V100 GPUs. \nThe code is in the supplementary material. \n\n\\\\\n\\textbf{Impact of Shot Number}.\nIn Figure~\\ref{fig:node-shot}, we show the performance of our model (CGFL-T) under different shot numbers (1 to 5) compared with some selected baselines. It is easy to see that CGFL achieves better accuracy across different shot numbers.\nThe result demonstrates our model's performance is robust for node classification. Note that we only show results on two datasets and the results on the other datasets (i.e., ogbn-arxiv and Tissue-PPI) are shown in Appendix~\\ref{sec:app:fsn}. The same goes for the following analyses. \n\\\\\n\\textbf{Impact of Training Label Rate}.\nWe further evaluate CGFL's performance under different training label rates (10\\%, 20\\%, 30\\%, 40\\%, 50\\%, 100\\%) compared with baseline methods for 3-shot node classification, as shown in Figure~\\ref{fig:node-lr}. It is easy to see that (1) CGFL has better results across different label rates; (2) CGFL achieves larger improvement over baseline models when label rate becomes lower (e.g., 10\\%), showing contrastive pre-training and self-distillation of CGFL lead to more significant improvement for label sparsity case. \n\\\\\n\\textbf{Impact of Data Augmentation}.\nAs a key step for contrastive learning in CGFL, graph augmentation plays an important role in affecting model performance. Here we conduct experiments to evaluate the model performance with different augmentation strategies. We consider three graph augmentations - node dropping (\\textbf{ND}), feature masking (\\textbf{FM}), edge removing (\\textbf{ER}), and their combinations for CGFL phase and report their performances in Figure~\\ref{fig:node-da}. It is easy to find that the combination of three augmentation strategies works better than a single augmentation or the combination of two augmentations. It demonstrates that various graph augmentations are able to generate sufficient contrastive pairs for a better model. \n\\\\\n\n\\vspace{-0.15in}\n\\subsection{Few-Shot Graph Classification}\n\\vspace{-0.1in}\n\\textbf{Overall Performance}.\nThe results of all models for 5/10-shot graph classification\nare reported in Table~\\ref{tab:graph-cls-1}. Similar to the findings obtained from Table~\\ref{tab:node-cls-1}, according to this table: (1) CGFL outperforms all baseline models, showing its superiority for few-shot graph classification;\n(2) The improvement of CGFL-I over baseline models ranges from 0.95\\% to 38.36\\% (5-shot) and from 0.8\\% to 34.33\\% (10-shot). Meanwhile, this value of CGFL-T ranges from 5.65\\% to 46.21\\% (5-shot) and from 2.18\\% to 41.51\\% (10-shot). This demonstrates the effect of contrastive pre-training and self-distillation in learning rich graph embedding from unlabeled data; (3) CGFL-T (or Teacher-T) outperforms CGFL-I (or Teacher-I), showing that unlabeled data in testing set improves model's generalization; {(4) CGFL is better than teacher model as contrastive distillation step further elevates the model.}\n\n\\vspace{-0.1in}\n\\section{Conclusion}\n\\vspace{-0.1in}\n\\label{sec:conclusion}\nIn this paper, to tackle the limitations of existing GFL models, we propose a general and effective framework named CGFL - \\textbf{C}ontrastive \\textbf{G}raph \\textbf{F}ew-shot \\textbf{L}earning. CGFL leverages a self-distilled contrastive learning procedure to boost GFL, in which the GNN encoder is pre-trained with contrastive learning and further elevated with knowledge distillation in a self-supervised manner. Additionally, we introduce an information-based method to compute the amount of graph information discarded by the GFL model. Extensive experiments on multiple \ngraph datasets demonstrate that CGFL outperforms state-of-the-art baseline methods for both node classification and graph classification tasks in the few-shot scenario. The discarded information value further shows the superiority of CGFL in learning node (or graph) embeddings. \n\n\\clearpage\n\\appendix\n\\maketitle\n\\section{Dataset Details}\n\\label{app: dataset}\nFor few-shot node classification, we use five different datasets: ogbn-arxiv~\\cite{hu2020open}, Tissue-PPI~\\cite{hamilton2017inductive}, Fold-PPI~\\cite{zitnik2017predicting}, Cora~\\cite{sen2008collective}, and Citeseer~\\cite{sen2008collective} to perform extensive empirical evaluations of different models. These datasets  vary from citation network to biochemical graph. The statistics of datasets are reported in Table~\\ref{tab:node-data-stat}. ", "table_source": "\\begin{table}[htbp]\n\\vspace{-0.1in}\n\\caption{Statistics of datasets used in the node classification task.}\n\\label{tab:node-data-stat}\n\\vspace{1mm}\n\\small\n\\renewcommand\\arraystretch{1}\n\\centering\n\\resizebox{0.67\\textwidth}{!}{\\begin{tabular*}{0.7\\textwidth}{c| @{\\extracolsep{\\fill}} |c|c|c|c|c}\n\\toprule\n{Dataset} &  \\# Graph & \\# Node & \\#  Edge &  \\# Feat. &  \\# Label  \\\\\n\\midrule\nogbn-arxiv       &1 &169,343 &1,166,243 &128 &40  \\\\ \nTissue-PPI       &24 &51,194 &1,350,412 &50 &10 \\\\ \nFold-PPI       &144 &274,606 &3,666,563 &512 &29 \\\\ \nCora       &1 &2,708 &10,556 &1,433 &7 \\\\ \nCiteseer       &1 &3,327 &9,228 &3,703 &6 \\\\ \n\\bottomrule\n\\end{tabular*}\n}\n\\end{table}", "cell_list_gold": [{"value": "1", "char_index": [402, 403], "type": "Data Stat.", "dataset": "ogbn-arxiv", "attribute name": ["number of Graph", "# Graph"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "169,343", "char_index": [405, 412], "type": "Data Stat.", "dataset": "ogbn-arxiv", "attribute name": ["number of Node", "# Node"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "1,166,243", "char_index": [414, 423], "type": "Data Stat.", "dataset": "ogbn-arxiv", "attribute name": ["number of Edge", "# Edge"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "128", "char_index": [425, 428], "type": "Data Stat.", "dataset": "ogbn-arxiv", "attribute name": ["number of Feat.", "# Feat."], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "40", "char_index": [430, 432], "type": "Data Stat.", "dataset": "ogbn-arxiv", "attribute name": ["number of Label", "# Label"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "24", "char_index": [456, 458], "type": "Data Stat.", "dataset": "Tissue-PPI", "attribute name": ["number of Graph", "# Graph"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "51,194", "char_index": [460, 466], "type": "Data Stat.", "dataset": "Tissue-PPI", "attribute name": ["number of Node", "# Node"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "1,350,412", "char_index": [468, 477], "type": "Data Stat.", "dataset": "Tissue-PPI", "attribute name": ["number of Edge", "# Edge"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "50", "char_index": [479, 481], "type": "Data Stat.", "dataset": "Tissue-PPI", "attribute name": ["number of Feat.", "# Feat."], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "10", "char_index": [483, 485], "type": "Data Stat.", "dataset": "Tissue-PPI", "attribute name": ["number of Label", "# Label"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "144", "char_index": [506, 509], "type": "Data Stat.", "dataset": "Fold-PPI", "attribute name": ["number of Graph", "# Graph"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "274,606", "char_index": [511, 518], "type": "Data Stat.", "dataset": "Fold-PPI", "attribute name": ["number of Node", "# Node"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "3,666,563", "char_index": [520, 529], "type": "Data Stat.", "dataset": "Fold-PPI", "attribute name": ["number of Edge", "# Edge"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "512", "char_index": [531, 534], "type": "Data Stat.", "dataset": "Fold-PPI", "attribute name": ["number of Feat.", "# Feat."], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "29", "char_index": [536, 538], "type": "Data Stat.", "dataset": "Fold-PPI", "attribute name": ["number of Label", "# Label"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "1", "char_index": [555, 556], "type": "Data Stat.", "dataset": "Cora", "attribute name": ["number of Graph", "# Graph"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "2,708", "char_index": [558, 563], "type": "Data Stat.", "dataset": "Cora", "attribute name": ["number of Node", "# Node"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "10,556", "char_index": [565, 571], "type": "Data Stat.", "dataset": "Cora", "attribute name": ["number of Edge", "# Edge"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "1,433", "char_index": [573, 578], "type": "Data Stat.", "dataset": "Cora", "attribute name": ["number of Feat.", "# Feat."], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "7", "char_index": [580, 581], "type": "Data Stat.", "dataset": "Cora", "attribute name": ["number of Label", "# Label"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "1", "char_index": [602, 603], "type": "Data Stat.", "dataset": "Citeseer", "attribute name": ["number of Graph", "# Graph"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "3,327", "char_index": [605, 610], "type": "Data Stat.", "dataset": "Citeseer", "attribute name": ["number of Node", "# Node"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "9,228", "char_index": [612, 617], "type": "Data Stat.", "dataset": "Citeseer", "attribute name": ["number of Edge", "# Edge"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "3,703", "char_index": [619, 624], "type": "Data Stat.", "dataset": "Citeseer", "attribute name": ["number of Feat.", "# Feat."], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "6", "char_index": [626, 627], "type": "Data Stat.", "dataset": "Citeseer", "attribute name": ["number of Label", "# Label"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}]}, "2210.00084v1_table3": {"table_code": "\\begin{table}[htbp]\n\\vspace{-0.1in}\n\\caption{Statistics of datasets used in the graph classification task.}\n\\label{tab:graph-data-stat}\n\\vspace{1mm}\n\\small\n\\renewcommand\\arraystretch{1}\n\\centering\n\\resizebox{0.6\\textwidth}{!}{\\begin{tabular*}{0.6\\textwidth}{c| @{\\extracolsep{\\fill}} |c|c|c|c|c}\n\\toprule\n\\multirow{2}{*}{Dataset} & \\multicolumn{2}{c|}{Class \\#} &\\multicolumn{3}{c}{Graph \\#}  \\\\\n\\cmidrule{2-3} \\cmidrule{4-6} &Train &Test & Training &Validation &Test  \\\\\n\\midrule\nLetter-High       &11 &4 &1,330 &320 &600 \\\\ \nTriangles       &7 &3 &1,126 &271 &603 \\\\ \nReddit-12K       &7 &4 &566 &141 &404 \\\\ \nEnzymes       &4 &2 &320 &80 &200 \\\\ \n\\bottomrule\n\\end{tabular*}\n}\n\\end{table}", "table_label": "{tab:graph-data-stat}", "table_numeric_cells": [["11", "11", 500, 502, 500, 502], ["4", "4", 504, 505, 504, 505], ["1,330", "1,330", 507, 512, 507, 512], ["320", "320", 514, 517, 514, 517], ["600", "600", 519, 522, 519, 522], ["7", "7", 544, 545, 544, 545], ["3", "3", 547, 548, 547, 548], ["1,126", "1,126", 550, 555, 550, 555], ["271", "271", 557, 560, 557, 560], ["603", "603", 562, 565, 562, 565], ["7", "7", 588, 589, 588, 589], ["4", "4", 591, 592, 591, 592], ["566", "566", 594, 597, 594, 597], ["141", "141", 599, 602, 599, 602], ["404", "404", 604, 607, 604, 607], ["4", "4", 627, 628, 627, 628], ["2", "2", 630, 631, 630, 631], ["320", "320", 633, 636, 633, 636], ["80", "80", 638, 640, 638, 640], ["200", "200", 642, 645, 642, 645]], "text_chunk_selected": "\\textbf{GFL Setting and Problem}. Let $\\mathcal{C}_{base}$ and $\\mathcal{C}_{novel}$ denote the base classes set and novel (new) classes set in training data $\\mathcal{T}_{train}$ and testing data $\\mathcal{T}_{test}$, respectively. \nSimilar to the general meta-learning problem~\\cite{finn2017model}, the purpose of graph few-shot learning (GFL) is to train a GNN encoder $f_\\theta(\\cdot)$ over $\\mathcal{C}_{base}$, such that the trained GNN encoder can be quickly adapted to $\\mathcal{C}_{novel}$ with few labels per class. Note that there is no overlapping between base classes and novel classes, i.e., $\\mathcal{C}_{base} \\cap \\mathcal{C}_{novel} = \\emptyset$. \nIn $K$-shot setting, during the meta-training phase,\na batch of classes (tasks) is randomly sampled from $\\mathcal{C}_{base}$, where $K$ labeled instances per class are sampled to form the support set $\\mathcal{S}$ for model training and the remaining instances are taken as the query set $\\mathcal{Q}$ for model evaluation. After sufficient training, the model is further transferred to the meta-testing phase to conduct $N$-way classification over $\\mathcal{C}_{novel}$ ($N$ is the number of novel classes), where each class is only with $K$ labeled instances. GFL applies to different graph mining problems, depending on the class meaning. Each class corresponds to a node label for the node classification problem or corresponds to a graph label for the graph classification problem. In this work, we will study both node classification and graph classification problems under few-shot setting, which are formally defined as follows:\n\nwhere $C = \\frac{1}{2}\\log (2\\pi e)$. By taking Eqn.~\\ref{eq:infoloss-7} into Eqn.~\\ref{eq:infoloss-6}, \nwe can optimize $\\mathcal{L}(\\bm{\\sigma})$ with Adam optimizer and obtain the optimal $\\bm{\\sigma}$ for computing the overall discarded information $H(G|Z)$. Furthermore, we can explain GFL model's capability by comparing discarded information of different models. Ideally, we expect the GFL model to encode valid node (or graph) embeddings as much as possible and therefore discard information as small as possible. \n\n\\subsection{Experimental Setup}\n\\label{sec:exp-set}\n\\textbf{Datasets}.\nWe use multiple graph datasets to conduct experiments: for the node classification task, we use ogbn-arxiv~\\citep{hu2020open}, Tissue-PPI~\\citep{hamilton2017inductive}, Fold-PPI~\\citep{zitnik2017predicting}, Cora~\\citep{sen2008collective}, and Citeseer~\\citep{sen2008collective}; for the graph classification task, we use datasets in ~\\citep{chauhan2020few}, i.e., Letter-High, Triangles, Reddit-12K, and Enzymes. The detailed information of datasets is illustrated in Appendix~\\ref{app: dataset}. \n\\\\\n\\textbf{Baseline Methods}. \nWe employ a variety of baseline methods for model comparison in two tasks. For few-shot node classification, we use node2vec~\\citep{grover2016node2vec}, DeepWalk~\\citep{perozzi2014deepwalk}, Meta-GNN~\\citep{zhou2019meta}, FS-GIN~\\citep{xu2018powerful}, FS-SGC~\\citep{wu2019simplifying}, No-Finetune~\\citep{triantafillou2019meta},\nFinetune~\\citep{triantafillou2019meta},\nKNN~~\\citep{triantafillou2019meta},  ProtoNet~\\citep{snell2017prototypical}, MAML~\\citep{finn2017model}, G-Meta~\\citep{huang2020graph}, and TENT~\\citep{wang2022task}. For few-shot graph classification, we utilize WL~\\citep{shervashidze2011weisfeiler}, Graphlet~\\citep{shervashidze2009efficient}, AWE~\\citep{ivanov2018anonymous}, Graph2Vec~\\citep{narayanan2017graph2vec}, Diffpool~\\citep{lee2019self}, CapsGNN~\\citep{xinyi2018capsule}, GIN~\\citep{xu2018powerful}, GIN-KNN~\\citep{xu2018powerful}, GSM-GCN~\\citep{chauhan2020few}, GSM-GAT~\\citep{chauhan2020few}, and AS-MAML~\\citep{ma2020adaptive}. Details of baselines are illustrated in Appendix~\\ref{app: implementation}. \n\\\\\n\\textbf{Experimental Settings}.\nIn CGFL, we use GCN~\\citep{kipf2016semi} as the GNN backbone for the node classification task, including two-layer graph convolution and one-layer FC. The graph classification task adds an extra average pooling operation as a readout layer. \nIn the pretraining phase, we pre-train GNN on the unlabeled graph dataset with contrastive learning. \nFor graph data augmentation, the node drop rate is 15\\%, the edge removing rate is 15\\%; and the feature masking rate is 20\\%. The mini-batch size is set to 2,048, and the learning rate is set to 0.05 with a decay factor = 0.9. Meanwhile, the $\\tau$ in exponential moving average is 0.999. We consider both inductive and transductive settings. For the inductive setting (\\textbf{CGFL-I}), we only use unlabeled data in the training set; for the transductive setting (\\textbf{CGFL-T}), we use unlabeled data in both training data and testing data. Additionally, for pre-trained models without knowledge distillation elevation, we refer to them as \\textbf{Teacher-I} and \\textbf{Teacher-T} for two settings, respectively. \nIn GFL phase, we employ MAML to fine-tune the model.\nWe implement CGFL by PyTorch and train it on NVIDIA V100 GPUs. \nThe code is in the supplementary material. \n\n\\vspace{-0.15in}\n\\subsection{Few-Shot Graph Classification}\n\\vspace{-0.1in}\n\\textbf{Overall Performance}.\nThe results of all models for 5/10-shot graph classification\nare reported in Table~\\ref{tab:graph-cls-1}. Similar to the findings obtained from Table~\\ref{tab:node-cls-1}, according to this table: (1) CGFL outperforms all baseline models, showing its superiority for few-shot graph classification;\n(2) The improvement of CGFL-I over baseline models ranges from 0.95\\% to 38.36\\% (5-shot) and from 0.8\\% to 34.33\\% (10-shot). Meanwhile, this value of CGFL-T ranges from 5.65\\% to 46.21\\% (5-shot) and from 2.18\\% to 41.51\\% (10-shot). This demonstrates the effect of contrastive pre-training and self-distillation in learning rich graph embedding from unlabeled data; (3) CGFL-T (or Teacher-T) outperforms CGFL-I (or Teacher-I), showing that unlabeled data in testing set improves model's generalization; {(4) CGFL is better than teacher model as contrastive distillation step further elevates the model.}\n\n\\textbf{Impact of Shot Number}.\nIn Figure~\\ref{fig:graph-shot}, we report our model's result under different shot numbers (5, 10, 15, 20) compared with some selected baselines. Similar to Figure~\\ref{fig:node-shot}, CGFL consistently outperforms baseline methods across different shot numbers, showing the robustness of CGFL. Note that we only show results on two datasets (e.g., Letter-High and Triangles), and the results on the other datasets are shown in Appendix~\\ref{sec:app:fsg}.\nThe same goes for the following analyses.  \n\\\\\n\\vspace{-0.1in}\n\\\\\n\\textbf{Impact of Training Label Rate}.\nIn Figure~\\ref{fig:graph-lr}, we report our model's performance under different training label rates compared with baseline models for 5-shot graph classification. The phenomenon is similar to the node task: \nour model achieves better accuracy across different label rates; the performance gaps between CGFL and baseline methods are larger when the label rate is lower. Again, this shows the significance of CGFL when labeled data is limited.  \n\n\\vspace{-0.1in}\n\\section{Conclusion}\n\\vspace{-0.1in}\n\\label{sec:conclusion}\nIn this paper, to tackle the limitations of existing GFL models, we propose a general and effective framework named CGFL - \\textbf{C}ontrastive \\textbf{G}raph \\textbf{F}ew-shot \\textbf{L}earning. CGFL leverages a self-distilled contrastive learning procedure to boost GFL, in which the GNN encoder is pre-trained with contrastive learning and further elevated with knowledge distillation in a self-supervised manner. Additionally, we introduce an information-based method to compute the amount of graph information discarded by the GFL model. Extensive experiments on multiple \ngraph datasets demonstrate that CGFL outperforms state-of-the-art baseline methods for both node classification and graph classification tasks in the few-shot scenario. The discarded information value further shows the superiority of CGFL in learning node (or graph) embeddings. \n\n\\clearpage\n\\appendix\n\\maketitle\n\\section{Dataset Details}\n\\label{app: dataset}\nFor few-shot node classification, we use five different datasets: ogbn-arxiv~\\cite{hu2020open}, Tissue-PPI~\\cite{hamilton2017inductive}, Fold-PPI~\\cite{zitnik2017predicting}, Cora~\\cite{sen2008collective}, and Citeseer~\\cite{sen2008collective} to perform extensive empirical evaluations of different models. These datasets  vary from citation network to biochemical graph. The statistics of datasets are reported in Table~\\ref{tab:node-data-stat}. \n\nFor few-shot graph classification, we use four different datasets~\\cite{chauhan2020few}: Reddit-12K, ENZYMES, Letter-High, and TRIANGLES, to perform extensive empirical evaluations of different models. These datasets vary from small average graph size (e.g., Letter-High) to large graph size (e.g., Reddit-12K). \nThe statistics of datasets are reported in Table~\\ref{tab:graph-data-stat}. ", "table_source": "\\begin{table}[htbp]\n\\vspace{-0.1in}\n\\caption{Statistics of datasets used in the graph classification task.}\n\\label{tab:graph-data-stat}\n\\vspace{1mm}\n\\small\n\\renewcommand\\arraystretch{1}\n\\centering\n\\resizebox{0.6\\textwidth}{!}{\\begin{tabular*}{0.6\\textwidth}{c| @{\\extracolsep{\\fill}} |c|c|c|c|c}\n\\toprule\n\\multirow{2}{*}{Dataset} & \\multicolumn{2}{c|}{Class \\#} &\\multicolumn{3}{c}{Graph \\#}  \\\\\n\\cmidrule{2-3} \\cmidrule{4-6} &Train &Test & Training &Validation &Test  \\\\\n\\midrule\nLetter-High       &11 &4 &1,330 &320 &600 \\\\ \nTriangles       &7 &3 &1,126 &271 &603 \\\\ \nReddit-12K       &7 &4 &566 &141 &404 \\\\ \nEnzymes       &4 &2 &320 &80 &200 \\\\ \n\\bottomrule\n\\end{tabular*}\n}\n\\end{table}", "cell_list_gold": [{"value": "11", "char_index": [500, 502], "type": "Data Stat.", "dataset": "Letter-High", "attribute name": ["number of Class", "Class #"], "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "4", "char_index": [504, 505], "type": "Data Stat.", "dataset": "Letter-High", "attribute name": ["number of Class", "Class #"], "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "1,330", "char_index": [507, 512], "type": "Data Stat.", "dataset": "Letter-High", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Training", "dataset features": {"xx": "yy"}}, {"value": "320", "char_index": [514, 517], "type": "Data Stat.", "dataset": "Letter-High", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Validation", "dataset features": {"xx": "yy"}}, {"value": "600", "char_index": [519, 522], "type": "Data Stat.", "dataset": "Letter-High", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "7", "char_index": [544, 545], "type": "Data Stat.", "dataset": "Triangles", "attribute name": ["number of Class", "Class #"], "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "3", "char_index": [547, 548], "type": "Data Stat.", "dataset": "Triangles", "attribute name": ["number of Class", "Class #"], "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "1,126", "char_index": [550, 555], "type": "Data Stat.", "dataset": "Triangles", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Training", "dataset features": {"xx": "yy"}}, {"value": "271", "char_index": [557, 560], "type": "Data Stat.", "dataset": "Triangles", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Validation", "dataset features": {"xx": "yy"}}, {"value": "603", "char_index": [562, 565], "type": "Data Stat.", "dataset": "Triangles", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "7", "char_index": [588, 589], "type": "Data Stat.", "dataset": "Reddit-12K", "attribute name": ["number of Class", "Class #"], "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "4", "char_index": [591, 592], "type": "Data Stat.", "dataset": "Reddit-12K", "attribute name": ["number of Class", "Class #"], "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "566", "char_index": [594, 597], "type": "Data Stat.", "dataset": "Reddit-12K", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Training", "dataset features": {"xx": "yy"}}, {"value": "141", "char_index": [599, 602], "type": "Data Stat.", "dataset": "Reddit-12K", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Validation", "dataset features": {"xx": "yy"}}, {"value": "404", "char_index": [604, 607], "type": "Data Stat.", "dataset": "Reddit-12K", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "4", "char_index": [627, 628], "type": "Data Stat.", "dataset": "Enzymes", "attribute name": ["number of Class", "Class #"], "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "2", "char_index": [630, 631], "type": "Data Stat.", "dataset": "Enzymes", "attribute name": ["number of Class", "Class #"], "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "320", "char_index": [633, 636], "type": "Data Stat.", "dataset": "Enzymes", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Training", "dataset features": {"xx": "yy"}}, {"value": "80", "char_index": [638, 640], "type": "Data Stat.", "dataset": "Enzymes", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Validation", "dataset features": {"xx": "yy"}}, {"value": "200", "char_index": [642, 645], "type": "Data Stat.", "dataset": "Enzymes", "attribute name": ["number of Graph", "Graph #"], "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}]}, "2210.00084v1_table5": {"table_code": "\\begin{table}[!htb]\n\\caption{{Discarded information for graph classification.}}\n\\vspace{1mm}\n\\small\n\\centering\n\\resizebox{0.5\\textwidth}{!}{\\begin{tabular*}{0.5\\textwidth}{c|c|c|c}\n\\toprule\n\\multirow{2}{*}{Method} & \\multirow{2}{*}{Layer} &\\multicolumn{2}{c}{Discarded Information}  \\\\ \n\\cmidrule{3-4} & &Reddit-12K &Enzymes \\\\\n\\midrule\n       &GNN-1   &5455.46 &2455.54 \\\\\nAS-MAML &GNN-2 &5024.15  &2243.25 \\\\\n       &FC     &4937.99 &2125.91\\\\\n       \\midrule\n       &GNN-1  &5401.30  &2273.80 \\\\ \nGSM-GAT &GNN-2 &5183.87  &2128.04 \\\\ \n       &FC     &4936.76  &1958.18\\\\ \n\\midrule\n                &GNN-1  &5323.04  &1864.98 \\\\ \nCGFL-I         &GNN-2  &5098.54  &1788.97 \\\\ \n                &FC     &4802.03  &1759.87 \\\\  \n\\midrule\n                 &GNN-1 &\\textbf{4823.91}  &\\textbf{1787.98} \\\\ \nCGFL-T          &GNN-2 &\\textbf{4546.23}  &\\textbf{1698.35} \\\\ \n                   &FC  &\\textbf{4329.39}  &\\textbf{1585.33} \\\\ \n\\bottomrule\n\\end{tabular*}\n}\n\\label{tab:info-loss-graph-cls-app}\n\\end{table}", "table_label": "{tab:info-loss-graph-cls-app}", "table_numeric_cells": [["5455.46", "5455.46", 354, 361, 354, 361], ["2455.54", "2455.54", 363, 370, 363, 370], ["5024.15", "5024.15", 390, 397, 390, 397], ["2243.25", "2243.25", 400, 407, 400, 407], ["4937.99", "4937.99", 427, 434, 427, 434], ["2125.91", "2125.91", 436, 443, 436, 443], ["5401.30", "5401.30", 478, 485, 478, 485], ["2273.80", "2273.80", 488, 495, 488, 495], ["5183.87", "5183.87", 516, 523, 516, 523], ["2128.04", "2128.04", 526, 533, 526, 533], ["4936.76", "4936.76", 554, 561, 554, 561], ["1958.18", "1958.18", 564, 571, 564, 571], ["5323.04", "5323.04", 609, 616, 609, 616], ["1864.98", "1864.98", 619, 626, 619, 626], ["5098.54", "5098.54", 655, 662, 655, 662], ["1788.97", "1788.97", 665, 672, 665, 672], ["4802.03", "4802.03", 702, 709, 702, 709], ["1759.87", "1759.87", 712, 719, 712, 719], ["4823.91", "\\textbf{4823.91}", 767, 774, 759, 775], ["1787.98", "\\textbf{1787.98}", 786, 793, 778, 794], ["4546.23", "\\textbf{4546.23}", 831, 838, 823, 839], ["1698.35", "\\textbf{1698.35}", 850, 857, 842, 858], ["4329.39", "\\textbf{4329.39}", 896, 903, 888, 904], ["1585.33", "\\textbf{1585.33}", 915, 922, 907, 923]], "text_chunk_selected": "\\begin{equation}\n    \\mathcal{L}_{\\theta,\\xi} = {\\lVert {z_{\\theta} - h_{\\xi}}\\rVert}_{2}^{2}=2-2\\cdot \\frac{{z_{\\theta}, h_{\\xi}}}{{\\lVert {z_{\\theta} }\\rVert}_{2} \\cdot {\\lVert {h_{\\xi}}\\rVert}_{2}}.\n\\label{eq:cosine-loss}\n\\end{equation}\n\nwhere $C = \\frac{1}{2}\\log (2\\pi e)$. By taking Eqn.~\\ref{eq:infoloss-7} into Eqn.~\\ref{eq:infoloss-6}, \nwe can optimize $\\mathcal{L}(\\bm{\\sigma})$ with Adam optimizer and obtain the optimal $\\bm{\\sigma}$ for computing the overall discarded information $H(G|Z)$. Furthermore, we can explain GFL model's capability by comparing discarded information of different models. Ideally, we expect the GFL model to encode valid node (or graph) embeddings as much as possible and therefore discard information as small as possible. \n\n\\subsection{Experimental Setup}\n\\label{sec:exp-set}\n\\textbf{Datasets}.\nWe use multiple graph datasets to conduct experiments: for the node classification task, we use ogbn-arxiv~\\citep{hu2020open}, Tissue-PPI~\\citep{hamilton2017inductive}, Fold-PPI~\\citep{zitnik2017predicting}, Cora~\\citep{sen2008collective}, and Citeseer~\\citep{sen2008collective}; for the graph classification task, we use datasets in ~\\citep{chauhan2020few}, i.e., Letter-High, Triangles, Reddit-12K, and Enzymes. The detailed information of datasets is illustrated in Appendix~\\ref{app: dataset}. \n\\\\\n\\textbf{Baseline Methods}. \nWe employ a variety of baseline methods for model comparison in two tasks. For few-shot node classification, we use node2vec~\\citep{grover2016node2vec}, DeepWalk~\\citep{perozzi2014deepwalk}, Meta-GNN~\\citep{zhou2019meta}, FS-GIN~\\citep{xu2018powerful}, FS-SGC~\\citep{wu2019simplifying}, No-Finetune~\\citep{triantafillou2019meta},\nFinetune~\\citep{triantafillou2019meta},\nKNN~~\\citep{triantafillou2019meta},  ProtoNet~\\citep{snell2017prototypical}, MAML~\\citep{finn2017model}, G-Meta~\\citep{huang2020graph}, and TENT~\\citep{wang2022task}. For few-shot graph classification, we utilize WL~\\citep{shervashidze2011weisfeiler}, Graphlet~\\citep{shervashidze2009efficient}, AWE~\\citep{ivanov2018anonymous}, Graph2Vec~\\citep{narayanan2017graph2vec}, Diffpool~\\citep{lee2019self}, CapsGNN~\\citep{xinyi2018capsule}, GIN~\\citep{xu2018powerful}, GIN-KNN~\\citep{xu2018powerful}, GSM-GCN~\\citep{chauhan2020few}, GSM-GAT~\\citep{chauhan2020few}, and AS-MAML~\\citep{ma2020adaptive}. Details of baselines are illustrated in Appendix~\\ref{app: implementation}. \n\\\\\n\\textbf{Experimental Settings}.\nIn CGFL, we use GCN~\\citep{kipf2016semi} as the GNN backbone for the node classification task, including two-layer graph convolution and one-layer FC. The graph classification task adds an extra average pooling operation as a readout layer. \nIn the pretraining phase, we pre-train GNN on the unlabeled graph dataset with contrastive learning. \nFor graph data augmentation, the node drop rate is 15\\%, the edge removing rate is 15\\%; and the feature masking rate is 20\\%. The mini-batch size is set to 2,048, and the learning rate is set to 0.05 with a decay factor = 0.9. Meanwhile, the $\\tau$ in exponential moving average is 0.999. We consider both inductive and transductive settings. For the inductive setting (\\textbf{CGFL-I}), we only use unlabeled data in the training set; for the transductive setting (\\textbf{CGFL-T}), we use unlabeled data in both training data and testing data. Additionally, for pre-trained models without knowledge distillation elevation, we refer to them as \\textbf{Teacher-I} and \\textbf{Teacher-T} for two settings, respectively. \nIn GFL phase, we employ MAML to fine-tune the model.\nWe implement CGFL by PyTorch and train it on NVIDIA V100 GPUs. \nThe code is in the supplementary material. \n\n\\textbf{Loss Information Comparison}.\nIn Section~\\ref{sec:inforamtion-loss}, we propose to compute graph information discarded in the GFL model. Here, we compare results between CGFL and a selected baseline method (G-Meta) in Table~\\ref{tab:info-loss-node-cls}. From this table, the amount of discarded information in each layer of CGFL is smaller than baseline models. This may be because CGFL can learn more label-irrelevant information from unlabeled graph data, which somehow shows the superiority of CGFL in learning node embeddings for node classification.  \n\n\\textbf{Loss Information Comparison}.\nFinally, we compute the discarded graph information of our model and a selected baseline method (GSM-GAT), as shown in Table~\\ref{tab:info-loss-graph-cls}.\nObviously, the amount of information discarded in CGFL is less than baseline models. It somehow shows the superiority of CGFL in learning graph embeddings for graph classification.\n\n\\vspace{-0.1in}\n\\section{Conclusion}\n\\vspace{-0.1in}\n\\label{sec:conclusion}\nIn this paper, to tackle the limitations of existing GFL models, we propose a general and effective framework named CGFL - \\textbf{C}ontrastive \\textbf{G}raph \\textbf{F}ew-shot \\textbf{L}earning. CGFL leverages a self-distilled contrastive learning procedure to boost GFL, in which the GNN encoder is pre-trained with contrastive learning and further elevated with knowledge distillation in a self-supervised manner. Additionally, we introduce an information-based method to compute the amount of graph information discarded by the GFL model. Extensive experiments on multiple \ngraph datasets demonstrate that CGFL outperforms state-of-the-art baseline methods for both node classification and graph classification tasks in the few-shot scenario. The discarded information value further shows the superiority of CGFL in learning node (or graph) embeddings. \n\n\\textbf{Discarded Information Comparison}. The discarded graph information of different models are shown in Table~\\ref{tab:info-loss-node-cls-app}.\nObviously, the amount of information discarded in CGFL is less than baseline models. \n\n\\textbf{Discarded Information Comparison}. The discarded graph information of different methods are reported in Table~\\ref{tab:info-loss-graph-cls-app}.\nAccording to this table, the amount of information discarded in CGFL is less than baseline models. ", "table_source": "\\begin{table}[!htb]\n\\caption{{Discarded information for graph classification.}}\n\\vspace{1mm}\n\\small\n\\centering\n\\resizebox{0.5\\textwidth}{!}{\\begin{tabular*}{0.5\\textwidth}{c|c|c|c}\n\\toprule\n\\multirow{2}{*}{Method} & \\multirow{2}{*}{Layer} &\\multicolumn{2}{c}{Discarded Information}  \\\\ \n\\cmidrule{3-4} & &Reddit-12K &Enzymes \\\\\n\\midrule\n       &GNN-1   &5455.46 &2455.54 \\\\\nAS-MAML &GNN-2 &5024.15  &2243.25 \\\\\n       &FC     &4937.99 &2125.91\\\\\n       \\midrule\n       &GNN-1  &5401.30  &2273.80 \\\\ \nGSM-GAT &GNN-2 &5183.87  &2128.04 \\\\ \n       &FC     &4936.76  &1958.18\\\\ \n\\midrule\n                &GNN-1  &5323.04  &1864.98 \\\\ \nCGFL-I         &GNN-2  &5098.54  &1788.97 \\\\ \n                &FC     &4802.03  &1759.87 \\\\  \n\\midrule\n                 &GNN-1 &\\textbf{4823.91}  &\\textbf{1787.98} \\\\ \nCGFL-T          &GNN-2 &\\textbf{4546.23}  &\\textbf{1698.35} \\\\ \n                   &FC  &\\textbf{4329.39}  &\\textbf{1585.33} \\\\ \n\\bottomrule\n\\end{tabular*}\n}\n\\label{tab:info-loss-graph-cls-app}\n\\end{table}", "cell_list_gold": [{"value": "5455.46", "char_index": [354, 361], "type": "Other"}, {"value": "2455.54", "char_index": [363, 370], "type": "Other"}, {"value": "5024.15", "char_index": [390, 397], "type": "Other"}, {"value": "2243.25", "char_index": [400, 407], "type": "Other"}, {"value": "4937.99", "char_index": [427, 434], "type": "Other"}, {"value": "2125.91", "char_index": [436, 443], "type": "Other"}, {"value": "5401.30", "char_index": [478, 485], "type": "Other"}, {"value": "2273.80", "char_index": [488, 495], "type": "Other"}, {"value": "5183.87", "char_index": [516, 523], "type": "Other"}, {"value": "2128.04", "char_index": [526, 533], "type": "Other"}, {"value": "4936.76", "char_index": [554, 561], "type": "Other"}, {"value": "1958.18", "char_index": [564, 571], "type": "Other"}, {"value": "5323.04", "char_index": [609, 616], "type": "Other"}, {"value": "1864.98", "char_index": [619, 626], "type": "Other"}, {"value": "5098.54", "char_index": [655, 662], "type": "Other"}, {"value": "1788.97", "char_index": [665, 672], "type": "Other"}, {"value": "4802.03", "char_index": [702, 709], "type": "Other"}, {"value": "1759.87", "char_index": [712, 719], "type": "Other"}, {"value": "4823.91", "char_index": [767, 774], "type": "Other"}, {"value": "1787.98", "char_index": [786, 793], "type": "Other"}, {"value": "4546.23", "char_index": [831, 838], "type": "Other"}, {"value": "1698.35", "char_index": [850, 857], "type": "Other"}, {"value": "4329.39", "char_index": [896, 903], "type": "Other"}, {"value": "1585.33", "char_index": [915, 922], "type": "Other"}]}, "2210.00084v1_table6": {"table_code": "\\begin{table}[t]\n\\caption{{Comparison with finetuning by MAML++ and ALFA.}}\n\\label{tab:node-alfa}\n\\vspace{1mm}\n\\small\n\\renewcommand\\arraystretch{1}\n\\centering\n\\resizebox{0.4\\textwidth}{!}{\\begin{tabular*}{0.45\\textwidth}{c| @{\\extracolsep{\\fill}} |c|c}\n\\toprule\n{Model} &  3-shot & 5-shot \\\\\n\\midrule\nMAML++       &55.9$\\pm$2.1 &59.4$\\pm$3.2 \\\\\nALFA       &54.8$\\pm$3.6 &58.8$\\pm$3.7 \\\\\nOurs (with MAML)       &55.2$\\pm$2.5 &58.7$\\pm$2.7  \\\\\n\\bottomrule\n\\end{tabular*}\n}\n\\end{table}", "table_label": "{tab:node-alfa}", "table_numeric_cells": [["55.9", "55.9$\\pm$2.1", 315, 319, 315, 327], ["59.4", "59.4$\\pm$3.2", 329, 333, 329, 341], ["54.8", "54.8$\\pm$3.6", 357, 361, 357, 369], ["58.8", "58.8$\\pm$3.7", 371, 375, 371, 383], ["55.2", "55.2$\\pm$2.5", 411, 415, 411, 423], ["58.7", "58.7$\\pm$2.7", 425, 429, 425, 437]], "text_chunk_selected": "\\textbf{GFL Setting and Problem}. Let $\\mathcal{C}_{base}$ and $\\mathcal{C}_{novel}$ denote the base classes set and novel (new) classes set in training data $\\mathcal{T}_{train}$ and testing data $\\mathcal{T}_{test}$, respectively. \nSimilar to the general meta-learning problem~\\cite{finn2017model}, the purpose of graph few-shot learning (GFL) is to train a GNN encoder $f_\\theta(\\cdot)$ over $\\mathcal{C}_{base}$, such that the trained GNN encoder can be quickly adapted to $\\mathcal{C}_{novel}$ with few labels per class. Note that there is no overlapping between base classes and novel classes, i.e., $\\mathcal{C}_{base} \\cap \\mathcal{C}_{novel} = \\emptyset$. \nIn $K$-shot setting, during the meta-training phase,\na batch of classes (tasks) is randomly sampled from $\\mathcal{C}_{base}$, where $K$ labeled instances per class are sampled to form the support set $\\mathcal{S}$ for model training and the remaining instances are taken as the query set $\\mathcal{Q}$ for model evaluation. After sufficient training, the model is further transferred to the meta-testing phase to conduct $N$-way classification over $\\mathcal{C}_{novel}$ ($N$ is the number of novel classes), where each class is only with $K$ labeled instances. GFL applies to different graph mining problems, depending on the class meaning. Each class corresponds to a node label for the node classification problem or corresponds to a graph label for the graph classification problem. In this work, we will study both node classification and graph classification problems under few-shot setting, which are formally defined as follows:\n\nwhere $\\tau \\in [0, 1]$ is the decay rate. Note that the target GNN stops the backpropagation from $\\mathcal{L}_{\\theta,\\xi}$, and it is only updated by EMA.\n\\textbf{Contrastive Distillation}.\nWith the pre-trained GNN $f_\\theta(\\cdot)$ obtained in the previous step, we introduce a self-distillation step to elevate $f_\\theta(\\cdot)$. This is inspired by the Born-again strategy~\\cite{furlanello2018born}, which implies a well-trained teacher can boost a random initialized identical student. The distillation step adopts a similar contrastive framework as the previous step, as shown in Figure~\\ref{fig:ssl}(a). Specifically, we load the pre-trained GNN $f_\\theta(\\cdot)$ and take it as the teacher model $f_{\\theta_{t}}(\\cdot)$. The teacher model is frozen and applied to distill a student model $f_{\\theta_{s}}(\\cdot)$.\nTwo augmented views ($G^{*}$, $G^{**}$) of a graph $G$ are generated and fed to $f_{\\theta_{t}}(\\cdot)$ and $f_{\\theta_{s}}(\\cdot)$, respectively. Later,\nthe student's normalized output is forced to approximate the teacher's normalized output as follows:\n\nwhere $x_{i}$ denotes a random variable of $i$-th node attribute in the graph. \nThen, we introduce a  noise perturbation-based method to approximate $H(x_{i}|{\\bf {z}})$. Specifically,  let $\\widetilde{x_i}=x_i+{\\bf \\epsilon}_i$ (${\\bf \\epsilon}_i \\sim \\mathcal{N}(0, {\\bf \\Sigma}_i={\\bf \\sigma}^2_i{\\bf I})$) and we aim to optimize the following loss function:\n\nwhere $\\bm{\\sigma}$ = $[\\sigma_1, \\sigma_2, \\cdots, \\sigma_n]$ are learnable parameters; \n$\\beta$ is trade-off weight. In particular, the first term of the above objective minimizes difference between the encoded embedding of noisy input and the hidden state while the second term encourages a high conditional entropy according to the maximum entropy principle. In this way, we have $p(\\widetilde{x_i}|{\\bf z})=p({\\bf \\epsilon}_i)$ and $H(x_i|{\\bf z})$ is approximated by $H(\\widetilde{x}_i|{\\bf z})$:\n\nwhere $C = \\frac{1}{2}\\log (2\\pi e)$. By taking Eqn.~\\ref{eq:infoloss-7} into Eqn.~\\ref{eq:infoloss-6}, \nwe can optimize $\\mathcal{L}(\\bm{\\sigma})$ with Adam optimizer and obtain the optimal $\\bm{\\sigma}$ for computing the overall discarded information $H(G|Z)$. Furthermore, we can explain GFL model's capability by comparing discarded information of different models. Ideally, we expect the GFL model to encode valid node (or graph) embeddings as much as possible and therefore discard information as small as possible. \n\n\\subsection{Experimental Setup}\n\\label{sec:exp-set}\n\\textbf{Datasets}.\nWe use multiple graph datasets to conduct experiments: for the node classification task, we use ogbn-arxiv~\\citep{hu2020open}, Tissue-PPI~\\citep{hamilton2017inductive}, Fold-PPI~\\citep{zitnik2017predicting}, Cora~\\citep{sen2008collective}, and Citeseer~\\citep{sen2008collective}; for the graph classification task, we use datasets in ~\\citep{chauhan2020few}, i.e., Letter-High, Triangles, Reddit-12K, and Enzymes. The detailed information of datasets is illustrated in Appendix~\\ref{app: dataset}. \n\\\\\n\\textbf{Baseline Methods}. \nWe employ a variety of baseline methods for model comparison in two tasks. For few-shot node classification, we use node2vec~\\citep{grover2016node2vec}, DeepWalk~\\citep{perozzi2014deepwalk}, Meta-GNN~\\citep{zhou2019meta}, FS-GIN~\\citep{xu2018powerful}, FS-SGC~\\citep{wu2019simplifying}, No-Finetune~\\citep{triantafillou2019meta},\nFinetune~\\citep{triantafillou2019meta},\nKNN~~\\citep{triantafillou2019meta},  ProtoNet~\\citep{snell2017prototypical}, MAML~\\citep{finn2017model}, G-Meta~\\citep{huang2020graph}, and TENT~\\citep{wang2022task}. For few-shot graph classification, we utilize WL~\\citep{shervashidze2011weisfeiler}, Graphlet~\\citep{shervashidze2009efficient}, AWE~\\citep{ivanov2018anonymous}, Graph2Vec~\\citep{narayanan2017graph2vec}, Diffpool~\\citep{lee2019self}, CapsGNN~\\citep{xinyi2018capsule}, GIN~\\citep{xu2018powerful}, GIN-KNN~\\citep{xu2018powerful}, GSM-GCN~\\citep{chauhan2020few}, GSM-GAT~\\citep{chauhan2020few}, and AS-MAML~\\citep{ma2020adaptive}. Details of baselines are illustrated in Appendix~\\ref{app: implementation}. \n\\\\\n\\textbf{Experimental Settings}.\nIn CGFL, we use GCN~\\citep{kipf2016semi} as the GNN backbone for the node classification task, including two-layer graph convolution and one-layer FC. The graph classification task adds an extra average pooling operation as a readout layer. \nIn the pretraining phase, we pre-train GNN on the unlabeled graph dataset with contrastive learning. \nFor graph data augmentation, the node drop rate is 15\\%, the edge removing rate is 15\\%; and the feature masking rate is 20\\%. The mini-batch size is set to 2,048, and the learning rate is set to 0.05 with a decay factor = 0.9. Meanwhile, the $\\tau$ in exponential moving average is 0.999. We consider both inductive and transductive settings. For the inductive setting (\\textbf{CGFL-I}), we only use unlabeled data in the training set; for the transductive setting (\\textbf{CGFL-T}), we use unlabeled data in both training data and testing data. Additionally, for pre-trained models without knowledge distillation elevation, we refer to them as \\textbf{Teacher-I} and \\textbf{Teacher-T} for two settings, respectively. \nIn GFL phase, we employ MAML to fine-tune the model.\nWe implement CGFL by PyTorch and train it on NVIDIA V100 GPUs. \nThe code is in the supplementary material. \n\n\\\\\n\\textbf{Impact of Shot Number}.\nIn Figure~\\ref{fig:node-shot}, we show the performance of our model (CGFL-T) under different shot numbers (1 to 5) compared with some selected baselines. It is easy to see that CGFL achieves better accuracy across different shot numbers.\nThe result demonstrates our model's performance is robust for node classification. Note that we only show results on two datasets and the results on the other datasets (i.e., ogbn-arxiv and Tissue-PPI) are shown in Appendix~\\ref{sec:app:fsn}. The same goes for the following analyses. \n\\\\\n\\textbf{Impact of Training Label Rate}.\nWe further evaluate CGFL's performance under different training label rates (10\\%, 20\\%, 30\\%, 40\\%, 50\\%, 100\\%) compared with baseline methods for 3-shot node classification, as shown in Figure~\\ref{fig:node-lr}. It is easy to see that (1) CGFL has better results across different label rates; (2) CGFL achieves larger improvement over baseline models when label rate becomes lower (e.g., 10\\%), showing contrastive pre-training and self-distillation of CGFL lead to more significant improvement for label sparsity case. \n\\\\\n\\textbf{Impact of Data Augmentation}.\nAs a key step for contrastive learning in CGFL, graph augmentation plays an important role in affecting model performance. Here we conduct experiments to evaluate the model performance with different augmentation strategies. We consider three graph augmentations - node dropping (\\textbf{ND}), feature masking (\\textbf{FM}), edge removing (\\textbf{ER}), and their combinations for CGFL phase and report their performances in Figure~\\ref{fig:node-da}. It is easy to find that the combination of three augmentation strategies works better than a single augmentation or the combination of two augmentations. It demonstrates that various graph augmentations are able to generate sufficient contrastive pairs for a better model. \n\\\\\n\n\\subsection{{Apply to MAML++ and ALFA in Few-shot Learing Phase}}\n{We implement MAML++ and ALFA (extended from MAML) in our CGFL to replace MAML under transductive setting on ogbn-arxiv dataset. The results are reported in  Table~\\ref{tab:node-alfa}. Our aim is to design a contrastive pretraining framework which is plug-and-play for graph few-shot learning. So our CGFL is not limited to MAML. It is easy to be extended with more new methods (e.g., MAML++). Replacing MAML to MAML++ in CGFL brings better accuracy, which implies our framework is general and extendable.}", "table_source": "\\begin{table}[t]\n\\caption{{Comparison with finetuning by MAML++ and ALFA.}}\n\\label{tab:node-alfa}\n\\vspace{1mm}\n\\small\n\\renewcommand\\arraystretch{1}\n\\centering\n\\resizebox{0.4\\textwidth}{!}{\\begin{tabular*}{0.45\\textwidth}{c| @{\\extracolsep{\\fill}} |c|c}\n\\toprule\n{Model} &  3-shot & 5-shot \\\\\n\\midrule\nMAML++       &55.9$\\pm$2.1 &59.4$\\pm$3.2 \\\\\nALFA       &54.8$\\pm$3.6 &58.8$\\pm$3.7 \\\\\nOurs (with MAML)       &55.2$\\pm$2.5 &58.7$\\pm$2.7  \\\\\n\\bottomrule\n\\end{tabular*}\n}\n\\end{table}", "cell_list_gold": [{"value": "55.9", "char_index": [315, 319], "type": "Result", "training data/set": "ogbn-arxiv", "test data/set": "ogbn-arxiv", "task": ["Few-shot node classification", "node classification"], "metric": "Accuracy", "experimental settings": {"3-shot": "true"}, "model": "MAML++", "model settings": {"xx": "yy"}}, {"value": "59.4", "char_index": [329, 333], "type": "Result", "training data/set": "ogbn-arxiv", "test data/set": "ogbn-arxiv", "task": ["Few-shot node classification", "node classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "MAML++", "model settings": {"xx": "yy"}}, {"value": "54.8", "char_index": [357, 361], "type": "Result", "training data/set": "ogbn-arxiv", "test data/set": "ogbn-arxiv", "task": ["Few-shot node classification", "node classification"], "metric": "Accuracy", "experimental settings": {"3-shot": "true"}, "model": "ALFA", "model settings": {"xx": "yy"}}, {"value": "58.8", "char_index": [371, 375], "type": "Result", "training data/set": "ogbn-arxiv", "test data/set": "ogbn-arxiv", "task": ["Few-shot node classification", "node classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": "ALFA", "model settings": {"xx": "yy"}}, {"value": "55.2", "char_index": [411, 415], "type": "Result", "training data/set": "ogbn-arxiv", "test data/set": "ogbn-arxiv", "task": ["Few-shot node classification", "node classification"], "metric": "Accuracy", "experimental settings": {"3-shot": "true"}, "model": ["CGFL MAML", "Ours (with MAML)"], "model settings": {"xx": "yy"}}, {"value": "58.7", "char_index": [425, 429], "type": "Result", "training data/set": "ogbn-arxiv", "test data/set": "ogbn-arxiv", "task": ["Few-shot node classification", "node classification"], "metric": "Accuracy", "experimental settings": {"5-shot": "true"}, "model": ["CGFL MAML", "Ours (with MAML)"], "model settings": {"xx": "yy"}}]}, "2210.00193v1_table0": {"table_code": "\\begin{table}\\label{tab:corpus_stats}\n\\centering\n\\footnotesize\n\\begin{tabular}{ccrr}\n\\toprule\n\\textbf{Bucket} & \\textbf{Split} & \\textbf{Portuguese} & \\textbf{Mandarin} \\\\\n\\midrule\n\\multirow{3}{3.4em}{Lexical} & Exemplar & 118 & 173\\\\\n& Dev & 848 & 524\\\\ \n& Test & 874 & 538\\\\ \n\\midrule\n\\multirow{3}{3.4em}{Entities} & Exemplar & 112 & 104\\\\\n& Dev & 935 & 883\\\\ \n& Test & 985 & 932\\\\ \n\\midrule\n\\multirow{3}{3.4em}{Random} & Exemplar & 111 & 111\\\\\n& Dev & 744 & 744\\\\ \n& Test & 757 & 757\\\\ \n\\midrule\n\\multirow{3}{3.4em}{Total} & Exemplar & 341 & 388\\\\\n& Dev & 2527 & 2151\\\\ \n& Test & 2616 & 2227\\\\ \n\\bottomrule\n\\end{tabular}\n\\caption{Number of sentence pairs by bucket, split, and language, as well as cross-bucket totals. Note, the \\texttt{random} bucket contains the same English source sentences across the Portuguese and Mandarin sets.}\n\\end{table}", "table_label": "{tab:corpus_stats}", "table_numeric_cells": [["118", "118", 223, 226, 223, 226], ["173", "173", 229, 232, 229, 232], ["848", "848", 243, 246, 243, 246], ["524", "524", 249, 252, 249, 252], ["874", "874", 265, 268, 265, 268], ["538", "538", 271, 274, 271, 274], ["112", "112", 330, 333, 330, 333], ["104", "104", 336, 339, 336, 339], ["935", "935", 350, 353, 350, 353], ["883", "883", 356, 359, 356, 359], ["985", "985", 372, 375, 372, 375], ["932", "932", 378, 381, 378, 381], ["111", "111", 435, 438, 435, 438], ["111", "111", 441, 444, 441, 444], ["744", "744", 455, 458, 455, 458], ["744", "744", 461, 464, 461, 464], ["757", "757", 477, 480, 477, 480], ["757", "757", 483, 486, 483, 486], ["341", "341", 539, 542, 539, 542], ["388", "388", 545, 548, 545, 548], ["2527", "2527", 559, 563, 559, 563], ["2151", "2151", 566, 570, 566, 570], ["2616", "2616", 583, 587, 583, 587], ["2227", "2227", 590, 594, 590, 594]], "text_chunk_selected": "\\begin{abstract}\nWe present FRMT, a new dataset and evaluation benchmark for Few-shot Region-aware Machine Translation, a type of style-targeted translation. The dataset consists of professional translations from English into two regional variants each of Portuguese and Mandarin Chinese. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms. We explore automatic evaluation metrics for FRMT and validate their correlation with expert human evaluation across both region-matched and mismatched rating scenarios. Finally, we present a number of baseline models for this task, and offer guidelines for how researchers can train, evaluate, and compare their own models. Our dataset and evaluation code are publicly available: \\url{https://bit.ly/frmt-task}.\n\\end{abstract}\n\n\\section{Related Work}\nPrevious work on textual ``style transfer'' or ``attribute rewriting'' is related in trying to control fine-level stylistic features of generated text. Earlier work in this space leverages supervised parallel data \\cite{jhamtani-etal-2017-shakespearizing}. Later work assumes labeled but non-parallel training data \\cite{shen17style, li-etal-2018-delete, niu18multitask}, or foregoes training-time labels entirely, as in our setting, relying only on few-shot exemplars provided at inference time \\cite{xu2020variational, riley-etal-2021-textsettr, garcia2021towards}. However, there is broad agreement that style transfer evaluation protocols are far from ideal \\cite{pang-gimpel-2019-unsupervised, briakou-etal-2021-review, hu2022tst}, due to the underspecification of stylistic attributes (e.g.~formality, sentiment) and the lack of standardization across studies. Region-aware translation addresses these issues, providing an excellent test-bed for exploring few-shot attribute control---MT evaluation methods are relatively mature, and many regional language varieties can be sufficiently delineated for the task.\n\n\\section{FRMT Dataset}\\label{sec:new-dataset}\nWe introduce the FRMT dataset for evaluating the quality of few-shot region-aware machine translation. The dataset covers two regions each for Portuguese (Brazil and Portugal) and Mandarin (Mainland and Taiwan).\nOur overall approach for constructing the dataset is to sample English sentences from Wikipedia and acquire professional human translations into the target regional varieties.\nFinal quality control is done through manual evaluation by an independent set of translators, using the MQM protocol \\cite{Freitag2021ExpertsEA} that we also employ to evaluate system translation quality.\n\n\\subsection{Data sampling method}\\label{sec:data_sampling}\nFRMT seeks to capture region-specific linguistic differences, as well as potential distractors in the form of entities that are strongly associated with one region, without necessarily having a different linguistic rendering (e.g., Lisbon vs.~S\\~{a}o Paulo). To this end, we divide the dataset into three buckets (\\texttt{lexical}, \\texttt{entity}, \\texttt{random}), each containing human translations of sentences extracted from different sets of English Wikipedia articles.\\footnote{\nAs Wikipedia data source we use the training split of {\\tt wiki40b} (v1.3.0) by \\citet{wiki40b}, available at \\scriptsize{\\url{https://www.tensorflow.org/datasets/catalog/wiki40b}}.}\n\n{\\bf Entities}:  We manually select a balanced number of entities per language region such that they are strongly associated with one region, relying primarily on world knowledge and following Wikipedia hyperlinks.\nSelection is done within each of a few broad entity types defined at the outset: people, locations, organizations, attractions/infrastructure, and other.\nTo mitigate availability bias in picking entities, we also took inspiration from automatically extracted entity mentions most distinctive (by log-odds) of either top-level web domain across the mC4 corpus \\cite{xue-etal-2021-mt5}, for {\\tt .pt} vs.~{\\tt .br}, and {\\tt .cn} vs.~{\\tt .tw}.\nThe final selection comprises 38 Mandarin-focused and 34 Portuguese-focused entities.\nAgain we extract 100 source sentences from the beginning of the English Wikipedia article about each selected entity.\n\n{\\bf Random}: \nTo include more naturally distributed phenomena, we also sampled 100 articles at random from the combined set of 28k articles appearing in Wikipedia's collections of ``featured'' or ``good''  articles.\\footnote{\n\\scriptsize{\\url{https://en.wikipedia.org/wiki/Wikipedia:Good_articles/all}} and\n\\scriptsize{\\url{https://en.wikipedia.org/wiki/Wikipedia:Featured_articles}} (as of 2021-12-15).\n}\nWe accept the inherent selection bias of such curated collections in exchange for improved text quality and factual accuracy.\nHere, we can extract less text from more articles, taking up to 20 contiguous sentences from the start\nof a randomly chosen section within each article.\nThe latter step counteracts the somewhat formulaic article structure that is common to the Wikipedia genre.\nUnlike the other two buckets, this one features one common set of sentences to be translated into all four target variants.\n\n\\subsection{Corpus statistics}\nFor each bucket, we split our data into exemplar, development (dev), and test data. The exemplars are intended to be the only pairs where the region label is shown to the model, such as via few-shot or in-context learning \\cite{brown-etal-22-gpt3}.\n\nTable~\\ref{tab:corpus_stats} reports the number of released sentence pairs for each combination of bucket, split, and language. For each document in our dataset, all sentences from that document appear only in a single split---this ensures that a system cannot ``cheat'' by memorizing word-region associations from the exemplars, or by overfitting to words and entities while hill-climbing on the validation set.", "table_source": "\\begin{table}\\label{tab:corpus_stats}\n\\centering\n\\footnotesize\n\\begin{tabular}{ccrr}\n\\toprule\n\\textbf{Bucket} & \\textbf{Split} & \\textbf{Portuguese} & \\textbf{Mandarin} \\\\\n\\midrule\n\\multirow{3}{3.4em}{Lexical} & Exemplar & 118 & 173\\\\\n& Dev & 848 & 524\\\\ \n& Test & 874 & 538\\\\ \n\\midrule\n\\multirow{3}{3.4em}{Entities} & Exemplar & 112 & 104\\\\\n& Dev & 935 & 883\\\\ \n& Test & 985 & 932\\\\ \n\\midrule\n\\multirow{3}{3.4em}{Random} & Exemplar & 111 & 111\\\\\n& Dev & 744 & 744\\\\ \n& Test & 757 & 757\\\\ \n\\midrule\n\\multirow{3}{3.4em}{Total} & Exemplar & 341 & 388\\\\\n& Dev & 2527 & 2151\\\\ \n& Test & 2616 & 2227\\\\ \n\\bottomrule\n\\end{tabular}\n\\caption{Number of sentence pairs by bucket, split, and language, as well as cross-bucket totals. Note, the \\texttt{random} bucket contains the same English source sentences across the Portuguese and Mandarin sets.}\n\\end{table}", "cell_list_gold": [{"value": "118", "char_index": [223, 226], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Lexical Exemplar", "dataset features": {"xx": "yy"}}, {"value": "173", "char_index": [229, 232], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Lexical Exemplar", "dataset features": {"xx": "yy"}}, {"value": "848", "char_index": [243, 246], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Lexical Dev", "dataset features": {"xx": "yy"}}, {"value": "524", "char_index": [249, 252], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Lexical Dev", "dataset features": {"xx": "yy"}}, {"value": "874", "char_index": [265, 268], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Lexical Test", "dataset features": {"xx": "yy"}}, {"value": "538", "char_index": [271, 274], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Lexical Test", "dataset features": {"xx": "yy"}}, {"value": "112", "char_index": [330, 333], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Entities Exemplar", "dataset features": {"xx": "yy"}}, {"value": "104", "char_index": [336, 339], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Entities Exemplar", "dataset features": {"xx": "yy"}}, {"value": "935", "char_index": [350, 353], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Entities Dev", "dataset features": {"xx": "yy"}}, {"value": "883", "char_index": [356, 359], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Entities Dev", "dataset features": {"xx": "yy"}}, {"value": "985", "char_index": [372, 375], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Entities Test", "dataset features": {"xx": "yy"}}, {"value": "932", "char_index": [378, 381], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Entities Test", "dataset features": {"xx": "yy"}}, {"value": "111", "char_index": [435, 438], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Exemplar", "dataset features": {"xx": "yy"}}, {"value": "111", "char_index": [441, 444], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Exemplar", "dataset features": {"xx": "yy"}}, {"value": "744", "char_index": [455, 458], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Dev", "dataset features": {"xx": "yy"}}, {"value": "744", "char_index": [461, 464], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Dev", "dataset features": {"xx": "yy"}}, {"value": "757", "char_index": [477, 480], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Test", "dataset features": {"xx": "yy"}}, {"value": "757", "char_index": [483, 486], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Test", "dataset features": {"xx": "yy"}}, {"value": "341", "char_index": [539, 542], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Exemplar", "dataset features": {"xx": "yy"}}, {"value": "388", "char_index": [545, 548], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Exemplar", "dataset features": {"xx": "yy"}}, {"value": "2527", "char_index": [559, 563], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Dev", "dataset features": {"xx": "yy"}}, {"value": "2151", "char_index": [566, 570], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Dev", "dataset features": {"xx": "yy"}}, {"value": "2616", "char_index": [583, 587], "type": "Data Stat.", "dataset": "Portuguese", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Test", "dataset features": {"xx": "yy"}}, {"value": "2227", "char_index": [590, 594], "type": "Data Stat.", "dataset": "Mandarin", "attribute name": ["Number of sentence pairs", "# sentence pairs"], "sub-set/group name": "Random Test", "dataset features": {"xx": "yy"}}]}, "2210.00193v1_table2": {"table_code": "\\begin{table*}\n\\centering\n\\footnotesize\n\\begin{tabular}{lcccccc|c}\n\\toprule\n& \\multicolumn{2}{c}{\\bf Lexical} & \\multicolumn{2}{c}{\\bf Entities} & \\multicolumn{2}{c}{\\bf Random} & \\bf FRMT \\\\\n\\bf Model & pt-BR & pt-PT & pt-BR & pt-PT & pt-BR & pt-PT & pt\\\\\\midrule\nUR & 37.4 (69.9) & 32.7 (68.0) & 46.7 (76.3) & 40.8 (73.6) & 39.8 (70.7) & 35.3 (69.2) & 38.7 (71.3)\\\\\nM4-UR & 46.7 (74.5) & 32.7 (69.7) & 53.5 (79.9) & 45.4 (77.5) & 43.1 (70.9) & 32.9 (68.4) & 42.0 (73.5)\\\\\nM4-Prompts & 54.1 (77.1) & 36.9 (72.1) & 56.9 (81.1) & 47.3 (78.4) & 56.1 (77.5) & 41.0 (73.7) & 48.2 (76.6)\\\\\nM4-Prompts FT & 45.5 (70.1) & 32.5 (67.4) & 48.6 (73.8) & 40.7 (72.8) & 48.1 (70.5) & 36.9 (69.0) & 41.7 (70.6)\\\\\nPaLM 8B & 38.6 (69.8) & 26.7 (65.8) & 45.9 (75.9) & 38.0 (73.6) & 39.3 (69.4) & 32.1 (67.8) & 36.5 (70.4)\\\\\nPaLM 62B & 49.5 (75.9) & 36.7 (72.4) & 55.4 (80.1) & 46.1 (77.8) & 50.3 (75.2) & 41.5 (73.5) & 46.3 (75.8)\\\\\nPaLM 540B & 53.7 (77.1) & \\textbf{40.1} (\\textbf{73.9}) & \\textbf{59.0} (\\textbf{81.2}) & \\textbf{49.5} (\\textbf{79.0}) & 54.8 (76.9) & \\textbf{45.6} (\\textbf{75.5}) & \\textbf{50.2} (77.3)\\\\\nOnline & \\textbf{56.2} (\\textbf{78.7}) & 35.6 (72.3) & 56.3 (\\textbf{81.2}) & 46.9 (78.3) & \\textbf{65.2} (\\textbf{80.5}) & 42.9 (75.0) & 49.8 (\\textbf{77.6}) \\\\\n\\midrule\n& zh-CN & zh-TW & zh-CN & zh-TW & zh-CN & zh-TW & zh\\\\\\midrule\nUR & 22.6 (58.5) & 13.8 (56.0) & 26.7 (67.1) & 19.5 (65.3) & 26.4 (62.1) & 20.4 (61.0) & 21.3 (61.7)\\\\\nM4-UR & 33.3 (65.0) & 18.9 (58.2) & 43.2 (73.0) & 31.4 (70.4) & 40.8 (65.4) & 30.8 (63.6) & 32.5 (65.9)\\\\\nM4-Prompts & 33.3 (64.9) & 18.3 (57.6) & 44.2 (72.5) & 32.0 (68.7) & 43.7 (67.0) & 32.2 (63.4) & 33.3 (65.6)\\\\\nM4-Prompts FT & 33.8 (65.7) & 18.8 (59.0) & 44.8 (73.2) & 31.6 (69.8) & 42.7 (66.7) & 31.5 (64.0) & 33.2 (66.4)\\\\\nPaLM 8B & 17.6 (55.7) & 13.3 (52.3) & 28.1 (65.7) & 24.4 (63.9) & 21.6 (56.3) & 18.2 (56.1) & 20.4 (58.3)\\\\\nPaLM 62B & 29.2 (62.2) & 20.4 (59.8) & 40.2 (71.8) & 33.0 (69.9) & 34.5 (64.0) & 26.0 (63.1) & 30.3 (65.1)\\\\\nPaLM 540B & 34.8 (66.5) & \\textbf{24.6} (\\textbf{63.3}) & 44.9 (74.7) & 35.2 (\\textbf{72.5}) & 40.0 (67.8) & 29.6 (66.0) & 34.5 (68.4)\\\\\nOnline & \\textbf{39.7} (\\textbf{68.0}) & 21.9 (61.8) & \\textbf{50.4} (\\textbf{75.0}) & \\textbf{37.0} (72.2) & \\textbf{56.1} (\\textbf{72.0}) & \\textbf{39.9} (\\textbf{68.7}) & \\textbf{40.1} (\\textbf{69.6}) \\\\\n \\bottomrule\n\\end{tabular}\n\\caption{FRMT per-bucket test set results, in the format: BLEU (BLEURT). The ``FRMT'' score is the geometric mean across regions of the arithmetic mean across buckets.}\n\\label{tab:results_auto}\n\\end{table*}", "table_label": "{tab:results_auto}", "table_numeric_cells": [["37.4", "37.4 (69.9)", 270, 274, 270, 281], ["32.7", "32.7 (68.0)", 284, 288, 284, 295], ["46.7", "46.7 (76.3)", 298, 302, 298, 309], ["40.8", "40.8 (73.6)", 312, 316, 312, 323], ["39.8", "39.8 (70.7)", 326, 330, 326, 337], ["35.3", "35.3 (69.2)", 340, 344, 340, 351], ["38.7", "38.7 (71.3)", 354, 358, 354, 365], ["46.7", "46.7 (74.5)", 376, 380, 376, 387], ["32.7", "32.7 (69.7)", 390, 394, 390, 401], ["53.5", "53.5 (79.9)", 404, 408, 404, 415], ["45.4", "45.4 (77.5)", 418, 422, 418, 429], ["43.1", "43.1 (70.9)", 432, 436, 432, 443], ["32.9", "32.9 (68.4)", 446, 450, 446, 457], ["42.0", "42.0 (73.5)", 460, 464, 460, 471], ["54.1", "54.1 (77.1)", 487, 491, 487, 498], ["36.9", "36.9 (72.1)", 501, 505, 501, 512], ["56.9", "56.9 (81.1)", 515, 519, 515, 526], ["47.3", "47.3 (78.4)", 529, 533, 529, 540], ["56.1", "56.1 (77.5)", 543, 547, 543, 554], ["41.0", "41.0 (73.7)", 557, 561, 557, 568], ["48.2", "48.2 (76.6)", 571, 575, 571, 582], ["45.5", "45.5 (70.1)", 601, 605, 601, 612], ["32.5", "32.5 (67.4)", 615, 619, 615, 626], ["48.6", "48.6 (73.8)", 629, 633, 629, 640], ["40.7", "40.7 (72.8)", 643, 647, 643, 654], ["48.1", "48.1 (70.5)", 657, 661, 657, 668], ["36.9", "36.9 (69.0)", 671, 675, 671, 682], ["41.7", "41.7 (70.6)", 685, 689, 685, 696], ["38.6", "38.6 (69.8)", 709, 713, 709, 720], ["26.7", "26.7 (65.8)", 723, 727, 723, 734], ["45.9", "45.9 (75.9)", 737, 741, 737, 748], ["38.0", "38.0 (73.6)", 751, 755, 751, 762], ["39.3", "39.3 (69.4)", 765, 769, 765, 776], ["32.1", "32.1 (67.8)", 779, 783, 779, 790], ["36.5", "36.5 (70.4)", 793, 797, 793, 804], ["49.5", "49.5 (75.9)", 818, 822, 818, 829], ["36.7", "36.7 (72.4)", 832, 836, 832, 843], ["55.4", "55.4 (80.1)", 846, 850, 846, 857], ["46.1", "46.1 (77.8)", 860, 864, 860, 871], ["50.3", "50.3 (75.2)", 874, 878, 874, 885], ["41.5", "41.5 (73.5)", 888, 892, 888, 899], ["46.3", "46.3 (75.8)", 902, 906, 902, 913], ["53.7", "53.7 (77.1)", 928, 932, 928, 939], ["40.1", "\\textbf{40.1} (\\textbf{73.9})", 950, 954, 942, 971], ["59.0", "\\textbf{59.0} (\\textbf{81.2})", 982, 986, 974, 1003], ["49.5", "\\textbf{49.5} (\\textbf{79.0})", 1014, 1018, 1006, 1035], ["54.8", "54.8 (76.9)", 1038, 1042, 1038, 1049], ["45.6", "\\textbf{45.6} (\\textbf{75.5})", 1060, 1064, 1052, 1081], ["50.2", "\\textbf{50.2} (77.3)", 1092, 1096, 1084, 1104], ["56.2", "\\textbf{56.2} (\\textbf{78.7})", 1124, 1128, 1116, 1145], ["35.6", "35.6 (72.3)", 1148, 1152, 1148, 1159], ["56.3", "56.3 (\\textbf{81.2})", 1162, 1166, 1162, 1182], ["46.9", "46.9 (78.3)", 1185, 1189, 1185, 1196], ["65.2", "\\textbf{65.2} (\\textbf{80.5})", 1207, 1211, 1199, 1228], ["42.9", "42.9 (75.0)", 1231, 1235, 1231, 1242], ["49.8", "49.8 (\\textbf{77.6})", 1245, 1249, 1245, 1265], ["22.6", "22.6 (58.5)", 1346, 1350, 1346, 1357], ["13.8", "13.8 (56.0)", 1360, 1364, 1360, 1371], ["26.7", "26.7 (67.1)", 1374, 1378, 1374, 1385], ["19.5", "19.5 (65.3)", 1388, 1392, 1388, 1399], ["26.4", "26.4 (62.1)", 1402, 1406, 1402, 1413], ["20.4", "20.4 (61.0)", 1416, 1420, 1416, 1427], ["21.3", "21.3 (61.7)", 1430, 1434, 1430, 1441], ["33.3", "33.3 (65.0)", 1452, 1456, 1452, 1463], ["18.9", "18.9 (58.2)", 1466, 1470, 1466, 1477], ["43.2", "43.2 (73.0)", 1480, 1484, 1480, 1491], ["31.4", "31.4 (70.4)", 1494, 1498, 1494, 1505], ["40.8", "40.8 (65.4)", 1508, 1512, 1508, 1519], ["30.8", "30.8 (63.6)", 1522, 1526, 1522, 1533], ["32.5", "32.5 (65.9)", 1536, 1540, 1536, 1547], ["33.3", "33.3 (64.9)", 1563, 1567, 1563, 1574], ["18.3", "18.3 (57.6)", 1577, 1581, 1577, 1588], ["44.2", "44.2 (72.5)", 1591, 1595, 1591, 1602], ["32.0", "32.0 (68.7)", 1605, 1609, 1605, 1616], ["43.7", "43.7 (67.0)", 1619, 1623, 1619, 1630], ["32.2", "32.2 (63.4)", 1633, 1637, 1633, 1644], ["33.3", "33.3 (65.6)", 1647, 1651, 1647, 1658], ["33.8", "33.8 (65.7)", 1677, 1681, 1677, 1688], ["18.8", "18.8 (59.0)", 1691, 1695, 1691, 1702], ["44.8", "44.8 (73.2)", 1705, 1709, 1705, 1716], ["31.6", "31.6 (69.8)", 1719, 1723, 1719, 1730], ["42.7", "42.7 (66.7)", 1733, 1737, 1733, 1744], ["31.5", "31.5 (64.0)", 1747, 1751, 1747, 1758], ["33.2", "33.2 (66.4)", 1761, 1765, 1761, 1772], ["17.6", "17.6 (55.7)", 1785, 1789, 1785, 1796], ["13.3", "13.3 (52.3)", 1799, 1803, 1799, 1810], ["28.1", "28.1 (65.7)", 1813, 1817, 1813, 1824], ["24.4", "24.4 (63.9)", 1827, 1831, 1827, 1838], ["21.6", "21.6 (56.3)", 1841, 1845, 1841, 1852], ["18.2", "18.2 (56.1)", 1855, 1859, 1855, 1866], ["20.4", "20.4 (58.3)", 1869, 1873, 1869, 1880], ["29.2", "29.2 (62.2)", 1894, 1898, 1894, 1905], ["20.4", "20.4 (59.8)", 1908, 1912, 1908, 1919], ["40.2", "40.2 (71.8)", 1922, 1926, 1922, 1933], ["33.0", "33.0 (69.9)", 1936, 1940, 1936, 1947], ["34.5", "34.5 (64.0)", 1950, 1954, 1950, 1961], ["26.0", "26.0 (63.1)", 1964, 1968, 1964, 1975], ["30.3", "30.3 (65.1)", 1978, 1982, 1978, 1989], ["34.8", "34.8 (66.5)", 2004, 2008, 2004, 2015], ["24.6", "\\textbf{24.6} (\\textbf{63.3})", 2026, 2030, 2018, 2047], ["44.9", "44.9 (74.7)", 2050, 2054, 2050, 2061], ["35.2", "35.2 (\\textbf{72.5})", 2064, 2068, 2064, 2084], ["40.0", "40.0 (67.8)", 2087, 2091, 2087, 2098], ["29.6", "29.6 (66.0)", 2101, 2105, 2101, 2112], ["34.5", "34.5 (68.4)", 2115, 2119, 2115, 2126], ["39.7", "\\textbf{39.7} (\\textbf{68.0})", 2146, 2150, 2138, 2167], ["21.9", "21.9 (61.8)", 2170, 2174, 2170, 2181], ["50.4", "\\textbf{50.4} (\\textbf{75.0})", 2192, 2196, 2184, 2213], ["37.0", "\\textbf{37.0} (72.2)", 2224, 2228, 2216, 2236], ["56.1", "\\textbf{56.1} (\\textbf{72.0})", 2247, 2251, 2239, 2268], ["39.9", "\\textbf{39.9} (\\textbf{68.7})", 2279, 2283, 2271, 2300], ["40.1", "\\textbf{40.1} (\\textbf{69.6})", 2311, 2315, 2303, 2332]], "text_chunk_selected": "\\section{FRMT Dataset}\\label{sec:new-dataset}\nWe introduce the FRMT dataset for evaluating the quality of few-shot region-aware machine translation. The dataset covers two regions each for Portuguese (Brazil and Portugal) and Mandarin (Mainland and Taiwan).\nOur overall approach for constructing the dataset is to sample English sentences from Wikipedia and acquire professional human translations into the target regional varieties.\nFinal quality control is done through manual evaluation by an independent set of translators, using the MQM protocol \\cite{Freitag2021ExpertsEA} that we also employ to evaluate system translation quality.\n\n\\subsection{Data sampling method}\\label{sec:data_sampling}\nFRMT seeks to capture region-specific linguistic differences, as well as potential distractors in the form of entities that are strongly associated with one region, without necessarily having a different linguistic rendering (e.g., Lisbon vs.~S\\~{a}o Paulo). To this end, we divide the dataset into three buckets (\\texttt{lexical}, \\texttt{entity}, \\texttt{random}), each containing human translations of sentences extracted from different sets of English Wikipedia articles.\\footnote{\nAs Wikipedia data source we use the training split of {\\tt wiki40b} (v1.3.0) by \\citet{wiki40b}, available at \\scriptsize{\\url{https://www.tensorflow.org/datasets/catalog/wiki40b}}.}\n\n\\subsection{Human evaluation}\\label{sec:human_eval_mqm}\nTo obtain the highest fidelity human ratings, we use the expert-based \nMultidimensional Quality Metrics (MQM) evaluation framework proposed by \\citet{Freitag2021ExpertsEA}. This is the same framework recommended by the WMT'21 Evaluation Campaign \\citep{freitag-etal-2021-results}. For our evaluation, expert raters are shown a chunk of 10 contiguous English sentences from our test set with the corresponding translations from one combination of model (or human) and target region. Raters then identify errors in the translations, assigning a category and severity to each; see \\citet{Freitag2021ExpertsEA} for details. Due to cost constraints, we evaluate 25\\%{} of the test set, evenly distributed across our three evaluation buckets. Each chunk is rated by three raters.\n\nIn Mandarin, there is an added complexity of script choice.  Mainland Mandarin is typically written in the ``simplified'' Han script, while Taiwan Mandarin is written in ``traditional'' script.  To disentangle lexical choice from script choice, we define lexical accuracy in a script-agnostic manner.  For example, for the word \\textit{pineapple}, if the target region is Taiwan, we count both the (expected) traditional and (unexpected) simplified forms of the Taiwan variant \\textit{f\u00e8ngl\u00ed} (\\trad{\u9cf3\u68a8} and \\simp{\u51e4\u68a8}) as correct, while counting both script forms of the Mainland variant \\textit{b\u014dlu\u00f3} (\\simp{\u83e0\u841d} and \\trad{\u83e0\u863f}) as incorrect.  This ensures that even if a model is outputting the wrong script, it will be rewarded or penalized based on its lexical choices. This also prevents the possibility of ``gaming'' our metric by only using the lexical forms and script of a single region.\n\n\\subsection{Reporting FRMT results}\\label{sec:recommendations}\nFor the FRMT \\emph{task} (as opposed to the \\emph{dataset}), we stipulate a key ``few-shot'' restriction: candidate models \\textbf{may not be intentionally exposed to any region-labeled data} at any point during training.  This restriction covers both region-labeled monolingual data as well as region-labeled parallel translation data.\\footnote{Models may train on multilingual web crawl data, as is common practice, as long as supervised region labels are not provided.  We allow that some implicit or explicit region labels may appear within the unsupervised data.} While it may not be difficult to obtain region labels for Brazil/Portugal or Mainland/Taiwan (e.g.~by filtering web pages on top-level web domain), we intend for FRMT to serve as a measure of few-shot generalization to \\emph{arbitrary} regions and language varieties, for which obtaining labels may be much harder.\n\nResearchers sharing FRMT results should \\textbf{report per-bucket BLEU and lexical accuracy} metrics on the test set, as shown in Tables \\ref{tab:results_auto} and \\ref{tab:lexical_accuracy}. These metrics are efficient to calculate with our provided evaluation scripts.\n\nFor Mandarin (on the full 25\\% sample), a large gap remains between expert translations and our baselines (Gold: $2.5$ vs.~PaLM: $8.8$).  Our results indicate that better region handling will be needed to close this gap; our baselines are not effectively leveraging region information (as evidenced by small match vs.~mismatch deltas), and even the best-case expert translation scores poorly in the mismatched region (Gold match: $2.5$ vs.~Gold mismatch: $5.2$).\\footnote{Note, this is ignoring orthographic differences, as we transliterate outputs to the rater's script.  See section \\S\\ref{sec:script_analysis} for analysis of script differences.}\n\nTable \\ref{tab:results_auto} shows the performance of our baseline models on the automated metrics BLEU and BLEURT\\@. The \\textbf{``FRMT'' metric} is a summary of per-language performance, calculated as the geometric mean across regions of the arithmetic mean across buckets. Recall, while BLEU scores serve as a lightweight point of comparison, we recommend additionally reporting the more costly BLEURT where possible, as this correlates more strongly with human judgments.", "table_source": "\\begin{table*}\n\\centering\n\\footnotesize\n\\begin{tabular}{lcccccc|c}\n\\toprule\n& \\multicolumn{2}{c}{\\bf Lexical} & \\multicolumn{2}{c}{\\bf Entities} & \\multicolumn{2}{c}{\\bf Random} & \\bf FRMT \\\\\n\\bf Model & pt-BR & pt-PT & pt-BR & pt-PT & pt-BR & pt-PT & pt\\\\\\midrule\nUR & 37.4 (69.9) & 32.7 (68.0) & 46.7 (76.3) & 40.8 (73.6) & 39.8 (70.7) & 35.3 (69.2) & 38.7 (71.3)\\\\\nM4-UR & 46.7 (74.5) & 32.7 (69.7) & 53.5 (79.9) & 45.4 (77.5) & 43.1 (70.9) & 32.9 (68.4) & 42.0 (73.5)\\\\\nM4-Prompts & 54.1 (77.1) & 36.9 (72.1) & 56.9 (81.1) & 47.3 (78.4) & 56.1 (77.5) & 41.0 (73.7) & 48.2 (76.6)\\\\\nM4-Prompts FT & 45.5 (70.1) & 32.5 (67.4) & 48.6 (73.8) & 40.7 (72.8) & 48.1 (70.5) & 36.9 (69.0) & 41.7 (70.6)\\\\\nPaLM 8B & 38.6 (69.8) & 26.7 (65.8) & 45.9 (75.9) & 38.0 (73.6) & 39.3 (69.4) & 32.1 (67.8) & 36.5 (70.4)\\\\\nPaLM 62B & 49.5 (75.9) & 36.7 (72.4) & 55.4 (80.1) & 46.1 (77.8) & 50.3 (75.2) & 41.5 (73.5) & 46.3 (75.8)\\\\\nPaLM 540B & 53.7 (77.1) & \\textbf{40.1} (\\textbf{73.9}) & \\textbf{59.0} (\\textbf{81.2}) & \\textbf{49.5} (\\textbf{79.0}) & 54.8 (76.9) & \\textbf{45.6} (\\textbf{75.5}) & \\textbf{50.2} (77.3)\\\\\nOnline & \\textbf{56.2} (\\textbf{78.7}) & 35.6 (72.3) & 56.3 (\\textbf{81.2}) & 46.9 (78.3) & \\textbf{65.2} (\\textbf{80.5}) & 42.9 (75.0) & 49.8 (\\textbf{77.6}) \\\\\n\\midrule\n& zh-CN & zh-TW & zh-CN & zh-TW & zh-CN & zh-TW & zh\\\\\\midrule\nUR & 22.6 (58.5) & 13.8 (56.0) & 26.7 (67.1) & 19.5 (65.3) & 26.4 (62.1) & 20.4 (61.0) & 21.3 (61.7)\\\\\nM4-UR & 33.3 (65.0) & 18.9 (58.2) & 43.2 (73.0) & 31.4 (70.4) & 40.8 (65.4) & 30.8 (63.6) & 32.5 (65.9)\\\\\nM4-Prompts & 33.3 (64.9) & 18.3 (57.6) & 44.2 (72.5) & 32.0 (68.7) & 43.7 (67.0) & 32.2 (63.4) & 33.3 (65.6)\\\\\nM4-Prompts FT & 33.8 (65.7) & 18.8 (59.0) & 44.8 (73.2) & 31.6 (69.8) & 42.7 (66.7) & 31.5 (64.0) & 33.2 (66.4)\\\\\nPaLM 8B & 17.6 (55.7) & 13.3 (52.3) & 28.1 (65.7) & 24.4 (63.9) & 21.6 (56.3) & 18.2 (56.1) & 20.4 (58.3)\\\\\nPaLM 62B & 29.2 (62.2) & 20.4 (59.8) & 40.2 (71.8) & 33.0 (69.9) & 34.5 (64.0) & 26.0 (63.1) & 30.3 (65.1)\\\\\nPaLM 540B & 34.8 (66.5) & \\textbf{24.6} (\\textbf{63.3}) & 44.9 (74.7) & 35.2 (\\textbf{72.5}) & 40.0 (67.8) & 29.6 (66.0) & 34.5 (68.4)\\\\\nOnline & \\textbf{39.7} (\\textbf{68.0}) & 21.9 (61.8) & \\textbf{50.4} (\\textbf{75.0}) & \\textbf{37.0} (72.2) & \\textbf{56.1} (\\textbf{72.0}) & \\textbf{39.9} (\\textbf{68.7}) & \\textbf{40.1} (\\textbf{69.6}) \\\\\n \\bottomrule\n\\end{tabular}\n\\caption{FRMT per-bucket test set results, in the format: BLEU (BLEURT). The ``FRMT'' score is the geometric mean across regions of the arithmetic mean across buckets.}\n\\label{tab:results_auto}\n\\end{table*}", "cell_list_gold": [{"value": "37.4", "char_index": [270, 274], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "32.7", "char_index": [284, 288], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "46.7", "char_index": [298, 302], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "40.8", "char_index": [312, 316], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "39.8", "char_index": [326, 330], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "35.3", "char_index": [340, 344], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "38.7", "char_index": [354, 358], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "46.7", "char_index": [376, 380], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "32.7", "char_index": [390, 394], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "53.5", "char_index": [404, 408], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "45.4", "char_index": [418, 422], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "43.1", "char_index": [432, 436], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "32.9", "char_index": [446, 450], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "42.0", "char_index": [460, 464], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "54.1", "char_index": [487, 491], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "36.9", "char_index": [501, 505], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "56.9", "char_index": [515, 519], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "47.3", "char_index": [529, 533], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "56.1", "char_index": [543, 547], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "41.0", "char_index": [557, 561], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "48.2", "char_index": [571, 575], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "45.5", "char_index": [601, 605], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "32.5", "char_index": [615, 619], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "48.6", "char_index": [629, 633], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "40.7", "char_index": [643, 647], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "48.1", "char_index": [657, 661], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "36.9", "char_index": [671, 675], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "41.7", "char_index": [685, 689], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "38.6", "char_index": [709, 713], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "26.7", "char_index": [723, 727], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "45.9", "char_index": [737, 741], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "38.0", "char_index": [751, 755], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "39.3", "char_index": [765, 769], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "32.1", "char_index": [779, 783], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "36.5", "char_index": [793, 797], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "49.5", "char_index": [818, 822], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "36.7", "char_index": [832, 836], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "55.4", "char_index": [846, 850], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "46.1", "char_index": [860, 864], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "50.3", "char_index": [874, 878], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "41.5", "char_index": [888, 892], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "46.3", "char_index": [902, 906], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "53.7", "char_index": [928, 932], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "40.1", "char_index": [950, 954], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "59.0", "char_index": [982, 986], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "49.5", "char_index": [1014, 1018], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "54.8", "char_index": [1038, 1042], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "45.6", "char_index": [1060, 1064], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "50.2", "char_index": [1092, 1096], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "56.2", "char_index": [1124, 1128], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "35.6", "char_index": [1148, 1152], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "56.3", "char_index": [1162, 1166], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "46.9", "char_index": [1185, 1189], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "65.2", "char_index": [1207, 1211], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "42.9", "char_index": [1231, 1235], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "49.8", "char_index": [1245, 1249], "type": "Result", "training data/set": "xx", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "22.6", "char_index": [1346, 1350], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "13.8", "char_index": [1360, 1364], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "26.7", "char_index": [1374, 1378], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "19.5", "char_index": [1388, 1392], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "26.4", "char_index": [1402, 1406], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "20.4", "char_index": [1416, 1420], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "21.3", "char_index": [1430, 1434], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "33.3", "char_index": [1452, 1456], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "18.9", "char_index": [1466, 1470], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "43.2", "char_index": [1480, 1484], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "31.4", "char_index": [1494, 1498], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "40.8", "char_index": [1508, 1512], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "30.8", "char_index": [1522, 1526], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "32.5", "char_index": [1536, 1540], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "33.3", "char_index": [1563, 1567], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "18.3", "char_index": [1577, 1581], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "44.2", "char_index": [1591, 1595], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "32.0", "char_index": [1605, 1609], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "43.7", "char_index": [1619, 1623], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "32.2", "char_index": [1633, 1637], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "33.3", "char_index": [1647, 1651], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "33.8", "char_index": [1677, 1681], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "18.8", "char_index": [1691, 1695], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "44.8", "char_index": [1705, 1709], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "31.6", "char_index": [1719, 1723], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "42.7", "char_index": [1733, 1737], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "31.5", "char_index": [1747, 1751], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "33.2", "char_index": [1761, 1765], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "17.6", "char_index": [1785, 1789], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "13.3", "char_index": [1799, 1803], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "28.1", "char_index": [1813, 1817], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "24.4", "char_index": [1827, 1831], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "21.6", "char_index": [1841, 1845], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "18.2", "char_index": [1855, 1859], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "20.4", "char_index": [1869, 1873], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "29.2", "char_index": [1894, 1898], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "20.4", "char_index": [1908, 1912], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "40.2", "char_index": [1922, 1926], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "33.0", "char_index": [1936, 1940], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "34.5", "char_index": [1950, 1954], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "26.0", "char_index": [1964, 1968], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "30.3", "char_index": [1978, 1982], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "34.8", "char_index": [2004, 2008], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "24.6", "char_index": [2026, 2030], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "44.9", "char_index": [2050, 2054], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "35.2", "char_index": [2064, 2068], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "40.0", "char_index": [2087, 2091], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "29.6", "char_index": [2101, 2105], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "34.5", "char_index": [2115, 2119], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "39.7", "char_index": [2146, 2150], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "21.9", "char_index": [2170, 2174], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "50.4", "char_index": [2192, 2196], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "37.0", "char_index": [2224, 2228], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "56.1", "char_index": [2247, 2251], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "39.9", "char_index": [2279, 2283], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "40.1", "char_index": [2311, 2315], "type": "Result", "training data/set": "xx", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}]}, "2210.00193v1_table3": {"table_code": "\\begin{table}\n\\centering\n\\footnotesize\n\\begin{tabular}{lcc}\n\\toprule\n\\textbf{Model} & \\textbf{pt} & \\textbf{zh} \\\\\n\\midrule\nGold & 98.6 & 94.4 \\\\\n\\midrule\nUR & 50.4 & 50.6 \\\\\nM4-UR & 51.2 & 50.9 \\\\\nM4-Prompts & 66.7 & 50.0 \\\\\nM4-Prompts FT & 66.7 & 51.0 \\\\\nPaLM 8B & 85.0 & 69.0 \\\\\nPaLM 62B & 90.4 & 70.8 \\\\\nPaLM 540B & \\textbf{93.2} & \\textbf{83.6} \\\\\nOnline & 50.0 & 50.0 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Lexical accuracy on FRMT test. PaLM outperforms other approaches, while region-agnostic models like Online are guaranteed 50\\%.}\n\\label{tab:lexical_accuracy}\n\\end{table}", "table_label": "{tab:lexical_accuracy}", "table_numeric_cells": [["98.6", "98.6", 131, 135, 131, 135], ["94.4", "94.4", 138, 142, 138, 142], ["50.4", "50.4", 160, 164, 160, 164], ["50.6", "50.6", 167, 171, 167, 171], ["51.2", "51.2", 183, 187, 183, 187], ["50.9", "50.9", 190, 194, 190, 194], ["66.7", "66.7", 211, 215, 211, 215], ["50.0", "50.0", 218, 222, 218, 222], ["66.7", "66.7", 242, 246, 242, 246], ["51.0", "51.0", 249, 253, 249, 253], ["85.0", "85.0", 267, 271, 267, 271], ["69.0", "69.0", 274, 278, 274, 278], ["90.4", "90.4", 293, 297, 293, 297], ["70.8", "70.8", 300, 304, 300, 304], ["93.2", "\\textbf{93.2}", 328, 332, 320, 333], ["83.6", "\\textbf{83.6}", 344, 348, 336, 349], ["50.0", "50.0", 362, 366, 362, 366], ["50.0", "50.0", 369, 373, 369, 373]], "text_chunk_selected": "\\section{Evaluation Metrics}\\label{sec:metrics}\nWhile human judgments are the gold standard for evaluating machine-generated texts, collecting them can be difficult, time-consuming, and expensive. Therefore, generally the best practical method for fast iteration on benchmarks is the use of automatic metrics that are shown to correlate well with human judgments. We hypothesize that common reference-based MT evaluation metrics might have differing sensitivities to regional differences, and so we conduct a human evaluation of several baseline models (see \\S\\ref{sec:results_human}) and compute correlation of several automatic metrics with the human judgments. We also explore two additional metrics specifically for region-awareness: a modified version of our human metric, and an automatic lexical accuracy metric.\n\n\\subsection{Human evaluation}\\label{sec:human_eval_mqm}\nTo obtain the highest fidelity human ratings, we use the expert-based \nMultidimensional Quality Metrics (MQM) evaluation framework proposed by \\citet{Freitag2021ExpertsEA}. This is the same framework recommended by the WMT'21 Evaluation Campaign \\citep{freitag-etal-2021-results}. For our evaluation, expert raters are shown a chunk of 10 contiguous English sentences from our test set with the corresponding translations from one combination of model (or human) and target region. Raters then identify errors in the translations, assigning a category and severity to each; see \\citet{Freitag2021ExpertsEA} for details. Due to cost constraints, we evaluate 25\\%{} of the test set, evenly distributed across our three evaluation buckets. Each chunk is rated by three raters.\n\n\\subsection{Lexical accuracy}\\label{sec:lexical_accuracy}\nTo assess a model's ability to select lexical forms appropriate to the target region, we define a lexical accuracy metric.  As discussed in section \\S\\ref{sec:data_sampling}, sentences in our \\texttt{lexical} bucket are chosen from Wikipedia articles containing specific words we expect to have distinct translations in the regions in question.  For instance, we include source sentences from the English Wikipedia article ``Bus'' in the Portuguese \\texttt{lexical} bucket, as the word for bus is distinct in Brazil and Portugal (\\textit{\u00f4nibus} vs.~\\textit{autocarro}).  As the expected output forms are known ahead of time, we can directly measure the rate at which a model selects region-appropriate variants.\n\nIn Mandarin, there is an added complexity of script choice.  Mainland Mandarin is typically written in the ``simplified'' Han script, while Taiwan Mandarin is written in ``traditional'' script.  To disentangle lexical choice from script choice, we define lexical accuracy in a script-agnostic manner.  For example, for the word \\textit{pineapple}, if the target region is Taiwan, we count both the (expected) traditional and (unexpected) simplified forms of the Taiwan variant \\textit{f\u00e8ngl\u00ed} (\\trad{\u9cf3\u68a8} and \\simp{\u51e4\u68a8}) as correct, while counting both script forms of the Mainland variant \\textit{b\u014dlu\u00f3} (\\simp{\u83e0\u841d} and \\trad{\u83e0\u863f}) as incorrect.  This ensures that even if a model is outputting the wrong script, it will be rewarded or penalized based on its lexical choices. This also prevents the possibility of ``gaming'' our metric by only using the lexical forms and script of a single region.\n\nResearchers sharing FRMT results should \\textbf{report per-bucket BLEU and lexical accuracy} metrics on the test set, as shown in Tables \\ref{tab:results_auto} and \\ref{tab:lexical_accuracy}. These metrics are efficient to calculate with our provided evaluation scripts.\n\nOur next three baseline models are different-sized versions of PaLM \\citep{chowdhery2022palm}, a large language model that has demonstrated remarkable zero-shot and few-shot performance on a variety of tasks, often without needing any fine-tuning. Because it was pre-trained on a multilingual corpus, we include it as a baseline, serving as a representative of the class of recent large decoder-only causal language models including GPT-3 \\citep{brown-etal-22-gpt3} and OPT-175B \\citep{zhang-etal-22-opt}. We call these models \\textbf{PaLM 540B}, \\textbf{PaLM 62B}, and \\textbf{PaLM 8B}, referring to their approximate parameter counts. The prompt for these models is larger than for the M4 Prompts models and is exemplar-based. It begins with ``Translate the following texts from English to X'', where ``X'' is the name of the language variety. This is followed by ten exemplars selected randomly from the \\texttt{lexical} bucket,\\footnote{The model has a fixed input sequence length, which includes the prompt, and a fixed output sequence length. Through rejection sampling, we ensure that the ten exemplars are short enough to leave at least 128 tokens for the input text, to match the 128 tokens allotted to the output.} where each exemplar is put on two lines: the first line contains the English text, prefixed by ``English:'', and the second containing the translation in the target variety, prefixed by the variety's name. At the end of the prompt, we show the model the input text and the language variety prefix, and decode from the model, taking everything up to the next newline as the output (or the end of the line if no newline was generated).\n\nTable \\ref{tab:results_auto} shows the performance of our baseline models on the automated metrics BLEU and BLEURT\\@. The \\textbf{``FRMT'' metric} is a summary of per-language performance, calculated as the geometric mean across regions of the arithmetic mean across buckets. Recall, while BLEU scores serve as a lightweight point of comparison, we recommend additionally reporting the more costly BLEURT where possible, as this correlates more strongly with human judgments.\n\nTable \\ref{tab:lexical_accuracy} shows \\textbf{lexical accuracy} performance, assessing whether specific terms receive region-appropriate translations. Here, the PaLM models outperform alternatives by a wide margin. As even the smallest PaLM model has more than $2\\times$ the parameters of our trained baselines (3.7B parameters each), this suggests that model capacity is a key ingredient for learning to use region-specific terminology in a few-shot manner. However, there is still a significant gap compared to human performance.", "table_source": "\\begin{table}\n\\centering\n\\footnotesize\n\\begin{tabular}{lcc}\n\\toprule\n\\textbf{Model} & \\textbf{pt} & \\textbf{zh} \\\\\n\\midrule\nGold & 98.6 & 94.4 \\\\\n\\midrule\nUR & 50.4 & 50.6 \\\\\nM4-UR & 51.2 & 50.9 \\\\\nM4-Prompts & 66.7 & 50.0 \\\\\nM4-Prompts FT & 66.7 & 51.0 \\\\\nPaLM 8B & 85.0 & 69.0 \\\\\nPaLM 62B & 90.4 & 70.8 \\\\\nPaLM 540B & \\textbf{93.2} & \\textbf{83.6} \\\\\nOnline & 50.0 & 50.0 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Lexical accuracy on FRMT test. PaLM outperforms other approaches, while region-agnostic models like Online are guaranteed 50\\%.}\n\\label{tab:lexical_accuracy}\n\\end{table}", "cell_list_gold": [{"value": "98.6", "char_index": [131, 135], "type": "Result", "training data/set": "xx", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "Gold", "model settings": {"xx": "yy"}}, {"value": "94.4", "char_index": [138, 142], "type": "Result", "training data/set": "xx", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "Gold", "model settings": {"xx": "yy"}}, {"value": "50.4", "char_index": [160, 164], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "50.6", "char_index": [167, 171], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "51.2", "char_index": [183, 187], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "50.9", "char_index": [190, 194], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "66.7", "char_index": [211, 215], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "50.0", "char_index": [218, 222], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "66.7", "char_index": [242, 246], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "51.0", "char_index": [249, 253], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "85.0", "char_index": [267, 271], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "69.0", "char_index": [274, 278], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "90.4", "char_index": [293, 297], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "70.8", "char_index": [300, 304], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "93.2", "char_index": [328, 332], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "83.6", "char_index": [344, 348], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "50.0", "char_index": [362, 366], "type": "Result", "training data/set": "xx", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "50.0", "char_index": [369, 373], "type": "Result", "training data/set": "xx", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "lexical accuracy", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}]}, "2210.00193v1_table4": {"table_code": "\\begin{table*}\n\\centering\n\\footnotesize\n\\begin{tabular}{lcccccc|c}\n\\toprule\n& \\multicolumn{2}{c}{\\bf Lexical} & \\multicolumn{2}{c}{\\bf Entities} & \\multicolumn{2}{c}{\\bf Random} & \\bf $\\Delta$FRMT \\\\\n\\bf Model & pt-BR & pt-PT & pt-BR & pt-PT & pt-BR & pt-PT & pt\\\\\\midrule\nUR & 8.4 (2.7) & \\scalebox{0.7}[1.0]{$-$}7.7 (\\scalebox{0.7}[1.0]{$-$}2.5) & 6.9 (1.4) & \\scalebox{0.7}[1.0]{$-$}6.3 (\\scalebox{0.7}[1.0]{$-$}1.3) & 5.2 (0.3) & \\scalebox{0.7}[1.0]{$-$}4.3 (\\scalebox{0.7}[1.0]{$-$}0.0) & 0.2 (0.1)\\\\\nM4 UR & 13.9 (5.2) & \\scalebox{0.7}[1.0]{$-$}13.0 (\\scalebox{0.7}[1.0]{$-$}5.0) & 8.2 (2.5) & \\scalebox{0.7}[1.0]{$-$}7.9 (\\scalebox{0.7}[1.0]{$-$}2.4) & 9.7 (2.9) & \\scalebox{0.7}[1.0]{$-$}9.3 (\\scalebox{0.7}[1.0]{$-$}2.8) & 0.2 (0.1)\\\\\nM4 Prompts & 19.5 (5.7) & \\scalebox{0.7}[1.0]{$-$}13.6 (\\scalebox{0.7}[1.0]{$-$}3.2) & 10.5 (2.9) & \\scalebox{0.7}[1.0]{$-$}7.8 (\\scalebox{0.7}[1.0]{$-$}2.3) & 15.6 (3.4) & \\scalebox{0.7}[1.0]{$-$}12.6 (\\scalebox{0.7}[1.0]{$-$}2.7) & 1.9 (0.6)\\\\\nM4 Prompts FT & 14.8 (4.9) & \\scalebox{0.7}[1.0]{$-$}9.8 (\\scalebox{0.7}[1.0]{$-$}2.8) & 8.7 (2.4) & \\scalebox{0.7}[1.0]{$-$}6.4 (\\scalebox{0.7}[1.0]{$-$}2.0) & 11.8 (2.8) & \\scalebox{0.7}[1.0]{$-$}9.2 (\\scalebox{0.7}[1.0]{$-$}2.2) & 1.6 (0.5)\\\\\nPaLM 8B & 13.6 (5.1) & \\scalebox{0.7}[1.0]{$-$}5.4 (\\scalebox{0.7}[1.0]{$-$}1.8) & 8.6 (2.7) & \\scalebox{0.7}[1.0]{$-$}3.3 (\\scalebox{0.7}[1.0]{$-$}1.6) & 7.6 (1.7) & \\scalebox{0.7}[1.0]{$-$}2.9 (\\scalebox{0.7}[1.0]{$-$}0.7) & 2.8 (0.9)\\\\\nPaLM 62B & 18.0 (6.2) & 0.3 (0.5) & 11.5 (3.4) & 0.3 (\\scalebox{0.7}[1.0]{$-$}0.6) & 11.5 (2.5) & \\scalebox{0.7}[1.0]{$-$}0.9 (\\scalebox{0.7}[1.0]{$-$}0.4) & 6.5 (1.9)\\\\\nPaLM 540B & 20.7 (6.4) & 0.2 (0.9) & 13.4 (3.7) & 0.2 (\\scalebox{0.7}[1.0]{$-$}0.5) & 13.1 (2.9) & \\scalebox{0.7}[1.0]{$-$}0.1 (0.0) & 7.7 (2.2)\\\\\nOnline & 20.6 (6.4) & \\scalebox{0.7}[1.0]{$-$}20.6 (\\scalebox{0.7}[1.0]{$-$}6.4) & 9.5 (2.8) & \\scalebox{0.7}[1.0]{$-$}9.5 (\\scalebox{0.7}[1.0]{$-$}2.8) & 22.3 (5.5) & \\scalebox{0.7}[1.0]{$-$}22.3 (\\scalebox{0.7}[1.0]{$-$}5.5) & 0.0 (0.0)\\\\\n\\midrule\n& zh-CN & zh-TW & zh-CN & zh-TW & zh-CN & zh-TW & zh\\\\\\midrule\nUR & 4.4 (1.1) & \\scalebox{0.7}[1.0]{$-$}3.4 (\\scalebox{0.7}[1.0]{$-$}1.0) & 5.0 (1.8) & \\scalebox{0.7}[1.0]{$-$}4.9 (\\scalebox{0.7}[1.0]{$-$}1.8) & 3.8 (0.9) & \\scalebox{0.7}[1.0]{$-$}3.0 (\\scalebox{0.7}[1.0]{$-$}0.8) & 0.3 (0.0)\\\\\nM4 UR & 14.5 (7.0) & \\scalebox{0.7}[1.0]{$-$}14.4 (\\scalebox{0.7}[1.0]{$-$}7.0) & 11.7 (2.5) & \\scalebox{0.7}[1.0]{$-$}11.5 (\\scalebox{0.7}[1.0]{$-$}2.5) & 10.0 (1.9) & \\scalebox{0.7}[1.0]{$-$}10.1 (\\scalebox{0.7}[1.0]{$-$}1.9) & 0.0 (0.0)\\\\\nM4 Prompts & 14.4 (5.6) & \\scalebox{0.7}[1.0]{$-$}14.4 (\\scalebox{0.7}[1.0]{$-$}5.8) & 11.7 (2.6) & \\scalebox{0.7}[1.0]{$-$}10.9 (\\scalebox{0.7}[1.0]{$-$}2.7) & 10.7 (2.1) & \\scalebox{0.7}[1.0]{$-$}10.1 (\\scalebox{0.7}[1.0]{$-$}1.6) & 0.1 (0.0)\\\\\nM4 Prompts FT & 14.2 (5.8) & \\scalebox{0.7}[1.0]{$-$}13.5 (\\scalebox{0.7}[1.0]{$-$}5.8) & 11.9 (2.7) & \\scalebox{0.7}[1.0]{$-$}11.1 (\\scalebox{0.7}[1.0]{$-$}2.6) & 9.9 (1.5) & \\scalebox{0.7}[1.0]{$-$}9.6 (\\scalebox{0.7}[1.0]{$-$}1.6) & 0.1 (\\scalebox{0.7}[1.0]{$-$}0.0)\\\\\nPaLM 8B & 6.2 (4.1) & \\scalebox{0.7}[1.0]{$-$}2.3 (\\scalebox{0.7}[1.0]{$-$}2.3) & 4.0 (1.0) & \\scalebox{0.7}[1.0]{$-$}0.7 (\\scalebox{0.7}[1.0]{$-$}0.9) & 4.5 (0.5) & \\scalebox{0.7}[1.0]{$-$}1.0 (0.1) & 1.7 (0.4)\\\\\nPaLM 62B & 12.3 (3.8) & \\scalebox{0.7}[1.0]{$-$}2.9 (\\scalebox{0.7}[1.0]{$-$}0.7) & 8.5 (1.8) & \\scalebox{0.7}[1.0]{$-$}3.0 (\\scalebox{0.7}[1.0]{$-$}1.0) & 8.7 (1.5) & \\scalebox{0.7}[1.0]{$-$}2.6 (0.2) & 3.3 (0.9)\\\\\nPaLM 540B & 15.0 (5.0) & \\scalebox{0.7}[1.0]{$-$}0.5 (1.0) & 10.2 (2.2) & \\scalebox{0.7}[1.0]{$-$}3.3 (\\scalebox{0.7}[1.0]{$-$}0.8) & 9.9 (2.1) & \\scalebox{0.7}[1.0]{$-$}1.4 (0.2) & 4.7 (1.6)\\\\\nOnline & 17.8 (6.2) & \\scalebox{0.7}[1.0]{$-$}17.8 (\\scalebox{0.7}[1.0]{$-$}6.2) & 13.4 (2.8) & \\scalebox{0.7}[1.0]{$-$}13.4 (\\scalebox{0.7}[1.0]{$-$}2.8) & 16.3 (3.3) & \\scalebox{0.7}[1.0]{$-$}16.3 (\\scalebox{0.7}[1.0]{$-$}3.3) & 0.0 (0.0)\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{FRMT test set deltas between matched and mismatched references, shown in the format: $\\Delta$BLEU~($\\Delta$BLEURT)\\@. Negative numbers in a minority region column indicate that the model achieved a higher score when evaluated with respect to the majority region, despite being asked to output for the minority region. The last column shows deltas between FRMT scores evaluated with respect to matched vs.~mismatched references.}\n\\label{tab:mismatched_references}\n\\end{table*}", "table_label": "{tab:mismatched_references}", "table_numeric_cells": [["8.4", "8.4 (2.7)", 278, 281, 278, 287], ["6.9", "6.9 (1.4)", 350, 353, 350, 359], ["5.2", "5.2 (0.3)", 422, 425, 422, 431], ["0.2", "0.2 (0.1)", 494, 497, 494, 503], ["13.9", "13.9 (5.2)", 514, 518, 514, 524], ["8.2", "8.2 (2.5)", 588, 591, 588, 597], ["9.7", "9.7 (2.9)", 660, 663, 660, 669], ["0.2", "0.2 (0.1)", 732, 735, 732, 741], ["19.5", "19.5 (5.7)", 757, 761, 757, 767], ["10.5", "10.5 (2.9)", 831, 835, 831, 841], ["15.6", "15.6 (3.4)", 904, 908, 904, 914], ["1.9", "1.9 (0.6)", 978, 981, 978, 987], ["14.8", "14.8 (4.9)", 1006, 1010, 1006, 1016], ["8.7", "8.7 (2.4)", 1079, 1082, 1079, 1088], ["11.8", "11.8 (2.8)", 1151, 1155, 1151, 1161], ["1.6", "1.6 (0.5)", 1224, 1227, 1224, 1233], ["13.6", "13.6 (5.1)", 1246, 1250, 1246, 1256], ["8.6", "8.6 (2.7)", 1319, 1322, 1319, 1328], ["7.6", "7.6 (1.7)", 1391, 1394, 1391, 1400], ["2.8", "2.8 (0.9)", 1463, 1466, 1463, 1472], ["18.0", "18.0 (6.2)", 1486, 1490, 1486, 1496], ["0.3", "0.3 (0.5)", 1499, 1502, 1499, 1508], ["11.5", "11.5 (3.4)", 1511, 1515, 1511, 1521], ["0.3", "0.3 (\\scalebox{0.7}[1.0]{$-$}0.6)", 1524, 1527, 1524, 1557], ["11.5", "11.5 (2.5)", 1560, 1564, 1560, 1570], ["6.5", "6.5 (1.9)", 1633, 1636, 1633, 1642], ["20.7", "20.7 (6.4)", 1657, 1661, 1657, 1667], ["0.2", "0.2 (0.9)", 1670, 1673, 1670, 1679], ["13.4", "13.4 (3.7)", 1682, 1686, 1682, 1692], ["0.2", "0.2 (\\scalebox{0.7}[1.0]{$-$}0.5)", 1695, 1698, 1695, 1728], ["13.1", "13.1 (2.9)", 1731, 1735, 1731, 1741], ["7.7", "7.7 (2.2)", 1780, 1783, 1780, 1789], ["20.6", "20.6 (6.4)", 1801, 1805, 1801, 1811], ["9.5", "9.5 (2.8)", 1875, 1878, 1875, 1884], ["22.3", "22.3 (5.5)", 1947, 1951, 1947, 1957], ["0.0", "0.0 (0.0)", 2021, 2024, 2021, 2030], ["4.4", "4.4 (1.1)", 2110, 2113, 2110, 2119], ["5.0", "5.0 (1.8)", 2182, 2185, 2182, 2191], ["3.8", "3.8 (0.9)", 2254, 2257, 2254, 2263], ["0.3", "0.3 (0.0)", 2326, 2329, 2326, 2335], ["14.5", "14.5 (7.0)", 2346, 2350, 2346, 2356], ["11.7", "11.7 (2.5)", 2420, 2424, 2420, 2430], ["10.0", "10.0 (1.9)", 2494, 2498, 2494, 2504], ["0.0", "0.0 (0.0)", 2568, 2571, 2568, 2577], ["14.4", "14.4 (5.6)", 2593, 2597, 2593, 2603], ["11.7", "11.7 (2.6)", 2667, 2671, 2667, 2677], ["10.7", "10.7 (2.1)", 2741, 2745, 2741, 2751], ["0.1", "0.1 (0.0)", 2815, 2818, 2815, 2824], ["14.2", "14.2 (5.8)", 2843, 2847, 2843, 2853], ["11.9", "11.9 (2.7)", 2917, 2921, 2917, 2927], ["9.9", "9.9 (1.5)", 2991, 2994, 2991, 3000], ["0.1", "0.1 (\\scalebox{0.7}[1.0]{$-$}0.0)", 3063, 3066, 3063, 3096], ["6.2", "6.2 (4.1)", 3109, 3112, 3109, 3118], ["4.0", "4.0 (1.0)", 3181, 3184, 3181, 3190], ["4.5", "4.5 (0.5)", 3253, 3256, 3253, 3262], ["1.7", "1.7 (0.4)", 3301, 3304, 3301, 3310], ["12.3", "12.3 (3.8)", 3324, 3328, 3324, 3334], ["8.5", "8.5 (1.8)", 3397, 3400, 3397, 3406], ["8.7", "8.7 (1.5)", 3469, 3472, 3469, 3478], ["3.3", "3.3 (0.9)", 3517, 3520, 3517, 3526], ["15.0", "15.0 (5.0)", 3541, 3545, 3541, 3551], ["10.2", "10.2 (2.2)", 3590, 3594, 3590, 3600], ["9.9", "9.9 (2.1)", 3663, 3666, 3663, 3672], ["4.7", "4.7 (1.6)", 3711, 3714, 3711, 3720], ["17.8", "17.8 (6.2)", 3732, 3736, 3732, 3742], ["13.4", "13.4 (2.8)", 3806, 3810, 3806, 3816], ["16.3", "16.3 (3.3)", 3880, 3884, 3880, 3890], ["0.0", "0.0 (0.0)", 3954, 3957, 3954, 3963]], "text_chunk_selected": "Popular shared tasks have not included region-targeted translation either:\nThe Workshop on Machine Translation (WMT) has included translation between similar languages \\cite[e.g.][]{akhbardeh-etal-2021-findings}, while the Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial) focuses mainly on classification and not translation \\cite[e.g.][]{vardial-2021-nlp}.\n\n\\textbf{BLEU \\citep{papineni-etal-2002-bleu}:} This metric is based on token $n$-gram precision. We use \\texttt{sacrebleu.corpus\\_bleu} from \\citet{post-2018-call}.\n\n\\textbf{chrF \\citep{popovic-2015-chrf}:} This metric is based on F1 using character $n$-grams. We use \\texttt{sacrebleu.corpus\\_chrf} from \\citet{post-2018-call}.\n\nTo derive a score that is more sensitive to the features of regional varieties, we filter MQM error categories to those most indicative of region mismatch.  Specifically, for each language (Portuguese and Mandarin), we train a logistic regression model over ratings of gold translations to predict whether the rater's region is matched (0) or mismatched (1) to the translator's region.  The model uses error types as features, and assigns positive coefficients to errors that are associated with region mismatch.  We designate features as ``mismatch predictive'' if they have both a positive coefficient and a significant $p$-value (<0.05).\\footnote{We believe a priori that no error type should be robustly indicative of \\emph{matching} region, as having a matched translator and rater should ideally result in no errors.}  Having identified these error types ($8$ in Portuguese, and $7$ in Mandarin), we run a second regression restricted to just these features.  We calculate ``Reweighted MQM'' (R-MQM) as the probability this model assigns to the mismatch category, giving scores in the range [$0$,$1$], as opposed to raw MQM scores which can cover [$0$,$\\infty$] if arbitrarily many errors are found.\n\nWe calculate the metric over all model outputs for the \\texttt{lexical} bucket, covering both regional outputs of a given language. For each term we calculate the number of sentences containing a matched variant and the number of sentences containing a mismatched variant; the sum of these over all terms are $N_{match}$ and $N_{mismatch}$, respectively. From this, we calculate the model's lexical accuracy (LA) score for the given language as:\n\nFigure \\ref{fig:mqm_scores} presents human evaluation metrics for our baseline models on the 25\\% sample of our test set described in \\S\\ref{sec:translate_quality_metrics}. We first consider gold translations: in order to validate that our process for collecting reference translations and human ratings elicited the sought-after linguistic variance, we can examine whether translations by raters in one speaker group were consistently rated higher by other raters in that same group (the ``matched'' case) than by raters in the alternative speaker group (the ``mismatched'' case). Indeed, we do see that this is the case; on average, matched reference/rater pairs received an MQM penalty of $1.73$, which is significantly lower (better) than the MQM penalty of $3.55$ received on average by mismatched pairs, as measured by a paired $t$-test ($t=-3.34; p<0.001$). This effect is strongest in the \\texttt{lexical} bucket, presumably due to the high rate of region-distinct terms in these sentences.\\footnote{We note that ratings tend to be somewhat better in the \\texttt{entity} bucket, across both matched and mismatched settings.  This could indicate that professional translators are less prone to errors when the source is ``culturally relevant'', e.g.~when translating sentences about an entity native to the area where the target language is spoken.}\n\nFor Mandarin (on the full 25\\% sample), a large gap remains between expert translations and our baselines (Gold: $2.5$ vs.~PaLM: $8.8$).  Our results indicate that better region handling will be needed to close this gap; our baselines are not effectively leveraging region information (as evidenced by small match vs.~mismatch deltas), and even the best-case expert translation scores poorly in the mismatched region (Gold match: $2.5$ vs.~Gold mismatch: $5.2$).\\footnote{Note, this is ignoring orthographic differences, as we transliterate outputs to the rater's script.  See section \\S\\ref{sec:script_analysis} for analysis of script differences.}\n\n\\subsection{Mismatched references}\\label{mismatched_references}\nAll else being equal, a model that has perfectly solved the FRMT task should achieve higher performance when evaluated against a reference in the target region than a reference from a different region. To measure the extent to which this holds for our baseline models, we show the delta between matched and mismatched reference on the test set in Table \\ref{tab:mismatched_references}.", "table_source": "\\begin{table*}\n\\centering\n\\footnotesize\n\\begin{tabular}{lcccccc|c}\n\\toprule\n& \\multicolumn{2}{c}{\\bf Lexical} & \\multicolumn{2}{c}{\\bf Entities} & \\multicolumn{2}{c}{\\bf Random} & \\bf $\\Delta$FRMT \\\\\n\\bf Model & pt-BR & pt-PT & pt-BR & pt-PT & pt-BR & pt-PT & pt\\\\\\midrule\nUR & 8.4 (2.7) & \\scalebox{0.7}[1.0]{$-$}7.7 (\\scalebox{0.7}[1.0]{$-$}2.5) & 6.9 (1.4) & \\scalebox{0.7}[1.0]{$-$}6.3 (\\scalebox{0.7}[1.0]{$-$}1.3) & 5.2 (0.3) & \\scalebox{0.7}[1.0]{$-$}4.3 (\\scalebox{0.7}[1.0]{$-$}0.0) & 0.2 (0.1)\\\\\nM4 UR & 13.9 (5.2) & \\scalebox{0.7}[1.0]{$-$}13.0 (\\scalebox{0.7}[1.0]{$-$}5.0) & 8.2 (2.5) & \\scalebox{0.7}[1.0]{$-$}7.9 (\\scalebox{0.7}[1.0]{$-$}2.4) & 9.7 (2.9) & \\scalebox{0.7}[1.0]{$-$}9.3 (\\scalebox{0.7}[1.0]{$-$}2.8) & 0.2 (0.1)\\\\\nM4 Prompts & 19.5 (5.7) & \\scalebox{0.7}[1.0]{$-$}13.6 (\\scalebox{0.7}[1.0]{$-$}3.2) & 10.5 (2.9) & \\scalebox{0.7}[1.0]{$-$}7.8 (\\scalebox{0.7}[1.0]{$-$}2.3) & 15.6 (3.4) & \\scalebox{0.7}[1.0]{$-$}12.6 (\\scalebox{0.7}[1.0]{$-$}2.7) & 1.9 (0.6)\\\\\nM4 Prompts FT & 14.8 (4.9) & \\scalebox{0.7}[1.0]{$-$}9.8 (\\scalebox{0.7}[1.0]{$-$}2.8) & 8.7 (2.4) & \\scalebox{0.7}[1.0]{$-$}6.4 (\\scalebox{0.7}[1.0]{$-$}2.0) & 11.8 (2.8) & \\scalebox{0.7}[1.0]{$-$}9.2 (\\scalebox{0.7}[1.0]{$-$}2.2) & 1.6 (0.5)\\\\\nPaLM 8B & 13.6 (5.1) & \\scalebox{0.7}[1.0]{$-$}5.4 (\\scalebox{0.7}[1.0]{$-$}1.8) & 8.6 (2.7) & \\scalebox{0.7}[1.0]{$-$}3.3 (\\scalebox{0.7}[1.0]{$-$}1.6) & 7.6 (1.7) & \\scalebox{0.7}[1.0]{$-$}2.9 (\\scalebox{0.7}[1.0]{$-$}0.7) & 2.8 (0.9)\\\\\nPaLM 62B & 18.0 (6.2) & 0.3 (0.5) & 11.5 (3.4) & 0.3 (\\scalebox{0.7}[1.0]{$-$}0.6) & 11.5 (2.5) & \\scalebox{0.7}[1.0]{$-$}0.9 (\\scalebox{0.7}[1.0]{$-$}0.4) & 6.5 (1.9)\\\\\nPaLM 540B & 20.7 (6.4) & 0.2 (0.9) & 13.4 (3.7) & 0.2 (\\scalebox{0.7}[1.0]{$-$}0.5) & 13.1 (2.9) & \\scalebox{0.7}[1.0]{$-$}0.1 (0.0) & 7.7 (2.2)\\\\\nOnline & 20.6 (6.4) & \\scalebox{0.7}[1.0]{$-$}20.6 (\\scalebox{0.7}[1.0]{$-$}6.4) & 9.5 (2.8) & \\scalebox{0.7}[1.0]{$-$}9.5 (\\scalebox{0.7}[1.0]{$-$}2.8) & 22.3 (5.5) & \\scalebox{0.7}[1.0]{$-$}22.3 (\\scalebox{0.7}[1.0]{$-$}5.5) & 0.0 (0.0)\\\\\n\\midrule\n& zh-CN & zh-TW & zh-CN & zh-TW & zh-CN & zh-TW & zh\\\\\\midrule\nUR & 4.4 (1.1) & \\scalebox{0.7}[1.0]{$-$}3.4 (\\scalebox{0.7}[1.0]{$-$}1.0) & 5.0 (1.8) & \\scalebox{0.7}[1.0]{$-$}4.9 (\\scalebox{0.7}[1.0]{$-$}1.8) & 3.8 (0.9) & \\scalebox{0.7}[1.0]{$-$}3.0 (\\scalebox{0.7}[1.0]{$-$}0.8) & 0.3 (0.0)\\\\\nM4 UR & 14.5 (7.0) & \\scalebox{0.7}[1.0]{$-$}14.4 (\\scalebox{0.7}[1.0]{$-$}7.0) & 11.7 (2.5) & \\scalebox{0.7}[1.0]{$-$}11.5 (\\scalebox{0.7}[1.0]{$-$}2.5) & 10.0 (1.9) & \\scalebox{0.7}[1.0]{$-$}10.1 (\\scalebox{0.7}[1.0]{$-$}1.9) & 0.0 (0.0)\\\\\nM4 Prompts & 14.4 (5.6) & \\scalebox{0.7}[1.0]{$-$}14.4 (\\scalebox{0.7}[1.0]{$-$}5.8) & 11.7 (2.6) & \\scalebox{0.7}[1.0]{$-$}10.9 (\\scalebox{0.7}[1.0]{$-$}2.7) & 10.7 (2.1) & \\scalebox{0.7}[1.0]{$-$}10.1 (\\scalebox{0.7}[1.0]{$-$}1.6) & 0.1 (0.0)\\\\\nM4 Prompts FT & 14.2 (5.8) & \\scalebox{0.7}[1.0]{$-$}13.5 (\\scalebox{0.7}[1.0]{$-$}5.8) & 11.9 (2.7) & \\scalebox{0.7}[1.0]{$-$}11.1 (\\scalebox{0.7}[1.0]{$-$}2.6) & 9.9 (1.5) & \\scalebox{0.7}[1.0]{$-$}9.6 (\\scalebox{0.7}[1.0]{$-$}1.6) & 0.1 (\\scalebox{0.7}[1.0]{$-$}0.0)\\\\\nPaLM 8B & 6.2 (4.1) & \\scalebox{0.7}[1.0]{$-$}2.3 (\\scalebox{0.7}[1.0]{$-$}2.3) & 4.0 (1.0) & \\scalebox{0.7}[1.0]{$-$}0.7 (\\scalebox{0.7}[1.0]{$-$}0.9) & 4.5 (0.5) & \\scalebox{0.7}[1.0]{$-$}1.0 (0.1) & 1.7 (0.4)\\\\\nPaLM 62B & 12.3 (3.8) & \\scalebox{0.7}[1.0]{$-$}2.9 (\\scalebox{0.7}[1.0]{$-$}0.7) & 8.5 (1.8) & \\scalebox{0.7}[1.0]{$-$}3.0 (\\scalebox{0.7}[1.0]{$-$}1.0) & 8.7 (1.5) & \\scalebox{0.7}[1.0]{$-$}2.6 (0.2) & 3.3 (0.9)\\\\\nPaLM 540B & 15.0 (5.0) & \\scalebox{0.7}[1.0]{$-$}0.5 (1.0) & 10.2 (2.2) & \\scalebox{0.7}[1.0]{$-$}3.3 (\\scalebox{0.7}[1.0]{$-$}0.8) & 9.9 (2.1) & \\scalebox{0.7}[1.0]{$-$}1.4 (0.2) & 4.7 (1.6)\\\\\nOnline & 17.8 (6.2) & \\scalebox{0.7}[1.0]{$-$}17.8 (\\scalebox{0.7}[1.0]{$-$}6.2) & 13.4 (2.8) & \\scalebox{0.7}[1.0]{$-$}13.4 (\\scalebox{0.7}[1.0]{$-$}2.8) & 16.3 (3.3) & \\scalebox{0.7}[1.0]{$-$}16.3 (\\scalebox{0.7}[1.0]{$-$}3.3) & 0.0 (0.0)\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{FRMT test set deltas between matched and mismatched references, shown in the format: $\\Delta$BLEU~($\\Delta$BLEURT)\\@. Negative numbers in a minority region column indicate that the model achieved a higher score when evaluated with respect to the majority region, despite being asked to output for the minority region. The last column shows deltas between FRMT scores evaluated with respect to matched vs.~mismatched references.}\n\\label{tab:mismatched_references}\n\\end{table*}", "cell_list_gold": [{"value": "8.4", "char_index": [278, 281], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "6.9", "char_index": [350, 353], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "5.2", "char_index": [422, 425], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "0.2", "char_index": [494, 497], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "13.9", "char_index": [514, 518], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "8.2", "char_index": [588, 591], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "9.7", "char_index": [660, 663], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "0.2", "char_index": [732, 735], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "19.5", "char_index": [757, 761], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "10.5", "char_index": [831, 835], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "15.6", "char_index": [904, 908], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "1.9", "char_index": [978, 981], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "14.8", "char_index": [1006, 1010], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "8.7", "char_index": [1079, 1082], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "11.8", "char_index": [1151, 1155], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "1.6", "char_index": [1224, 1227], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "13.6", "char_index": [1246, 1250], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "8.6", "char_index": [1319, 1322], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "7.6", "char_index": [1391, 1394], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "2.8", "char_index": [1463, 1466], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "18.0", "char_index": [1486, 1490], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [1499, 1502], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "11.5", "char_index": [1511, 1515], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [1524, 1527], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "11.5", "char_index": [1560, 1564], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "6.5", "char_index": [1633, 1636], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "20.7", "char_index": [1657, 1661], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.2", "char_index": [1670, 1673], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "13.4", "char_index": [1682, 1686], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.2", "char_index": [1695, 1698], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "13.1", "char_index": [1731, 1735], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "7.7", "char_index": [1780, 1783], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "20.6", "char_index": [1801, 1805], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "9.5", "char_index": [1875, 1878], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "22.3", "char_index": [1947, 1951], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [2021, 2024], "type": "Result", "training data/set": "xx", "test data/set": "FRMT pt", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "4.4", "char_index": [2110, 2113], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "5.0", "char_index": [2182, 2185], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "3.8", "char_index": [2254, 2257], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [2326, 2329], "type": "Result", "training data/set": "mC4 OPUS", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "UR", "model settings": {"xx": "yy"}}, {"value": "14.5", "char_index": [2346, 2350], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "11.7", "char_index": [2420, 2424], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "10.0", "char_index": [2494, 2498], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [2568, 2571], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-UR", "model settings": {"xx": "yy"}}, {"value": "14.4", "char_index": [2593, 2597], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "11.7", "char_index": [2667, 2671], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "10.7", "char_index": [2741, 2745], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "0.1", "char_index": [2815, 2818], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts", "model settings": {"xx": "yy"}}, {"value": "14.2", "char_index": [2843, 2847], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "11.9", "char_index": [2917, 2921], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "9.9", "char_index": [2991, 2994], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "0.1", "char_index": [3063, 3066], "type": "Result", "training data/set": "a mixture of monolingual and parallel data from 112 languages mined from the web", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "M4-Prompts FT", "model settings": {"xx": "yy"}}, {"value": "6.2", "char_index": [3109, 3112], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "4.0", "char_index": [3181, 3184], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "4.5", "char_index": [3253, 3256], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "1.7", "char_index": [3301, 3304], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 8B", "model settings": {"xx": "yy"}}, {"value": "12.3", "char_index": [3324, 3328], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "8.5", "char_index": [3397, 3400], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "8.7", "char_index": [3469, 3472], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "3.3", "char_index": [3517, 3520], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 62B", "model settings": {"xx": "yy"}}, {"value": "15.0", "char_index": [3541, 3545], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "10.2", "char_index": [3590, 3594], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "9.9", "char_index": [3663, 3666], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "4.7", "char_index": [3711, 3714], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "17.8", "char_index": [3732, 3736], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "13.4", "char_index": [3806, 3810], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "16.3", "char_index": [3880, 3884], "type": "Result", "training data/set": "xx", "test data/set": "FRMT Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [3954, 3957], "type": "Result", "training data/set": "xx", "test data/set": "FRMT zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"xx": "yy"}, "model": "Online", "model settings": {"xx": "yy"}}]}, "2210.00193v1_table5": {"table_code": "\\begin{table*}\n\\centering\n\\footnotesize\n\\begin{tabular}{rcccccc|c}\n\\toprule\n& \\multicolumn{2}{c}{\\bf Lexical} & \\multicolumn{2}{c}{\\bf Entities} & \\multicolumn{2}{c}{\\bf Random} & \\bf FRMT \\\\\n\\bf Exemplars & pt-BR & pt-PT & pt-BR & pt-PT & pt-BR & pt-PT & pt\\\\\\midrule\n0 & 0.0 (-1.0) & 0.0 (-1.2) & 0.0 (-1.4) & 0.0 (-1.6) & 0.1 (-0.8) & 0.0 (-1.3) & 0.0 (1.2)\\\\\n1 & 51.3 (77.8) & 38.2 (74.4) & 56.5 (80.8) & 47.5 (78.5) & 53.7 (\\textbf{76.6}) & 45.1 (73.8) & 48.4 (77.0) \\\\\n2 & 51.8 (77.7) & 38.8 (75.0) & 57.3 (80.8) & 46.9 (78.5) & 52.9 (76.1) & 45.0 (74.4) & 48.5 (77.1) \\\\\n5 & 52.8 (77.9) & 39.6 (75.0) & 57.9 (80.8) & 46.8 (78.5) & 54.5 (76.4) & 45.2 (74.6) & 49.1 (77.2) \\\\\n10 & \\textbf{54.0} (\\textbf{78.7}) & \\textbf{40.6} (\\textbf{75.3}) & \\textbf{58.3} (\\textbf{80.9}) & \\textbf{48.2} (\\textbf{78.9}) & \\textbf{54.6} (76.5) & \\textbf{46.2} (\\textbf{74.9}) & \\textbf{50.0} (\\textbf{77.5}) \\\\\\midrule\n& zh-CN & zh-TW & zh-CN & zh-TW & zh-CN & zh-TW & zh\\\\\\midrule\n0 & 0.0 (0.9) & 0.0 (3.2) & 0.0 (0.8) & 0.1 (6.5) & 0.1 (1.6) & 0.3 (4.3) & 0.1 (2.3)\\\\\n1 & 37.2 (70.1) & 25.7 (67.5) & 47.4 (74.2) & 37.7 (73.0) & 39.7 (67.0) & 32.3 (65.5) & 36.3 (69.5)\\\\\n2 & \\textbf{37.7} (70.5) & \\textbf{27.0} (\\textbf{67.8}) & 48.3 (74.8) & 38.5 (\\textbf{73.3}) & 39.3 (\\textbf{67.1}) & 32.2 (\\textbf{65.7}) & \\textbf{36.9} (\\textbf{69.8})\\\\\n5 & 37.2 (70.2) & 26.4 (67.2) & \\textbf{48.4} (74.7) & \\textbf{39.0} (73.2) & \\textbf{39.9} (66.8) & \\textbf{32.4} (65.4) & \\textbf{36.9} (69.6)\\\\\n10 & 37.6 (\\textbf{70.7}) & 25.5 (67.2) & 48.1 (\\textbf{74.9}) & 38.6 (72.6) & 38.9 (66.9) & 30.4 (64.7) & 36.2 (69.5)\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{FRMT dev set results of PaLM 540B, when varying the number of exemplars, shown in the format: BLEU~(BLEURT)\\@. Across both languages, even one exemplar is sufficient for strong results. In Portuguese, increasing to $10$ exemplars gives marginal additional gains.}\n\\label{tab:palm_exemplars}\n\\end{table*}", "table_label": "{tab:palm_exemplars}", "table_numeric_cells": [["0", "0", 269, 270, 269, 270], ["0.0", "0.0 (-1.0)", 273, 276, 273, 283], ["0.0", "0.0 (-1.2)", 286, 289, 286, 296], ["0.0", "0.0 (-1.4)", 299, 302, 299, 309], ["0.0", "0.0 (-1.6)", 312, 315, 312, 322], ["0.1", "0.1 (-0.8)", 325, 328, 325, 335], ["0.0", "0.0 (-1.3)", 338, 341, 338, 348], ["0.0", "0.0 (1.2)", 351, 354, 351, 360], ["1", "1", 363, 364, 363, 364], ["51.3", "51.3 (77.8)", 367, 371, 367, 378], ["38.2", "38.2 (74.4)", 381, 385, 381, 392], ["56.5", "56.5 (80.8)", 395, 399, 395, 406], ["47.5", "47.5 (78.5)", 409, 413, 409, 420], ["53.7", "53.7 (\\textbf{76.6})", 423, 427, 423, 443], ["45.1", "45.1 (73.8)", 446, 450, 446, 457], ["48.4", "48.4 (77.0)", 460, 464, 460, 471], ["2", "2", 475, 476, 475, 476], ["51.8", "51.8 (77.7)", 479, 483, 479, 490], ["38.8", "38.8 (75.0)", 493, 497, 493, 504], ["57.3", "57.3 (80.8)", 507, 511, 507, 518], ["46.9", "46.9 (78.5)", 521, 525, 521, 532], ["52.9", "52.9 (76.1)", 535, 539, 535, 546], ["45.0", "45.0 (74.4)", 549, 553, 549, 560], ["48.5", "48.5 (77.1)", 563, 567, 563, 574], ["5", "5", 578, 579, 578, 579], ["52.8", "52.8 (77.9)", 582, 586, 582, 593], ["39.6", "39.6 (75.0)", 596, 600, 596, 607], ["57.9", "57.9 (80.8)", 610, 614, 610, 621], ["46.8", "46.8 (78.5)", 624, 628, 624, 635], ["54.5", "54.5 (76.4)", 638, 642, 638, 649], ["45.2", "45.2 (74.6)", 652, 656, 652, 663], ["49.1", "49.1 (77.2)", 666, 670, 666, 677], ["10", "10", 681, 683, 681, 683], ["54.0", "\\textbf{54.0} (\\textbf{78.7})", 694, 698, 686, 715], ["40.6", "\\textbf{40.6} (\\textbf{75.3})", 726, 730, 718, 747], ["58.3", "\\textbf{58.3} (\\textbf{80.9})", 758, 762, 750, 779], ["48.2", "\\textbf{48.2} (\\textbf{78.9})", 790, 794, 782, 811], ["54.6", "\\textbf{54.6} (76.5)", 822, 826, 814, 834], ["46.2", "\\textbf{46.2} (\\textbf{74.9})", 845, 849, 837, 866], ["50.0", "\\textbf{50.0} (\\textbf{77.5})", 877, 881, 869, 898], ["0", "0", 973, 974, 973, 974], ["0.0", "0.0 (0.9)", 977, 980, 977, 986], ["0.0", "0.0 (3.2)", 989, 992, 989, 998], ["0.0", "0.0 (0.8)", 1001, 1004, 1001, 1010], ["0.1", "0.1 (6.5)", 1013, 1016, 1013, 1022], ["0.1", "0.1 (1.6)", 1025, 1028, 1025, 1034], ["0.3", "0.3 (4.3)", 1037, 1040, 1037, 1046], ["0.1", "0.1 (2.3)", 1049, 1052, 1049, 1058], ["1", "1", 1061, 1062, 1061, 1062], ["37.2", "37.2 (70.1)", 1065, 1069, 1065, 1076], ["25.7", "25.7 (67.5)", 1079, 1083, 1079, 1090], ["47.4", "47.4 (74.2)", 1093, 1097, 1093, 1104], ["37.7", "37.7 (73.0)", 1107, 1111, 1107, 1118], ["39.7", "39.7 (67.0)", 1121, 1125, 1121, 1132], ["32.3", "32.3 (65.5)", 1135, 1139, 1135, 1146], ["36.3", "36.3 (69.5)", 1149, 1153, 1149, 1160], ["2", "2", 1163, 1164, 1163, 1164], ["37.7", "\\textbf{37.7} (70.5)", 1175, 1179, 1167, 1187], ["27.0", "\\textbf{27.0} (\\textbf{67.8})", 1198, 1202, 1190, 1219], ["48.3", "48.3 (74.8)", 1222, 1226, 1222, 1233], ["38.5", "38.5 (\\textbf{73.3})", 1236, 1240, 1236, 1256], ["39.3", "39.3 (\\textbf{67.1})", 1259, 1263, 1259, 1279], ["32.2", "32.2 (\\textbf{65.7})", 1282, 1286, 1282, 1302], ["36.9", "\\textbf{36.9} (\\textbf{69.8})", 1313, 1317, 1305, 1334], ["5", "5", 1337, 1338, 1337, 1338], ["37.2", "37.2 (70.2)", 1341, 1345, 1341, 1352], ["26.4", "26.4 (67.2)", 1355, 1359, 1355, 1366], ["48.4", "\\textbf{48.4} (74.7)", 1377, 1381, 1369, 1389], ["39.0", "\\textbf{39.0} (73.2)", 1400, 1404, 1392, 1412], ["39.9", "\\textbf{39.9} (66.8)", 1423, 1427, 1415, 1435], ["32.4", "\\textbf{32.4} (65.4)", 1446, 1450, 1438, 1458], ["36.9", "\\textbf{36.9} (69.6)", 1469, 1473, 1461, 1481], ["10", "10", 1484, 1486, 1484, 1486], ["37.6", "37.6 (\\textbf{70.7})", 1489, 1493, 1489, 1509], ["25.5", "25.5 (67.2)", 1512, 1516, 1512, 1523], ["48.1", "48.1 (\\textbf{74.9})", 1526, 1530, 1526, 1546], ["38.6", "38.6 (72.6)", 1549, 1553, 1549, 1560], ["38.9", "38.9 (66.9)", 1563, 1567, 1563, 1574], ["30.4", "30.4 (64.7)", 1577, 1581, 1577, 1588], ["36.2", "36.2 (69.5)", 1591, 1595, 1591, 1602]], "text_chunk_selected": "\\subsection{Data sampling method}\\label{sec:data_sampling}\nFRMT seeks to capture region-specific linguistic differences, as well as potential distractors in the form of entities that are strongly associated with one region, without necessarily having a different linguistic rendering (e.g., Lisbon vs.~S\\~{a}o Paulo). To this end, we divide the dataset into three buckets (\\texttt{lexical}, \\texttt{entity}, \\texttt{random}), each containing human translations of sentences extracted from different sets of English Wikipedia articles.\\footnote{\nAs Wikipedia data source we use the training split of {\\tt wiki40b} (v1.3.0) by \\citet{wiki40b}, available at \\scriptsize{\\url{https://www.tensorflow.org/datasets/catalog/wiki40b}}.}\n\n\\textbf{BLEU \\citep{papineni-etal-2002-bleu}:} This metric is based on token $n$-gram precision. We use \\texttt{sacrebleu.corpus\\_bleu} from \\citet{post-2018-call}.\n\n\\textbf{chrF \\citep{popovic-2015-chrf}:} This metric is based on F1 using character $n$-grams. We use \\texttt{sacrebleu.corpus\\_chrf} from \\citet{post-2018-call}.\n\n\\textbf{BLEURT \\citep{sellam-etal-2020-bleurt}:} This is a learned, model-based metric. While the original paper showed that BLEURT has good correlation with human judgments of translation quality, we note that BLEURT has not, to our knowledge, been evaluated with respect to human judgments of \\textit{region-specific} translation quality. We use the authors' released checkpoint.\n\n\\textbf{BLEURT-D\\{3,6,12\\} \\citep{sellam-etal-2020-bleurt}:} These are distilled versions of BLEURT that are less resource-intensive to run. They have 3, 6, and 12 layers, respectively. As above, we use checkpoints released by the authors.\n\nResearchers sharing FRMT results should \\textbf{report per-bucket BLEU and lexical accuracy} metrics on the test set, as shown in Tables \\ref{tab:results_auto} and \\ref{tab:lexical_accuracy}. These metrics are efficient to calculate with our provided evaluation scripts.\n\nOur next three baseline models are different-sized versions of PaLM \\citep{chowdhery2022palm}, a large language model that has demonstrated remarkable zero-shot and few-shot performance on a variety of tasks, often without needing any fine-tuning. Because it was pre-trained on a multilingual corpus, we include it as a baseline, serving as a representative of the class of recent large decoder-only causal language models including GPT-3 \\citep{brown-etal-22-gpt3} and OPT-175B \\citep{zhang-etal-22-opt}. We call these models \\textbf{PaLM 540B}, \\textbf{PaLM 62B}, and \\textbf{PaLM 8B}, referring to their approximate parameter counts. The prompt for these models is larger than for the M4 Prompts models and is exemplar-based. It begins with ``Translate the following texts from English to X'', where ``X'' is the name of the language variety. This is followed by ten exemplars selected randomly from the \\texttt{lexical} bucket,\\footnote{The model has a fixed input sequence length, which includes the prompt, and a fixed output sequence length. Through rejection sampling, we ensure that the ten exemplars are short enough to leave at least 128 tokens for the input text, to match the 128 tokens allotted to the output.} where each exemplar is put on two lines: the first line contains the English text, prefixed by ``English:'', and the second containing the translation in the target variety, prefixed by the variety's name. At the end of the prompt, we show the model the input text and the language variety prefix, and decode from the model, taking everything up to the next newline as the output (or the end of the line if no newline was generated).\n\nTo test sensitivity to the number and choice of exemplars, we evaluate PaLM 540B while varying the set of exemplars used.  Table \\ref{tab:palm_exemplars} shows the effect of ablating the number of exemplars in the range $0$--$10$.  We observe that a single exemplar is sufficient to achieve strong results, and gains from additional exemplars are marginal.", "table_source": "\\begin{table*}\n\\centering\n\\footnotesize\n\\begin{tabular}{rcccccc|c}\n\\toprule\n& \\multicolumn{2}{c}{\\bf Lexical} & \\multicolumn{2}{c}{\\bf Entities} & \\multicolumn{2}{c}{\\bf Random} & \\bf FRMT \\\\\n\\bf Exemplars & pt-BR & pt-PT & pt-BR & pt-PT & pt-BR & pt-PT & pt\\\\\\midrule\n0 & 0.0 (-1.0) & 0.0 (-1.2) & 0.0 (-1.4) & 0.0 (-1.6) & 0.1 (-0.8) & 0.0 (-1.3) & 0.0 (1.2)\\\\\n1 & 51.3 (77.8) & 38.2 (74.4) & 56.5 (80.8) & 47.5 (78.5) & 53.7 (\\textbf{76.6}) & 45.1 (73.8) & 48.4 (77.0) \\\\\n2 & 51.8 (77.7) & 38.8 (75.0) & 57.3 (80.8) & 46.9 (78.5) & 52.9 (76.1) & 45.0 (74.4) & 48.5 (77.1) \\\\\n5 & 52.8 (77.9) & 39.6 (75.0) & 57.9 (80.8) & 46.8 (78.5) & 54.5 (76.4) & 45.2 (74.6) & 49.1 (77.2) \\\\\n10 & \\textbf{54.0} (\\textbf{78.7}) & \\textbf{40.6} (\\textbf{75.3}) & \\textbf{58.3} (\\textbf{80.9}) & \\textbf{48.2} (\\textbf{78.9}) & \\textbf{54.6} (76.5) & \\textbf{46.2} (\\textbf{74.9}) & \\textbf{50.0} (\\textbf{77.5}) \\\\\\midrule\n& zh-CN & zh-TW & zh-CN & zh-TW & zh-CN & zh-TW & zh\\\\\\midrule\n0 & 0.0 (0.9) & 0.0 (3.2) & 0.0 (0.8) & 0.1 (6.5) & 0.1 (1.6) & 0.3 (4.3) & 0.1 (2.3)\\\\\n1 & 37.2 (70.1) & 25.7 (67.5) & 47.4 (74.2) & 37.7 (73.0) & 39.7 (67.0) & 32.3 (65.5) & 36.3 (69.5)\\\\\n2 & \\textbf{37.7} (70.5) & \\textbf{27.0} (\\textbf{67.8}) & 48.3 (74.8) & 38.5 (\\textbf{73.3}) & 39.3 (\\textbf{67.1}) & 32.2 (\\textbf{65.7}) & \\textbf{36.9} (\\textbf{69.8})\\\\\n5 & 37.2 (70.2) & 26.4 (67.2) & \\textbf{48.4} (74.7) & \\textbf{39.0} (73.2) & \\textbf{39.9} (66.8) & \\textbf{32.4} (65.4) & \\textbf{36.9} (69.6)\\\\\n10 & 37.6 (\\textbf{70.7}) & 25.5 (67.2) & 48.1 (\\textbf{74.9}) & 38.6 (72.6) & 38.9 (66.9) & 30.4 (64.7) & 36.2 (69.5)\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{FRMT dev set results of PaLM 540B, when varying the number of exemplars, shown in the format: BLEU~(BLEURT)\\@. Across both languages, even one exemplar is sufficient for strong results. In Portuguese, increasing to $10$ exemplars gives marginal additional gains.}\n\\label{tab:palm_exemplars}\n\\end{table*}", "cell_list_gold": [{"value": "0", "char_index": [269, 270], "type": "Other"}, {"value": "0.0", "char_index": [273, 276], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [286, 289], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [299, 302], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [312, 315], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.1", "char_index": [325, 328], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [338, 341], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [351, 354], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [363, 364], "type": "Other"}, {"value": "51.3", "char_index": [367, 371], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "38.2", "char_index": [381, 385], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "56.5", "char_index": [395, 399], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "47.5", "char_index": [409, 413], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "53.7", "char_index": [423, 427], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "45.1", "char_index": [446, 450], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "48.4", "char_index": [460, 464], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "2", "char_index": [475, 476], "type": "Other"}, {"value": "51.8", "char_index": [479, 483], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "38.8", "char_index": [493, 497], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "57.3", "char_index": [507, 511], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "46.9", "char_index": [521, 525], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "52.9", "char_index": [535, 539], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "45.0", "char_index": [549, 553], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "48.5", "char_index": [563, 567], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "5", "char_index": [578, 579], "type": "Other"}, {"value": "52.8", "char_index": [582, 586], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "39.6", "char_index": [596, 600], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "57.9", "char_index": [610, 614], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "46.8", "char_index": [624, 628], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "54.5", "char_index": [638, 642], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "45.2", "char_index": [652, 656], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "49.1", "char_index": [666, 670], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "10", "char_index": [681, 683], "type": "Other"}, {"value": "54.0", "char_index": [694, 698], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "40.6", "char_index": [726, 730], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "58.3", "char_index": [758, 762], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "48.2", "char_index": [790, 794], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "54.6", "char_index": [822, 826], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-BR", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "46.2", "char_index": [845, 849], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random pt-PT", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "50.0", "char_index": [877, 881], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0", "char_index": [973, 974], "type": "Other"}, {"value": "0.0", "char_index": [977, 980], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [989, 992], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.0", "char_index": [1001, 1004], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.1", "char_index": [1013, 1016], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.1", "char_index": [1025, 1028], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [1037, 1040], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "0.1", "char_index": [1049, 1052], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "0"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [1061, 1062], "type": "Other"}, {"value": "37.2", "char_index": [1065, 1069], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "25.7", "char_index": [1079, 1083], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "47.4", "char_index": [1093, 1097], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "37.7", "char_index": [1107, 1111], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "39.7", "char_index": [1121, 1125], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "32.3", "char_index": [1135, 1139], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "36.3", "char_index": [1149, 1153], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "1"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "2", "char_index": [1163, 1164], "type": "Other"}, {"value": "37.7", "char_index": [1175, 1179], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "27.0", "char_index": [1198, 1202], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "48.3", "char_index": [1222, 1226], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "38.5", "char_index": [1236, 1240], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "39.3", "char_index": [1259, 1263], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "32.2", "char_index": [1282, 1286], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "36.9", "char_index": [1313, 1317], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "2"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "5", "char_index": [1337, 1338], "type": "Other"}, {"value": "37.2", "char_index": [1341, 1345], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "26.4", "char_index": [1355, 1359], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "48.4", "char_index": [1377, 1381], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "39.0", "char_index": [1400, 1404], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "39.9", "char_index": [1423, 1427], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "32.4", "char_index": [1446, 1450], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "36.9", "char_index": [1469, 1473], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "5"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "10", "char_index": [1484, 1486], "type": "Other"}, {"value": "37.6", "char_index": [1489, 1493], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "25.5", "char_index": [1512, 1516], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Lexical zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "48.1", "char_index": [1526, 1530], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "38.6", "char_index": [1549, 1553], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Entities zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "38.9", "char_index": [1563, 1567], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-CN", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "30.4", "char_index": [1577, 1581], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev Random zh-TW", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}, {"value": "36.2", "char_index": [1591, 1595], "type": "Result", "training data/set": "a multilingual corpus", "test data/set": "FRMT dev zh", "task": ["Few-Shot Region-Aware Machine Translation", "FRMT", "Machine Translation"], "metric": "delta BLEU", "experimental settings": {"Exemplars": "10"}, "model": "PaLM 540B", "model settings": {"xx": "yy"}}]}, "2210.00213v1_table0": {"table_code": "\\begin{table*}\n\\centering\n\\caption{Results for Zero-shot setup for proposed method HyperHawkes under different setups against the proposed baselines. The table considers both the variants HyperHawkes-FNN and HyperHawkes-FNN-RNN as proposed methodology for ZSL}\n\\label{Tab:zsl}\n\\tabcolsep=0.05cm\n\\begin{tabular}{c|c|cccc|cccc} \n\\hline\\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Experimental\\\\Setup\\end{tabular}}      & \\multirow{2}{*}{Dataset} & \\multicolumn{4}{c|}{MNLL}                                                                                                                                                                       & \\multicolumn{4}{c}{MAE}                                                                                                                                                                         \\\\ \n\\cline{3-10}\n                                                                                  &                          & FNHP    & \\begin{tabular}[c]{@{}c@{}}FNHP-\\\\Descriptor\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}HyperHawkes-\\\\FNN\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}HyperHawkes-\\\\FNN-RNN\\end{tabular} & FNHP   & \\begin{tabular}[c]{@{}c@{}}FNHP-\\\\Descriptor\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}HyperHawkes-\\\\FNN\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}HyperHawkes-\\\\FNN-RNN\\end{tabular}  \\\\ \n\\hline\n\\multirow{2}{*}{Zero-shot}                                                        & Yelp                     & -4.2023 & -3.3334                                                   & \\textbf{\\textbf{-5.7045}}                                 & -5.4934                                                       & 0.0025 & 0.0015                                                    & \\textbf{0.0013}                                           & 0.0014                                                         \\\\\n                                                                                  & Meme                     & -3.9011 & -2.9421                                                   & \\textbf{\\textbf{-5.4339}}                                 & -5.3403                                                       & 0.0056 & 0.0072                                                    & 0.0036                                                    & \\textbf{0.0018}                                                \\\\ \n\\hline\\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Generalized\\\\zero-shot\\end{tabular}}   & Yelp                     & -4.9279 & -4.2075                                                   & -5.2473                                                   & \\textbf{\\textbf{-5.6869}}                                     & 0.0025 & 0.0016                                                    & 0.0021                                                    & \\textbf{0.0014}                                                \\\\\n                                                                                  & Meme                     & -4.2735 & -4.0250                                                   & -5.2475                                                   & \\textbf{\\textbf{-5.2922}}                                     & 0.0036 & 0.0033                                                    & 0.0030                                                    & \\textbf{0.0026}                                                \\\\ \n\\hline\\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Standard\\\\event modeling\\end{tabular}} & Yelp                     & -4.2700 & -3.8543                                                   & -4.9030                                                   & \\textbf{\\textbf{-4.9475}}                                     & 0.0047 & 0.0042                                                    & \\textbf{0.0024}                                           & 0.0025                                                         \\\\\n                                                                                  & Meme                     & -4.6991 & -2.9633                                                   & -2.5325                                                   & \\textbf{-4.8796}                                                      & 0.0046 & 0.0072                                                    & 0.0078                                                    & \\textbf{0.0027}                                                \\\\\n\\hline\\hline\n\\end{tabular}\n\\end{table*}", "table_label": "{Tab:zsl}", "table_numeric_cells": [["-4.2023", "-4.2023", 1473, 1480, 1473, 1480], ["-3.3334", "-3.3334", 1483, 1490, 1483, 1490], ["-5.7045", "\\textbf{\\textbf{-5.7045}}", 1559, 1566, 1543, 1568], ["-5.4934", "-5.4934", 1603, 1610, 1603, 1610], ["0.0025", "0.0025", 1667, 1673, 1667, 1673], ["0.0015", "0.0015", 1676, 1682, 1676, 1682], ["0.0013", "\\textbf{0.0013}", 1744, 1750, 1736, 1751], ["0.0014", "0.0014", 1796, 1802, 1796, 1802], ["-3.9011", "-3.9011", 1973, 1980, 1973, 1980], ["-2.9421", "-2.9421", 1983, 1990, 1983, 1990], ["-5.4339", "\\textbf{\\textbf{-5.4339}}", 2059, 2066, 2043, 2068], ["-5.3403", "-5.3403", 2103, 2110, 2103, 2110], ["0.0056", "0.0056", 2167, 2173, 2167, 2173], ["0.0072", "0.0072", 2176, 2182, 2176, 2182], ["0.0036", "0.0036", 2236, 2242, 2236, 2242], ["0.0018", "\\textbf{0.0018}", 2304, 2310, 2296, 2311], ["-4.9279", "-4.9279", 2487, 2494, 2487, 2494], ["-4.2075", "-4.2075", 2497, 2504, 2497, 2504], ["-5.2473", "-5.2473", 2557, 2564, 2557, 2564], ["-5.6869", "\\textbf{\\textbf{-5.6869}}", 2633, 2640, 2617, 2642], ["0.0025", "0.0025", 2681, 2687, 2681, 2687], ["0.0016", "0.0016", 2690, 2696, 2690, 2696], ["0.0021", "0.0021", 2750, 2756, 2750, 2756], ["0.0014", "\\textbf{0.0014}", 2818, 2824, 2810, 2825], ["-4.2735", "-4.2735", 2987, 2994, 2987, 2994], ["-4.0250", "-4.0250", 2997, 3004, 2997, 3004], ["-5.2475", "-5.2475", 3057, 3064, 3057, 3064], ["-5.2922", "\\textbf{\\textbf{-5.2922}}", 3133, 3140, 3117, 3142], ["0.0036", "0.0036", 3181, 3187, 3181, 3187], ["0.0033", "0.0033", 3190, 3196, 3190, 3196], ["0.0030", "0.0030", 3250, 3256, 3250, 3256], ["0.0026", "\\textbf{0.0026}", 3318, 3324, 3310, 3325], ["-4.2700", "-4.2700", 3501, 3508, 3501, 3508], ["-3.8543", "-3.8543", 3511, 3518, 3511, 3518], ["-4.9030", "-4.9030", 3571, 3578, 3571, 3578], ["-4.9475", "\\textbf{\\textbf{-4.9475}}", 3647, 3654, 3631, 3656], ["0.0047", "0.0047", 3695, 3701, 3695, 3701], ["0.0042", "0.0042", 3704, 3710, 3704, 3710], ["0.0024", "\\textbf{0.0024}", 3772, 3778, 3764, 3779], ["0.0025", "0.0025", 3824, 3830, 3824, 3830], ["-4.6991", "-4.6991", 4001, 4008, 4001, 4008], ["-2.9633", "-2.9633", 4011, 4018, 4011, 4018], ["-2.5325", "-2.5325", 4071, 4078, 4071, 4078], ["-4.8796", "\\textbf{-4.8796}", 4139, 4146, 4131, 4147], ["0.0046", "0.0046", 4203, 4209, 4203, 4209], ["0.0072", "0.0072", 4212, 4218, 4212, 4218], ["0.0078", "0.0078", 4272, 4278, 4272, 4278], ["0.0027", "\\textbf{0.0027}", 4340, 4346, 4332, 4347]], "text_chunk_selected": "\\paragraph{\\textbf{Continual Learning:}} aims to create a learning paradigm which is able to model a stream of tasks while avoiding catastrophic forgetting. Different techniques \\cite{kirkpatrick2017overcoming,li2017learning,lopez2017gradient,von2019continual}  have been proposed in this regard by consolidating knowledge either in various spaces like data, weight or meta space.\n\\cite{von2019continual} performs continual learning through hypernetwork which learns task conditioned weights of base model. \n\\paragraph{\n\\textbf{Hypernetworks:}} They have been introduced as a metanetwork which can generate weights for another network~\\cite{ha2017hypernetworks}. It is used for various tasks like meta learning \\cite{zhao2020meta}, neural architecture search \\cite{zoph2016neural}, natural language understanding~\\cite{he2022hyperprompt} etc.\n\n\\begin{equation}\n\\label{eq:log_lik_nhp}\n\\begin{split}\n& \\log p(\\{t^i_j\\}_{j=1}^{n^i} ; W^i) = \\sum_{j=1}^{n^i} \\log p(t^i_j|\\mathcal{H}^i_j;W^i) = \\\\\n& \\sum_{j=1}^{n^i} \\big( \\log (\\frac{ \\partial}{\\partial \\tau^i_j} \\Phi(\\tau^i_j |\\boldsymbol{h}^i_{j-1}; W^i) )\n - \\Phi(\\tau^i_j|\\boldsymbol{h}^i_{j-1}; W^i) \\big) \n \\end{split}\n\\end{equation}\n\n\\begin{itemize}\n\\item \\textbf{HyperHawkes-FNN:} Hypernetwork is considered only for the FNN  modeling the cumulative hazard function.  \n\\item \\textbf{HyperHawkes-FNN-RNN:} This variant uses two separate hypernetworks, one to model  the RNN modeling the history and the second to model the FNN.\n\\end{itemize}\n\n\\subsection{HyperHawkes for Zero-shot modeling}\nIn this section  we discuss how we employ \\textit{HyperHawkes} for zero-shot event modeling. Our goal is to train the model on seen sequences $\\mathcal{D}^S$ with event sequences $\\mathcal{T}^s$ and task descriptors $\\mathbf{d}^s$ and predict event times of unseen sequences in $\\mathcal{D}^U$ given a task descriptor $\\mathbf{d}^u$. We employ \\textit{HyperHawkes} for performing zero-shot learning on event sequences. The central idea of the proposed approach is to predict the parameters for the neural Hawkes process for the unseen task $\\mathbf{d}^u$. This is achieved using hypernetwork which considers the sequence descriptor as input and parameters for neural Hawkes process as output as discussed in the previous section. Consequently, we can get parameters for RNN ($W_r^u$) using Equation \\ref{eq:hrnn} and FFN ($W_t^u$)  using Equation \\ref{eq:hfnn} and use them to model the cumulative hazard function for an unseen sequence. These parameters can then be used for predicting  events in the sequence $\\mathcal{T}^u$. \n\n\\begin{equation}\n\\label{eq:log_lik_nhp_cl}\n\\begin{split}\n&\\sum_{j=1}^{n^s}  -\\log p(t^s_j|\\mathcal{H}^s_j;(f_r(\\mathbf{d}^s;\\theta_{fr}), \nf_t(\\mathbf{d}^s;\\theta_{ft}))) \\\\\n & +\\frac{\\beta}{s-1}\\sum_{c=1}^{s-1}\\biggl(\\parallel f_r(\\mathbf{d}^c;\\theta_{fr})-f_r(\\mathbf{d}^c; \\bar{\\theta}_{fr})\\!\\!\\parallel^2 \\\\\n& + \\parallel f_t(\\mathbf{d}^c; \\theta_{ft}) - f_t(\\mathbf{d}^c; \\bar{\\theta}_{ft}) \\parallel^2 \\biggr)\n \\end{split}\n\\end{equation}\n\n\\section{Experiments}\n\\subsection{Datasets}\nDue to paucity of standard datasets for event modeling tasks which contain meta descriptions as well, we use the following two datasets: \\textbf{1)Yelp:}\\footnote[1]{https://www.kaggle.com/datasets/yelp-dataset/}: This is a dataset comprising of business information and their check-in information. Each business is associated with 82 attributes like \\textit{Wheelchair Accessible, Accepts Insurance, By Appointment Only, Business Category, Business Timings} etc. Also, they are associated with latitude-longitude pairs. Moreover, these businesses are associated with a fine-grained category. For higher granularity, we convert them into 22 broad categories using the hierarchy mentioned in their website \\footnote[2]{https://www.yelp.com/developers/documentation/v3/all\\_category\\_list}. Using these attributes, we create a vector of length 1229 representing a business. This vector acts as a descriptor of the business. We select businesses with more than 5000 check-ins. For continual learning, we have considered another sample of dataset consisting of business with more than 10k checkins, hence considering 26 sequences of business. This is done to reduce the number of sequences for better visualization of performance of each sequence. \\textbf{2) Meme:}\\footnote[3]{https://snap.stanford.edu/data/memetracker9.html}: This dataset\\cite{leskovec2009meme} tracks the popular phrases and quotes which appear appear most frequently over time in news media and blogs. Each meme is associated with the content and timestamps when they were quoted in the media. We select top 200 english phrases and doc2vec representation of the meme content is considered as the descriptor of length 100. We have considered memes from April, 2009 with an average number of events as 970. \n\n\\subsection{Experimental Setup}\nWe consider these experimental setups to evaluate the performance of our model - \n\\textbf{1) Zero-Shot:} Training is done on seen sequences and testing is done on unseen sequences.  \\textbf{2) Generalized Zero-Shot:} In this, testing is done by randomly sampling 20\\% of events from seen sequences and unseen sequences.  \\textbf{3) Standard Event Modeling:} Training is done on the first 70\\% of the events from all sequences. Testing is done on the last 20\\% events for unseen sequences. \nMean negative log-likelihood (MNLL) and mean absolute error (MAE) are considered as evaluation metrics for both zero-shot and continual learning setup. Lower MNLL and MAE indicates better performance.\n\n\\section{Results and Analysis}\n\\subsection{Zero-Shot Learning}\nResults for zero-shot setup are presented in Table \\ref{Tab:zsl}. A better model is expected to have lower MNLL and MAE. We can observe that the proposed method HyperHawkes performs better than the baselines in terms of both evaluation metrics MNLL and MAE. We can observe that zero-shot setup, which consists of predictive performance for unseen sequences, exhibits significantly lower MNLL and MAE for HyperHawkes-FNN as compared to FNHP and FNHP-Descriptor. Also, HyperHawkes-FNN-RNN yields lower MAE for Meme dataset, however, MNLL for both the variants of the proposed methods is close. Comparing the results of generalized zero-shot setup where we consider instances from seen sequences as well as unseen sequences, we can observe that HyperHawkes-FNN-RNN performs better for both the datasets. The final section of the table discusses results for standard event modeling where we test on the last 20\\% of the events of unseen sequences. HyperHawkes-FNN-RNN performs well here as well except for MAE for Yelp dataset. We can also observe that HyperHawkes-FNN-RNN performs slightly better for generalized zero-shot and standard event modeling setup. Moreover, it is interesting to note that inclusion of sequence descriptors within FNHP in FNHP-Descriptor has not helped much in prediction of unseen sequences. In fact, in various cases, it has performed worse than FNHP itself. This confirms the necessity of an event sequence specific framework which can predict better for unseen sequences as well. Therefore, our results indicate that the proposed variants of HyperHawkes perform consistently and significantly better than the baselines for both the datasets.  ", "table_source": "\\begin{table*}\n\\centering\n\\caption{Results for Zero-shot setup for proposed method HyperHawkes under different setups against the proposed baselines. The table considers both the variants HyperHawkes-FNN and HyperHawkes-FNN-RNN as proposed methodology for ZSL}\n\\label{Tab:zsl}\n\\tabcolsep=0.05cm\n\\begin{tabular}{c|c|cccc|cccc} \n\\hline\\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Experimental\\\\Setup\\end{tabular}}      & \\multirow{2}{*}{Dataset} & \\multicolumn{4}{c|}{MNLL}                                                                                                                                                                       & \\multicolumn{4}{c}{MAE}                                                                                                                                                                         \\\\ \n\\cline{3-10}\n                                                                                  &                          & FNHP    & \\begin{tabular}[c]{@{}c@{}}FNHP-\\\\Descriptor\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}HyperHawkes-\\\\FNN\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}HyperHawkes-\\\\FNN-RNN\\end{tabular} & FNHP   & \\begin{tabular}[c]{@{}c@{}}FNHP-\\\\Descriptor\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}HyperHawkes-\\\\FNN\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}HyperHawkes-\\\\FNN-RNN\\end{tabular}  \\\\ \n\\hline\n\\multirow{2}{*}{Zero-shot}                                                        & Yelp                     & -4.2023 & -3.3334                                                   & \\textbf{\\textbf{-5.7045}}                                 & -5.4934                                                       & 0.0025 & 0.0015                                                    & \\textbf{0.0013}                                           & 0.0014                                                         \\\\\n                                                                                  & Meme                     & -3.9011 & -2.9421                                                   & \\textbf{\\textbf{-5.4339}}                                 & -5.3403                                                       & 0.0056 & 0.0072                                                    & 0.0036                                                    & \\textbf{0.0018}                                                \\\\ \n\\hline\\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Generalized\\\\zero-shot\\end{tabular}}   & Yelp                     & -4.9279 & -4.2075                                                   & -5.2473                                                   & \\textbf{\\textbf{-5.6869}}                                     & 0.0025 & 0.0016                                                    & 0.0021                                                    & \\textbf{0.0014}                                                \\\\\n                                                                                  & Meme                     & -4.2735 & -4.0250                                                   & -5.2475                                                   & \\textbf{\\textbf{-5.2922}}                                     & 0.0036 & 0.0033                                                    & 0.0030                                                    & \\textbf{0.0026}                                                \\\\ \n\\hline\\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Standard\\\\event modeling\\end{tabular}} & Yelp                     & -4.2700 & -3.8543                                                   & -4.9030                                                   & \\textbf{\\textbf{-4.9475}}                                     & 0.0047 & 0.0042                                                    & \\textbf{0.0024}                                           & 0.0025                                                         \\\\\n                                                                                  & Meme                     & -4.6991 & -2.9633                                                   & -2.5325                                                   & \\textbf{-4.8796}                                                      & 0.0046 & 0.0072                                                    & 0.0078                                                    & \\textbf{0.0027}                                                \\\\\n\\hline\\hline\n\\end{tabular}\n\\end{table*}", "cell_list_gold": [{"value": "-4.2023", "char_index": [1473, 1480], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Zero-shot": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "-3.3334", "char_index": [1483, 1490], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Zero-shot": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "-5.7045", "char_index": [1559, 1566], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Zero-shot": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "-5.4934", "char_index": [1603, 1610], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Zero-shot": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "0.0025", "char_index": [1667, 1673], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Zero-shot": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "0.0015", "char_index": [1676, 1682], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Zero-shot": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "0.0013", "char_index": [1744, 1750], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Zero-shot": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "0.0014", "char_index": [1796, 1802], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Zero-shot": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "-3.9011", "char_index": [1973, 1980], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Zero-shot": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "-2.9421", "char_index": [1983, 1990], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Zero-shot": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "-5.4339", "char_index": [2059, 2066], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Zero-shot": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "-5.3403", "char_index": [2103, 2110], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Zero-shot": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "0.0056", "char_index": [2167, 2173], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Zero-shot": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "0.0072", "char_index": [2176, 2182], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Zero-shot": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "0.0036", "char_index": [2236, 2242], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Zero-shot": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "0.0018", "char_index": [2304, 2310], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Zero-shot": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "-4.9279", "char_index": [2487, 2494], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Generalized zero-shot": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "-4.2075", "char_index": [2497, 2504], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Generalized zero-shot": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "-5.2473", "char_index": [2557, 2564], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Generalized zero-shot": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "-5.6869", "char_index": [2633, 2640], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Generalized zero-shot": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "0.0025", "char_index": [2681, 2687], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Generalized zero-shot": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "0.0016", "char_index": [2690, 2696], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Generalized zero-shot": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "0.0021", "char_index": [2750, 2756], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Generalized zero-shot": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "0.0014", "char_index": [2818, 2824], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Generalized zero-shot": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "-4.2735", "char_index": [2987, 2994], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Generalized zero-shot": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "-4.0250", "char_index": [2997, 3004], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Generalized zero-shot": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "-5.2475", "char_index": [3057, 3064], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Generalized zero-shot": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "-5.2922", "char_index": [3133, 3140], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Generalized zero-shot": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "0.0036", "char_index": [3181, 3187], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Generalized zero-shot": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "0.0033", "char_index": [3190, 3196], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Generalized zero-shot": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "0.0030", "char_index": [3250, 3256], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Generalized zero-shot": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "0.0026", "char_index": [3318, 3324], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Generalized zero-shot": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "-4.2700", "char_index": [3501, 3508], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Standard event modeling": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "-3.8543", "char_index": [3511, 3518], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Standard event modeling": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "-4.9030", "char_index": [3571, 3578], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Standard event modeling": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "-4.9475", "char_index": [3647, 3654], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Standard event modeling": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "0.0047", "char_index": [3695, 3701], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Standard event modeling": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "0.0042", "char_index": [3704, 3710], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Standard event modeling": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "0.0024", "char_index": [3772, 3778], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Standard event modeling": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "0.0025", "char_index": [3824, 3830], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Standard event modeling": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "-4.6991", "char_index": [4001, 4008], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Standard event modeling": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "-2.9633", "char_index": [4011, 4018], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Standard event modeling": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "-2.5325", "char_index": [4071, 4078], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Standard event modeling": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "-4.8796", "char_index": [4139, 4146], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Standard event modeling": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}, {"value": "0.0046", "char_index": [4203, 4209], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Standard event modeling": "true"}, "model": "FNHP", "model settings": {"xx": "yy"}}, {"value": "0.0072", "char_index": [4212, 4218], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Standard event modeling": "true"}, "model": "FNHP-Descriptor", "model settings": {"xx": "yy"}}, {"value": "0.0078", "char_index": [4272, 4278], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Standard event modeling": "true"}, "model": "HyperHawkes-FNN", "model settings": {"xx": "yy"}}, {"value": "0.0027", "char_index": [4340, 4346], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Standard event modeling": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"xx": "yy"}}]}, "2210.00213v1_table1": {"table_code": "\\begin{table*}\n\\centering\n\\caption{Results for Continual Learning by comparing the performance of the proposed HyperHawkes with regularization against under HyperHawkes without regularization (Lower MNLL and MAE indicates better performance) \n}\n\\label{Tab:cl}\n\\tabcolsep=0.2cm\n\\begin{tabular}{c|cc|cc||cc|cc} \n\\hhline{=====:t:====}\n\\multirow{3}{*}{Dataset} & \\multicolumn{4}{c||}{HyperHawkes-FNN}                                                          & \\multicolumn{4}{c}{HyperHawkes-FNN-RNN}                                                                           \\\\ \n\\cline{2-9}\n                         & \\multicolumn{2}{c|}{MNLL}                               & \\multicolumn{2}{c||}{MAE}            & \\multicolumn{2}{c|}{MNLL}                               & \\multicolumn{2}{c}{MAE}                                 \\\\ \n\\cline{2-9}\n                         & WithoutCL & WithCL                                      & WithoutCL & WithCL                   & WithoutCL & WithCL                                      & WithoutCL & WithCL                                      \\\\ \n\\hline\nYelp                     & -4.8627   & \\textbf{\\textbf{\\textbf{\\textbf{-5.6928}}}} & 0.00159    & \\textbf{\\textbf{0.00149}} & -5.2629   & \\textbf{\\textbf{\\textbf{\\textbf{-5.7727}}}} & 0.00152    & \\textbf{\\textbf{\\textbf{\\textbf{0.00150}}}}  \\\\\nMeme                     & -2.8550   & \\textbf{\\textbf{-5.1462}}                   & 0.00548    & \\textbf{0.00471}          & -3.8254   & \\textbf{\\textbf{-5.1192}}                   & 0.00508    & \\textbf{\\textbf{0.00471}}                    \\\\\n\\hhline{=====:b:====}\n\\end{tabular}\n\\end{table*}", "table_label": "{Tab:cl}", "table_numeric_cells": [["-4.8627", "-4.8627", 1116, 1123, 1116, 1123], ["-5.6928", "\\textbf{\\textbf{\\textbf{\\textbf{-5.6928}}}}", 1160, 1167, 1128, 1171], ["0.00159", "0.00159", 1174, 1181, 1174, 1181], ["0.00149", "\\textbf{\\textbf{0.00149}}", 1203, 1210, 1187, 1212], ["-5.2629", "-5.2629", 1215, 1222, 1215, 1222], ["-5.7727", "\\textbf{\\textbf{\\textbf{\\textbf{-5.7727}}}}", 1259, 1266, 1227, 1270], ["0.00152", "0.00152", 1273, 1280, 1273, 1280], ["0.00150", "\\textbf{\\textbf{\\textbf{\\textbf{0.00150}}}}", 1318, 1325, 1286, 1329], ["-2.8550", "-2.8550", 1361, 1368, 1361, 1368], ["-5.1462", "\\textbf{\\textbf{-5.1462}}", 1389, 1396, 1373, 1398], ["0.00548", "0.00548", 1419, 1426, 1419, 1426], ["0.00471", "\\textbf{0.00471}", 1440, 1447, 1432, 1448], ["-3.8254", "-3.8254", 1460, 1467, 1460, 1467], ["-5.1192", "\\textbf{\\textbf{-5.1192}}", 1488, 1495, 1472, 1497], ["0.00508", "0.00508", 1518, 1525, 1518, 1525], ["0.00471", "\\textbf{\\textbf{0.00471}}", 1547, 1554, 1531, 1556]], "text_chunk_selected": "\\textbf{Training}\nWe adopt a training procedure where we train hypernetwork using the maximum likelihood estimation for the NHP model. For each seen sequence $\\mathcal{T}^s$ from $D^S$, we sample a mini-batch consisting of events. The sequence descriptor $\\mathbf{d}^s$ of this sequence is used to generate parameters of the neural Hawkes process using the hyper-network and its parameters.  These values are then used in Equation \\ref{eq:log_lik_nhp} to find the log-likelihood of the event times of the seen sequences. So, the log-likelihood described in Equation \\ref{eq:log_lik_nhp} will now be:\n\\setlength{\\abovedisplayskip}{0pt}\n\\setlength{\\belowdisplayskip}{0pt}\n\\setlength{\\abovedisplayshortskip}{0pt}\n\\setlength{\\belowdisplayshortskip}{0pt}\n\nGiven a sequence description $\\mathbf{d}^s$ for the descriptor $\\mathcal{T}^s$, our descriptor conditioned hypernetwork $f_r(\\cdot)$ can generate parameters $W_r^s$ and $f_t(\\cdot)$ can generate parameters $W_t^s$. To perform continual learning, we use regularization to penalize changes in $\\{W_r^c, W_t^c\\}$ generated for past sequences in order to retain information from those sequences and to learn continually. The  regularization is applied to the hypernetwork parameters while learning a new event sequence, and this prevents adaptation of the hypernetworks parameters completely to the new event sequence.  For a new event sequence $\\mathcal{T}^s$ and its corresponding descriptor $\\mathbf{d}^s$, the hypernetwork parameters are learnt by minimizing the following continual learning loss over events in the  sequence:\n\\setlength{\\abovedisplayskip}{0pt}\n\\setlength{\\belowdisplayskip}{0pt}\n\\setlength{\\abovedisplayshortskip}{0pt}\n\\setlength{\\belowdisplayshortskip}{0pt}\n\n\\begin{equation}\n\\label{eq:log_lik_nhp_cl}\n\\begin{split}\n&\\sum_{j=1}^{n^s}  -\\log p(t^s_j|\\mathcal{H}^s_j;(f_r(\\mathbf{d}^s;\\theta_{fr}), \nf_t(\\mathbf{d}^s;\\theta_{ft}))) \\\\\n & +\\frac{\\beta}{s-1}\\sum_{c=1}^{s-1}\\biggl(\\parallel f_r(\\mathbf{d}^c;\\theta_{fr})-f_r(\\mathbf{d}^c; \\bar{\\theta}_{fr})\\!\\!\\parallel^2 \\\\\n& + \\parallel f_t(\\mathbf{d}^c; \\theta_{ft}) - f_t(\\mathbf{d}^c; \\bar{\\theta}_{ft}) \\parallel^2 \\biggr)\n \\end{split}\n\\end{equation}\n\nwhere $\\{\\bar{\\theta}_{fr}, \\bar{\\theta}_{ft}\\}$ represents the stored hypernetwork parameters after learning until  sequence $s-1$ and $\\{\\theta_{fr}, \\theta_{ft}\\}$ represent the hypernetwork parameters learnt considering the event sequence $s$ and regularization to avoid forgetting. The regularization term ensures that the newly learnt hyper-network parameters will be able to produce the required main network parameters from the past event sequences given the sequence descriptor without forgetting and the regularization constant $\\beta$ captures the importance associated with it.   So, in this way, we try to retain the information from previous sequences at a meta-level. By including a simple regularization term within the framework of \\textit{HyperHawkes}, our model is capable of learning sequences continually without forgetting knowledge learnt from previous sequences. We are able to achieve this because of the use of sequence-conditioned hypernetwork on the top of neural Hawkes process, emphasizing its usefulness for continual learning over event sequences in addition to zero shot learning. \n\n\\subsection{Baselines}\nTo the best of our knowledge, the proposed problem statement is the first work along this direction. Therefore, we propose our own baselines as - \\textbf{1) FNHP:} This includes fully neural Hawkes process \\cite{omi2019fully}. This approach doesn't incorporate the sequence descriptor. \\textbf{2) FNHP-Descriptor:} In this variant, we use concatenated descriptor and time as input to RNN and FNN.  For Continual learning setup, we compare against HyperHawkes without any regularization as baseline. \n\n\\subsection{Experimental Setup}\nWe consider these experimental setups to evaluate the performance of our model - \n\\textbf{1) Zero-Shot:} Training is done on seen sequences and testing is done on unseen sequences.  \\textbf{2) Generalized Zero-Shot:} In this, testing is done by randomly sampling 20\\% of events from seen sequences and unseen sequences.  \\textbf{3) Standard Event Modeling:} Training is done on the first 70\\% of the events from all sequences. Testing is done on the last 20\\% events for unseen sequences. \nMean negative log-likelihood (MNLL) and mean absolute error (MAE) are considered as evaluation metrics for both zero-shot and continual learning setup. Lower MNLL and MAE indicates better performance.\n\n\\section{Results and Analysis}\n\\subsection{Zero-Shot Learning}\nResults for zero-shot setup are presented in Table \\ref{Tab:zsl}. A better model is expected to have lower MNLL and MAE. We can observe that the proposed method HyperHawkes performs better than the baselines in terms of both evaluation metrics MNLL and MAE. We can observe that zero-shot setup, which consists of predictive performance for unseen sequences, exhibits significantly lower MNLL and MAE for HyperHawkes-FNN as compared to FNHP and FNHP-Descriptor. Also, HyperHawkes-FNN-RNN yields lower MAE for Meme dataset, however, MNLL for both the variants of the proposed methods is close. Comparing the results of generalized zero-shot setup where we consider instances from seen sequences as well as unseen sequences, we can observe that HyperHawkes-FNN-RNN performs better for both the datasets. The final section of the table discusses results for standard event modeling where we test on the last 20\\% of the events of unseen sequences. HyperHawkes-FNN-RNN performs well here as well except for MAE for Yelp dataset. We can also observe that HyperHawkes-FNN-RNN performs slightly better for generalized zero-shot and standard event modeling setup. Moreover, it is interesting to note that inclusion of sequence descriptors within FNHP in FNHP-Descriptor has not helped much in prediction of unseen sequences. In fact, in various cases, it has performed worse than FNHP itself. This confirms the necessity of an event sequence specific framework which can predict better for unseen sequences as well. Therefore, our results indicate that the proposed variants of HyperHawkes perform consistently and significantly better than the baselines for both the datasets.  \n\n\\subsection{Continual Learning}\nTable \\ref{Tab:cl} presents the averaged results over all tasks by enabling HyperHawkes for continual learning. We can observe that averaged performance for the proposed model is better than the case when no regularization is incorporated. Also, we can observe that both the proposed variants perform better than the model without using regularization (corresponds to the case when $\\beta$ from Equation \\ref{eq:log_lik_nhp_cl} is set to 0). Hence the use of regularization within the framework of HyperHawkes supports that proposed method can avoid catastrophic forgetting. Fig \\ref{fig:cl_plots} displays sequence-wise performance for both the datasets for the proposed variants HyperHawkes-FNN and HyperHawkes-FNN-RNN.\\ref{fig:avg_mnll_yelp}) displays average MNLL over previous sequences for both the models for Yelp. This shows that while training the model without regularization over new sequences, the network is unable to retain information learnt from previous sequences, hence MNLL increases as we train new sequences. However, with the use of regularization, we can avoid catastrophic forgetting, hence having lower MNLL for successive tasks. Similar behavior is observed by \\ref{fig:cl_chfn_rnn_averaged_mae_meme}) as well which displays average MAE over previous sequences for both the variants, with and without CL for Meme. So, this corroborates that use of regularization with HyperHawkes can help in backward transfer. \\ref{fig:mnll_yelp}) shows MNLL for each sequence using the model HyperHawkes-FNN-RNN with regularization. MNLL for the model with regularization is having lower MNLL as compared to the model without any regularization. This essentially reflects that the proposed model is able to forward transfer the knowledge learnt from previous sequences as well. So, the model is able to perform forward and backward transfer, which are important continual learning desiderata. \\ref{fig:ablation}) displays the effect of various regularization parameters for Yelp and Meme dataset for HyperHawkes-FNN. A possible explanation could be for Meme for $\\beta$, the model is not able to learn from previous sequences and for large $\\beta$ might not be able to learn from new sequences. To conclude, presented results suggest that proposed framework can aid in avoiding catastrophic forgetting while learning continually.", "table_source": "\\begin{table*}\n\\centering\n\\caption{Results for Continual Learning by comparing the performance of the proposed HyperHawkes with regularization against under HyperHawkes without regularization (Lower MNLL and MAE indicates better performance) \n}\n\\label{Tab:cl}\n\\tabcolsep=0.2cm\n\\begin{tabular}{c|cc|cc||cc|cc} \n\\hhline{=====:t:====}\n\\multirow{3}{*}{Dataset} & \\multicolumn{4}{c||}{HyperHawkes-FNN}                                                          & \\multicolumn{4}{c}{HyperHawkes-FNN-RNN}                                                                           \\\\ \n\\cline{2-9}\n                         & \\multicolumn{2}{c|}{MNLL}                               & \\multicolumn{2}{c||}{MAE}            & \\multicolumn{2}{c|}{MNLL}                               & \\multicolumn{2}{c}{MAE}                                 \\\\ \n\\cline{2-9}\n                         & WithoutCL & WithCL                                      & WithoutCL & WithCL                   & WithoutCL & WithCL                                      & WithoutCL & WithCL                                      \\\\ \n\\hline\nYelp                     & -4.8627   & \\textbf{\\textbf{\\textbf{\\textbf{-5.6928}}}} & 0.00159    & \\textbf{\\textbf{0.00149}} & -5.2629   & \\textbf{\\textbf{\\textbf{\\textbf{-5.7727}}}} & 0.00152    & \\textbf{\\textbf{\\textbf{\\textbf{0.00150}}}}  \\\\\nMeme                     & -2.8550   & \\textbf{\\textbf{-5.1462}}                   & 0.00548    & \\textbf{0.00471}          & -3.8254   & \\textbf{\\textbf{-5.1192}}                   & 0.00508    & \\textbf{\\textbf{0.00471}}                    \\\\\n\\hhline{=====:b:====}\n\\end{tabular}\n\\end{table*}", "cell_list_gold": [{"value": "-4.8627", "char_index": [1116, 1123], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN", "model settings": {"WithoutCL": "true"}}, {"value": "-5.6928", "char_index": [1160, 1167], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN", "model settings": {"WithCL": "true"}}, {"value": "0.00159", "char_index": [1174, 1181], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN", "model settings": {"WithoutCL": "true"}}, {"value": "0.00149", "char_index": [1203, 1210], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN", "model settings": {"WithCL": "true"}}, {"value": "-5.2629", "char_index": [1215, 1222], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"WithoutCL": "true"}}, {"value": "-5.7727", "char_index": [1259, 1266], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"WithCL": "true"}}, {"value": "0.00152", "char_index": [1273, 1280], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"WithoutCL": "true"}}, {"value": "0.00150", "char_index": [1318, 1325], "type": "Result", "training data/set": "Yelp", "test data/set": "Yelp", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"WithCL": "true"}}, {"value": "-2.8550", "char_index": [1361, 1368], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN", "model settings": {"WithoutCL": "true"}}, {"value": "-5.1462", "char_index": [1389, 1396], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN", "model settings": {"WithCL": "true"}}, {"value": "0.00548", "char_index": [1419, 1426], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN", "model settings": {"WithoutCL": "true"}}, {"value": "0.00471", "char_index": [1440, 1447], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN", "model settings": {"WithCL": "true"}}, {"value": "-3.8254", "char_index": [1460, 1467], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"WithoutCL": "true"}}, {"value": "-5.1192", "char_index": [1488, 1495], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MNLL", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"WithCL": "true"}}, {"value": "0.00508", "char_index": [1518, 1525], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"WithoutCL": "true"}}, {"value": "0.00471", "char_index": [1547, 1554], "type": "Result", "training data/set": "Meme", "test data/set": "Meme", "task": ["event modeling", "time-to-event modeling"], "metric": "MAE", "experimental settings": {"Continual Learning": "true"}, "model": "HyperHawkes-FNN-RNN", "model settings": {"WithCL": "true"}}]}, "2210.00448v1_table10": {"table_code": "\\begin{table}[H]\n  \\centering\n    \\small\n    \\caption{Inference speed on Jetson Nano at maximum power mode}\n  \\begin{tabular}{p{3cm}p{2.5cm}p{2cm}}\n    \\toprule\n    \\cmidrule(r){1-3}\nModel Architecture   & Input Resolution \\newline (pixels) & Inference Per \\newline second \\\\\n    \\midrule\nEfficientNetB0       & 384x288                   & 19                   \\\\\nMobileNet V3   Large & 512x384                   & 25                   \\\\\nMobileNet V3   Large & 224x224                   & 40          \\\\                 \n    \\bottomrule\n    \n  \\end{tabular}\n  \\label{tab:11}\n\\end{table}", "table_label": "{tab:11}", "table_numeric_cells": [["384x288", "384x288", 312, 319, 312, 319], ["19", "19", 340, 342, 340, 342], ["512x384", "512x384", 387, 394, 387, 394], ["25", "25", 415, 417, 415, 417], ["224x224", "224x224", 462, 469, 462, 469], ["40", "40", 490, 492, 490, 492]], "text_chunk_selected": "] \n\\section{Introduction}\nWaste generation has increased dramatically in the 21st century due to the growth in the global population. According to the World Bank Group's report \\cite{WorldBankGroup2018What2.0}, 2.01 billion tonnes of municipal solid waste are generated worldwide every year, with only about 19\\% adequately recycled  \\cite{WorldBankGroup2018What2.0}. Recycling not only helps to preserve raw material but, more importantly, reduces the landfills required, which is an undesirable way of waste disposal due to its high demand for space and the danger of introducing contaminants into the ground or even groundwater system. A major step of recycling is to separate the waste into specific categories according to its material. Failure to do so will significantly harm the effectiveness of recycling. Traditionally, workers at the recycling company sort the waste into corresponding categories by hand, which is an inefficient method and requires unnecessary labor. Therefore, people are now in dire need of a more advanced and automated waste separation system.\\par\nImage classification machine learning (ML) algorithms have been used to build automatic waste classification systems and assist waste management. This research aimed to improve the existing classification system and pursue the commercialization of an artificial intelligence (AI) street bin. Specifically, we focused on reducing the power consumption of the controller board to extend AI bin\u2019s battery life, increasing its waste classification accuracy and reducing its price. \\par\nThe only intelligent recycling bin on the market, Bin-e  \\cite{Bin-e}, uses image classification algorithms to separate the trash into four categories: plastic, paper, metal and glass. It achieves a waste segmentation accuracy of 92\\% \\cite{Bin-e}. However, the price USD 5200 \\cite{Bin-e} is expensive for a rubbish bin. Also, Bin-e\u2019s operation requires a 230V power supply, so it cannot replace the traditional trash bins at the locations without plug sockets. Therefore, it can hardly increase the household and street recycling rate. As a result, this paper proposes low-cost battery-powered real-time recycling waste segmentation bin systems to fill this gap.\\par\nIn this research, we trained a light MobileNet image classification model for Jetson Nano with TrashNet  \\cite{GaryThungGarythung/trashnet:Classification}, an open-source waste dataset, and 1800 new training samples collected by us. The model demonstrated great performance with a high test accuracy of 95.98\\% and a small parameter size of 3.0 M. Jetson Nano only consumed 4.7 W when it ran the model. We also found a cheaper and less energy-hungry device, K210 \\cite{CanaanKendryteK210} to further reduce power consumption. The K210 board only consumed 0.89 W at inference time and the model on it achieved a high test accuracy of 96.64\\%, which made it a more practical solution for trash bin applications. The code used in this study is available on Github \\cite{XueyingLiMolvcanAN-SMART-RECYCLING-BIN-USING-WASTE-IMAGE-CLASSIFICATION-AT-THE-EDGE}.\\par\nThis paper is organized as follows. In Section \\ref{sec:2}, the related works performed in waste classification and AI trash bins are introduced. Section \\ref{sec:3} introduces the system design and explains the background theories that support this study. The methodology of data collection and model training is introduced in section \\ref{sec:4}. Section \\ref{sec:5} analyses the test results and evaluates the system performance on Jetson Nano and K210. Finally, Section \\ref{sec:6} concludes the achievements and provides suggestions for future research.\n\nThe development of machine learning and image classification enables the bin to sort the waste based on visual input like a human. The convolutional neural network (CNN) is a branch of image classification algorithms that performs mainly convolution operations on the pixels \\cite{Yamashita2018ConvolutionalRadiology}. It is the most popular choice due to its high accuracy and power efficiency compared to other methods and is used in all the four papers \\cite{Ziouzios2019AClassification}\\cite{White2020WasteNet:Bins}\\cite{Jimeno2021DevelopmentProcessing}\\cite{Sallang2021AEnvironment}. It gives the bin ability to differentiate between spoons and cups, which is tremendous progress compared to the sensor-based approaches. Traditionally, the CNN models run on a cloud raising the data transmission latency and user privacy security problems. To solve these problems, recent research moved the computation to an edge embedded system. However, edge computing has the drawback of limiting computation resources, so the model size is important in selecting the CNN model structure.\\par\n\nThe waste classification models in the previous study \\cite{Abdulmahmood2021ImprovingAnalysis} have five output classes: \"paper\", \"metal\", \"plastic\", \"cardboard\" and \"glass\". They will be reproduced in the next section as the benchmark models of our study. Our model will be trained with two new classes, \"empty\" and \"hand\". \"Empty\" means there is nothing in the photo, while the \"hand\" group detects a human's hand to avoid trapping the user's hand by the door. When these two groups are detected, the bin waits and continues detecting. \n\\subsection{The Benchmark models }\\label{bench}\nThe waste classification models in \\cite{Abdulmahmood2021ImprovingAnalysis} consist of an input layer, a pre-processing augmentation layer, an EfficientNet B0 base model layer, a global average pooling 2D layer and a dense layer. The first model has an input layer size of 512x384 pixels, while the second model input has 384x288 pixels. Table \\ref{tab:2} shows the configuration of the augmentation layer and the model training hyperparameters for reproduction. \\par\n\nAfter the model was reproduced, it was saved into TensorFlow SavedModel format and transferred to Jetson Nano. The 384x288 model was optimized successfully through TensorRT on Jetson Nano, but the 512x384 model caused an OOM error. The throughput of the accelerated model was 19 inferences per second (IPS) at the maximum power mode.\\par\nNevertheless, the 384x288 model occupied 94.9\\% of memory. The system would freeze up if we ran other applications simultaneously. To implement the classification model in an AI bin application, it is essential to replace the original base model with the lighter model, such as MobileNet V3, which has lower memory usage and power consumption. \n\\subsection{MobileNet V3 model}\nThe EfficientNet architecture used in the benchmark model has demonstrated outstanding performance on the ImageNet dataset. In particular, EfficientNet B7 has the same top-1 accuracy, 84.3\\%, as the GPipe while being 8.4x \\cite{Howard2017MobileNets:Applications}. However, as EfficientNet B0 is the smallest model in the family, with a parameter size of 4.0 M without top fully connected layers, so MobileNet V3 Large, which only has 3.0M parameters and top-1 accuracy of 1.1\\% less than EfficientNet on ImageNet, is chosen as the substitute. \nMobileNet is a series of CNN architectures developed by Google. It contains three architectures: MobileNet V1 \\cite{Howard2017MobileNets:Applications}, V2 \\cite{Sandler2018MobileNetV2:Bottlenecks} and V3 \\cite{Howard2019SearchingMobileNetV3}. MobileNet V2 achieved higher accuracy on ImageNet with a smaller parameter size than MobileNet V1. The MobileNet V3 optimized the latency of MobileNet V2. \\par\n\n\\subsection{Train the models with transfer learning}\nThe two datasets were used to train new classification models with TensorFlow version 2.9. Firstly, we replaced the EfficientNet B0 model layer in the benchmark model with MobileNet V3 architecture initialized with the pre-trained weights on ImageNet. The MobileNet model has a smaller parameter size and memory usage of the model. Then, the output size of the dense layer was changed to seven to add two new classes to the model. Finally, a resizing layer is inserted between the data augmentation layer and the base model layer to resize the input from 512x384 pixels to 224x224 pixels. By compressing the input after data augmentation, we obtained a clearer image for base model input than compressing the data before the data augmentation layer. \\par\nFigure \\ref{fig:6} illustrates the structure of our model. The model was trained with the hyperparameters shown in Table \\ref{tab:6}. The final model was converted to the TensorRT model with TensorFlow version 2.5, and its performance was measured on Jetson Nano. \n\n\\section{Results and discussion}\\label{sec:5}\nThe accuracies of models obtained from transfer learning will be analyzed in this section. Our best model, the 224x224 pixels MobileNet V3 Large model, achieved an accuracy of 95.98\\% in classifying the images into seven groups. The model had 40 IPS on Jetson Nano at 5 W power mode and consumed 30\\% less power than the EfficientNet B0 model. The K210 model achieved a further 96.64\\% on waste classification while the board's power only drew 0.89 W. \n\\subsection{Accuracies of models trained with TrashNet dataset on Jetson Nano }\nAs mentioned in section \\ref{bench}, The EfficientNet B0 model has achieved high top-1 accuracy on original test data. However, we could not use this compute-intensive model directly in the application. So, we aimed to construct a less resource-intensive model while maintaining higher accuracy than the benchmark model.\\par\n\nWe trained EfficientNetB0 and MobileNet V3 Large with the new training data. The same hyperparameters in Table \\ref{tab:6} were used to train both models. \nThe result is shown in Table \\ref{tab:10}. The accuracy on 5-class test set of the EfficientNet B0 model increased from 94.40\\% in Table \\ref{tab:9} to 95.23\\%. This indicates that the new dataset had a positive effect on the models and helped them to learn more general features that assist classification tasks. Our photos have different waste samples, lighting and background, so the accuracy on TrashNet will not increase significantly. \\par\n\nFinally, The model was accelerated into the TensorRT model and ran on the Jetson Nano. Table \\ref{tab:11} shows the inference speed of the three models. Compared to the 384x288 EfficientNet model, the MobileNet V3 computation speed increased because of the reduction of parameter size. The speed doubled for MobileNet V3 with a resolution of 224x224 pixels", "table_source": "\\begin{table}[H]\n  \\centering\n    \\small\n    \\caption{Inference speed on Jetson Nano at maximum power mode}\n  \\begin{tabular}{p{3cm}p{2.5cm}p{2cm}}\n    \\toprule\n    \\cmidrule(r){1-3}\nModel Architecture   & Input Resolution \\newline (pixels) & Inference Per \\newline second \\\\\n    \\midrule\nEfficientNetB0       & 384x288                   & 19                   \\\\\nMobileNet V3   Large & 512x384                   & 25                   \\\\\nMobileNet V3   Large & 224x224                   & 40          \\\\                 \n    \\bottomrule\n    \n  \\end{tabular}\n  \\label{tab:11}\n\\end{table}", "cell_list_gold": [{"value": "384x288", "char_index": [312, 319], "type": "Hyper-parameter/Architecture", "model": "EfficientNetB0", "parameter/architecture name": "Input Resolution (pixels)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "19", "char_index": [340, 342], "type": "Hyper-parameter/Architecture", "model": "EfficientNetB0", "parameter/architecture name": "Inference Per second", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "512x384", "char_index": [387, 394], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Large", "parameter/architecture name": "Input Resolution (pixels)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "25", "char_index": [415, 417], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Large", "parameter/architecture name": "Inference Per second", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "224x224", "char_index": [462, 469], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Large", "parameter/architecture name": "Input Resolution (pixels)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "40", "char_index": [490, 492], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Large", "parameter/architecture name": "Inference Per second", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}]}, "2210.00448v1_table5": {"table_code": "\\begin{table}[H]\n  \\centering\n    \\small\n    \\caption{Training parameters used to build the classification model on Jetson Nano}\n  \\begin{tabular}{ll}\n    \\toprule\n    \\cmidrule(r){1-2}\n    Training Parameter     & Value \\\\\n    \\midrule\n    Learning rate scheduler & Constant learning rate scheduler \\\\\n    Train/Validation/Test split ratio & 72/18/10\\\\\n    Optimizer & Adam optimiser \\\\\n    Training epochs & 50\\\\\n    Learning rate&\t4.3e-05\\\\\n    Fine-tuning training epochs\t&10\\\\\nFine-tuning learning rate&\t4e-06\\\\\nLoss functions&\tSparse categorical cross entropy\\\\\nClassifier activation function\t&Softmax\\\\\nInclude top layers in base model&\tFalse\\\\\n    Batch size & 16 \\\\\n    Base model dropout rate& 0.2 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\label{tab:6}\n\\end{table}", "table_label": "{tab:6}", "table_numeric_cells": [["50", "50", 410, 412, 410, 412], ["4.3e-05", "4.3e-05", 434, 441, 434, 441], ["10", "10", 477, 479, 477, 479], ["4e-06", "4e-06", 509, 514, 509, 514], ["16", "16", 669, 671, 669, 671], ["0.2", "0.2", 704, 707, 704, 707]], "text_chunk_selected": "The waste classification models in the previous study \\cite{Abdulmahmood2021ImprovingAnalysis} have five output classes: \"paper\", \"metal\", \"plastic\", \"cardboard\" and \"glass\". They will be reproduced in the next section as the benchmark models of our study. Our model will be trained with two new classes, \"empty\" and \"hand\". \"Empty\" means there is nothing in the photo, while the \"hand\" group detects a human's hand to avoid trapping the user's hand by the door. When these two groups are detected, the bin waits and continues detecting. \n\\subsection{The Benchmark models }\\label{bench}\nThe waste classification models in \\cite{Abdulmahmood2021ImprovingAnalysis} consist of an input layer, a pre-processing augmentation layer, an EfficientNet B0 base model layer, a global average pooling 2D layer and a dense layer. The first model has an input layer size of 512x384 pixels, while the second model input has 384x288 pixels. Table \\ref{tab:2} shows the configuration of the augmentation layer and the model training hyperparameters for reproduction. \\par\n\nTo begin with, the base model is initialized with pre-trained weights on ImageNet. The two models are trained separately by setting the corresponding input sizes. The learning rate was set to 4.3e-05 at the first 50 epochs and was reduced to 4e-06 at the eight epochs afterwards. Eventually, both models achieved the same test accuracy of 95.38\\%. Figure \\ref{fig:2} shows the confusion matrix of the test results and indicates that most mistakes are made in classifying between paper and cardboard.\\par\n\nAfter the model was reproduced, it was saved into TensorFlow SavedModel format and transferred to Jetson Nano. The 384x288 model was optimized successfully through TensorRT on Jetson Nano, but the 512x384 model caused an OOM error. The throughput of the accelerated model was 19 inferences per second (IPS) at the maximum power mode.\\par\nNevertheless, the 384x288 model occupied 94.9\\% of memory. The system would freeze up if we ran other applications simultaneously. To implement the classification model in an AI bin application, it is essential to replace the original base model with the lighter model, such as MobileNet V3, which has lower memory usage and power consumption. \n\\subsection{MobileNet V3 model}\nThe EfficientNet architecture used in the benchmark model has demonstrated outstanding performance on the ImageNet dataset. In particular, EfficientNet B7 has the same top-1 accuracy, 84.3\\%, as the GPipe while being 8.4x \\cite{Howard2017MobileNets:Applications}. However, as EfficientNet B0 is the smallest model in the family, with a parameter size of 4.0 M without top fully connected layers, so MobileNet V3 Large, which only has 3.0M parameters and top-1 accuracy of 1.1\\% less than EfficientNet on ImageNet, is chosen as the substitute. \nMobileNet is a series of CNN architectures developed by Google. It contains three architectures: MobileNet V1 \\cite{Howard2017MobileNets:Applications}, V2 \\cite{Sandler2018MobileNetV2:Bottlenecks} and V3 \\cite{Howard2019SearchingMobileNetV3}. MobileNet V2 achieved higher accuracy on ImageNet with a smaller parameter size than MobileNet V1. The MobileNet V3 optimized the latency of MobileNet V2. \\par\n\n\\section{Methodology}\\label{sec:4}\nThis section introduces the development of the classification model on Jetson Nano and K210. We collected 1872 new recycling waste data to train new models. Experiments were carried out to evaluate the models' accuracy and power consumption.\\par\n\\subsection{Collect more training data}\nFirstly, more recycling waste images were collected to increase the training dataset. As the TrashNet dataset only contains waste images with white background, making the classification model vulnerable to variation in lighting conditions and dirt or contamination in the background. Waste photos with a black background were taken to tell the model the background color is irrelevant in classification and increase the robustness of real-world cases. Additionally, we want to add two classes, \"empty\" and \"hand\" to the model, but we do not have images belonging to these categories to train the model. Therefore, we built a detection model with plywood material and collected a domain-specific waste dataset.\\par\nThe waste samples were collected from a nearby recycling bin and from people directly. Figure \\ref{fig:3} shows a waste photo taken by the camera at the top.\\par\n\n\\subsection{Train the models with transfer learning}\nThe two datasets were used to train new classification models with TensorFlow version 2.9. Firstly, we replaced the EfficientNet B0 model layer in the benchmark model with MobileNet V3 architecture initialized with the pre-trained weights on ImageNet. The MobileNet model has a smaller parameter size and memory usage of the model. Then, the output size of the dense layer was changed to seven to add two new classes to the model. Finally, a resizing layer is inserted between the data augmentation layer and the base model layer to resize the input from 512x384 pixels to 224x224 pixels. By compressing the input after data augmentation, we obtained a clearer image for base model input than compressing the data before the data augmentation layer. \\par\nFigure \\ref{fig:6} illustrates the structure of our model. The model was trained with the hyperparameters shown in Table \\ref{tab:6}. The final model was converted to the TensorRT model with TensorFlow version 2.5, and its performance was measured on Jetson Nano. \n\n\\section{Results and discussion}\\label{sec:5}\nThe accuracies of models obtained from transfer learning will be analyzed in this section. Our best model, the 224x224 pixels MobileNet V3 Large model, achieved an accuracy of 95.98\\% in classifying the images into seven groups. The model had 40 IPS on Jetson Nano at 5 W power mode and consumed 30\\% less power than the EfficientNet B0 model. The K210 model achieved a further 96.64\\% on waste classification while the board's power only drew 0.89 W. \n\\subsection{Accuracies of models trained with TrashNet dataset on Jetson Nano }\nAs mentioned in section \\ref{bench}, The EfficientNet B0 model has achieved high top-1 accuracy on original test data. However, we could not use this compute-intensive model directly in the application. So, we aimed to construct a less resource-intensive model while maintaining higher accuracy than the benchmark model.\\par\n\nWe trained and tested EfficientNetB0, MobileNet V3 Large and MobileNet V3 Small with new TrashNet training set using the hyperparameters in Table \\ref{tab:6}. All the models converged quickly and reached 90\\% of the highest validation accuracy after 8 epochs. Table \\ref{tab:9} compares the results we obtained.\n\nWe trained EfficientNetB0 and MobileNet V3 Large with the new training data. The same hyperparameters in Table \\ref{tab:6} were used to train both models. \nThe result is shown in Table \\ref{tab:10}. The accuracy on 5-class test set of the EfficientNet B0 model increased from 94.40\\% in Table \\ref{tab:9} to 95.23\\%. This indicates that the new dataset had a positive effect on the models and helped them to learn more general features that assist classification tasks. Our photos have different waste samples, lighting and background, so the accuracy on TrashNet will not increase significantly. \\par", "table_source": "\\begin{table}[H]\n  \\centering\n    \\small\n    \\caption{Training parameters used to build the classification model on Jetson Nano}\n  \\begin{tabular}{ll}\n    \\toprule\n    \\cmidrule(r){1-2}\n    Training Parameter     & Value \\\\\n    \\midrule\n    Learning rate scheduler & Constant learning rate scheduler \\\\\n    Train/Validation/Test split ratio & 72/18/10\\\\\n    Optimizer & Adam optimiser \\\\\n    Training epochs & 50\\\\\n    Learning rate&\t4.3e-05\\\\\n    Fine-tuning training epochs\t&10\\\\\nFine-tuning learning rate&\t4e-06\\\\\nLoss functions&\tSparse categorical cross entropy\\\\\nClassifier activation function\t&Softmax\\\\\nInclude top layers in base model&\tFalse\\\\\n    Batch size & 16 \\\\\n    Base model dropout rate& 0.2 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\label{tab:6}\n\\end{table}", "cell_list_gold": [{"value": "50", "char_index": [410, 412], "type": "Hyper-parameter/Architecture", "model": "MobileNet", "parameter/architecture name": "Training epochs", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "4.3e-05", "char_index": [434, 441], "type": "Hyper-parameter/Architecture", "model": "MobileNet", "parameter/architecture name": "Learning rate", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "10", "char_index": [477, 479], "type": "Hyper-parameter/Architecture", "model": "MobileNet", "parameter/architecture name": "Fine-tuning training epochs", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "4e-06", "char_index": [509, 514], "type": "Hyper-parameter/Architecture", "model": "MobileNet", "parameter/architecture name": "Fine-tuning learning rate", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "16", "char_index": [669, 671], "type": "Hyper-parameter/Architecture", "model": "MobileNet", "parameter/architecture name": "Batch size", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "0.2", "char_index": [704, 707], "type": "Hyper-parameter/Architecture", "model": "MobileNet", "parameter/architecture name": "Base model dropout rate", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}]}, "2210.00448v1_table7": {"table_code": "\\begin{table}[H]\n  \\centering\n    \\small\n    \\caption{Training parameters used to build K210 model }\n  \\begin{tabular}{p{4cm}p{4cm}}\n    \\toprule\n    \\cmidrule(r){1-2}\n    Training Parameter     & Value \\\\\n    \\midrule\n    Learning rate scheduler & Constant learning rate scheduler \\\\\n    Optimizer & Adam optimiser \\\\\n    Learning rate&\t4.3e-05\\\\\nLoss functions&\tSparse categorical cross entropy\\\\\nClassifier activation function\t&Softmax\\\\\nInclude top layers in base model&\tFalse\\\\\n    Batch size & 16 \\\\\nDropout rate in base model&\t0.001\\\\\nDropout rate at output&\t0.001\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\label{tab:8}\n\\end{table}", "table_label": "{tab:8}", "table_numeric_cells": [["4.3e-05", "4.3e-05", 338, 345, 338, 345], ["16", "16", 500, 502, 500, 502], ["0.001", "0.001", 534, 539, 534, 539], ["0.001", "0.001", 566, 571, 566, 571]], "text_chunk_selected": "] \n\\section{Introduction}\nWaste generation has increased dramatically in the 21st century due to the growth in the global population. According to the World Bank Group's report \\cite{WorldBankGroup2018What2.0}, 2.01 billion tonnes of municipal solid waste are generated worldwide every year, with only about 19\\% adequately recycled  \\cite{WorldBankGroup2018What2.0}. Recycling not only helps to preserve raw material but, more importantly, reduces the landfills required, which is an undesirable way of waste disposal due to its high demand for space and the danger of introducing contaminants into the ground or even groundwater system. A major step of recycling is to separate the waste into specific categories according to its material. Failure to do so will significantly harm the effectiveness of recycling. Traditionally, workers at the recycling company sort the waste into corresponding categories by hand, which is an inefficient method and requires unnecessary labor. Therefore, people are now in dire need of a more advanced and automated waste separation system.\\par\nImage classification machine learning (ML) algorithms have been used to build automatic waste classification systems and assist waste management. This research aimed to improve the existing classification system and pursue the commercialization of an artificial intelligence (AI) street bin. Specifically, we focused on reducing the power consumption of the controller board to extend AI bin\u2019s battery life, increasing its waste classification accuracy and reducing its price. \\par\nThe only intelligent recycling bin on the market, Bin-e  \\cite{Bin-e}, uses image classification algorithms to separate the trash into four categories: plastic, paper, metal and glass. It achieves a waste segmentation accuracy of 92\\% \\cite{Bin-e}. However, the price USD 5200 \\cite{Bin-e} is expensive for a rubbish bin. Also, Bin-e\u2019s operation requires a 230V power supply, so it cannot replace the traditional trash bins at the locations without plug sockets. Therefore, it can hardly increase the household and street recycling rate. As a result, this paper proposes low-cost battery-powered real-time recycling waste segmentation bin systems to fill this gap.\\par\nIn this research, we trained a light MobileNet image classification model for Jetson Nano with TrashNet  \\cite{GaryThungGarythung/trashnet:Classification}, an open-source waste dataset, and 1800 new training samples collected by us. The model demonstrated great performance with a high test accuracy of 95.98\\% and a small parameter size of 3.0 M. Jetson Nano only consumed 4.7 W when it ran the model. We also found a cheaper and less energy-hungry device, K210 \\cite{CanaanKendryteK210} to further reduce power consumption. The K210 board only consumed 0.89 W at inference time and the model on it achieved a high test accuracy of 96.64\\%, which made it a more practical solution for trash bin applications. The code used in this study is available on Github \\cite{XueyingLiMolvcanAN-SMART-RECYCLING-BIN-USING-WASTE-IMAGE-CLASSIFICATION-AT-THE-EDGE}.\\par\nThis paper is organized as follows. In Section \\ref{sec:2}, the related works performed in waste classification and AI trash bins are introduced. Section \\ref{sec:3} introduces the system design and explains the background theories that support this study. The methodology of data collection and model training is introduced in section \\ref{sec:4}. Section \\ref{sec:5} analyses the test results and evaluates the system performance on Jetson Nano and K210. Finally, Section \\ref{sec:6} concludes the achievements and provides suggestions for future research.\n\n\\section{Related works}\\label{sec:2}\nThe United Kingdom (UK) government has planned to increase the household recycling rate in England to 50\\% by 2020, but only 44\\% of municipal waste was reused and recycled in 2020 \\cite{2022Progress2020}. The inconvenience and lack of knowledge are two crucial factors that prevent people from recycling \\cite{Knickmeyer2020SocialAreas}. Automatic waste segmentation bins were designed to overcome these problems by helping people classify and send the waste into the corresponding containers, making waste disposal more convenient. Table \\ref{tab:1} outlines the approaches and the primary hardware components involved in related studies to construct the waste segmentation systems. \n\nThe waste classification models in the previous study \\cite{Abdulmahmood2021ImprovingAnalysis} have five output classes: \"paper\", \"metal\", \"plastic\", \"cardboard\" and \"glass\". They will be reproduced in the next section as the benchmark models of our study. Our model will be trained with two new classes, \"empty\" and \"hand\". \"Empty\" means there is nothing in the photo, while the \"hand\" group detects a human's hand to avoid trapping the user's hand by the door. When these two groups are detected, the bin waits and continues detecting. \n\\subsection{The Benchmark models }\\label{bench}\nThe waste classification models in \\cite{Abdulmahmood2021ImprovingAnalysis} consist of an input layer, a pre-processing augmentation layer, an EfficientNet B0 base model layer, a global average pooling 2D layer and a dense layer. The first model has an input layer size of 512x384 pixels, while the second model input has 384x288 pixels. Table \\ref{tab:2} shows the configuration of the augmentation layer and the model training hyperparameters for reproduction. \\par\n\nTo begin with, the base model is initialized with pre-trained weights on ImageNet. The two models are trained separately by setting the corresponding input sizes. The learning rate was set to 4.3e-05 at the first 50 epochs and was reduced to 4e-06 at the eight epochs afterwards. Eventually, both models achieved the same test accuracy of 95.38\\%. Figure \\ref{fig:2} shows the confusion matrix of the test results and indicates that most mistakes are made in classifying between paper and cardboard.\\par\n\nAfter the model was reproduced, it was saved into TensorFlow SavedModel format and transferred to Jetson Nano. The 384x288 model was optimized successfully through TensorRT on Jetson Nano, but the 512x384 model caused an OOM error. The throughput of the accelerated model was 19 inferences per second (IPS) at the maximum power mode.\\par\nNevertheless, the 384x288 model occupied 94.9\\% of memory. The system would freeze up if we ran other applications simultaneously. To implement the classification model in an AI bin application, it is essential to replace the original base model with the lighter model, such as MobileNet V3, which has lower memory usage and power consumption. \n\\subsection{MobileNet V3 model}\nThe EfficientNet architecture used in the benchmark model has demonstrated outstanding performance on the ImageNet dataset. In particular, EfficientNet B7 has the same top-1 accuracy, 84.3\\%, as the GPipe while being 8.4x \\cite{Howard2017MobileNets:Applications}. However, as EfficientNet B0 is the smallest model in the family, with a parameter size of 4.0 M without top fully connected layers, so MobileNet V3 Large, which only has 3.0M parameters and top-1 accuracy of 1.1\\% less than EfficientNet on ImageNet, is chosen as the substitute. \nMobileNet is a series of CNN architectures developed by Google. It contains three architectures: MobileNet V1 \\cite{Howard2017MobileNets:Applications}, V2 \\cite{Sandler2018MobileNetV2:Bottlenecks} and V3 \\cite{Howard2019SearchingMobileNetV3}. MobileNet V2 achieved higher accuracy on ImageNet with a smaller parameter size than MobileNet V1. The MobileNet V3 optimized the latency of MobileNet V2. \\par\n\n\\subsection{Train the models with transfer learning}\nThe two datasets were used to train new classification models with TensorFlow version 2.9. Firstly, we replaced the EfficientNet B0 model layer in the benchmark model with MobileNet V3 architecture initialized with the pre-trained weights on ImageNet. The MobileNet model has a smaller parameter size and memory usage of the model. Then, the output size of the dense layer was changed to seven to add two new classes to the model. Finally, a resizing layer is inserted between the data augmentation layer and the base model layer to resize the input from 512x384 pixels to 224x224 pixels. By compressing the input after data augmentation, we obtained a clearer image for base model input than compressing the data before the data augmentation layer. \\par\nFigure \\ref{fig:6} illustrates the structure of our model. The model was trained with the hyperparameters shown in Table \\ref{tab:6}. The final model was converted to the TensorRT model with TensorFlow version 2.5, and its performance was measured on Jetson Nano. \n\n\\subsection{Build and implement the K210 model}\nAfter building the AI application on Jetson Nano, we used a cheaper and less power-hungry processor, K210, to run the waste classification model. K210 used its own model format called kmodel. Sipeed provides sample code to train classification model based on MobileNet V1 and convert it to kmodel model format with nncase \\cite{SipeedSipeedMaix_train}. They applied data augmentation to the image data, as shown in Table \\ref{tab:7}, and built a model with structure with TensorFlow, shown in Figure \\ref{fig:7}. \n\nWe kept the same data augmentation setup and used the same training hyperparameters applied in the previous training, as shown in Table \\ref{tab:8}. All training data is resized into 224x224 pixels before training. The model used the same test/validation split 70/15 as our previous models on Jetson Nano. We first trained 110 epochs to find the stopping epochs and then trained the model with the stopping epochs. ", "table_source": "\\begin{table}[H]\n  \\centering\n    \\small\n    \\caption{Training parameters used to build K210 model }\n  \\begin{tabular}{p{4cm}p{4cm}}\n    \\toprule\n    \\cmidrule(r){1-2}\n    Training Parameter     & Value \\\\\n    \\midrule\n    Learning rate scheduler & Constant learning rate scheduler \\\\\n    Optimizer & Adam optimiser \\\\\n    Learning rate&\t4.3e-05\\\\\nLoss functions&\tSparse categorical cross entropy\\\\\nClassifier activation function\t&Softmax\\\\\nInclude top layers in base model&\tFalse\\\\\n    Batch size & 16 \\\\\nDropout rate in base model&\t0.001\\\\\nDropout rate at output&\t0.001\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\label{tab:8}\n\\end{table}", "cell_list_gold": [{"value": "4.3e-05", "char_index": [338, 345], "type": "Hyper-parameter/Architecture", "model": ["MobileNet", "MobileNet V1"], "parameter/architecture name": "Learning rate", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "16", "char_index": [500, 502], "type": "Hyper-parameter/Architecture", "model": ["MobileNet", "MobileNet V1"], "parameter/architecture name": "Batch size", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "0.001", "char_index": [534, 539], "type": "Hyper-parameter/Architecture", "model": ["MobileNet", "MobileNet V1"], "parameter/architecture name": "Dropout rate in base model", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "0.001", "char_index": [566, 571], "type": "Hyper-parameter/Architecture", "model": ["MobileNet", "MobileNet V1"], "parameter/architecture name": "Dropout rate at output", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}]}, "2210.00448v1_table8": {"table_code": "\\begin{table*}[!ht]\n \\caption{Accuracy of three models on 70/15/15 split}\n  \\centering\n  \\begin{adjustbox}{width=1\\textwidth}\n      \\small\n\\begin{tabular}{p{3cm}p{2cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}}\n\\toprule\n\\cmidrule(r){1-6}\nModel \\newline Architecture & Total \\newline parameters (M) & Total keras(.h5)\\newline model size (MB) & Training \\newline accuracy (\\%) & Top-1 Validation \\newline Accuracy (\\%) & Top-1 Accuracy \\newline on test set (\\%) \\\\\n\\midrule\nEfficientNetB0     & 4.2                  & 15.8                               & 99.44                  & 96.32                          & 94.40                           \\\\\nMobileNet V3 Large & 3.0                  & 11.7                               & 99.50                  & 94.94                          & 95.52                           \\\\\nMobileNet V3 Small & 0.94                 & 3.87                               & 98.94                  & 93.33                          & 93.56                          \\\\\n\\bottomrule\n\\end{tabular}\n  \\end{adjustbox}\n  \\label{tab:9}\n\\end{table*}", "table_label": "{tab:9}", "table_numeric_cells": [["4.2", "4.2", 483, 486, 483, 486], ["15.8", "15.8", 506, 510, 506, 510], ["99.44", "99.44", 543, 548, 543, 548], ["96.32", "96.32", 568, 573, 568, 573], ["94.40", "94.40", 601, 606, 601, 606], ["3.0", "3.0", 657, 660, 657, 660], ["11.7", "11.7", 680, 684, 680, 684], ["99.50", "99.50", 717, 722, 717, 722], ["94.94", "94.94", 742, 747, 742, 747], ["95.52", "95.52", 775, 780, 775, 780], ["0.94", "0.94", 831, 835, 831, 835], ["3.87", "3.87", 854, 858, 854, 858], ["98.94", "98.94", 891, 896, 891, 896], ["93.33", "93.33", 916, 921, 916, 921], ["93.56", "93.56", 949, 954, 949, 954]], "text_chunk_selected": "] \n\\section{Introduction}\nWaste generation has increased dramatically in the 21st century due to the growth in the global population. According to the World Bank Group's report \\cite{WorldBankGroup2018What2.0}, 2.01 billion tonnes of municipal solid waste are generated worldwide every year, with only about 19\\% adequately recycled  \\cite{WorldBankGroup2018What2.0}. Recycling not only helps to preserve raw material but, more importantly, reduces the landfills required, which is an undesirable way of waste disposal due to its high demand for space and the danger of introducing contaminants into the ground or even groundwater system. A major step of recycling is to separate the waste into specific categories according to its material. Failure to do so will significantly harm the effectiveness of recycling. Traditionally, workers at the recycling company sort the waste into corresponding categories by hand, which is an inefficient method and requires unnecessary labor. Therefore, people are now in dire need of a more advanced and automated waste separation system.\\par\nImage classification machine learning (ML) algorithms have been used to build automatic waste classification systems and assist waste management. This research aimed to improve the existing classification system and pursue the commercialization of an artificial intelligence (AI) street bin. Specifically, we focused on reducing the power consumption of the controller board to extend AI bin\u2019s battery life, increasing its waste classification accuracy and reducing its price. \\par\nThe only intelligent recycling bin on the market, Bin-e  \\cite{Bin-e}, uses image classification algorithms to separate the trash into four categories: plastic, paper, metal and glass. It achieves a waste segmentation accuracy of 92\\% \\cite{Bin-e}. However, the price USD 5200 \\cite{Bin-e} is expensive for a rubbish bin. Also, Bin-e\u2019s operation requires a 230V power supply, so it cannot replace the traditional trash bins at the locations without plug sockets. Therefore, it can hardly increase the household and street recycling rate. As a result, this paper proposes low-cost battery-powered real-time recycling waste segmentation bin systems to fill this gap.\\par\nIn this research, we trained a light MobileNet image classification model for Jetson Nano with TrashNet  \\cite{GaryThungGarythung/trashnet:Classification}, an open-source waste dataset, and 1800 new training samples collected by us. The model demonstrated great performance with a high test accuracy of 95.98\\% and a small parameter size of 3.0 M. Jetson Nano only consumed 4.7 W when it ran the model. We also found a cheaper and less energy-hungry device, K210 \\cite{CanaanKendryteK210} to further reduce power consumption. The K210 board only consumed 0.89 W at inference time and the model on it achieved a high test accuracy of 96.64\\%, which made it a more practical solution for trash bin applications. The code used in this study is available on Github \\cite{XueyingLiMolvcanAN-SMART-RECYCLING-BIN-USING-WASTE-IMAGE-CLASSIFICATION-AT-THE-EDGE}.\\par\nThis paper is organized as follows. In Section \\ref{sec:2}, the related works performed in waste classification and AI trash bins are introduced. Section \\ref{sec:3} introduces the system design and explains the background theories that support this study. The methodology of data collection and model training is introduced in section \\ref{sec:4}. Section \\ref{sec:5} analyses the test results and evaluates the system performance on Jetson Nano and K210. Finally, Section \\ref{sec:6} concludes the achievements and provides suggestions for future research.\n\nAfter the model was reproduced, it was saved into TensorFlow SavedModel format and transferred to Jetson Nano. The 384x288 model was optimized successfully through TensorRT on Jetson Nano, but the 512x384 model caused an OOM error. The throughput of the accelerated model was 19 inferences per second (IPS) at the maximum power mode.\\par\nNevertheless, the 384x288 model occupied 94.9\\% of memory. The system would freeze up if we ran other applications simultaneously. To implement the classification model in an AI bin application, it is essential to replace the original base model with the lighter model, such as MobileNet V3, which has lower memory usage and power consumption. \n\\subsection{MobileNet V3 model}\nThe EfficientNet architecture used in the benchmark model has demonstrated outstanding performance on the ImageNet dataset. In particular, EfficientNet B7 has the same top-1 accuracy, 84.3\\%, as the GPipe while being 8.4x \\cite{Howard2017MobileNets:Applications}. However, as EfficientNet B0 is the smallest model in the family, with a parameter size of 4.0 M without top fully connected layers, so MobileNet V3 Large, which only has 3.0M parameters and top-1 accuracy of 1.1\\% less than EfficientNet on ImageNet, is chosen as the substitute. \nMobileNet is a series of CNN architectures developed by Google. It contains three architectures: MobileNet V1 \\cite{Howard2017MobileNets:Applications}, V2 \\cite{Sandler2018MobileNetV2:Bottlenecks} and V3 \\cite{Howard2019SearchingMobileNetV3}. MobileNet V2 achieved higher accuracy on ImageNet with a smaller parameter size than MobileNet V1. The MobileNet V3 optimized the latency of MobileNet V2. \\par\n\n\\subsection{Train the models with transfer learning}\nThe two datasets were used to train new classification models with TensorFlow version 2.9. Firstly, we replaced the EfficientNet B0 model layer in the benchmark model with MobileNet V3 architecture initialized with the pre-trained weights on ImageNet. The MobileNet model has a smaller parameter size and memory usage of the model. Then, the output size of the dense layer was changed to seven to add two new classes to the model. Finally, a resizing layer is inserted between the data augmentation layer and the base model layer to resize the input from 512x384 pixels to 224x224 pixels. By compressing the input after data augmentation, we obtained a clearer image for base model input than compressing the data before the data augmentation layer. \\par\nFigure \\ref{fig:6} illustrates the structure of our model. The model was trained with the hyperparameters shown in Table \\ref{tab:6}. The final model was converted to the TensorRT model with TensorFlow version 2.5, and its performance was measured on Jetson Nano. \n\nWe tested both the implementation of MobileNet V1 and V2 on K210. K210 only supported MobileNet V1 with alpha equal to 0.75 and MobileNet V2 with alpha equal to 0.5. OOM error occurs when a larger model is deployed. The ImageNet  accuracies of the V1 (alpha = 0.75) and V2 (alpha = 0 .5) were 68.4\\% \\cite{Tensorflow/models/research/slim/nets/mobilenet_v1.md} and 65.4\\% \\cite{Tensorflow/models/research/slim/nets/mobilenet/} correspondingly, so only MobileNet V1 architecture was trained.\\par\nThe trained model is accelerated into kmodel format and loaded onto K210 together with the firmware using Kflash provided by Kendryte \\cite{KendryteKendryteKflash.py}. We used the firmware that only supports basic Application Programming Interface (API) and integrated development environment (IDE) on K210 to reduce memory usage.\n\n\\section{Results and discussion}\\label{sec:5}\nThe accuracies of models obtained from transfer learning will be analyzed in this section. Our best model, the 224x224 pixels MobileNet V3 Large model, achieved an accuracy of 95.98\\% in classifying the images into seven groups. The model had 40 IPS on Jetson Nano at 5 W power mode and consumed 30\\% less power than the EfficientNet B0 model. The K210 model achieved a further 96.64\\% on waste classification while the board's power only drew 0.89 W. \n\\subsection{Accuracies of models trained with TrashNet dataset on Jetson Nano }\nAs mentioned in section \\ref{bench}, The EfficientNet B0 model has achieved high top-1 accuracy on original test data. However, we could not use this compute-intensive model directly in the application. So, we aimed to construct a less resource-intensive model while maintaining higher accuracy than the benchmark model.\\par\n\nFirstly, we applied a 70/15/15 training/validation/test split to the TrashNet dataset by randomly separating 15\\% of TrashNet data to form the TrashNet test set. The rest of the TrashNet dataset became a new TrashNet training set. This ratio was used because the author in \\cite{YuTowardsClassification} demonstrated that both 15\\% and 10\\% could produce the same level of classification accuracy on the TrashNet dataset, which is higher than 20\\% and 25\\% with statistical analysis. Other research \\cite{Aral2019ClassificationModels}\\cite{Bircanoglu2018RecycleNet:Networks} also utilized 70\\% of the TrashNet data as training data. So, we consider that 70/15/15 would also be suitable for model training.\\par\n\nWe trained and tested EfficientNetB0, MobileNet V3 Large and MobileNet V3 Small with new TrashNet training set using the hyperparameters in Table \\ref{tab:6}. All the models converged quickly and reached 90\\% of the highest validation accuracy after 8 epochs. Table \\ref{tab:9} compares the results we obtained.\n\nWe trained EfficientNetB0 and MobileNet V3 Large with the new training data. The same hyperparameters in Table \\ref{tab:6} were used to train both models. \nThe result is shown in Table \\ref{tab:10}. The accuracy on 5-class test set of the EfficientNet B0 model increased from 94.40\\% in Table \\ref{tab:9} to 95.23\\%. This indicates that the new dataset had a positive effect on the models and helped them to learn more general features that assist classification tasks. Our photos have different waste samples, lighting and background, so the accuracy on TrashNet will not increase significantly. \\par", "table_source": "\\begin{table*}[!ht]\n \\caption{Accuracy of three models on 70/15/15 split}\n  \\centering\n  \\begin{adjustbox}{width=1\\textwidth}\n      \\small\n\\begin{tabular}{p{3cm}p{2cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}}\n\\toprule\n\\cmidrule(r){1-6}\nModel \\newline Architecture & Total \\newline parameters (M) & Total keras(.h5)\\newline model size (MB) & Training \\newline accuracy (\\%) & Top-1 Validation \\newline Accuracy (\\%) & Top-1 Accuracy \\newline on test set (\\%) \\\\\n\\midrule\nEfficientNetB0     & 4.2                  & 15.8                               & 99.44                  & 96.32                          & 94.40                           \\\\\nMobileNet V3 Large & 3.0                  & 11.7                               & 99.50                  & 94.94                          & 95.52                           \\\\\nMobileNet V3 Small & 0.94                 & 3.87                               & 98.94                  & 93.33                          & 93.56                          \\\\\n\\bottomrule\n\\end{tabular}\n  \\end{adjustbox}\n  \\label{tab:9}\n\\end{table*}", "cell_list_gold": [{"value": "4.2", "char_index": [483, 486], "type": "Hyper-parameter/Architecture", "model": "EfficientNetB0", "parameter/architecture name": "Total parameters (M)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "15.8", "char_index": [506, 510], "type": "Hyper-parameter/Architecture", "model": "EfficientNetB0", "parameter/architecture name": "Total keras(.h5) model size (MB)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "99.44", "char_index": [543, 548], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Training accuracy", "experimental settings": {"xx": "yy"}, "model": "EfficientNetB0", "model settings": {"xx": "yy"}}, {"value": "96.32", "char_index": [568, 573], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Validation accuracy", "experimental settings": {"xx": "yy"}, "model": "EfficientNetB0", "model settings": {"xx": "yy"}}, {"value": "94.40", "char_index": [601, 606], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on test set", "experimental settings": {"xx": "yy"}, "model": "EfficientNetB0", "model settings": {"xx": "yy"}}, {"value": "3.0", "char_index": [657, 660], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Large", "parameter/architecture name": "Total parameters (M)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "11.7", "char_index": [680, 684], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Large", "parameter/architecture name": "Total keras(.h5) model size (MB)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "99.50", "char_index": [717, 722], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Training accuracy", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "94.94", "char_index": [742, 747], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Validation accuracy", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "95.52", "char_index": [775, 780], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "0.94", "char_index": [831, 835], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Small", "parameter/architecture name": "Total parameters (M)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "3.87", "char_index": [854, 858], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Small", "parameter/architecture name": "Total keras(.h5) model size (MB)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "98.94", "char_index": [891, 896], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Training accuracy", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Small", "model settings": {"xx": "yy"}}, {"value": "93.33", "char_index": [916, 921], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Validation accuracy", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Small", "model settings": {"xx": "yy"}}, {"value": "93.56", "char_index": [949, 954], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Small", "model settings": {"xx": "yy"}}]}, "2210.00448v1_table9": {"table_code": "\\begin{table*}[!ht]\n \\caption{Accuracies of models trained by the final training data}\n  \\centering\n  \\begin{adjustbox}{width=1\\textwidth}\n      \\small\n\\begin{tabular}{p{3cm}p{2cm}p{1.2cm}p{2cm}p{2.4cm}p{2cm}p{2.4cm}}\n\\toprule\n\\cmidrule(r){1-7}\nModel \\newline Architecture & Base model \\newline input resolution (pixels) & Training accuracy (\\%) & Top-1 Accuracy on 5-class test set (\\%) & Per-class precision of 5-class test set (\\%) & Top-1 Accuracy \\newline on new 7-class test set (\\%) & Per-class precision of 7-class test set (\\%) \\\\\n\\midrule\nEfficientNet B0    & 512x384                                                                           & 99.59                  & 95.23                                   & 94.79                                        & 95.98                                       & 95.97                                        \\\\\nMobileNet V3 Large & 512x384                                                                           & 99.46                  & 94.68                                   & 94.53                                        & 95.38                                       & 95.68                                        \\\\\nMobileNet V3 Large & 224x224                                                                           & 99.46                  & 95.79                                   & 95.70                                        & 95.98                                       & 96.41                                         \\\\\n\\bottomrule\n\\end{tabular}\n  \\end{adjustbox}\n  \\label{tab:10}\n\\end{table*}", "table_label": "{tab:10}", "table_numeric_cells": [["512x384", "512x384", 570, 577, 570, 577], ["99.59", "99.59", 654, 659, 654, 659], ["95.23", "95.23", 679, 684, 679, 684], ["94.79", "94.79", 721, 726, 721, 726], ["95.98", "95.98", 768, 773, 768, 773], ["95.97", "95.97", 814, 819, 814, 819], ["512x384", "512x384", 883, 890, 883, 890], ["99.46", "99.46", 967, 972, 967, 972], ["94.68", "94.68", 992, 997, 992, 997], ["94.53", "94.53", 1034, 1039, 1034, 1039], ["95.38", "95.38", 1081, 1086, 1081, 1086], ["95.68", "95.68", 1127, 1132, 1127, 1132], ["224x224", "224x224", 1196, 1203, 1196, 1203], ["99.46", "99.46", 1280, 1285, 1280, 1285], ["95.79", "95.79", 1305, 1310, 1305, 1310], ["95.70", "95.70", 1347, 1352, 1347, 1352], ["95.98", "95.98", 1394, 1399, 1394, 1399], ["96.41", "96.41", 1440, 1445, 1440, 1445]], "text_chunk_selected": "] \n\\section{Introduction}\nWaste generation has increased dramatically in the 21st century due to the growth in the global population. According to the World Bank Group's report \\cite{WorldBankGroup2018What2.0}, 2.01 billion tonnes of municipal solid waste are generated worldwide every year, with only about 19\\% adequately recycled  \\cite{WorldBankGroup2018What2.0}. Recycling not only helps to preserve raw material but, more importantly, reduces the landfills required, which is an undesirable way of waste disposal due to its high demand for space and the danger of introducing contaminants into the ground or even groundwater system. A major step of recycling is to separate the waste into specific categories according to its material. Failure to do so will significantly harm the effectiveness of recycling. Traditionally, workers at the recycling company sort the waste into corresponding categories by hand, which is an inefficient method and requires unnecessary labor. Therefore, people are now in dire need of a more advanced and automated waste separation system.\\par\nImage classification machine learning (ML) algorithms have been used to build automatic waste classification systems and assist waste management. This research aimed to improve the existing classification system and pursue the commercialization of an artificial intelligence (AI) street bin. Specifically, we focused on reducing the power consumption of the controller board to extend AI bin\u2019s battery life, increasing its waste classification accuracy and reducing its price. \\par\nThe only intelligent recycling bin on the market, Bin-e  \\cite{Bin-e}, uses image classification algorithms to separate the trash into four categories: plastic, paper, metal and glass. It achieves a waste segmentation accuracy of 92\\% \\cite{Bin-e}. However, the price USD 5200 \\cite{Bin-e} is expensive for a rubbish bin. Also, Bin-e\u2019s operation requires a 230V power supply, so it cannot replace the traditional trash bins at the locations without plug sockets. Therefore, it can hardly increase the household and street recycling rate. As a result, this paper proposes low-cost battery-powered real-time recycling waste segmentation bin systems to fill this gap.\\par\nIn this research, we trained a light MobileNet image classification model for Jetson Nano with TrashNet  \\cite{GaryThungGarythung/trashnet:Classification}, an open-source waste dataset, and 1800 new training samples collected by us. The model demonstrated great performance with a high test accuracy of 95.98\\% and a small parameter size of 3.0 M. Jetson Nano only consumed 4.7 W when it ran the model. We also found a cheaper and less energy-hungry device, K210 \\cite{CanaanKendryteK210} to further reduce power consumption. The K210 board only consumed 0.89 W at inference time and the model on it achieved a high test accuracy of 96.64\\%, which made it a more practical solution for trash bin applications. The code used in this study is available on Github \\cite{XueyingLiMolvcanAN-SMART-RECYCLING-BIN-USING-WASTE-IMAGE-CLASSIFICATION-AT-THE-EDGE}.\\par\nThis paper is organized as follows. In Section \\ref{sec:2}, the related works performed in waste classification and AI trash bins are introduced. Section \\ref{sec:3} introduces the system design and explains the background theories that support this study. The methodology of data collection and model training is introduced in section \\ref{sec:4}. Section \\ref{sec:5} analyses the test results and evaluates the system performance on Jetson Nano and K210. Finally, Section \\ref{sec:6} concludes the achievements and provides suggestions for future research.\n\nAfter the model was reproduced, it was saved into TensorFlow SavedModel format and transferred to Jetson Nano. The 384x288 model was optimized successfully through TensorRT on Jetson Nano, but the 512x384 model caused an OOM error. The throughput of the accelerated model was 19 inferences per second (IPS) at the maximum power mode.\\par\nNevertheless, the 384x288 model occupied 94.9\\% of memory. The system would freeze up if we ran other applications simultaneously. To implement the classification model in an AI bin application, it is essential to replace the original base model with the lighter model, such as MobileNet V3, which has lower memory usage and power consumption. \n\\subsection{MobileNet V3 model}\nThe EfficientNet architecture used in the benchmark model has demonstrated outstanding performance on the ImageNet dataset. In particular, EfficientNet B7 has the same top-1 accuracy, 84.3\\%, as the GPipe while being 8.4x \\cite{Howard2017MobileNets:Applications}. However, as EfficientNet B0 is the smallest model in the family, with a parameter size of 4.0 M without top fully connected layers, so MobileNet V3 Large, which only has 3.0M parameters and top-1 accuracy of 1.1\\% less than EfficientNet on ImageNet, is chosen as the substitute. \nMobileNet is a series of CNN architectures developed by Google. It contains three architectures: MobileNet V1 \\cite{Howard2017MobileNets:Applications}, V2 \\cite{Sandler2018MobileNetV2:Bottlenecks} and V3 \\cite{Howard2019SearchingMobileNetV3}. MobileNet V2 achieved higher accuracy on ImageNet with a smaller parameter size than MobileNet V1. The MobileNet V3 optimized the latency of MobileNet V2. \\par\n\n\\subsection{Train the models with transfer learning}\nThe two datasets were used to train new classification models with TensorFlow version 2.9. Firstly, we replaced the EfficientNet B0 model layer in the benchmark model with MobileNet V3 architecture initialized with the pre-trained weights on ImageNet. The MobileNet model has a smaller parameter size and memory usage of the model. Then, the output size of the dense layer was changed to seven to add two new classes to the model. Finally, a resizing layer is inserted between the data augmentation layer and the base model layer to resize the input from 512x384 pixels to 224x224 pixels. By compressing the input after data augmentation, we obtained a clearer image for base model input than compressing the data before the data augmentation layer. \\par\nFigure \\ref{fig:6} illustrates the structure of our model. The model was trained with the hyperparameters shown in Table \\ref{tab:6}. The final model was converted to the TensorRT model with TensorFlow version 2.5, and its performance was measured on Jetson Nano. \n\nWe tested both the implementation of MobileNet V1 and V2 on K210. K210 only supported MobileNet V1 with alpha equal to 0.75 and MobileNet V2 with alpha equal to 0.5. OOM error occurs when a larger model is deployed. The ImageNet  accuracies of the V1 (alpha = 0.75) and V2 (alpha = 0 .5) were 68.4\\% \\cite{Tensorflow/models/research/slim/nets/mobilenet_v1.md} and 65.4\\% \\cite{Tensorflow/models/research/slim/nets/mobilenet/} correspondingly, so only MobileNet V1 architecture was trained.\\par\nThe trained model is accelerated into kmodel format and loaded onto K210 together with the firmware using Kflash provided by Kendryte \\cite{KendryteKendryteKflash.py}. We used the firmware that only supports basic Application Programming Interface (API) and integrated development environment (IDE) on K210 to reduce memory usage.\n\n\\section{Results and discussion}\\label{sec:5}\nThe accuracies of models obtained from transfer learning will be analyzed in this section. Our best model, the 224x224 pixels MobileNet V3 Large model, achieved an accuracy of 95.98\\% in classifying the images into seven groups. The model had 40 IPS on Jetson Nano at 5 W power mode and consumed 30\\% less power than the EfficientNet B0 model. The K210 model achieved a further 96.64\\% on waste classification while the board's power only drew 0.89 W. \n\\subsection{Accuracies of models trained with TrashNet dataset on Jetson Nano }\nAs mentioned in section \\ref{bench}, The EfficientNet B0 model has achieved high top-1 accuracy on original test data. However, we could not use this compute-intensive model directly in the application. So, we aimed to construct a less resource-intensive model while maintaining higher accuracy than the benchmark model.\\par\n\nFirstly, we applied a 70/15/15 training/validation/test split to the TrashNet dataset by randomly separating 15\\% of TrashNet data to form the TrashNet test set. The rest of the TrashNet dataset became a new TrashNet training set. This ratio was used because the author in \\cite{YuTowardsClassification} demonstrated that both 15\\% and 10\\% could produce the same level of classification accuracy on the TrashNet dataset, which is higher than 20\\% and 25\\% with statistical analysis. Other research \\cite{Aral2019ClassificationModels}\\cite{Bircanoglu2018RecycleNet:Networks} also utilized 70\\% of the TrashNet data as training data. So, we consider that 70/15/15 would also be suitable for model training.\\par\n\nWe trained and tested EfficientNetB0, MobileNet V3 Large and MobileNet V3 Small with new TrashNet training set using the hyperparameters in Table \\ref{tab:6}. All the models converged quickly and reached 90\\% of the highest validation accuracy after 8 epochs. Table \\ref{tab:9} compares the results we obtained.\n\nWe trained EfficientNetB0 and MobileNet V3 Large with the new training data. The same hyperparameters in Table \\ref{tab:6} were used to train both models. \nThe result is shown in Table \\ref{tab:10}. The accuracy on 5-class test set of the EfficientNet B0 model increased from 94.40\\% in Table \\ref{tab:9} to 95.23\\%. This indicates that the new dataset had a positive effect on the models and helped them to learn more general features that assist classification tasks. Our photos have different waste samples, lighting and background, so the accuracy on TrashNet will not increase significantly. \\par", "table_source": "\\begin{table*}[!ht]\n \\caption{Accuracies of models trained by the final training data}\n  \\centering\n  \\begin{adjustbox}{width=1\\textwidth}\n      \\small\n\\begin{tabular}{p{3cm}p{2cm}p{1.2cm}p{2cm}p{2.4cm}p{2cm}p{2.4cm}}\n\\toprule\n\\cmidrule(r){1-7}\nModel \\newline Architecture & Base model \\newline input resolution (pixels) & Training accuracy (\\%) & Top-1 Accuracy on 5-class test set (\\%) & Per-class precision of 5-class test set (\\%) & Top-1 Accuracy \\newline on new 7-class test set (\\%) & Per-class precision of 7-class test set (\\%) \\\\\n\\midrule\nEfficientNet B0    & 512x384                                                                           & 99.59                  & 95.23                                   & 94.79                                        & 95.98                                       & 95.97                                        \\\\\nMobileNet V3 Large & 512x384                                                                           & 99.46                  & 94.68                                   & 94.53                                        & 95.38                                       & 95.68                                        \\\\\nMobileNet V3 Large & 224x224                                                                           & 99.46                  & 95.79                                   & 95.70                                        & 95.98                                       & 96.41                                         \\\\\n\\bottomrule\n\\end{tabular}\n  \\end{adjustbox}\n  \\label{tab:10}\n\\end{table*}", "cell_list_gold": [{"value": "512x384", "char_index": [570, 577], "type": "Hyper-parameter/Architecture", "model": "EfficientNet B0", "parameter/architecture name": "Base model input resolution (pixels)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "99.59", "char_index": [654, 659], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Training accuracy", "experimental settings": {"xx": "yy"}, "model": "EfficientNet B0", "model settings": {"xx": "yy"}}, {"value": "95.23", "char_index": [679, 684], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on 5-class test set", "experimental settings": {"xx": "yy"}, "model": "EfficientNet B0", "model settings": {"xx": "yy"}}, {"value": "94.79", "char_index": [721, 726], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Per-class precision of 5-class test set", "experimental settings": {"xx": "yy"}, "model": "EfficientNet B0", "model settings": {"xx": "yy"}}, {"value": "95.98", "char_index": [768, 773], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on new 7-class test set", "experimental settings": {"xx": "yy"}, "model": "EfficientNet B0", "model settings": {"xx": "yy"}}, {"value": "95.97", "char_index": [814, 819], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Per-class precision of 7-class test set", "experimental settings": {"xx": "yy"}, "model": "EfficientNet B0", "model settings": {"xx": "yy"}}, {"value": "512x384", "char_index": [883, 890], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Large", "parameter/architecture name": "Base model input resolution (pixels)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "99.46", "char_index": [967, 972], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Training accuracy", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "94.68", "char_index": [992, 997], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on 5-class test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "94.53", "char_index": [1034, 1039], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Per-class precision of 5-class test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "95.38", "char_index": [1081, 1086], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on new 7-class test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "95.68", "char_index": [1127, 1132], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Per-class precision of 7-class test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "224x224", "char_index": [1196, 1203], "type": "Hyper-parameter/Architecture", "model": "MobileNet V3 Large", "parameter/architecture name": "Base model input resolution (pixels)", "dataset": ["TrashNet collected 1871 images", "The two datasets", "TrashNet", "new TrashNet"]}, {"value": "99.46", "char_index": [1280, 1285], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Training accuracy", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "95.79", "char_index": [1305, 1310], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on 5-class test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "95.70", "char_index": [1347, 1352], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Per-class precision of 5-class test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "95.98", "char_index": [1394, 1399], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Top-1 Accuracy on new 7-class test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}, {"value": "96.41", "char_index": [1440, 1445], "type": "Result", "training data/set": ["TrashNet", "new TrashNet"], "test data/set": ["TrashNet", "new TrashNet"], "task": ["image classification", "waste image classification"], "metric": "Per-class precision of 7-class test set", "experimental settings": {"xx": "yy"}, "model": "MobileNet V3 Large", "model settings": {"xx": "yy"}}]}, "2210.00543v1_table0": {"table_code": "\\begin{table}[t]\n\\centering\n\\setlength{\\tabcolsep}{11mm}{\n\\begin{tabular}{@{}cc@{}}\n\\toprule\nError Types          & Ratio \\\\ \\midrule\n\\bf{Under-spcified}       & \\bf{9.0}\\% \\\\\nOver-specified       & 5.5\\% \\\\\nSelf-reference       & 3.0\\% \\\\\nWrong part-of-speech & 1.0\\% \\\\\nOpposite             & 1.0\\% \\\\ \\bottomrule\n\\end{tabular}}\n\\caption{Ratio of each error type of the definitions generated in \\citet{huang2021definition}.}\n\\label{pic:error ratio}\n\\end{table}", "table_label": "{pic:error ratio}", "table_numeric_cells": [["9.0", "\\bf{9.0}\\%", 166, 169, 162, 172], ["5.5", "5.5\\%", 199, 202, 199, 204], ["3.0", "3.0\\%", 231, 234, 231, 236], ["1.0", "1.0\\%", 263, 266, 263, 268], ["1.0", "1.0\\%", 295, 298, 295, 300]], "text_chunk_selected": "\\section{Introduction} \\label{introduction}\nWhen readers find some expressions unfamiliar during reading a text, machines can help. The task of Definition Generation (DG) aims to generate a textual definition for a given word or phrase (the target), according to a surrounding context (the local context)~\\cite{ni-wang-2017-learning}. In addition to assisting readers in comprehending expressions, the task of DG is also useful for generating definition when building dictionaries. \nRecently, pre-trained encoder-decoder models have achieved great successes on this task~\\cite{huang2021definition,kong2022multitasking}. Despite their successes, the definitions produced by these pre-trained models often contain several types of errors~\\cite{noraset2017definition,huang2021definition}. According to Table~\\ref{pic:error ratio}, ``under-specific problem'' is the most frequent error that the generated definition conforms to the general semantics but loses certain parts of meaning of the target word. As presented in Table~\\ref{under-specific case}, the definition produced by T5 model is under-specific as it omits the meaning of \\emph{great} in the word ``double'' under the context ``\\emph{ate a double portion}''. The under-specific problem harms the accuracy of the generated definitions and in turn limits the applications of definition generation techniques in many scenarios. \n\n\\section{Related Work}\n\\subsection{Definition Generation}\nThe task of Definition Generation is firstly proposed by~\\citet{noraset2017definition}. They used word embedding to generate its corresponding definition, and utilize definition generation as an auxiliary task for reverse dictionary and word embedding training. Some later works explore more application scenarios and model architectures for definition generation. \\citet{ni-wang-2017-learning} propose a dual-encoder model to generate the proper definition of the given word under a specific context, and use it for explaining emerging words on the Internet. \\citet{gadetsky2018conditional} use both local and global information of the words in their model for word disambiguation. Following them, \\citet{ishiwatari2019learning} design gate mechanisms to fuse multi-source information of the word and context. Furthermore, some works attempt to utilize other information of the target word. \\citet{washio2019bridging} build relation of defined and defining words using word pair embedding \\cite{joshi2018pair2vec}. Different from former works that using distributed representations of target words, \\citet{yang2019incorporating} introduce target words' concepts in HowNet~\\cite{dong2003hownet} as fine-grained knowledge in Chinese definition modeling. Also, there exist literature works based on refined methods to learn the target words. Both \\citet{li2020explicit} and \\citet{reid2020vcdm} decompose the meaning of the target word into a group of latent variables and rely on variational inference for estimation. \n\nRecently, pre-trained encoder-decoder models have been used in definition generation and achieved great success. \\citet{bevilacqua2020generationary} use special tokens to mark the target word in the context and feed them into a BART model~\\cite{lewis2019bart}. \\citet{huang2021definition} fine-tune a T5 model and re-rank all the candidate results from the T5 model to obtain definitions in a proper specificity. \\citet{kong2022multitasking} design a MASS model based on multi-task framework to generate simple definition in an unsupervised manner. Despite of their promising performances on definition generation, the under-specific problem has been less investigated. Although \\citet{huang2021definition} design a scoring mechanism that measures definitions' specificity, we argue that the fundamental reason of the under-specific problem lies in the lack of fine-grained semantic learning in pre-trained encoder-decoder models, which we leverage contrastive learning to address in this work.\n\n\\subsection{Task Formulation}\n\\label{Task Formulation}\nGiven a word or phrase $W = \\{w_i,...,w_j\\}$ and its surrounding context $C = \\{w_0,...,W_k\\}(0<i<j<k)$, the task of definition generation is to generate the definition $D = \\{d_0,...d_T\\}$ to explain the meaning of $W$ under $C$. This process can be formulated as:\n\nFor evaluation, we follow previous works and acquire three popular datasets, which are ensembled by \\citet{ishiwatari2019learning}\\footnote{\\url{http://www.tkl.iis.u-tokyo.ac.jp/~ishiwatari/naacl_data.zip}}.\nEach entry in a dataset consists of three elements: (1) a target word or phrase, (2) the corresponding definition, and (3) one usage example of the target as a local context. If a target has multiple definitions and examples, we treat them as different entries. For fair comparison, each dataset is split into $\\textit{train}$, $\\textit{dev}$ and $\\textit{test}$ sets according to \\citet{ishiwatari2019learning}. The statistics of these datasets are shown in Table~\\ref{Statistics of datasets}.\n\n\\subsection{Manual Evaluation}\nTo adequately evaluate the generated definitions, we also adopt three kinds of manual metrics: (1) Acccuracy (Acc.) ~\\cite{ishiwatari2019learning} learning measures the semantic similarity between the generated definitions and the target words; (2) Fluency (Flu.) evaluates the readability of the generated definitions without considering the semantic alignment; (3) Under-specified (Under-spec.)~\\cite{noraset2017definition}  calculates the ratio of under-specific definitions in the generated cases, which is curated to assess the model's capabilities in addressing the under-specific problem. The lower the ratio is, the better the model is in capturing fine-grained semantics during definition generation. Note that both Acc. and Flu. metrics are likert-scale of \\{0,1,2,3,4,5\\}.\n\nAccording to Table~\\ref{manual-evaluation}, the definitions generated by the proposed method T5-Contrast are better than those by other two models in terms of all the three metrics. Notably, the under-specific ratio significantly drops from 7.6\\% (T5-Base) to 4.8\\% (Ours). The manual evaluation results imply that the definitions produced by our method are more accurate, fluent, and fine-grained as compared to other pre-trained models.\n\n\\subsection{Case Study}\nFor better understanding, we show some example definitions generated by these compared models in Table~\\ref{tab:case study}. It is obvious that T5-Base produces an under-specific definition ``\\emph{a positive criticism}'' for the target word ``\\emph{praise}'' in the context \\emph{he always appreciated praise for this work}. The generated definition roughly expresses the positive meaning of the target word \\emph{appreciate}, but fails to provide the accurate meaning of \\emph{approval and commendation} in \\emph{praise}. In this case, this example definition by T5-Base is under-specific. As for T5-Reranking, it generates the word ``\\emph{goodwill}'', which is a multi-sense word where the one sense is ``a kindly feeling of support'' and the other sense is ``the favor or advantage of a business''. As such, this definition by T5-Reranking is also inaccurate to describe the word \\emph{praise}. On the contrary, the definition generated by our model that ``\\emph{an expression of admiration or approval}'' is more specific, which shows the effectiveness of our proposed method to remedy the under-specific problem. Due to the space limit, we give more sampled examples in Appendix \\ref{Additonal Case Study}. ", "table_source": "\\begin{table}[t]\n\\centering\n\\setlength{\\tabcolsep}{11mm}{\n\\begin{tabular}{@{}cc@{}}\n\\toprule\nError Types          & Ratio \\\\ \\midrule\n\\bf{Under-spcified}       & \\bf{9.0}\\% \\\\\nOver-specified       & 5.5\\% \\\\\nSelf-reference       & 3.0\\% \\\\\nWrong part-of-speech & 1.0\\% \\\\\nOpposite             & 1.0\\% \\\\ \\bottomrule\n\\end{tabular}}\n\\caption{Ratio of each error type of the definitions generated in \\citet{huang2021definition}.}\n\\label{pic:error ratio}\n\\end{table}", "cell_list_gold": [{"value": "9.0", "char_index": [166, 169], "type": "Other"}, {"value": "5.5", "char_index": [199, 202], "type": "Other"}, {"value": "3.0", "char_index": [231, 234], "type": "Other"}, {"value": "1.0", "char_index": [263, 266], "type": "Other"}, {"value": "1.0", "char_index": [295, 298], "type": "Other"}]}, "2210.00543v1_table3": {"table_code": "\\begin{table*}[!t]\\small\n\\centering\n\\setlength{\\tabcolsep}{2.5mm}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n                                    & \\multicolumn{2}{c}{WordNet} & \\multicolumn{2}{c}{Oxford} & \\multicolumn{2}{c}{Urban} \\\\ \\midrule\n                                    & BLEU         & NIST         & BLEU         & NIST        & BLEU        & NIST        \\\\\n\\toprule\nI-Attention                          & 23.77        & 44.30        & 17.45        & 35.79       & 8.81        & 19.43       \\\\\nLocal                          & 24.78        & 40.32        & 17.58        & 31.30       & 8.99        & 17.39       \\\\\nGlobal                         & 23.59        & 49.70        & 14.95        & 32.79       & 5.15        & 10.45       \\\\\nLOG-CaD                           & 25.19        & 43.54        & 18.57        & 38.22       & 9.93        & 19.29       \\\\\nT5-Reranking      & \\bf{32.72}        & 64.57        & 26.52        & 74.17       & 17.71       & 35.53       \\\\ \\bottomrule\nT5-Contrast (Ours) &     32.05$_{(-2.1\\%)}$         &       \\bf{74.71}$_{(+15.5\\%)}$       & \\bf{27.11}$_{(+2.2\\%)}$        & \\bf{79.42}$_{(+7.1\\%)}$       & \\bf{19.44}$_{(+9.8\\%)}$       & \\bf{41.01}$_{(+15.4\\%)}$       \\\\\nT5-Base &     31.72         &      57.35        & 25.44        & 66.92       & 17.66       & 26.86       \n\\\\\\bottomrule\n\\end{tabular}}\n\\caption{Automatic evaluation results on test sets of three datasets. The best results in each dataset are in bold. We also add the quantitative comparison results between our method and the strongest baseline model T5-Reranking.}\n\\label{Experimental_Results}\n\\end{table*}", "table_label": "{Experimental_Results}", "table_numeric_cells": [["23.77", "23.77", 416, 421, 416, 421], ["44.30", "44.30", 431, 436, 431, 436], ["17.45", "17.45", 446, 451, 446, 451], ["35.79", "35.79", 461, 466, 461, 466], ["8.81", "8.81", 475, 479, 475, 479], ["19.43", "19.43", 489, 494, 489, 494], ["24.78", "24.78", 537, 542, 537, 542], ["40.32", "40.32", 552, 557, 552, 557], ["17.58", "17.58", 567, 572, 567, 572], ["31.30", "31.30", 582, 587, 582, 587], ["8.99", "8.99", 596, 600, 596, 600], ["17.39", "17.39", 610, 615, 610, 615], ["23.59", "23.59", 658, 663, 658, 663], ["49.70", "49.70", 673, 678, 673, 678], ["14.95", "14.95", 688, 693, 688, 693], ["32.79", "32.79", 703, 708, 703, 708], ["5.15", "5.15", 717, 721, 717, 721], ["10.45", "10.45", 731, 736, 731, 736], ["25.19", "25.19", 782, 787, 782, 787], ["43.54", "43.54", 797, 802, 797, 802], ["18.57", "18.57", 812, 817, 812, 817], ["38.22", "38.22", 827, 832, 827, 832], ["9.93", "9.93", 841, 845, 841, 845], ["19.29", "19.29", 855, 860, 855, 860], ["32.72", "\\bf{32.72}", 894, 899, 890, 900], ["64.57", "64.57", 910, 915, 910, 915], ["26.52", "26.52", 925, 930, 925, 930], ["74.17", "74.17", 940, 945, 940, 945], ["17.71", "17.71", 954, 959, 954, 959], ["35.53", "35.53", 968, 973, 968, 973], ["31.72", "31.72", 1233, 1238, 1233, 1238], ["57.35", "57.35", 1254, 1259, 1254, 1259], ["25.44", "25.44", 1269, 1274, 1269, 1274], ["66.92", "66.92", 1284, 1289, 1284, 1289], ["17.66", "17.66", 1298, 1303, 1298, 1303], ["26.86", "26.86", 1312, 1317, 1312, 1317]], "text_chunk_selected": "\\subsection{Task Formulation}\n\\label{Task Formulation}\nGiven a word or phrase $W = \\{w_i,...,w_j\\}$ and its surrounding context $C = \\{w_0,...,W_k\\}(0<i<j<k)$, the task of definition generation is to generate the definition $D = \\{d_0,...d_T\\}$ to explain the meaning of $W$ under $C$. This process can be formulated as:\n\n\\begin{equation}\\label{encoder}\n\\mathbf{H}_0={\\rm Emb}({\\rm Splice}(W, C))\n\\end{equation}\n\nFormally, we denote the {target word encoding} generated by the encoder in T5 as $\\mathbf{H}_{target}$, and the {definition encoding} generated by the decoder as $\\mathbf{G}^{L_D}$. In general, target word encoding $\\mathbf{H}_{target}$ is obtained by extracting the encoding of the target word's position in $\\mathbf{H}^{L_E}$, and\ndefinition encoding $\\mathbf{G}^{L_D}$ is generated by the decoder to decode and get the definition sequence later. \n\nAfter encoding, we use a pooling function $\\rm f()$ to aggregate the $\\mathbf{H}_{target}$ and $\\mathbf{G}^{L_D}$ respectively, and obtain {target word representation} $\\mathbf{h}$ and {definition representation} $\\mathbf{g}$ with the same length:\n\nFor evaluation, we follow previous works and acquire three popular datasets, which are ensembled by \\citet{ishiwatari2019learning}\\footnote{\\url{http://www.tkl.iis.u-tokyo.ac.jp/~ishiwatari/naacl_data.zip}}.\nEach entry in a dataset consists of three elements: (1) a target word or phrase, (2) the corresponding definition, and (3) one usage example of the target as a local context. If a target has multiple definitions and examples, we treat them as different entries. For fair comparison, each dataset is split into $\\textit{train}$, $\\textit{dev}$ and $\\textit{test}$ sets according to \\citet{ishiwatari2019learning}. The statistics of these datasets are shown in Table~\\ref{Statistics of datasets}.\n\n\\paragraph{WordNet dataset} The Wordnet dataset is collected by \\citet{noraset2017definition} from the Wordnet dictionary and the GNU Collaborative International Dictionary of English\\footnote{\\url{http://wwwgcide.gnu.org.ua}}. In this work, we follow \\citet{ishiwatari2019learning} and use the extended version of WordNet dataset, where usage examples for each entry are added and the entries without usage examples are removed.\n\n\\subsection{Automatic Metrics}\nFollowing common practice, we adopt two automatic evaluation metrics to assess the quality of the definitions generated by each model.\n\\paragraph{BLEU}\nThe metric BLEU \\cite{papineni2002bleu} has been widely used in previous works to measure the closeness between the generated results and human reference. It measures the geometric average of the precision over hypothesis n-grams with an additional penalty to discourage short definition.\n\\paragraph{NIST}\nNIST~\\cite{doddington2002automatic} is similar to BLEU, but considers up-weighting rare, informative n-grams. We use NLTK\\footnote{\\url{https://www.nltk.org}} tool to calculate NIST metric.\n\nTable~\\ref{Experimental_Results} shows the automatic comparison results of each compared model on the three datasets. Considering the absolute scores, the proposed method T5-Contrast significantly outperforms other 5 models on almost every metric across the three datasets. Although the BLEU score on WordNet dataset obtained by our method is slightly lower (2.09\\%) than T5-Reranking \\cite{huang2021definition}, the NIST score of our method in WordNet dataset is notably higher (15.70\\%) than theirs. This strongly demonstrates the effectiveness and generalization of the proposed method in generating high-quality definitions for a given word under a context. ", "table_source": "\\begin{table*}[!t]\\small\n\\centering\n\\setlength{\\tabcolsep}{2.5mm}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n                                    & \\multicolumn{2}{c}{WordNet} & \\multicolumn{2}{c}{Oxford} & \\multicolumn{2}{c}{Urban} \\\\ \\midrule\n                                    & BLEU         & NIST         & BLEU         & NIST        & BLEU        & NIST        \\\\\n\\toprule\nI-Attention                          & 23.77        & 44.30        & 17.45        & 35.79       & 8.81        & 19.43       \\\\\nLocal                          & 24.78        & 40.32        & 17.58        & 31.30       & 8.99        & 17.39       \\\\\nGlobal                         & 23.59        & 49.70        & 14.95        & 32.79       & 5.15        & 10.45       \\\\\nLOG-CaD                           & 25.19        & 43.54        & 18.57        & 38.22       & 9.93        & 19.29       \\\\\nT5-Reranking      & \\bf{32.72}        & 64.57        & 26.52        & 74.17       & 17.71       & 35.53       \\\\ \\bottomrule\nT5-Contrast (Ours) &     32.05$_{(-2.1\\%)}$         &       \\bf{74.71}$_{(+15.5\\%)}$       & \\bf{27.11}$_{(+2.2\\%)}$        & \\bf{79.42}$_{(+7.1\\%)}$       & \\bf{19.44}$_{(+9.8\\%)}$       & \\bf{41.01}$_{(+15.4\\%)}$       \\\\\nT5-Base &     31.72         &      57.35        & 25.44        & 66.92       & 17.66       & 26.86       \n\\\\\\bottomrule\n\\end{tabular}}\n\\caption{Automatic evaluation results on test sets of three datasets. The best results in each dataset are in bold. We also add the quantitative comparison results between our method and the strongest baseline model T5-Reranking.}\n\\label{Experimental_Results}\n\\end{table*}", "cell_list_gold": [{"value": "23.77", "char_index": [416, 421], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "I-Attention", "model settings": {"xx": "yy"}}, {"value": "44.30", "char_index": [431, 436], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "I-Attention", "model settings": {"xx": "yy"}}, {"value": "17.45", "char_index": [446, 451], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "I-Attention", "model settings": {"xx": "yy"}}, {"value": "35.79", "char_index": [461, 466], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "I-Attention", "model settings": {"xx": "yy"}}, {"value": "8.81", "char_index": [475, 479], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "I-Attention", "model settings": {"xx": "yy"}}, {"value": "19.43", "char_index": [489, 494], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "I-Attention", "model settings": {"xx": "yy"}}, {"value": "24.78", "char_index": [537, 542], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Local", "model settings": {"xx": "yy"}}, {"value": "40.32", "char_index": [552, 557], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "Local", "model settings": {"xx": "yy"}}, {"value": "17.58", "char_index": [567, 572], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Local", "model settings": {"xx": "yy"}}, {"value": "31.30", "char_index": [582, 587], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "Local", "model settings": {"xx": "yy"}}, {"value": "8.99", "char_index": [596, 600], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Local", "model settings": {"xx": "yy"}}, {"value": "17.39", "char_index": [610, 615], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "Local", "model settings": {"xx": "yy"}}, {"value": "23.59", "char_index": [658, 663], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Global", "model settings": {"xx": "yy"}}, {"value": "49.70", "char_index": [673, 678], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "Global", "model settings": {"xx": "yy"}}, {"value": "14.95", "char_index": [688, 693], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Global", "model settings": {"xx": "yy"}}, {"value": "32.79", "char_index": [703, 708], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "Global", "model settings": {"xx": "yy"}}, {"value": "5.15", "char_index": [717, 721], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "Global", "model settings": {"xx": "yy"}}, {"value": "10.45", "char_index": [731, 736], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "Global", "model settings": {"xx": "yy"}}, {"value": "25.19", "char_index": [782, 787], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "LOG-CaD", "model settings": {"xx": "yy"}}, {"value": "43.54", "char_index": [797, 802], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "LOG-CaD", "model settings": {"xx": "yy"}}, {"value": "18.57", "char_index": [812, 817], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "LOG-CaD", "model settings": {"xx": "yy"}}, {"value": "38.22", "char_index": [827, 832], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "LOG-CaD", "model settings": {"xx": "yy"}}, {"value": "9.93", "char_index": [841, 845], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "LOG-CaD", "model settings": {"xx": "yy"}}, {"value": "19.29", "char_index": [855, 860], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "LOG-CaD", "model settings": {"xx": "yy"}}, {"value": "32.72", "char_index": [894, 899], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Reranking", "model settings": {"xx": "yy"}}, {"value": "64.57", "char_index": [910, 915], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Reranking", "model settings": {"xx": "yy"}}, {"value": "26.52", "char_index": [925, 930], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Reranking", "model settings": {"xx": "yy"}}, {"value": "74.17", "char_index": [940, 945], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Reranking", "model settings": {"xx": "yy"}}, {"value": "17.71", "char_index": [954, 959], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Reranking", "model settings": {"xx": "yy"}}, {"value": "35.53", "char_index": [968, 973], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Reranking", "model settings": {"xx": "yy"}}, {"value": "32.05", "char_index": [1020, 1025], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Contrast", "model settings": {"xx": "yy"}}, {"value": "74.71", "char_index": [1059, 1064], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Contrast", "model settings": {"xx": "yy"}}, {"value": "27.11", "char_index": [1092, 1097], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Contrast", "model settings": {"xx": "yy"}}, {"value": "79.42", "char_index": [1125, 1130], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Contrast", "model settings": {"xx": "yy"}}, {"value": "19.44", "char_index": [1157, 1162], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Contrast", "model settings": {"xx": "yy"}}, {"value": "41.01", "char_index": [1189, 1194], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Contrast", "model settings": {"xx": "yy"}}, {"value": "31.72", "char_index": [1233, 1238], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"xx": "yy"}}, {"value": "57.35", "char_index": [1254, 1259], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"xx": "yy"}}, {"value": "25.44", "char_index": [1269, 1274], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"xx": "yy"}}, {"value": "66.92", "char_index": [1284, 1289], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"xx": "yy"}}, {"value": "17.66", "char_index": [1298, 1303], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"xx": "yy"}}, {"value": "26.86", "char_index": [1312, 1317], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"xx": "yy"}}]}, "2210.00543v1_table4": {"table_code": "\\begin{table*}[!t]\n\\centering\n\\label{tab-Experimental Results}\n\\setlength{\\tabcolsep}{5.5mm}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n                                    & \\multicolumn{2}{c}{WordNet} & \\multicolumn{2}{c}{Oxford} & \\multicolumn{2}{c}{Urban} \\\\ \\midrule\n                                    & BLEU         & NIST         & BLEU         & NIST        & BLEU        & NIST        \\\\\n\\toprule\nOurs                          & \\bf{32.05}        & \\bf{74.71}      & 27.10        & 79.42        & \\bf{19.44}        & \\bf{41.01}              \\\\\nw/ $\\rm Mean()$                          & 31.07        & 71.48      & \\bf{27.13}        & \\bf{80.33}        & 18.57        & 40.29             \\\\\nw/ One-stage training                         & 31.75        & 73.79      & 27.06        & 79.90        &  16.49       &   31.46           \\\\\n\\bottomrule\nT5-Base                           & 31.72        & 57.35        & 25.44        & 66.92        & 17.66        & 26.86              \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Ablation study results on test sets. The best numbers are in bold.}\n\\label{ablation study}\n\\end{table*}", "table_label": "{ablation study}", "table_numeric_cells": [["32.05", "\\bf{32.05}", 440, 445, 436, 446], ["74.71", "\\bf{74.71}", 460, 465, 456, 466], ["27.10", "27.10", 474, 479, 474, 479], ["79.42", "79.42", 489, 494, 489, 494], ["19.44", "\\bf{19.44}", 508, 513, 504, 514], ["41.01", "\\bf{41.01}", 528, 533, 524, 534], ["31.07", "31.07", 594, 599, 594, 599], ["71.48", "71.48", 609, 614, 609, 614], ["27.13", "\\bf{27.13}", 626, 631, 622, 632], ["80.33", "\\bf{80.33}", 646, 651, 642, 652], ["18.57", "18.57", 662, 667, 662, 667], ["40.29", "40.29", 677, 682, 677, 682], ["31.75", "31.75", 746, 751, 746, 751], ["73.79", "73.79", 761, 766, 761, 766], ["27.06", "27.06", 774, 779, 774, 779], ["79.90", "79.90", 789, 794, 789, 794], ["16.49", "16.49", 805, 810, 805, 810], ["31.46", "31.46", 821, 826, 821, 826], ["31.72", "31.72", 888, 893, 888, 893], ["57.35", "57.35", 903, 908, 903, 908], ["25.44", "25.44", 918, 923, 918, 923], ["66.92", "66.92", 933, 938, 933, 938], ["17.66", "17.66", 948, 953, 948, 953], ["26.86", "26.86", 963, 968, 963, 968]], "text_chunk_selected": "\\subsection{Task Formulation}\n\\label{Task Formulation}\nGiven a word or phrase $W = \\{w_i,...,w_j\\}$ and its surrounding context $C = \\{w_0,...,W_k\\}(0<i<j<k)$, the task of definition generation is to generate the definition $D = \\{d_0,...d_T\\}$ to explain the meaning of $W$ under $C$. This process can be formulated as:\n\n\\begin{equation}\\label{encoder}\n\\mathbf{H}_0={\\rm Emb}({\\rm Splice}(W, C))\n\\end{equation}\n\nAfter encoding, we use a pooling function $\\rm f()$ to aggregate the $\\mathbf{H}_{target}$ and $\\mathbf{G}^{L_D}$ respectively, and obtain {target word representation} $\\mathbf{h}$ and {definition representation} $\\mathbf{g}$ with the same length:\n\n\\begin{equation}\\label{eq:contrastive}\nL_C = \\sum_{i=1}^N -log\\frac{e^{{\\rm sim}(h_i,g_i)}/\\tau}{\\sum_{j=1}^N e^{{\\rm sim}(h_i,g_j)}/\\tau}\n\\end{equation}\n\nFor evaluation, we follow previous works and acquire three popular datasets, which are ensembled by \\citet{ishiwatari2019learning}\\footnote{\\url{http://www.tkl.iis.u-tokyo.ac.jp/~ishiwatari/naacl_data.zip}}.\nEach entry in a dataset consists of three elements: (1) a target word or phrase, (2) the corresponding definition, and (3) one usage example of the target as a local context. If a target has multiple definitions and examples, we treat them as different entries. For fair comparison, each dataset is split into $\\textit{train}$, $\\textit{dev}$ and $\\textit{test}$ sets according to \\citet{ishiwatari2019learning}. The statistics of these datasets are shown in Table~\\ref{Statistics of datasets}.\n\n\\subsection{Automatic Metrics}\nFollowing common practice, we adopt two automatic evaluation metrics to assess the quality of the definitions generated by each model.\n\\paragraph{BLEU}\nThe metric BLEU \\cite{papineni2002bleu} has been widely used in previous works to measure the closeness between the generated results and human reference. It measures the geometric average of the precision over hypothesis n-grams with an additional penalty to discourage short definition.\n\\paragraph{NIST}\nNIST~\\cite{doddington2002automatic} is similar to BLEU, but considers up-weighting rare, informative n-grams. We use NLTK\\footnote{\\url{https://www.nltk.org}} tool to calculate NIST metric.\n\n\\subsection{Experimental Setups}\n\\label{Settings}\nWe train all models in PyTorch\\footnote{\\url{https://github.com/pytorch/pytorch}} \\cite{paszke2019pytorch}, and use the HuggingFace\\footnote{\\url{https://github.com/huggingface/}} \\cite{wolf2019huggingface} implementation of T5. We train each model on a V100 GPU. For compared models, we replicate experiments following the implementations details released by \\citet{huang2021definition}. For training our model, we use the base version of T5 with the same size of \\citet{huang2021definition}. \nFor each dataset, we finetune it using Adam ~\\cite{kingma2014adam} optimizer with an initial learning rate of 3e-4 and the batch size of 16. In all the experiments, we train our model with a two-stage strategy as described in the previous section. Please refer to Appendix~\\ref{Detailed Training Setting} for the detailed training settings in each stage, like max-epoch and early-stop threshold.\n\nAs shown in Table~\\ref{ablation study}, replacing the pooling function $Max()$ with the mean-pooling $Mean()$ will bring in different changes on different datasets. Whereas the automatic scores drop a lot on WordNet and Urban datasets, they increase a bit on Oxford dataset. This indicates that the choice of pooling function might be empirically motivated, and in general the effect of contrastive learning does not vary a lot when the pooling function changes.", "table_source": "\\begin{table*}[!t]\n\\centering\n\\label{tab-Experimental Results}\n\\setlength{\\tabcolsep}{5.5mm}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n                                    & \\multicolumn{2}{c}{WordNet} & \\multicolumn{2}{c}{Oxford} & \\multicolumn{2}{c}{Urban} \\\\ \\midrule\n                                    & BLEU         & NIST         & BLEU         & NIST        & BLEU        & NIST        \\\\\n\\toprule\nOurs                          & \\bf{32.05}        & \\bf{74.71}      & 27.10        & 79.42        & \\bf{19.44}        & \\bf{41.01}              \\\\\nw/ $\\rm Mean()$                          & 31.07        & 71.48      & \\bf{27.13}        & \\bf{80.33}        & 18.57        & 40.29             \\\\\nw/ One-stage training                         & 31.75        & 73.79      & 27.06        & 79.90        &  16.49       &   31.46           \\\\\n\\bottomrule\nT5-Base                           & 31.72        & 57.35        & 25.44        & 66.92        & 17.66        & 26.86              \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Ablation study results on test sets. The best numbers are in bold.}\n\\label{ablation study}\n\\end{table*}", "cell_list_gold": [{"value": "32.05", "char_index": [440, 445], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"xx": "yy"}}, {"value": "74.71", "char_index": [460, 465], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"xx": "yy"}}, {"value": "27.10", "char_index": [474, 479], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"xx": "yy"}}, {"value": "79.42", "char_index": [489, 494], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"xx": "yy"}}, {"value": "19.44", "char_index": [508, 513], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"xx": "yy"}}, {"value": "41.01", "char_index": [528, 533], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"xx": "yy"}}, {"value": "31.07", "char_index": [594, 599], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ Mean()": "true"}}, {"value": "71.48", "char_index": [609, 614], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ Mean()": "true"}}, {"value": "27.13", "char_index": [626, 631], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ Mean()": "true"}}, {"value": "80.33", "char_index": [646, 651], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ Mean()": "true"}}, {"value": "18.57", "char_index": [662, 667], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ Mean()": "true"}}, {"value": "40.29", "char_index": [677, 682], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ Mean()": "true"}}, {"value": "31.75", "char_index": [746, 751], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ One-stage training": "true"}}, {"value": "73.79", "char_index": [761, 766], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ One-stage training": "true"}}, {"value": "27.06", "char_index": [774, 779], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ One-stage training": "true"}}, {"value": "79.90", "char_index": [789, 794], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ One-stage training": "true"}}, {"value": "16.49", "char_index": [805, 810], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ One-stage training": "true"}}, {"value": "31.46", "char_index": [821, 826], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"w/ One-stage training": "true"}}, {"value": "31.72", "char_index": [888, 893], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"w/ One-stage training": "true"}}, {"value": "57.35", "char_index": [903, 908], "type": "Result", "training data/set": "WordNet", "test data/set": "WordNet", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"w/ One-stage training": "true"}}, {"value": "25.44", "char_index": [918, 923], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"w/ One-stage training": "true"}}, {"value": "66.92", "char_index": [933, 938], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"w/ One-stage training": "true"}}, {"value": "17.66", "char_index": [948, 953], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"w/ One-stage training": "true"}}, {"value": "26.86", "char_index": [963, 968], "type": "Result", "training data/set": "Urban", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": "T5-Base", "model settings": {"w/ One-stage training": "true"}}]}, "2210.00543v1_table5": {"table_code": "\\begin{table}[htbp]\n\\centering\n\\begin{tabular}{c|c|c}\n\\hline\n$\\lambda$   & BLEU & NIST \\\\ \\hline\n1.0 & 7.71 & 25.67 \\\\ \\hline\n0.8 &  27.11    &   79.42   \\\\ \\hline\n0.6 &   27.23   &    79.86  \\\\ \\hline\n0.4 &   26.66   &    77.68  \\\\ \\hline\n0.2 &    26.54  &    78.60  \\\\ \\hline\n0.0 & 25.44 & 66.92 \\\\ \\hline\n\\end{tabular}\n\\caption{Different $\\lambda$ settings on Oxford test set.}\\label{hyper-param}\n\\end{table}", "table_label": "{hyper-param}", "table_numeric_cells": [["1.0", "1.0", 97, 100, 97, 100], ["7.71", "7.71", 103, 107, 103, 107], ["25.67", "25.67", 110, 115, 110, 115], ["0.8", "0.8", 126, 129, 126, 129], ["27.11", "27.11", 133, 138, 133, 138], ["79.42", "79.42", 146, 151, 146, 151], ["0.6", "0.6", 164, 167, 164, 167], ["27.23", "27.23", 172, 177, 172, 177], ["79.86", "79.86", 185, 190, 185, 190], ["0.4", "0.4", 202, 205, 202, 205], ["26.66", "26.66", 210, 215, 210, 215], ["77.68", "77.68", 223, 228, 223, 228], ["0.2", "0.2", 240, 243, 240, 243], ["26.54", "26.54", 249, 254, 249, 254], ["78.60", "78.60", 261, 266, 261, 266], ["0.0", "0.0", 278, 281, 278, 281], ["25.44", "25.44", 284, 289, 284, 289], ["66.92", "66.92", 292, 297, 292, 297]], "text_chunk_selected": "\\subsection{Task Formulation}\n\\label{Task Formulation}\nGiven a word or phrase $W = \\{w_i,...,w_j\\}$ and its surrounding context $C = \\{w_0,...,W_k\\}(0<i<j<k)$, the task of definition generation is to generate the definition $D = \\{d_0,...d_T\\}$ to explain the meaning of $W$ under $C$. This process can be formulated as:\n\nFinally, a softmax function is added upon a linear head to transform $\\mathbf{G}^{L_D}$ into a prediction distribution matrix $\\mathbf{V}\\in \\mathbb{R}^{|V|\\times |D|}$. Here $|V|$ and $|D|$ stand for the vocabulary size and the length of the ground-truth definition, respectively. To optimize, a cross-entropy loss is applied to measure the discrepancy between the generated distribution and the ground-truth distribution. \n\nFormally, we denote the {target word encoding} generated by the encoder in T5 as $\\mathbf{H}_{target}$, and the {definition encoding} generated by the decoder as $\\mathbf{G}^{L_D}$. In general, target word encoding $\\mathbf{H}_{target}$ is obtained by extracting the encoding of the target word's position in $\\mathbf{H}^{L_E}$, and\ndefinition encoding $\\mathbf{G}^{L_D}$ is generated by the decoder to decode and get the definition sequence later. \n\nwhere $N$ denotes a mini-batch of training samples. The $\\tau$ is a temperature hyper-parameter and $\\rm sim(,)$ stands for the cosine similarity function. During learning, the contrastive loss in Eq.~\\ref{eq:contrastive} enforces the model to concentrate on the discrepancy between the two views of the same semantic unit, i.e., the target word. \n\nAs depicted in Figure~\\ref{overview}, our full training strategy follows a two-stage paradigm. At the first stage, we finetune our model only with the generation loss. In the second stage, we combine the contrastive loss in the training and optimize the model with mixed loss $L_{Final}$:\n\nFor evaluation, we follow previous works and acquire three popular datasets, which are ensembled by \\citet{ishiwatari2019learning}\\footnote{\\url{http://www.tkl.iis.u-tokyo.ac.jp/~ishiwatari/naacl_data.zip}}.\nEach entry in a dataset consists of three elements: (1) a target word or phrase, (2) the corresponding definition, and (3) one usage example of the target as a local context. If a target has multiple definitions and examples, we treat them as different entries. For fair comparison, each dataset is split into $\\textit{train}$, $\\textit{dev}$ and $\\textit{test}$ sets according to \\citet{ishiwatari2019learning}. The statistics of these datasets are shown in Table~\\ref{Statistics of datasets}.\n\n\\subsection{Experimental Setups}\n\\label{Settings}\nWe train all models in PyTorch\\footnote{\\url{https://github.com/pytorch/pytorch}} \\cite{paszke2019pytorch}, and use the HuggingFace\\footnote{\\url{https://github.com/huggingface/}} \\cite{wolf2019huggingface} implementation of T5. We train each model on a V100 GPU. For compared models, we replicate experiments following the implementations details released by \\citet{huang2021definition}. For training our model, we use the base version of T5 with the same size of \\citet{huang2021definition}. \nFor each dataset, we finetune it using Adam ~\\cite{kingma2014adam} optimizer with an initial learning rate of 3e-4 and the batch size of 16. In all the experiments, we train our model with a two-stage strategy as described in the previous section. Please refer to Appendix~\\ref{Detailed Training Setting} for the detailed training settings in each stage, like max-epoch and early-stop threshold.\n\n\\subsection{Analysis on Hyper-Parameter}\nTo explore how our method would be affected by the choice of the hyper-parameter $\\lambda$ in Eq.~\\ref{eq:Final_Loss}, we remain other settings the same as we mentioned in Section~\\ref{Settings} and set different $\\lambda$ for each model to observe the performance change. The results on the Oxford dataset are reported in Table~\\ref{hyper-param}. As shown, when $\\lambda$ is set to 0.0, the model is ``degraded'' to the compared T5-Base model. Considering T5-Base model is fine-tuned only using the generation loss in our setting, it is identical to a variant without contrastive loss in the second training stage. To this end, their performances are the same. Also, the performance of the model when $\\lambda$ is set to 1.0 (without generation loss in the second training stage) is pretty bad. We attribute it to the fact that our task requires the ability of language generation and thus still need generation loss to guide contrastive learning in the right way. Besides the above extreme values of $\\lambda$, we find the model achieves better performance when $\\lambda$ is higher ($\\lambda$=0.8 and $\\lambda$=0.6). It further illustrates that after the first stage of generation-only training, the model will benefit more from our fine-grained contrastive learning.", "table_source": "\\begin{table}[htbp]\n\\centering\n\\begin{tabular}{c|c|c}\n\\hline\n$\\lambda$   & BLEU & NIST \\\\ \\hline\n1.0 & 7.71 & 25.67 \\\\ \\hline\n0.8 &  27.11    &   79.42   \\\\ \\hline\n0.6 &   27.23   &    79.86  \\\\ \\hline\n0.4 &   26.66   &    77.68  \\\\ \\hline\n0.2 &    26.54  &    78.60  \\\\ \\hline\n0.0 & 25.44 & 66.92 \\\\ \\hline\n\\end{tabular}\n\\caption{Different $\\lambda$ settings on Oxford test set.}\\label{hyper-param}\n\\end{table}", "cell_list_gold": [{"value": "1.0", "char_index": [97, 100], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Oxford"}, {"value": "7.71", "char_index": [103, 107], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "1.0"}}, {"value": "25.67", "char_index": [110, 115], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "1.0"}}, {"value": "0.8", "char_index": [126, 129], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Oxford"}, {"value": "27.11", "char_index": [133, 138], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.8"}}, {"value": "79.42", "char_index": [146, 151], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.8"}}, {"value": "0.6", "char_index": [164, 167], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Oxford"}, {"value": "27.23", "char_index": [172, 177], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.6"}}, {"value": "79.86", "char_index": [185, 190], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.6"}}, {"value": "0.4", "char_index": [202, 205], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Oxford"}, {"value": "26.66", "char_index": [210, 215], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.4"}}, {"value": "77.68", "char_index": [223, 228], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.4"}}, {"value": "0.2", "char_index": [240, 243], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Oxford"}, {"value": "26.54", "char_index": [249, 254], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.2"}}, {"value": "78.60", "char_index": [261, 266], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.2"}}, {"value": "0.0", "char_index": [278, 281], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Oxford"}, {"value": "25.44", "char_index": [284, 289], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "BLEU", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.0"}}, {"value": "66.92", "char_index": [292, 297], "type": "Result", "training data/set": "Oxford", "test data/set": "Oxford", "task": "Definition Generation", "metric": "NIST", "experimental settings": {"xx": "yy"}, "model": ["T5-Contrast", "Ours"], "model settings": {"\\lambda": "0.0"}}]}, "2210.00543v1_table8": {"table_code": "\\begin{table*}[h!]\n\\centering\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n\\multicolumn{1}{|l|}{Stage} & \\multicolumn{1}{l|}{Dataset} & \\multicolumn{1}{l|}{Max-epoch} & \\multicolumn{1}{l|}{Early-stop} & \\multicolumn{1}{l|}{Pooling method} & \\multicolumn{1}{l|}{$\\lambda$} \\\\ \\hline\n\\multirow{3}{*}{1}          & WordNet                      & 140                            & 40                              & None                                & 0.0                         \\\\ \\cline{2-6} \n                            & Oxford                       & 50                             & 10                              & None                                & 0.0                         \\\\ \\cline{2-6} \n                            & Urban                        & 30                             & 5                               & None                                & 0.0                         \\\\ \\hline\n\\multirow{3}{*}{2}          & WordNet                      & 70                             & 40                              & Max                                 & 0.8                         \\\\ \\cline{2-6} \n                            & Oxford                       & 50                             & 10                              & Max                                 & 0.8                         \\\\ \\cline{2-6} \n                            & Urban                        & 15                             & 5                               & Max                                 & 0.8                         \\\\ \\hline\n\\end{tabular}\n\\caption{Detailed settings on each of our training stages, including max-epoch, early-stop threshold, pooling method and loss weight $\\lambda$.}\\label{tab-settings}\n\\end{table*}", "table_label": "{tab-settings}", "table_numeric_cells": [["1", "\\multirow{3}{*}{1}", 291, 292, 275, 293], ["140", "140", 336, 339, 336, 339], ["40", "40", 369, 371, 369, 371], ["0.0", "0.0", 441, 444, 441, 444], ["50", "50", 546, 548, 546, 548], ["10", "10", 579, 581, 579, 581], ["0.0", "0.0", 651, 654, 651, 654], ["30", "30", 756, 758, 756, 758], ["5", "5", 789, 790, 789, 790], ["0.0", "0.0", 861, 864, 861, 864], ["2", "\\multirow{3}{*}{2}", 915, 916, 899, 917], ["70", "70", 960, 962, 960, 962], ["40", "40", 993, 995, 993, 995], ["0.8", "0.8", 1065, 1068, 1065, 1068], ["50", "50", 1170, 1172, 1170, 1172], ["10", "10", 1203, 1205, 1203, 1205], ["0.8", "0.8", 1275, 1278, 1275, 1278], ["15", "15", 1380, 1382, 1380, 1382], ["5", "5", 1413, 1414, 1413, 1414], ["0.8", "0.8", 1485, 1488, 1485, 1488]], "text_chunk_selected": "\\begin{equation}\\label{task formulation}\nP(D|W,C) = \\prod \\limits_{t=0}^T p(d_t|d_{<t},W,C)\n\\end{equation}\n\n\\begin{equation}\\label{encoder}\n\\mathbf{H}_0={\\rm Emb}({\\rm Splice}(W, C))\n\\end{equation}\n\nFinally, a softmax function is added upon a linear head to transform $\\mathbf{G}^{L_D}$ into a prediction distribution matrix $\\mathbf{V}\\in \\mathbb{R}^{|V|\\times |D|}$. Here $|V|$ and $|D|$ stand for the vocabulary size and the length of the ground-truth definition, respectively. To optimize, a cross-entropy loss is applied to measure the discrepancy between the generated distribution and the ground-truth distribution. \n\nAfter encoding, we use a pooling function $\\rm f()$ to aggregate the $\\mathbf{H}_{target}$ and $\\mathbf{G}^{L_D}$ respectively, and obtain {target word representation} $\\mathbf{h}$ and {definition representation} $\\mathbf{g}$ with the same length:\n\n\\begin{equation}\\label{}\n\\mathbf{h} = {\\rm f}(\\mathbf{H}_{target})\n\\end{equation}\n\n\\begin{equation}\\label{}\n\\mathbf{g} = {\\rm f}(\\mathbf{G}^{L_D})\n\\end{equation}\n\n\\begin{equation}\\label{eq:contrastive}\nL_C = \\sum_{i=1}^N -log\\frac{e^{{\\rm sim}(h_i,g_i)}/\\tau}{\\sum_{j=1}^N e^{{\\rm sim}(h_i,g_j)}/\\tau}\n\\end{equation}\n\n\\begin{equation}\\label{eq:Final_Loss}\nL_{Final} = \\lambda*L_C + (1-\\lambda)*L_G\n\\end{equation}", "table_source": "\\begin{table*}[h!]\n\\centering\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n\\multicolumn{1}{|l|}{Stage} & \\multicolumn{1}{l|}{Dataset} & \\multicolumn{1}{l|}{Max-epoch} & \\multicolumn{1}{l|}{Early-stop} & \\multicolumn{1}{l|}{Pooling method} & \\multicolumn{1}{l|}{$\\lambda$} \\\\ \\hline\n\\multirow{3}{*}{1}          & WordNet                      & 140                            & 40                              & None                                & 0.0                         \\\\ \\cline{2-6} \n                            & Oxford                       & 50                             & 10                              & None                                & 0.0                         \\\\ \\cline{2-6} \n                            & Urban                        & 30                             & 5                               & None                                & 0.0                         \\\\ \\hline\n\\multirow{3}{*}{2}          & WordNet                      & 70                             & 40                              & Max                                 & 0.8                         \\\\ \\cline{2-6} \n                            & Oxford                       & 50                             & 10                              & Max                                 & 0.8                         \\\\ \\cline{2-6} \n                            & Urban                        & 15                             & 5                               & Max                                 & 0.8                         \\\\ \\hline\n\\end{tabular}\n\\caption{Detailed settings on each of our training stages, including max-epoch, early-stop threshold, pooling method and loss weight $\\lambda$.}\\label{tab-settings}\n\\end{table*}", "cell_list_gold": [{"value": "1", "char_index": [291, 292], "type": "Other"}, {"value": "140", "char_index": [336, 339], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Max-epoch", "dataset": "WordNet"}, {"value": "40", "char_index": [369, 371], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Early-stop", "dataset": "WordNet"}, {"value": "0.0", "char_index": [441, 444], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "WordNet"}, {"value": "50", "char_index": [546, 548], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Max-epoch", "dataset": "Oxford"}, {"value": "10", "char_index": [579, 581], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Early-stop", "dataset": "Oxford"}, {"value": "0.0", "char_index": [651, 654], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Oxford"}, {"value": "30", "char_index": [756, 758], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Max-epoch", "dataset": "Urban"}, {"value": "5", "char_index": [789, 790], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Early-stop", "dataset": "Urban"}, {"value": "0.0", "char_index": [861, 864], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Urban"}, {"value": "2", "char_index": [915, 916], "type": "Other"}, {"value": "70", "char_index": [960, 962], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Max-epoch", "dataset": "WordNet"}, {"value": "40", "char_index": [993, 995], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Early-stop", "dataset": "WordNet"}, {"value": "0.8", "char_index": [1065, 1068], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "WordNet"}, {"value": "50", "char_index": [1170, 1172], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Max-epoch", "dataset": "Oxford"}, {"value": "10", "char_index": [1203, 1205], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Early-stop", "dataset": "Oxford"}, {"value": "0.8", "char_index": [1275, 1278], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Oxford"}, {"value": "15", "char_index": [1380, 1382], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Max-epoch", "dataset": "Urban"}, {"value": "5", "char_index": [1413, 1414], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "Early-stop", "dataset": "Urban"}, {"value": "0.8", "char_index": [1485, 1488], "type": "Hyper-parameter/Architecture", "model": ["T5-Contrast", "Ours"], "parameter/architecture name": "\\lambda", "dataset": "Urban"}]}, "2210.00627v1_table1": {"table_code": "\\begin{table}[t]\n\\small\n\\centering\n\\setlength\\tabcolsep{1.0pt}\n\\def1.1{1.1}\n\\begin{tabular}{C{3.5cm}|C{1.1cm}C{1.1cm}|C{1.1cm}C{1.1cm}}\n\\specialrule{.1em}{.05em}{.05em}\n\\multirow{2}{*}{method} & \\multicolumn{2}{c|}{ ZJU-MoCap} & \\multicolumn{2}{c}{AIST} \\\\ \n&  PSNR $\\uparrow$ &  SSIM $\\uparrow$ & PSNR $\\uparrow$ &  SSIM $\\uparrow$ \\\\ \\hline\npixelNeRF~\\cite{yu2021pixelnerf} & 22.13 & 0.8604 & 16.79 & 0.6308\\\\\nNHP~\\cite{kwon2021neural} & 24.01 & 0.8953 & 16.58 & 0.6934\\\\ \n\\textbf{MonoNHR (Ours)} & \\textbf{25.36} & \\textbf{0.9093} & \\textbf{17.61} & \\textbf{0.7186} \\\\ \\specialrule{.1em}{.05em}{.05em}\n\\end{tabular}\n\\vspace*{-3mm}\n\\caption{\n{Comparison of state-of-the-art NeRF-based neural human rendering methods on ZJU-MoCap and AIST.} \n}\n\\label{table:sota}\n\\end{table}", "table_label": "{table:sota}", "table_numeric_cells": [["22.13", "22.13", 378, 383, 378, 383], ["0.8604", "0.8604", 386, 392, 386, 392], ["16.79", "16.79", 395, 400, 395, 400], ["0.6308", "0.6308", 403, 409, 403, 409], ["24.01", "24.01", 440, 445, 440, 445], ["0.8953", "0.8953", 448, 454, 448, 454], ["16.58", "16.58", 457, 462, 457, 462], ["0.6934", "0.6934", 465, 471, 465, 471], ["25.36", "\\textbf{25.36}", 509, 514, 501, 515], ["0.9093", "\\textbf{0.9093}", 526, 532, 518, 533], ["17.61", "\\textbf{17.61}", 544, 549, 536, 550], ["0.7186", "\\textbf{0.7186}", 561, 567, 553, 568]], "text_chunk_selected": "\\author{\nHongsuk Choi$^{*1,3}$\n\\and\nGyeongsik Moon$^{*2}$\n\\and\nMatthieu Armando$^3$\n\\and\nVincent Leroy$^3$\n\\and\nKyoung Mu Lee$^1$\n\\and\nGr\\'egory Rogez$^3$\n\\and\n\\\\\n$^1$ Dept. of ECE \\& ASRI, Seoul National University, Korea\n\\\\\n$^2$ Meta Reality Labs Research\n\\hspace{1.1cm}\n$^3$ NAVER LABS Europe\n\\\\\n\\small \\texttt {redstonepo@gmail.com, mks0601@fb.com, kyoungmu@snu.ac.kr}\\\\ \\small \\texttt {\\{matthieu.armando,vincent.leroy,gregory.rogez\\}@naverlabs.com}\n}\n\n\\subsection{Backbone}\nFirst, a backbone extracts image features that will be used to condition the neural radiance field. Similarly to~\\cite{yu2021pixelnerf}, this allows MonoNHR to learn priors relevant to human rendering, which ultimately enables our method to work on subjects that were not seen during training.\nWe use a ResNet18~\\cite{he2016deep} as backbone.\nIt takes a masked human image $\\mathbf{I} \\in \\mathbb{R}^{3 \\times 256 \\times 256}$ as input and produces a feature map $\\mathbf{F} \\in \\mathbb{R}^{c_p \\times 128 \\times 128}$, with\n$c_p=64$ the feature's channel dimension. An off-the-shelf semantic part segmentation method, PGN~\\cite{gong2018instance}, is used to segment the subjects in the images.\nThe feature map $\\mathbf{F}$ is used in two ways:\n\n\\noindent\\textbf{Mesh feature.} Following NHP~\\cite{kwon2021neural}, we sample per-vertex image features from $\\mathbf{F}$, given SMPL~\\cite{loper2015smpl} mesh vertices.\nWe project these 3D vertices onto the image plane and use bilinear interpolation to sample the corresponding image features. \nThen, we concatenate the per-vertex image features and the vertices' root joint (~\\textit{i.e.}, pelvis)-relative depths, to distinguish the features of both occluded and occluding parts.\nThe concatenated feature is called the \\textcolor[RGB]{98,163,59}{mesh feature} and denoted as \\textcolor[RGB]{98,163,59}{$\\mathbf{M} \\in \\mathbb{R}^{M \\times (c_p+1)}$}, where $M$ denotes the number of SMPL mesh vertices. We feed these to the geometry branch.\n\n\\noindent\\textbf{Pixel feature.} We sample 3D query points from a ray of a target view (rendering view), following the quadrature rule of the volume rendering discussed by Max~\\cite{max1995optical}. The rays are bounded by the given mesh's 3D bounding box, following NB~\\cite{peng2021neural} and NHP~\\cite{kwon2021neural}. \nThe 3D query point $\\mathbf{x}\\in \\mathbb{R}^3$ is equipped with appearance information from its projection on the image plane. We coin this information the \\textcolor[RGB]{252,99,159}{pixel feature $\\mathbf{p} \\in \\mathbb{R}^{c_p}$}.\nThe pixel feature $\\mathbf{p}$ is fed to both geometry and texture branches with $\\mathbf{x}$'s \\textcolor[RGB]{54,93,183}{root joint-relative depth $\\mathrm{z}$}.\nThe root joint-relative depth is defined in the input view's camera-centered coordinate system.\n\n\\begin{equation}\n    \\mathbf{c}(\\mathbf{x}) = M_{\\mathbf{c}}(\\mathbf{p}, \\mathrm{z}, \\mathbf{R}\\mathbf{d}; \\sigma),\n\\end{equation}\n\nwhere $\\delta_i = || \\mathbf{x}_{i + 1} - \\mathbf{x}_{i} ||_2$ is the distance between adjacent sampled points, and $\\mathbf{r}$ is the camera ray. \nIn practice, we set $N$ to 64 following~\\cite{peng2021neural,kwon2021neural}.\n\n\\begin{equation}\n    L_{\\text{render}} = \\sum\\limits_{\\mathbf{r} \\in \\mathcal{R}} \\normsq{\\hat{C}(\\mathbf{r}) - C(\\mathbf{r})},\n\\end{equation}\n\n\\subsection{Comparison with state-of-the-art methods}\n\\noindent\\textbf{Overall comparison.}\nThe Figure~\\ref{fig:sota} and Table~\\ref{table:sota} show that MonoNHR produces the best results on both ZJU-MoCap~\\cite{peng2021neural} and AIST~\\cite{li2021learn,tsuchida2019aist}.", "table_source": "\\begin{table}[t]\n\\small\n\\centering\n\\setlength\\tabcolsep{1.0pt}\n\\def1.1{1.1}\n\\begin{tabular}{C{3.5cm}|C{1.1cm}C{1.1cm}|C{1.1cm}C{1.1cm}}\n\\specialrule{.1em}{.05em}{.05em}\n\\multirow{2}{*}{method} & \\multicolumn{2}{c|}{ ZJU-MoCap} & \\multicolumn{2}{c}{AIST} \\\\ \n&  PSNR $\\uparrow$ &  SSIM $\\uparrow$ & PSNR $\\uparrow$ &  SSIM $\\uparrow$ \\\\ \\hline\npixelNeRF~\\cite{yu2021pixelnerf} & 22.13 & 0.8604 & 16.79 & 0.6308\\\\\nNHP~\\cite{kwon2021neural} & 24.01 & 0.8953 & 16.58 & 0.6934\\\\ \n\\textbf{MonoNHR (Ours)} & \\textbf{25.36} & \\textbf{0.9093} & \\textbf{17.61} & \\textbf{0.7186} \\\\ \\specialrule{.1em}{.05em}{.05em}\n\\end{tabular}\n\\vspace*{-3mm}\n\\caption{\n{Comparison of state-of-the-art NeRF-based neural human rendering methods on ZJU-MoCap and AIST.} \n}\n\\label{table:sota}\n\\end{table}", "cell_list_gold": [{"value": "22.13", "char_index": [378, 383], "type": "Result", "training data/set": "ZJU-MoCap", "test data/set": "ZJU-MoCap", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "pixelNeRF", "model settings": {"xx": "yy"}}, {"value": "0.8604", "char_index": [386, 392], "type": "Result", "training data/set": "ZJU-MoCap", "test data/set": "ZJU-MoCap", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "pixelNeRF", "model settings": {"xx": "yy"}}, {"value": "16.79", "char_index": [395, 400], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "pixelNeRF", "model settings": {"xx": "yy"}}, {"value": "0.6308", "char_index": [403, 409], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "pixelNeRF", "model settings": {"xx": "yy"}}, {"value": "24.01", "char_index": [440, 445], "type": "Result", "training data/set": "ZJU-MoCap", "test data/set": "ZJU-MoCap", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "NHP", "model settings": {"xx": "yy"}}, {"value": "0.8953", "char_index": [448, 454], "type": "Result", "training data/set": "ZJU-MoCap", "test data/set": "ZJU-MoCap", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "NHP", "model settings": {"xx": "yy"}}, {"value": "16.58", "char_index": [457, 462], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "NHP", "model settings": {"xx": "yy"}}, {"value": "0.6934", "char_index": [465, 471], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "NHP", "model settings": {"xx": "yy"}}, {"value": "25.36", "char_index": [509, 514], "type": "Result", "training data/set": "ZJU-MoCap", "test data/set": "ZJU-MoCap", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"xx": "yy"}}, {"value": "0.9093", "char_index": [526, 532], "type": "Result", "training data/set": "ZJU-MoCap", "test data/set": "ZJU-MoCap", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"xx": "yy"}}, {"value": "17.61", "char_index": [544, 549], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"xx": "yy"}}, {"value": "0.7186", "char_index": [561, 567], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"xx": "yy"}}]}, "2210.00627v1_table3": {"table_code": "\\begin{table}[t]\n\\small\n\\centering\n\\setlength\\tabcolsep{1.0pt}\n\\def1.1{1.1}\n\\begin{tabular}{C{3.0cm}|C{1.1cm}}\n\\specialrule{.1em}{.05em}{.05em}\nmethod & CD $\\downarrow$ \\\\ \\hline\nPIFu~\\cite{saito2019pifu} & 79.67 \\\\\nPaMIR~\\cite{zheng2021pamir} & 74.24 \\\\ \n\\textbf{MonoNHR (Ours)} &  \\textbf{23.94}\\\\\n\\specialrule{.1em}{.05em}{.05em}\n\\end{tabular}\n\\vspace*{-3mm}\n\\caption{\n{Comparison between state-of-the-art non-NeRF methods and ours on CAPE.} \n}\n\\label{table:sota_cape}\n\\end{table}", "table_label": "{table:sota_cape}", "table_numeric_cells": [["79.67", "79.67", 207, 212, 207, 212], ["74.24", "74.24", 246, 251, 246, 251], ["23.94", "\\textbf{23.94}", 291, 296, 283, 297]], "text_chunk_selected": "\\noindent\\textbf{Mesh feature.} Following NHP~\\cite{kwon2021neural}, we sample per-vertex image features from $\\mathbf{F}$, given SMPL~\\cite{loper2015smpl} mesh vertices.\nWe project these 3D vertices onto the image plane and use bilinear interpolation to sample the corresponding image features. \nThen, we concatenate the per-vertex image features and the vertices' root joint (~\\textit{i.e.}, pelvis)-relative depths, to distinguish the features of both occluded and occluding parts.\nThe concatenated feature is called the \\textcolor[RGB]{98,163,59}{mesh feature} and denoted as \\textcolor[RGB]{98,163,59}{$\\mathbf{M} \\in \\mathbb{R}^{M \\times (c_p+1)}$}, where $M$ denotes the number of SMPL mesh vertices. We feed these to the geometry branch.\n\n\\noindent\\textbf{Pixel feature.} We sample 3D query points from a ray of a target view (rendering view), following the quadrature rule of the volume rendering discussed by Max~\\cite{max1995optical}. The rays are bounded by the given mesh's 3D bounding box, following NB~\\cite{peng2021neural} and NHP~\\cite{kwon2021neural}. \nThe 3D query point $\\mathbf{x}\\in \\mathbb{R}^3$ is equipped with appearance information from its projection on the image plane. We coin this information the \\textcolor[RGB]{252,99,159}{pixel feature $\\mathbf{p} \\in \\mathbb{R}^{c_p}$}.\nThe pixel feature $\\mathbf{p}$ is fed to both geometry and texture branches with $\\mathbf{x}$'s \\textcolor[RGB]{54,93,183}{root joint-relative depth $\\mathrm{z}$}.\nThe root joint-relative depth is defined in the input view's camera-centered coordinate system.\n\n\\subsection{Volume rendering}\nGiven a target view, we use a classical differentiable ray-marching algorithm~\\cite{kajiya1984ray} to render the target image following NeRF~\\cite{mildenhall2020nerf}.\nConcretely, the final color of a pixel is computed as the integral of RGB values along the ray shot from the camera center $\\mathbf{o} \\in \\mathbb{R}^{3}$, weighted by predicted volume densities.\nThe integral is approximated via stratified sampling~\\cite{mildenhall2020nerf}, and we use the quadrature rule~\\cite{max1995optical} to limit memory usage and predict continuous radiance fields in practice.\nFor each pixel, we sample along the ray $N$ query points$\\{\\mathbf{x}\\}_{i=1}^{N}$, where $\\mathbf{x}_i = (\\mathbf{o} + z_i\\cdot\\mathbf{d})$ and $z_i \\in [z_\\text{near},z_\\text{far}]$.\n$z_\\text{near}$ and $z_\\text{far}$ are the absolute depths of the two intersections between the ray and the given SMPL mesh's 3D bounding box. \nThen, the pixel color of the ray is computed as below:\n\n\\section{Experiments}\n\\subsection{Datasets}\n\\noindent\\textbf{Datasets.}\nWe train and test MonoNHR on ZJU-MoCap~\\cite{peng2021neural}, AIST~\\cite{li2021learn,tsuchida2019aist}, CAPE~\\cite{ma2020learning}, and HUMBI~\\cite{yu2018humbi}.\nFor ZJU-MoCap, we split 10 videos of 10 subjects (1 video per 1 subject) to 7 training videos and 3 testing videos.\nFor AIST, we follow the official training and testing splits.\nThe training set has 834 videos of 20 dancing subjects, and the testing set has 374 videos of 10 dancing subjects.\nPlease note that the testing splits of all datasets contain poses and identities never seen during training.\nCAPE is used to compare 3D geometry with non-NeRF methods as the above two datasets do not provide GT 3D geometry.\nWe use the above datasets for numerical studies, for more meaningful comparisons with prior work, but we also train on HUMBI for qualitative evaluation, for its larger size and diversity (see section~\\ref{subsec:qualitative_results}). More details about data settings can be found in the supplementary material.\n\n\\subsection{Comparison with state-of-the-art methods}\n\\noindent\\textbf{Overall comparison.}\nThe Figure~\\ref{fig:sota} and Table~\\ref{table:sota} show that MonoNHR produces the best results on both ZJU-MoCap~\\cite{peng2021neural} and AIST~\\cite{li2021learn,tsuchida2019aist}.\n\n\\noindent\\textbf{Comparison using estimated 3D meshes.}\nWe also compare MonoNHR with NHP~\\cite{kwon2021neural} on ZJU-MoCap~\\cite{peng2021neural} using estimated body meshes of SPIN~\\cite{kolotouros2019learning} in Table~\\ref{table:sota_mesh_estimate}.\nSPIN is a monocular method that regresses SMPL~\\cite{loper2015smpl} parameters to produce a body mesh.\nMonoNHR highly outperforms NHP, which also uses a body mesh at test time, considering that PSNR is in a log scale. \nFigure~\\ref{fig:estimated_sota} further validates MonoNHR's robustness on  monocular images.\nMonoNHR preserves the subject's 3D shape (\\textit{e.g.}, loose clothes around the torso of the right-hand subject) while NHP relatively shrinks the subject's overall shape (\\textit{e.g.}, the arms of the right-hand subject).\nAlso, the results of MonoNHR show more realistic textures in novel views than NHP (\\textit{e.g.}, the shirt of the left-hand subject).\n\n\\noindent\\textbf{3D geometry comparison.} \nFigure~\\ref{fig:geo_comparison} compares the 3D geometry reconstruction of MonoNHR with NHP~\\cite{kwon2021neural} on ZJU-MoCap.\nMonoNHR clearly distinguishes the geometry of the top subjects' shirts and pants, while NHP shows vague boundaries between them. \nAlso, on the SPIN example on the right (b), the result from MonoNHR reflects the widened deformation of the subject's shirt in the input image, but NHP recovers the shirt geometry in a tightened shape.\nThe results prove that MonoNHR learns better geometry features from a monocular image.\n\n\\noindent\\textbf{Comparison with non-NeRF methods.}\nFigure~\\ref{fig:sota_cape} shows that MonoNHR produces much better qualitative results than PIFu~\\cite{saito2019pifu} and PaMIR~\\cite{zheng2021pamir} on ZJU-Mocap.\nNote that it is difficult to measure their PSNR/SSIM due to the pixel-level misalignment between their rendered images and the GT ones.\nTable~\\ref{table:sota_cape} shows that ours achieves much better CD on CAPE~\\cite{ma2020learning}.\nThe results of PIFu and PaMIR are obtained by running their officially released codes and pre-trained weights.\nWe tested with MonoNHR trained on AIST.", "table_source": "\\begin{table}[t]\n\\small\n\\centering\n\\setlength\\tabcolsep{1.0pt}\n\\def1.1{1.1}\n\\begin{tabular}{C{3.0cm}|C{1.1cm}}\n\\specialrule{.1em}{.05em}{.05em}\nmethod & CD $\\downarrow$ \\\\ \\hline\nPIFu~\\cite{saito2019pifu} & 79.67 \\\\\nPaMIR~\\cite{zheng2021pamir} & 74.24 \\\\ \n\\textbf{MonoNHR (Ours)} &  \\textbf{23.94}\\\\\n\\specialrule{.1em}{.05em}{.05em}\n\\end{tabular}\n\\vspace*{-3mm}\n\\caption{\n{Comparison between state-of-the-art non-NeRF methods and ours on CAPE.} \n}\n\\label{table:sota_cape}\n\\end{table}", "cell_list_gold": [{"value": "79.67", "char_index": [207, 212], "type": "Result", "training data/set": "CAPE", "test data/set": "CAPE", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["CD", "Chamfer distance"], "experimental settings": {"xx": "yy"}, "model": "PIFu", "model settings": {"xx": "yy"}}, {"value": "74.24", "char_index": [246, 251], "type": "Result", "training data/set": "CAPE", "test data/set": "CAPE", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["CD", "Chamfer distance"], "experimental settings": {"xx": "yy"}, "model": "PaMIR", "model settings": {"xx": "yy"}}, {"value": "23.94", "char_index": [291, 296], "type": "Result", "training data/set": "CAPE", "test data/set": "CAPE", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["CD", "Chamfer distance"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"xx": "yy"}}]}, "2210.00627v1_table4": {"table_code": "\\begin{table}[t]\n\\small\n\\centering\n\\setlength\\tabcolsep{1.0pt}\n\\def1.1{1.1}\n\\begin{tabular}{C{4.8cm}|C{1.1cm}|C{1.1cm}}\n\\specialrule{.1em}{.05em}{.05em}\nsetting & PSNR $\\uparrow$ & SSIM $\\uparrow$ \\\\ \\hline\nw/o Mesh Inpainter & 17.52 & 0.7064  \\\\ \nw/o disentanglement & 17.53 & 0.7167  \\\\\nw/o geo. cond. & 17.38 &  0.6634 \\\\\n\\textbf{Ours: full model} &  \\textbf{17.61} & \\textbf{0.7186} \\\\ \\specialrule{.1em}{.05em}{.05em}\n\\end{tabular}\n\\vspace*{-3mm}\n\\caption{\n{Ablation studies on AIST.}\n}\n\\label{table:ablation} \n\\end{table}", "table_label": "{table:ablation}", "table_numeric_cells": [["17.52", "17.52", 228, 233, 228, 233], ["0.7064", "0.7064", 236, 242, 236, 242], ["17.53", "17.53", 270, 275, 270, 275], ["0.7167", "0.7167", 278, 284, 278, 284], ["17.38", "17.38", 306, 311, 306, 311], ["0.6634", "0.6634", 315, 321, 315, 321], ["17.61", "\\textbf{17.61}", 362, 367, 354, 368], ["0.7186", "\\textbf{0.7186}", 379, 385, 371, 386]], "text_chunk_selected": "\\noindent\\textbf{Mesh feature.} Following NHP~\\cite{kwon2021neural}, we sample per-vertex image features from $\\mathbf{F}$, given SMPL~\\cite{loper2015smpl} mesh vertices.\nWe project these 3D vertices onto the image plane and use bilinear interpolation to sample the corresponding image features. \nThen, we concatenate the per-vertex image features and the vertices' root joint (~\\textit{i.e.}, pelvis)-relative depths, to distinguish the features of both occluded and occluding parts.\nThe concatenated feature is called the \\textcolor[RGB]{98,163,59}{mesh feature} and denoted as \\textcolor[RGB]{98,163,59}{$\\mathbf{M} \\in \\mathbb{R}^{M \\times (c_p+1)}$}, where $M$ denotes the number of SMPL mesh vertices. We feed these to the geometry branch.\n\n\\noindent\\textbf{Pixel feature.} We sample 3D query points from a ray of a target view (rendering view), following the quadrature rule of the volume rendering discussed by Max~\\cite{max1995optical}. The rays are bounded by the given mesh's 3D bounding box, following NB~\\cite{peng2021neural} and NHP~\\cite{kwon2021neural}. \nThe 3D query point $\\mathbf{x}\\in \\mathbb{R}^3$ is equipped with appearance information from its projection on the image plane. We coin this information the \\textcolor[RGB]{252,99,159}{pixel feature $\\mathbf{p} \\in \\mathbb{R}^{c_p}$}.\nThe pixel feature $\\mathbf{p}$ is fed to both geometry and texture branches with $\\mathbf{x}$'s \\textcolor[RGB]{54,93,183}{root joint-relative depth $\\mathrm{z}$}.\nThe root joint-relative depth is defined in the input view's camera-centered coordinate system.\n\n\\noindent\\textbf{Sparse 3D convolution.} \nWe work with a sparse 3D volume, in which lie the sparse features of $\\mathbf{M}$.\nThis sparse 3D volume is fed to a sparse 3D convolutional neural network (CNN)~\\cite{graham2018spconv} that \nextracts multi-scale feature volumes, denoted mesh volumes.\nFrom each mesh volume, we sample the feature corresponding to $\\mathbf{x}$ based on its 3D camera coordinates using trilinear interpolation to get the \\textcolor[RGB]{252,22,39}{voxel feature $\\mathbf{v} \\in \\mathbb{R}^{c_v}$}, where\n$c_v=192$ is the feature's channel dimension.\nThe voxel feature $\\mathbf{v}$ is used only for the geometry estimation; therefore, it is a \\emph{geometry-dedicated feature}.\n\n\\noindent\\textbf{Comparison using estimated 3D meshes.}\nWe also compare MonoNHR with NHP~\\cite{kwon2021neural} on ZJU-MoCap~\\cite{peng2021neural} using estimated body meshes of SPIN~\\cite{kolotouros2019learning} in Table~\\ref{table:sota_mesh_estimate}.\nSPIN is a monocular method that regresses SMPL~\\cite{loper2015smpl} parameters to produce a body mesh.\nMonoNHR highly outperforms NHP, which also uses a body mesh at test time, considering that PSNR is in a log scale. \nFigure~\\ref{fig:estimated_sota} further validates MonoNHR's robustness on  monocular images.\nMonoNHR preserves the subject's 3D shape (\\textit{e.g.}, loose clothes around the torso of the right-hand subject) while NHP relatively shrinks the subject's overall shape (\\textit{e.g.}, the arms of the right-hand subject).\nAlso, the results of MonoNHR show more realistic textures in novel views than NHP (\\textit{e.g.}, the shirt of the left-hand subject).\n\n\\noindent\\textbf{Comparison with non-NeRF methods.}\nFigure~\\ref{fig:sota_cape} shows that MonoNHR produces much better qualitative results than PIFu~\\cite{saito2019pifu} and PaMIR~\\cite{zheng2021pamir} on ZJU-Mocap.\nNote that it is difficult to measure their PSNR/SSIM due to the pixel-level misalignment between their rendered images and the GT ones.\nTable~\\ref{table:sota_cape} shows that ours achieves much better CD on CAPE~\\cite{ma2020learning}.\nThe results of PIFu and PaMIR are obtained by running their officially released codes and pre-trained weights.\nWe tested with MonoNHR trained on AIST.\n\n\\subsection{Ablation studies}\n\\noindent\\textbf{Disentanglement of geometry and texture.}\nTo show the benefit of the proposed disentanglement, we design a model where voxel features $\\mathbf{v}$ are fed to both geometry and texture branches.\nResults confirm that disentangling geometry and texture improves predictions both qualitatively and quantitatively (Table~\\ref{table:ablation}).\nAs shown in Figure~\\ref{fig:ablation_disentangle}, noise on the invisible surface's texture is removed, and the overall visual quality is enhanced.\n\n\\noindent\\textbf{Mesh Inpainter.}\nUsing the Mesh Inpainter improves results both qualitatively (Fig.~\\ref{fig:ablation_mesh_inpainter}) and quantitatively (Table~\\ref{table:ablation}).\nThe critical challenge of the monocular setting is inferring the geometry and texture of invisible areas.\nTo this end, the backbone should produce proper features for unseen parts based on the visible ones, which amounts to learning human priors, such as symmetry of human parts and color similarity between surfaces belonging to the same part.\nWhile these could be learned by the rendering loss at the end of the network, we tried to facilitate it by giving a more direct signal to the backbone using mesh inpainting.\n\n\\noindent\\textbf{Geometry-conditioned texture estimation.}\nConditioning on geometry when estimating texture is essential (Fig.~\\ref{fig:ablation_geo_condition} and Table~\\ref{table:ablation}).\nIndeed, geometry provides information on whether a 3D query point is occupied or not, and such occupancy information is important to produce accurate textures at correct locations.", "table_source": "\\begin{table}[t]\n\\small\n\\centering\n\\setlength\\tabcolsep{1.0pt}\n\\def1.1{1.1}\n\\begin{tabular}{C{4.8cm}|C{1.1cm}|C{1.1cm}}\n\\specialrule{.1em}{.05em}{.05em}\nsetting & PSNR $\\uparrow$ & SSIM $\\uparrow$ \\\\ \\hline\nw/o Mesh Inpainter & 17.52 & 0.7064  \\\\ \nw/o disentanglement & 17.53 & 0.7167  \\\\\nw/o geo. cond. & 17.38 &  0.6634 \\\\\n\\textbf{Ours: full model} &  \\textbf{17.61} & \\textbf{0.7186} \\\\ \\specialrule{.1em}{.05em}{.05em}\n\\end{tabular}\n\\vspace*{-3mm}\n\\caption{\n{Ablation studies on AIST.}\n}\n\\label{table:ablation} \n\\end{table}", "cell_list_gold": [{"value": "17.52", "char_index": [228, 233], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"setting": "w/o Mesh Inpainter"}}, {"value": "0.7064", "char_index": [236, 242], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"setting": "w/o Mesh Inpainter"}}, {"value": "17.53", "char_index": [270, 275], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"setting": "w/o disentanglement"}}, {"value": "0.7167", "char_index": [278, 284], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"setting": "w/o disentanglement"}}, {"value": "17.38", "char_index": [306, 311], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"setting": "w/o geo. cond."}}, {"value": "0.6634", "char_index": [315, 321], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"setting": "w/o geo. cond."}}, {"value": "17.61", "char_index": [362, 367], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"setting": "full model"}}, {"value": "0.7186", "char_index": [379, 385], "type": "Result", "training data/set": "AIST", "test data/set": "AIST", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"setting": "full model"}}]}, "2210.00627v1_table5": {"table_code": "\\begin{table}[t]\n\\small\n\\centering\n\\setlength\\tabcolsep{1.0pt}\n\\def1.1{1.1}\n\\begin{tabular}{C{3.0cm}|C{1.1cm}|C{1.1cm}}\n\\specialrule{.1em}{.05em}{.05em}\nmethod & PSNR $\\uparrow$ & SSIM $\\uparrow$ \\\\ \\hline\nNHP~\\cite{kwon2021neural} & 17.472 & 0.7167\\\\ \n\\textbf{MonoNHR (Ours)} &  \\textbf{19.750} & \\textbf{0.7463}\\\\\n\\specialrule{.1em}{.05em}{.05em}\n\\end{tabular}\n\\caption{\n{Comparison with NHP~\\cite{kwon2021neural} on HUMBI.} \n}\n\\label{table:nhp_humbi}\n\\end{table}", "table_label": "{table:nhp_humbi}", "table_numeric_cells": [["17.472", "17.472", 234, 240, 234, 240], ["0.7167", "0.7167", 243, 249, 243, 249], ["19.750", "\\textbf{19.750}", 288, 294, 280, 295], ["0.7463", "\\textbf{0.7463}", 306, 312, 298, 313]], "text_chunk_selected": "\\noindent\\textbf{Mesh feature.} Following NHP~\\cite{kwon2021neural}, we sample per-vertex image features from $\\mathbf{F}$, given SMPL~\\cite{loper2015smpl} mesh vertices.\nWe project these 3D vertices onto the image plane and use bilinear interpolation to sample the corresponding image features. \nThen, we concatenate the per-vertex image features and the vertices' root joint (~\\textit{i.e.}, pelvis)-relative depths, to distinguish the features of both occluded and occluding parts.\nThe concatenated feature is called the \\textcolor[RGB]{98,163,59}{mesh feature} and denoted as \\textcolor[RGB]{98,163,59}{$\\mathbf{M} \\in \\mathbb{R}^{M \\times (c_p+1)}$}, where $M$ denotes the number of SMPL mesh vertices. We feed these to the geometry branch.\n\n\\section{Experiments}\n\\subsection{Datasets}\n\\noindent\\textbf{Datasets.}\nWe train and test MonoNHR on ZJU-MoCap~\\cite{peng2021neural}, AIST~\\cite{li2021learn,tsuchida2019aist}, CAPE~\\cite{ma2020learning}, and HUMBI~\\cite{yu2018humbi}.\nFor ZJU-MoCap, we split 10 videos of 10 subjects (1 video per 1 subject) to 7 training videos and 3 testing videos.\nFor AIST, we follow the official training and testing splits.\nThe training set has 834 videos of 20 dancing subjects, and the testing set has 374 videos of 10 dancing subjects.\nPlease note that the testing splits of all datasets contain poses and identities never seen during training.\nCAPE is used to compare 3D geometry with non-NeRF methods as the above two datasets do not provide GT 3D geometry.\nWe use the above datasets for numerical studies, for more meaningful comparisons with prior work, but we also train on HUMBI for qualitative evaluation, for its larger size and diversity (see section~\\ref{subsec:qualitative_results}). More details about data settings can be found in the supplementary material.\n\n\\noindent\\textbf{Comparison using estimated 3D meshes.}\nWe also compare MonoNHR with NHP~\\cite{kwon2021neural} on ZJU-MoCap~\\cite{peng2021neural} using estimated body meshes of SPIN~\\cite{kolotouros2019learning} in Table~\\ref{table:sota_mesh_estimate}.\nSPIN is a monocular method that regresses SMPL~\\cite{loper2015smpl} parameters to produce a body mesh.\nMonoNHR highly outperforms NHP, which also uses a body mesh at test time, considering that PSNR is in a log scale. \nFigure~\\ref{fig:estimated_sota} further validates MonoNHR's robustness on  monocular images.\nMonoNHR preserves the subject's 3D shape (\\textit{e.g.}, loose clothes around the torso of the right-hand subject) while NHP relatively shrinks the subject's overall shape (\\textit{e.g.}, the arms of the right-hand subject).\nAlso, the results of MonoNHR show more realistic textures in novel views than NHP (\\textit{e.g.}, the shirt of the left-hand subject).\n\n\\noindent\\textbf{Comparison with non-NeRF methods.}\nFigure~\\ref{fig:sota_cape} shows that MonoNHR produces much better qualitative results than PIFu~\\cite{saito2019pifu} and PaMIR~\\cite{zheng2021pamir} on ZJU-Mocap.\nNote that it is difficult to measure their PSNR/SSIM due to the pixel-level misalignment between their rendered images and the GT ones.\nTable~\\ref{table:sota_cape} shows that ours achieves much better CD on CAPE~\\cite{ma2020learning}.\nThe results of PIFu and PaMIR are obtained by running their officially released codes and pre-trained weights.\nWe tested with MonoNHR trained on AIST.\n\n\\subsection{Qualitative results} \\label{subsec:qualitative_results}\nThe HUMBI dataset~\\cite{yu2020humbi} has a high diversity of human subjects in terms of body shape, age, ethnicity, clothing and accessories. We train on this data in order to evaluate the robustness and generalization capabilities of our method.\nFigures~\\ref{fig:banner} and \\ref{fig:humbi_inpaint_comparison} show some results on test subjects unseen during training. In the latter, we show the benefits of using neural rendering by comparing with surface renderings of the inpainted SMPL mesh. Despite the accuracy of the SMPL annotations on this dataset, we can clearly see the inherent limits of texture based approaches. They typically fail in regions where the true surface largely differs from the template shape which is often the case \\eg with hair or wide cloths. In contrast, even though our approach makes use of the SMPL vertices, we exhibit good robustness to these cases.\n\n\\section{Qualitative results}\n\\label{supp:quality}\nWe provide a video\\footnote{\\url{https://youtu.be/9-hfGf7dRw4}} showing more qualitative results and comparisons on the ZJU-MoCap dataset~\\cite{peng2021neural}, as well as the HUMBI dataset~\\cite{yu2020humbi}. Below are more details and comments regarding the video.\n\\subsection{Results on ZJU-MoCap}\nWe show video results on ZJU-MoCap, both on dynamic scenes (where the input frame changes for each rendered image) and static scenes (where a video is generated given a single input image). Comparisons with NHP~\\cite{kwon2021neural} show that our method is quite stable with respect to time, and does not suffer from the jittering effect present in the NHP results.\nOther results show that MonoNHR can better render loose clothing, suggesting that it is able to correctly infer volumetric information around the underlying body-model, whereas NHP renderings tend to stick to the body-model.\nFinally, in some cases, NHP has trouble generalizing to a new subject given only a single image. \n\n\\subsection{Results on HUMBI} \\label{supp:humbi}\nIn order to challenge the generalization capabilities of MonoNHR, we also train and test it on the HUMBI dataset~\\cite{yu2020humbi}, which has a high diversity of human subjects in terms of body shape, age, race, clothing and accessories. We train our method on 9822 frames, taken from 334 subjects, with various poses. Qualitative results on diverse test subjects and poses are included in the attached video. They demonstrate that our method generalizes well to new people, unseen during training.\nAlso, some interesting effects can be noted in the attached video. \nFor example, our method learns to correctly reconstruct long hair in the back, given only a single frontal view of a person.\n\nWe retrain NHP on HUMBI for an additional comparison. We use the same protocol as MonoNHR (see section \\ref{supp:humbi}). Implementation details are given in section \\ref{supp:nhp}.\nTable \\ref{table:nhp_humbi} shows the average numerical results on 13 test subjects, unseen during training. (For each subject, we test on 5 views of one frame).\nFigure \\ref{fig:nhp_humbi} shows some qualitative comparisons on some of those test subjects.\nOur method produces more realistic and visually pleasing renderings. However, like NHP, it still suffers from artifacts for rendering the opposite surface of an observed view, as can be seen in the second row. These come from the difficulty of extracting proper features for novel views only from observed image features.", "table_source": "\\begin{table}[t]\n\\small\n\\centering\n\\setlength\\tabcolsep{1.0pt}\n\\def1.1{1.1}\n\\begin{tabular}{C{3.0cm}|C{1.1cm}|C{1.1cm}}\n\\specialrule{.1em}{.05em}{.05em}\nmethod & PSNR $\\uparrow$ & SSIM $\\uparrow$ \\\\ \\hline\nNHP~\\cite{kwon2021neural} & 17.472 & 0.7167\\\\ \n\\textbf{MonoNHR (Ours)} &  \\textbf{19.750} & \\textbf{0.7463}\\\\\n\\specialrule{.1em}{.05em}{.05em}\n\\end{tabular}\n\\caption{\n{Comparison with NHP~\\cite{kwon2021neural} on HUMBI.} \n}\n\\label{table:nhp_humbi}\n\\end{table}", "cell_list_gold": [{"value": "17.472", "char_index": [234, 240], "type": "Result", "training data/set": "HUMBI", "test data/set": "HUMBI", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "NHP", "model settings": {"xx": "yy"}}, {"value": "0.7167", "char_index": [243, 249], "type": "Result", "training data/set": "HUMBI", "test data/set": "HUMBI", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "NHP", "model settings": {"xx": "yy"}}, {"value": "19.750", "char_index": [288, 294], "type": "Result", "training data/set": "HUMBI", "test data/set": "HUMBI", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["PSNR", "peak signal-to-noise ratio"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"xx": "yy"}}, {"value": "0.7463", "char_index": [306, 312], "type": "Result", "training data/set": "HUMBI", "test data/set": "HUMBI", "task": ["human rendering", "free-viewpoint renderings"], "metric": ["SSIM", "structural similarity index"], "experimental settings": {"xx": "yy"}, "model": "MonoNHR", "model settings": {"xx": "yy"}}]}, "2210.00627v1_table6": {"table_code": "\\begin{table*}[!t]\n\\centering\n\\scalebox{0.9}{\n\\centering\n\\begin{tabular}{c|c|c}\n\\hline\n& layer description & output dimensions  \\\\ \\hline \\hline\nlayer index & input volume & D $\\times$ H $\\times$ W $\\times$ (64+1) \\\\ \\hline\n1-2 & (kernel size 3, out channel 32, stride 1, dilation 1) $\\times$ 2 & D $\\times$ H $\\times$ W $\\times$ 32 \\\\\n3 & (kernel size 3, out channel 32, stride 2, dilation 1) & D/2 $\\times$ H/2 $\\times$ W/2 $\\times$ 32 \\\\\n4-5 & (kernel size 3, out channel 32, stride 1, dilation 1) $\\times$ 2 & D/2 $\\times$ H/2 $\\times$ W/2 $\\times$ 32 \\\\\n6 & (kernel size 3, out channel 32, stride 2, dilation 1) & D/4 $\\times$ H/4 $\\times$ W/4 $\\times$ 32 \\\\\n7-9 & (kernel size 3, out channel 32, stride 1, dilation 2) $\\times$ 3 & D/4 $\\times$ H/4 $\\times$ W/4 $\\times$ 32 \\\\\n10 & (kernel size 3, out channel 64, stride 2, dilation 1) & D/8 $\\times$ H/8 $\\times$ W/8 $\\times$ 64 \\\\\n11-13 & (kernel size 3, out channel 64, stride 1, dilation 2) $\\times$ 3 & D/8 $\\times$ H/8 $\\times$ W/8 $\\times$ 64 \\\\\n14 & (kernel size 3, out channel 64, stride 2, dilation 1) & D/16 $\\times$ H/16 $\\times$ W/16 $\\times$ 64 \\\\\n15-17 & (kernel size 3, out channel 64, stride 1, dilation 2) $\\times$ 3 & D/16 $\\times$ H/16 $\\times$ W/16 $\\times$ 64 \\\\\n\\hline\n\\end{tabular}\n} \n\\vspace{3mm}\n\\caption{Architecture of the geometry branch's sparse 3D CNN. Each convolution layer is followed by 1D batch normalization and ReLU.}\n\\label{tab:sparse_3d_cnn}\n\\end{table*}", "table_label": "{tab:sparse_3d_cnn}", "table_numeric_cells": [["3", "3", 336, 337, 336, 337], ["6", "6", 559, 560, 559, 560], ["10", "10", 782, 784, 782, 784], ["14", "14", 1008, 1010, 1008, 1010]], "text_chunk_selected": "\\author{\nHongsuk Choi$^{*1,3}$\n\\and\nGyeongsik Moon$^{*2}$\n\\and\nMatthieu Armando$^3$\n\\and\nVincent Leroy$^3$\n\\and\nKyoung Mu Lee$^1$\n\\and\nGr\\'egory Rogez$^3$\n\\and\n\\\\\n$^1$ Dept. of ECE \\& ASRI, Seoul National University, Korea\n\\\\\n$^2$ Meta Reality Labs Research\n\\hspace{1.1cm}\n$^3$ NAVER LABS Europe\n\\\\\n\\small \\texttt {redstonepo@gmail.com, mks0601@fb.com, kyoungmu@snu.ac.kr}\\\\ \\small \\texttt {\\{matthieu.armando,vincent.leroy,gregory.rogez\\}@naverlabs.com}\n}\n\n\\subsection{Backbone}\nFirst, a backbone extracts image features that will be used to condition the neural radiance field. Similarly to~\\cite{yu2021pixelnerf}, this allows MonoNHR to learn priors relevant to human rendering, which ultimately enables our method to work on subjects that were not seen during training.\nWe use a ResNet18~\\cite{he2016deep} as backbone.\nIt takes a masked human image $\\mathbf{I} \\in \\mathbb{R}^{3 \\times 256 \\times 256}$ as input and produces a feature map $\\mathbf{F} \\in \\mathbb{R}^{c_p \\times 128 \\times 128}$, with\n$c_p=64$ the feature's channel dimension. An off-the-shelf semantic part segmentation method, PGN~\\cite{gong2018instance}, is used to segment the subjects in the images.\nThe feature map $\\mathbf{F}$ is used in two ways:\n\n\\noindent\\textbf{Mesh feature.} Following NHP~\\cite{kwon2021neural}, we sample per-vertex image features from $\\mathbf{F}$, given SMPL~\\cite{loper2015smpl} mesh vertices.\nWe project these 3D vertices onto the image plane and use bilinear interpolation to sample the corresponding image features. \nThen, we concatenate the per-vertex image features and the vertices' root joint (~\\textit{i.e.}, pelvis)-relative depths, to distinguish the features of both occluded and occluding parts.\nThe concatenated feature is called the \\textcolor[RGB]{98,163,59}{mesh feature} and denoted as \\textcolor[RGB]{98,163,59}{$\\mathbf{M} \\in \\mathbb{R}^{M \\times (c_p+1)}$}, where $M$ denotes the number of SMPL mesh vertices. We feed these to the geometry branch.\n\n\\subsection{Mesh Inpainter}~\\label{sec:mesh_inpainter}\nThis module reconstructs the colors of mesh vertices from a monocular image. \nIt is a simple MLP network that consists of two layers. \nIt takes the mesh feature $\\mathbf{M}$ as input and regresses the vertices' colors $\\hat{\\mathbf{K}} \\in \\mathbb{R}^{M \\times 3}$.\n$\\hat{\\mathbf{K}}$ is only used during training.\nThe vertices' GT colors $\\mathbf{K}$ are obtained by 1.\nprojecting the vertices to multi-view images considering visibility, 2. sampling the colors from each image with bilinear interpolation and 3. averaging the visible vertices' sampled colors.\nThe visibility of vertices are obtained by rasterizing mesh vertices.\n\n\\noindent\\textbf{Sparse 3D convolution.} \nWe work with a sparse 3D volume, in which lie the sparse features of $\\mathbf{M}$.\nThis sparse 3D volume is fed to a sparse 3D convolutional neural network (CNN)~\\cite{graham2018spconv} that \nextracts multi-scale feature volumes, denoted mesh volumes.\nFrom each mesh volume, we sample the feature corresponding to $\\mathbf{x}$ based on its 3D camera coordinates using trilinear interpolation to get the \\textcolor[RGB]{252,22,39}{voxel feature $\\mathbf{v} \\in \\mathbb{R}^{c_v}$}, where\n$c_v=192$ is the feature's channel dimension.\nThe voxel feature $\\mathbf{v}$ is used only for the geometry estimation; therefore, it is a \\emph{geometry-dedicated feature}.\n\n\\section{Reproducibility}\n\\label{supp:rep}\n\\subsection{MonoNHR}\n\\noindent\\textbf{Backbone.}\nThe backbone takes the input image $\\mathbf{I} \\in \\mathbb{R}^{3 \\times 256 \\times 256}$ and produces the feature map $\\mathbf{F} \\in \\mathbb{R}^{64 \\times 128 \\times 128}$.\nWe use ResNet18~\\cite{he2016deep} from which we removed the classifier head, as MonoNHR's backbone network architecture.\nFollowing pixelNeRF~\\cite{yu2021pixelnerf}, the backbone extracts multiple feature maps at different scales, right after the first pooling operation of ResNet18 and after ResNet 3 layers.\nThe multi-scale feature maps have the following shapes (channel dimension $\\times$ height $\\times$ width) :\n\\\\\n1. $64 \\times 128 \\times 128$\\\\\n2. $64 \\times 64 \\times 64$\\\\\n3. $128 \\times 32 \\times 32$\\\\\n4. $256 \\times 16 \\times 16$\\\\\n\n\\noindent\\textbf{Mesh inpainter.}\nThe mesh inpainter takes the mesh feature $\\mathbf{M} \\in \\mathbb{R}^{M \\times (64+1)}$ as input and estimates the vertices' colors $\\hat{\\mathbf{K}} \\in \\mathbb{R}^{M \\times 3}$, where $M$ is the number of SMPL~\\cite{loper2015smpl} mesh vertices, which is 6890.\nThe mesh inpainter consists of two MLP layers with a ReLU~\\cite{nair2010rectified} activation between them.\nThe first layer reduces the channel dimension from (64+1) to 32, and the second layer reduces it to 3, which corresponds to RGB channels.\n\n\\noindent\\textbf{Geometry branch - Sparse 3D convolution.} \nThe geometry branch first builds a sparse 3D volume with the mesh feature $\\mathbf{M}$, given a SMPL mesh similarly to NB~\\cite{peng2021neural} and NHP~\\cite{kwon2021neural}. \nWe obtain the 3D bounding box of the SMPL mesh in the input camera's coordinate system with $5cm$ padding along the depth, height, and width dimensions.\nThe 3D bounding box is transformed into a discretized volume with a voxel size of $5mm \\times 5mm \\times 5mm$, where the depth, height, and width ($D \\times H \\times W$) depend on the SMPL mesh.\nWe calculate the discretized coordinates of mesh vertices, which correspond to the discretized volume's voxels, and fill the voxels with $\\mathbf{M}$ to get the sparse 3D volume.\nThe sparse 3D volume is fed to a sparse 3D CNN~\\cite{graham2018spconv}. \nWe describe the architecture of this sparse 3D CNN in Table~\\ref{tab:sparse_3d_cnn}.\nThe output features of the 5th, 9th, 13th, and 17th layers are the mesh volumes defined in Section 3.3 of the main manuscript.\nWe sample the features of 3D query points from each mesh volume and concatenate the sampled features to get the voxel feature $\\mathbf{v} \\in \\mathbb{R}^{192}$, which is the geometry-dedicated feature.", "table_source": "\\begin{table*}[!t]\n\\centering\n\\scalebox{0.9}{\n\\centering\n\\begin{tabular}{c|c|c}\n\\hline\n& layer description & output dimensions  \\\\ \\hline \\hline\nlayer index & input volume & D $\\times$ H $\\times$ W $\\times$ (64+1) \\\\ \\hline\n1-2 & (kernel size 3, out channel 32, stride 1, dilation 1) $\\times$ 2 & D $\\times$ H $\\times$ W $\\times$ 32 \\\\\n3 & (kernel size 3, out channel 32, stride 2, dilation 1) & D/2 $\\times$ H/2 $\\times$ W/2 $\\times$ 32 \\\\\n4-5 & (kernel size 3, out channel 32, stride 1, dilation 1) $\\times$ 2 & D/2 $\\times$ H/2 $\\times$ W/2 $\\times$ 32 \\\\\n6 & (kernel size 3, out channel 32, stride 2, dilation 1) & D/4 $\\times$ H/4 $\\times$ W/4 $\\times$ 32 \\\\\n7-9 & (kernel size 3, out channel 32, stride 1, dilation 2) $\\times$ 3 & D/4 $\\times$ H/4 $\\times$ W/4 $\\times$ 32 \\\\\n10 & (kernel size 3, out channel 64, stride 2, dilation 1) & D/8 $\\times$ H/8 $\\times$ W/8 $\\times$ 64 \\\\\n11-13 & (kernel size 3, out channel 64, stride 1, dilation 2) $\\times$ 3 & D/8 $\\times$ H/8 $\\times$ W/8 $\\times$ 64 \\\\\n14 & (kernel size 3, out channel 64, stride 2, dilation 1) & D/16 $\\times$ H/16 $\\times$ W/16 $\\times$ 64 \\\\\n15-17 & (kernel size 3, out channel 64, stride 1, dilation 2) $\\times$ 3 & D/16 $\\times$ H/16 $\\times$ W/16 $\\times$ 64 \\\\\n\\hline\n\\end{tabular}\n} \n\\vspace{3mm}\n\\caption{Architecture of the geometry branch's sparse 3D CNN. Each convolution layer is followed by 1D batch normalization and ReLU.}\n\\label{tab:sparse_3d_cnn}\n\\end{table*}", "cell_list_gold": [{"value": "3", "char_index": [336, 337], "type": "Other"}, {"value": "6", "char_index": [559, 560], "type": "Other"}, {"value": "10", "char_index": [782, 784], "type": "Other"}, {"value": "14", "char_index": [1008, 1010], "type": "Other"}]}, "2210.00698v1_table0": {"table_code": "\\begin{table*}[t]\n\\caption{\nPerformance evaluations of the model on Cityscapes validation set.\n}\n\\centerline{\n\\setlength{\\tabcolsep}{3mm}{\n\\begin{tabular}{lccccc}\n\\toprule\n    Methods   & \\multicolumn{1}{l}{Backbone}  & \\multicolumn{1}{l}{Coarse} & \\multicolumn{1}{l}{ImageNet} & \\multicolumn{1}{l}{mIOU(\\%)} & \\multicolumn{1}{l}{Params(M)} \\\\\\hline\n    \n    PSPNet~\\cite{PSANet:ECCV2018} & Dilated-ResNet-101  & $\\times$  & $\\checkmark$  &76.2  & $\\times$   \\\\\n    PSANet~\\cite{PSANet:ECCV2018} & Dilated-ResNet-101   & $\\times$  & $\\checkmark$  & 77.3  & $\\times$ \\\\\n    PADNet~\\cite{PADNet:CVPR2018} & Dilated-ResNet-101   & $\\times$  & $\\checkmark$  & 78.1  & $\\times$  \\\\\n    DenseASPP~\\cite{DenseASPP:CVPR2018}  & WDenseNet-161    & $\\times$  & $\\checkmark$  & 77.8  & $\\times$ \\\\\n    DeepLabv3~\\cite{DeepLabv3:2017}  & ResNet-101   & $\\checkmark$  & $\\checkmark$  & 79.5  & $\\times$  \\\\\\hline\n    \n    Auto-DeepLab~\\cite{AutoDeepLab:CVPR2019}  & $\\times$  & $\\checkmark$  & $\\times$   & 80.3  & 44.42 \\\\\n    HRNetV2~\\cite{HRNet:arxiv2019} & $\\times$  & $\\times$ & $\\checkmark$  & 80.9  & 69.06 \\\\\\hline\n    Ours  & $\\times$  & $\\times$  & $\\checkmark$  &  81.4  & 68.67 \\\\\n    Ours + RSP  & $\\times$  & $\\times$  & $\\checkmark$  & 81.0  & 17.20 \\\\\n    \n\\bottomrule\n\\end{tabular}}\n}    \n\\label{tbl:experiment}\n\\end{table*}", "table_label": "{tbl:experiment}", "table_numeric_cells": [["76.2", "76.2", 440, 444, 440, 444], ["77.3", "77.3", 549, 553, 549, 553], ["78.1", "78.1", 656, 660, 656, 660], ["77.8", "77.8", 767, 771, 767, 771], ["79.5", "79.5", 873, 877, 873, 877], ["80.3", "80.3", 994, 998, 994, 998], ["44.42", "44.42", 1002, 1007, 1002, 1007], ["80.9", "80.9", 1087, 1091, 1087, 1091], ["69.06", "69.06", 1095, 1100, 1095, 1100], ["81.4", "81.4", 1163, 1167, 1163, 1167], ["68.67", "68.67", 1171, 1176, 1171, 1176], ["81.0", "81.0", 1238, 1242, 1238, 1242], ["17.20", "17.20", 1246, 1251, 1246, 1251]], "text_chunk_selected": "Based on Eq.(\\ref{eq:LayerRelation}), we have $X_L = f_L$.  Then, Eq.(\\ref{eq:GradientforLayer1}) can be rewritten as\n\nFor cell structure search, only one type of cell is used: the {\\bf Normal Cell (Norm-Cell)} as in Fig.~\\ref{fig:N:cell}. We use two intermediate nodes to construct each cell structure. Each cell keeps the number of channels and neither reduces the spatial dimension from $W \\times H$ to $\\frac{W}{2} \\times \\frac{H}{2} $ nor enlarges the spatial dimension from $W \\times H$ to $2W \\times 2H$.\n\nReferring to Fig.~\\ref{fig:overview} ({\\em bottom left}) and Fig.~\\ref{fig:N:cell}, for each Norm-Cell with one input, feature maps derived by previous layer with different shapes are reshaped by convolution.\nThe input maps are then fed into subsequent operations after adding together, which are searched by DARTS to generate a feature map with the same size $C \\times {H} \\times {W}$.  \n\nFig.~\\ref{fig:PAM} depicts the details of this PAM.  Given a node, the idea of PAM aims at calculating the attention value of each path and choosing the number of outputs you want (denoted by red color in Fig.~\\ref{fig:PAM}(a)) with higher attention to construct the final macrostructure. The PAM uses a $1\\times1$ convolution and an interpolation operation to normalize all five inputs (denoted by four blue-arrows and one green-arrow in Fig.~\\ref{fig:PAM}(b)) to the same size $C\\times H\\times W$. Then, they are sent to a {\\bf Channel Attention Module (CAM)} to further calculate their channel attention.  Details of the CAM are depicted in Fig.~\\ref{fig:CAM}.  For any pair of $i^{th}$ and $j^{th}$ inputs $F_{i}^c$ and $F_{j}^c$ on the $c^{th}$ channel, we perform an inner product between them to obtain channel attention $S^{c}_{i,j} \\in R^{C \\times C}$:\n\\vspace{-0.15cm}\n\nwhere $S_{i,j}^c$ is the impact of the $i^{th}$ path on the $j^{th}$ path on the $c^{th}$ channel. Then, the attention $S_i^{Path}$ of the $i^{th}$ path is obtained as follows:\n\\vspace{-0.2cm}\n\nThe \\textbf{Cityscapes} dataset~\\cite{Cityscapes:CVPR2016} is a recent large-scale urban scene dataset containing a diverse set of stereo video sequences from 50 cities. Cityscapes dataset contains high quality pixel-level annotations of $5,000$ images with size $1,024 \\times 2,048$. There are $2,975$, $500$, and $1,525$ for training, validation, and test images, respectively, and additional $20,000$ weakly annotated frames. It is an order of magnitude larger than similar previous datasets.\n\nWe consider 2 intermediate nodes in all cells with one input.\nFor each cell, we keep the channel numbers and the height and width of the feature tensor. Fig.~\\ref{fig:N:cell_result} shows the searched cell architectures by NAS on the Cityscapes dataset. Fig.~\\ref{fig:search_accuracy} shows that validation accuracy got more than 40\\% during macro search. Compared to the accuracy in AutoDeepLab~\\cite{AutoDeepLab:CVPR2019}, we got higher accuracy during search.\n$512 \\times 102$ random image crops are used. In DARTS search, batch size is 6 due to GPU memory limitation, architecture search optimization is conducted for $300$ epochs.\nIn the learning of network weight $w$, SGD optimizer with momentum $0.95$, and weight decay $0.0005$ are used. For learning the architecture, SGD optimizer with learning rate $0.005$ and weight decay $0.0001$ are used. The entire architecture search optimization takes about five days on two V100 GPUs.\n\nTable~\\ref{tbl:experiment} shows that our NAS model outperforms the SoTA on the Cityscapes.\nWithout any pretraining, our best model significantly outperforms all the SoTA method. Last row of Table~\\ref{tbl:experiment} shows that the light-weigh RSP design uses only 1/4 parameter size of SoTA methods but still outperforms them.  Fig.~\\ref{fig:cityscapes_results} shows that the visualization of our model on Cityscapes~\\cite{Cityscapes:CVPR2016} validation and test set.", "table_source": "\\begin{table*}[t]\n\\caption{\nPerformance evaluations of the model on Cityscapes validation set.\n}\n\\centerline{\n\\setlength{\\tabcolsep}{3mm}{\n\\begin{tabular}{lccccc}\n\\toprule\n    Methods   & \\multicolumn{1}{l}{Backbone}  & \\multicolumn{1}{l}{Coarse} & \\multicolumn{1}{l}{ImageNet} & \\multicolumn{1}{l}{mIOU(\\%)} & \\multicolumn{1}{l}{Params(M)} \\\\\\hline\n    \n    PSPNet~\\cite{PSANet:ECCV2018} & Dilated-ResNet-101  & $\\times$  & $\\checkmark$  &76.2  & $\\times$   \\\\\n    PSANet~\\cite{PSANet:ECCV2018} & Dilated-ResNet-101   & $\\times$  & $\\checkmark$  & 77.3  & $\\times$ \\\\\n    PADNet~\\cite{PADNet:CVPR2018} & Dilated-ResNet-101   & $\\times$  & $\\checkmark$  & 78.1  & $\\times$  \\\\\n    DenseASPP~\\cite{DenseASPP:CVPR2018}  & WDenseNet-161    & $\\times$  & $\\checkmark$  & 77.8  & $\\times$ \\\\\n    DeepLabv3~\\cite{DeepLabv3:2017}  & ResNet-101   & $\\checkmark$  & $\\checkmark$  & 79.5  & $\\times$  \\\\\\hline\n    \n    Auto-DeepLab~\\cite{AutoDeepLab:CVPR2019}  & $\\times$  & $\\checkmark$  & $\\times$   & 80.3  & 44.42 \\\\\n    HRNetV2~\\cite{HRNet:arxiv2019} & $\\times$  & $\\times$ & $\\checkmark$  & 80.9  & 69.06 \\\\\\hline\n    Ours  & $\\times$  & $\\times$  & $\\checkmark$  &  81.4  & 68.67 \\\\\n    Ours + RSP  & $\\times$  & $\\times$  & $\\checkmark$  & 81.0  & 17.20 \\\\\n    \n\\bottomrule\n\\end{tabular}}\n}    \n\\label{tbl:experiment}\n\\end{table*}", "cell_list_gold": [{"value": "76.2", "char_index": [440, 444], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "PSPNet", "model settings": {"Backbone": "Dilated-ResNet-101", "Coarse": "false", "ImageNet pretraining": "true", "Params (M)": "unknown"}}, {"value": "77.3", "char_index": [549, 553], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "PSANet", "model settings": {"Backbone": "Dilated-ResNet-101", "Coarse": "false", "ImageNet pretraining": "true", "Params (M)": "unknown"}}, {"value": "78.1", "char_index": [656, 660], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "PADNet", "model settings": {"Backbone": "Dilated-ResNet-101", "Coarse": "false", "ImageNet pretraining": "true", "Params (M)": "unknown"}}, {"value": "77.8", "char_index": [767, 771], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "DenseASPP", "model settings": {"Backbone": "WDenseNet-161", "Coarse": "false", "ImageNet pretraining": "true", "Params (M)": "unknown"}}, {"value": "79.5", "char_index": [873, 877], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "DeepLabv3", "model settings": {"Backbone": "ResNet-101", "Coarse": "true", "ImageNet pretraining": "true", "Params (M)": "unknown"}}, {"value": "80.3", "char_index": [994, 998], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "Auto-DeepLab", "model settings": {"Backbone": "false", "Coarse": "true", "ImageNet pretraining": "false", "Params (M)": "44.42"}}, {"value": "44.42", "char_index": [1002, 1007], "type": "Hyper-parameter/Architecture", "model": "Auto-DeepLab", "parameter/architecture name": ["Params", "model size"], "dataset": "Cityscapes"}, {"value": "80.9", "char_index": [1087, 1091], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "HRNetV2", "model settings": {"Backbone": "false", "Coarse": "false", "ImageNet pretraining": "true", "Params (M)": "69.06"}}, {"value": "69.06", "char_index": [1095, 1100], "type": "Hyper-parameter/Architecture", "model": "HRNetV2", "parameter/architecture name": ["Params", "model size"], "dataset": "Cityscapes"}, {"value": "81.4", "char_index": [1163, 1167], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "Ours", "model settings": {"Backbone": "false", "Coarse": "false", "ImageNet pretraining": "true", "Params (M)": "68.67"}}, {"value": "68.67", "char_index": [1171, 1176], "type": "Hyper-parameter/Architecture", "model": "Ours", "parameter/architecture name": ["Params", "model size"], "dataset": "Cityscapes"}, {"value": "81.0", "char_index": [1238, 1242], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "Ours + RSP", "model settings": {"Backbone": "false", "Coarse": "false", "ImageNet pretraining": "true", "Params (M)": "17.20"}}, {"value": "17.20", "char_index": [1246, 1251], "type": "Hyper-parameter/Architecture", "model": "Ours + RSP", "parameter/architecture name": ["Params", "model size"], "dataset": "Cityscapes"}]}, "2210.00698v1_table1": {"table_code": "\\begin{table*}[t]\n\\caption{\nPerformance evaluations of the model on Cityscapes validation set. Training with the Mapillary Vistas dataset.\n\\vspace{-0.2cm}\n}\n\\centerline{\n\\setlength{\\tabcolsep}{3mm}{\n\\begin{tabular}{lcccc}\n\\toprule\n    Methods   & \\multicolumn{1}{l}{Backbone}  & \\multicolumn{1}{l}{Mapillary}  & \\multicolumn{1}{l}{mIOU(\\%)} & \\multicolumn{1}{l}{Params(M)} \\\\\\hline\n    \n    Mapillary~\\cite{MapillaryVistas:ICCV2017} & ResNeXt-101 & $\\checkmark$  & 80.6  & $\\times$ \\\\\n    HANet~\\cite{HANet:CVPR2020} & ResNet-101     & $\\checkmark$    & 81.7  & $\\times$ \\\\ \n    HRNetV2+OCR~\\cite{HRNet:arxiv2019} & HRNetV2  & $\\checkmark$  & 81.8   & 70.37 \\\\\n    DecoupleSegNets  & Wide-ResNet  & $\\checkmark$   & 81.6  & $\\times$ \\\\\n    DCNAS~\\cite{DCNAS:CVPR2021}  & $\\times$  & $\\checkmark$   & 81.3  & $\\times$ \\\\\\hline\n    Ours  & $\\times$  & $\\times$    & 81.4  & 68.67 \\\\\n    Ours  & $\\times$  & $\\checkmark$    & 82.1  & 68.67  \\\\\n    Ours + RSP  & $\\times$  & $\\checkmark$    & 81.7  & 17.20 \\\\\n    \n\\bottomrule\n\\end{tabular}}\n}    \n\\label{tbl:experiment2}\n\\end{table*}", "table_label": "{tbl:experiment2}", "table_numeric_cells": [["80.6", "80.6", 465, 469, 465, 469], ["81.7", "81.7", 554, 558, 554, 558], ["81.8", "81.8", 643, 647, 643, 647], ["70.37", "70.37", 652, 657, 652, 657], ["81.6", "81.6", 716, 720, 716, 720], ["81.3", "81.3", 800, 804, 800, 804], ["81.4", "81.4", 864, 868, 864, 868], ["68.67", "68.67", 872, 877, 872, 877], ["82.1", "82.1", 923, 927, 923, 927], ["68.67", "68.67", 931, 936, 931, 936], ["81.7", "81.7", 989, 993, 989, 993], ["17.20", "17.20", 997, 1002, 997, 1002]], "text_chunk_selected": "For cell structure search, only one type of cell is used: the {\\bf Normal Cell (Norm-Cell)} as in Fig.~\\ref{fig:N:cell}. We use two intermediate nodes to construct each cell structure. Each cell keeps the number of channels and neither reduces the spatial dimension from $W \\times H$ to $\\frac{W}{2} \\times \\frac{H}{2} $ nor enlarges the spatial dimension from $W \\times H$ to $2W \\times 2H$.\n\nReferring to Fig.~\\ref{fig:overview} ({\\em bottom left}) and Fig.~\\ref{fig:N:cell}, for each Norm-Cell with one input, feature maps derived by previous layer with different shapes are reshaped by convolution.\nThe input maps are then fed into subsequent operations after adding together, which are searched by DARTS to generate a feature map with the same size $C \\times {H} \\times {W}$.  \n\nFig.~\\ref{fig:PAM} depicts the details of this PAM.  Given a node, the idea of PAM aims at calculating the attention value of each path and choosing the number of outputs you want (denoted by red color in Fig.~\\ref{fig:PAM}(a)) with higher attention to construct the final macrostructure. The PAM uses a $1\\times1$ convolution and an interpolation operation to normalize all five inputs (denoted by four blue-arrows and one green-arrow in Fig.~\\ref{fig:PAM}(b)) to the same size $C\\times H\\times W$. Then, they are sent to a {\\bf Channel Attention Module (CAM)} to further calculate their channel attention.  Details of the CAM are depicted in Fig.~\\ref{fig:CAM}.  For any pair of $i^{th}$ and $j^{th}$ inputs $F_{i}^c$ and $F_{j}^c$ on the $c^{th}$ channel, we perform an inner product between them to obtain channel attention $S^{c}_{i,j} \\in R^{C \\times C}$:\n\\vspace{-0.15cm}\n\n\\section{Experimental Results}\nExperiments are conducted in the Cityscapes~\\cite{Cityscapes:CVPR2016} urban scene understanding dataset for evaluation.\nAuto-DeepLab~\\cite{AutoDeepLab:CVPR2019}, U-Net~\\cite{UNet:MICCAI2015} and NasUnet~\\cite{NAS:UNet:IEEEAccess2019} are compared in Cityscapes as baseline. We use the standard {\\em mean intersection-over-union} (mIOU) as a performance evaluation metric for semantic segmentation.\n\nThe \\textbf{Cityscapes} dataset~\\cite{Cityscapes:CVPR2016} is a recent large-scale urban scene dataset containing a diverse set of stereo video sequences from 50 cities. Cityscapes dataset contains high quality pixel-level annotations of $5,000$ images with size $1,024 \\times 2,048$. There are $2,975$, $500$, and $1,525$ for training, validation, and test images, respectively, and additional $20,000$ weakly annotated frames. It is an order of magnitude larger than similar previous datasets.\n\nWe consider 2 intermediate nodes in all cells with one input.\nFor each cell, we keep the channel numbers and the height and width of the feature tensor. Fig.~\\ref{fig:N:cell_result} shows the searched cell architectures by NAS on the Cityscapes dataset. Fig.~\\ref{fig:search_accuracy} shows that validation accuracy got more than 40\\% during macro search. Compared to the accuracy in AutoDeepLab~\\cite{AutoDeepLab:CVPR2019}, we got higher accuracy during search.\n$512 \\times 102$ random image crops are used. In DARTS search, batch size is 6 due to GPU memory limitation, architecture search optimization is conducted for $300$ epochs.\nIn the learning of network weight $w$, SGD optimizer with momentum $0.95$, and weight decay $0.0005$ are used. For learning the architecture, SGD optimizer with learning rate $0.005$ and weight decay $0.0001$ are used. The entire architecture search optimization takes about five days on two V100 GPUs.\n\nTable~\\ref{tbl:experiment} shows that our NAS model outperforms the SoTA on the Cityscapes.\nWithout any pretraining, our best model significantly outperforms all the SoTA method. Last row of Table~\\ref{tbl:experiment} shows that the light-weigh RSP design uses only 1/4 parameter size of SoTA methods but still outperforms them.  Fig.~\\ref{fig:cityscapes_results} shows that the visualization of our model on Cityscapes~\\cite{Cityscapes:CVPR2016} validation and test set.\n\nThe \\textbf{Mapillary Vistas Dataset} is a large-scale street-level image dataset containing 25,000 high-resolution images annotated into 66/124 object categories of which 37/70 classes are instance-specific labels (v.1.2 and v2.0, respectively). Annotation is performed in a dense and fine-grained style by using polygons for delineating individual objects. Dataset contains images from all around the world, captured at various conditions regarding weather, season and daytime. Images come from different imaging devices (mobile phones, tablets, action cameras, professional capturing rigs) and differently experienced photographers. We also adopted the Mapillary Vistas Dataset~\\cite{MapillaryVistas:ICCV2017} during our training procedure. Because of the class number of cityscapes is less than Mapillary Vistas Dataset, we have to map the category into the corresponding ones in Cityscapes. Table~\\ref{tbl:experiment2} shows that our NAS model outperforms the SoTA on the Cityscapes with adopting Mapillary Vistas Dataset~\\cite{MapillaryVistas:ICCV2017}.  Clearly, our light-weight RSP design still outperforms other SoTA methods even with only 1/4 parameter size. ", "table_source": "\\begin{table*}[t]\n\\caption{\nPerformance evaluations of the model on Cityscapes validation set. Training with the Mapillary Vistas dataset.\n\\vspace{-0.2cm}\n}\n\\centerline{\n\\setlength{\\tabcolsep}{3mm}{\n\\begin{tabular}{lcccc}\n\\toprule\n    Methods   & \\multicolumn{1}{l}{Backbone}  & \\multicolumn{1}{l}{Mapillary}  & \\multicolumn{1}{l}{mIOU(\\%)} & \\multicolumn{1}{l}{Params(M)} \\\\\\hline\n    \n    Mapillary~\\cite{MapillaryVistas:ICCV2017} & ResNeXt-101 & $\\checkmark$  & 80.6  & $\\times$ \\\\\n    HANet~\\cite{HANet:CVPR2020} & ResNet-101     & $\\checkmark$    & 81.7  & $\\times$ \\\\ \n    HRNetV2+OCR~\\cite{HRNet:arxiv2019} & HRNetV2  & $\\checkmark$  & 81.8   & 70.37 \\\\\n    DecoupleSegNets  & Wide-ResNet  & $\\checkmark$   & 81.6  & $\\times$ \\\\\n    DCNAS~\\cite{DCNAS:CVPR2021}  & $\\times$  & $\\checkmark$   & 81.3  & $\\times$ \\\\\\hline\n    Ours  & $\\times$  & $\\times$    & 81.4  & 68.67 \\\\\n    Ours  & $\\times$  & $\\checkmark$    & 82.1  & 68.67  \\\\\n    Ours + RSP  & $\\times$  & $\\checkmark$    & 81.7  & 17.20 \\\\\n    \n\\bottomrule\n\\end{tabular}}\n}    \n\\label{tbl:experiment2}\n\\end{table*}", "cell_list_gold": [{"value": "80.6", "char_index": [465, 469], "type": "Result", "training data/set": "Mapillary Vistas", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"Mapillary": "true"}, "model": "Mapillary", "model settings": {"Backbone": "ResNeXt-101", "Params (M)": "unknown"}}, {"value": "81.7", "char_index": [554, 558], "type": "Result", "training data/set": "Mapillary Vistas", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"Mapillary": "true"}, "model": "HANet", "model settings": {"Backbone": "ResNet-101", "Params (M)": "unknown"}}, {"value": "81.8", "char_index": [643, 647], "type": "Result", "training data/set": "Mapillary Vistas", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"Mapillary": "true"}, "model": "HRNetV2+OCR", "model settings": {"Backbone": "HRNetV2", "Params (M)": "70.37"}}, {"value": "70.37", "char_index": [652, 657], "type": "Hyper-parameter/Architecture", "model": "HRNetV2+OCR", "parameter/architecture name": ["Params", "model size"], "dataset": "Cityscapes Mapillary Vistas"}, {"value": "81.6", "char_index": [716, 720], "type": "Result", "training data/set": "Mapillary Vistas", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"Mapillary": "true"}, "model": "DecoupleSegNets", "model settings": {"Backbone": "Wide-ResNet", "Params (M)": "unknown"}}, {"value": "81.3", "char_index": [800, 804], "type": "Result", "training data/set": "Mapillary Vistas", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"Mapillary": "true"}, "model": "DCNAS", "model settings": {"Backbone": "false", "Params (M)": "unknown"}}, {"value": "81.4", "char_index": [864, 868], "type": "Result", "training data/set": "Mapillary Vistas", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"Mapillary": "false"}, "model": "Ours", "model settings": {"Backbone": "false", "Params (M)": "68.67"}}, {"value": "68.67", "char_index": [872, 877], "type": "Hyper-parameter/Architecture", "model": "Ours", "parameter/architecture name": ["Params", "model size"], "dataset": "Cityscapes Mapillary Vistas"}, {"value": "82.1", "char_index": [923, 927], "type": "Result", "training data/set": "Mapillary Vistas", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"Mapillary": "true"}, "model": "Ours", "model settings": {"Backbone": "false", "Params (M)": "68.67"}}, {"value": "68.67", "char_index": [931, 936], "type": "Hyper-parameter/Architecture", "model": "Ours", "parameter/architecture name": ["Params", "model size"], "dataset": "Cityscapes Mapillary Vistas"}, {"value": "81.7", "char_index": [989, 993], "type": "Result", "training data/set": "Mapillary Vistas", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"Mapillary": "true"}, "model": "Ours + RSP", "model settings": {"Backbone": "false", "Params (M)": "17.20"}}, {"value": "17.20", "char_index": [997, 1002], "type": "Hyper-parameter/Architecture", "model": "Ours + RSP", "parameter/architecture name": ["Params", "model size"], "dataset": "Cityscapes Mapillary Vistas"}]}, "2210.00698v1_table2": {"table_code": "\\begin{table}[t]\n\\caption{\nPerformance evaluation on the Cityscapes validation set.\n}\n\\centerline{\n\\setlength{\\tabcolsep}{1mm}{\n\\begin{tabular}{lccc}\n\\toprule\n    Methods   & \\multicolumn{1}{l}{OCR}  & \\multicolumn{1}{l}{ImageNet}  & \\multicolumn{1}{l}{mIOU(\\%)} \\\\\\hline\n    \n    HRNetV2~\\cite{HRNet:arxiv2019} & $\\times$   & $\\times$  & 76.16  \\\\\n    HRNetV2   & $\\checkmark$   & $\\times$  & 78.2  \\\\\n    HRNetV2   & $\\times$   & $\\checkmark$  & 80.9  \\\\\n    HRNetV2   & $\\checkmark$   & $\\checkmark$  & 81.6  \\\\\n    DCNAS~\\cite{DCNAS:CVPR2021}    & $\\times$      & $\\times$   & 81.9 \\\\\\hline\n    \n    Ours & $\\times$  & $\\checkmark$    & 81.4  \\\\\n    Ours & $\\checkmark$  & $\\checkmark$    & 83.2 \\\\\n    Ours + RSP & $\\checkmark$  & $\\checkmark$    & 82.5 \\\\\n    \n\\bottomrule\n\\end{tabular}}\n}    \n\\label{tbl:experiment3}\n\\end{table}", "table_label": "{tbl:experiment3}", "table_numeric_cells": [["76.16", "76.16", 339, 344, 339, 344], ["78.2", "78.2", 394, 398, 394, 398], ["80.9", "80.9", 448, 452, 448, 452], ["81.6", "81.6", 506, 510, 506, 510], ["81.9", "81.9", 581, 585, 581, 585], ["81.4", "81.4", 641, 645, 641, 645], ["83.2", "83.2", 695, 699, 695, 699], ["82.5", "82.5", 754, 758, 754, 758]], "text_chunk_selected": "Based on Eq.(\\ref{eq:LayerRelation}), we have $X_L = f_L$.  Then, Eq.(\\ref{eq:GradientforLayer1}) can be rewritten as\n\nFor cell structure search, only one type of cell is used: the {\\bf Normal Cell (Norm-Cell)} as in Fig.~\\ref{fig:N:cell}. We use two intermediate nodes to construct each cell structure. Each cell keeps the number of channels and neither reduces the spatial dimension from $W \\times H$ to $\\frac{W}{2} \\times \\frac{H}{2} $ nor enlarges the spatial dimension from $W \\times H$ to $2W \\times 2H$.\n\nReferring to Fig.~\\ref{fig:overview} ({\\em bottom left}) and Fig.~\\ref{fig:N:cell}, for each Norm-Cell with one input, feature maps derived by previous layer with different shapes are reshaped by convolution.\nThe input maps are then fed into subsequent operations after adding together, which are searched by DARTS to generate a feature map with the same size $C \\times {H} \\times {W}$.  \n\nFig.~\\ref{fig:PAM} depicts the details of this PAM.  Given a node, the idea of PAM aims at calculating the attention value of each path and choosing the number of outputs you want (denoted by red color in Fig.~\\ref{fig:PAM}(a)) with higher attention to construct the final macrostructure. The PAM uses a $1\\times1$ convolution and an interpolation operation to normalize all five inputs (denoted by four blue-arrows and one green-arrow in Fig.~\\ref{fig:PAM}(b)) to the same size $C\\times H\\times W$. Then, they are sent to a {\\bf Channel Attention Module (CAM)} to further calculate their channel attention.  Details of the CAM are depicted in Fig.~\\ref{fig:CAM}.  For any pair of $i^{th}$ and $j^{th}$ inputs $F_{i}^c$ and $F_{j}^c$ on the $c^{th}$ channel, we perform an inner product between them to obtain channel attention $S^{c}_{i,j} \\in R^{C \\times C}$:\n\\vspace{-0.15cm}\n\nThe \\textbf{Cityscapes} dataset~\\cite{Cityscapes:CVPR2016} is a recent large-scale urban scene dataset containing a diverse set of stereo video sequences from 50 cities. Cityscapes dataset contains high quality pixel-level annotations of $5,000$ images with size $1,024 \\times 2,048$. There are $2,975$, $500$, and $1,525$ for training, validation, and test images, respectively, and additional $20,000$ weakly annotated frames. It is an order of magnitude larger than similar previous datasets.\n\nWe consider 2 intermediate nodes in all cells with one input.\nFor each cell, we keep the channel numbers and the height and width of the feature tensor. Fig.~\\ref{fig:N:cell_result} shows the searched cell architectures by NAS on the Cityscapes dataset. Fig.~\\ref{fig:search_accuracy} shows that validation accuracy got more than 40\\% during macro search. Compared to the accuracy in AutoDeepLab~\\cite{AutoDeepLab:CVPR2019}, we got higher accuracy during search.\n$512 \\times 102$ random image crops are used. In DARTS search, batch size is 6 due to GPU memory limitation, architecture search optimization is conducted for $300$ epochs.\nIn the learning of network weight $w$, SGD optimizer with momentum $0.95$, and weight decay $0.0005$ are used. For learning the architecture, SGD optimizer with learning rate $0.005$ and weight decay $0.0001$ are used. The entire architecture search optimization takes about five days on two V100 GPUs.\n\nTable~\\ref{tbl:experiment} shows that our NAS model outperforms the SoTA on the Cityscapes.\nWithout any pretraining, our best model significantly outperforms all the SoTA method. Last row of Table~\\ref{tbl:experiment} shows that the light-weigh RSP design uses only 1/4 parameter size of SoTA methods but still outperforms them.  Fig.~\\ref{fig:cityscapes_results} shows that the visualization of our model on Cityscapes~\\cite{Cityscapes:CVPR2016} validation and test set.\n\nTo better aggregate the context, we also adopted the Object-Contextual\nRepresentations(OCR)~\\cite{OCR:ECCV2020}. Table~\\ref{tbl:experiment3} shows that our NAS model outperforms the other model with pretraining and Object-Contextual Representations(OCR)~\\cite{OCR:ECCV2020} on the Cityscapes dataset.  Without using any backbone, our light-weight architecture still outperforms HRNET and DCNAS.  ", "table_source": "\\begin{table}[t]\n\\caption{\nPerformance evaluation on the Cityscapes validation set.\n}\n\\centerline{\n\\setlength{\\tabcolsep}{1mm}{\n\\begin{tabular}{lccc}\n\\toprule\n    Methods   & \\multicolumn{1}{l}{OCR}  & \\multicolumn{1}{l}{ImageNet}  & \\multicolumn{1}{l}{mIOU(\\%)} \\\\\\hline\n    \n    HRNetV2~\\cite{HRNet:arxiv2019} & $\\times$   & $\\times$  & 76.16  \\\\\n    HRNetV2   & $\\checkmark$   & $\\times$  & 78.2  \\\\\n    HRNetV2   & $\\times$   & $\\checkmark$  & 80.9  \\\\\n    HRNetV2   & $\\checkmark$   & $\\checkmark$  & 81.6  \\\\\n    DCNAS~\\cite{DCNAS:CVPR2021}    & $\\times$      & $\\times$   & 81.9 \\\\\\hline\n    \n    Ours & $\\times$  & $\\checkmark$    & 81.4  \\\\\n    Ours & $\\checkmark$  & $\\checkmark$    & 83.2 \\\\\n    Ours + RSP & $\\checkmark$  & $\\checkmark$    & 82.5 \\\\\n    \n\\bottomrule\n\\end{tabular}}\n}    \n\\label{tbl:experiment3}\n\\end{table}", "cell_list_gold": [{"value": "76.16", "char_index": [339, 344], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "HRNetV2", "model settings": {"OCR": "false", "ImageNet pretraining": "false"}}, {"value": "78.2", "char_index": [394, 398], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "HRNetV2", "model settings": {"OCR": "true", "ImageNet pretraining": "false"}}, {"value": "80.9", "char_index": [448, 452], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "HRNetV2", "model settings": {"OCR": "false", "ImageNet pretraining": "true"}}, {"value": "81.6", "char_index": [506, 510], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "HRNetV2", "model settings": {"OCR": "true", "ImageNet pretraining": "true"}}, {"value": "81.9", "char_index": [581, 585], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "DCNAS", "model settings": {"OCR": "false", "ImageNet pretraining": "false"}}, {"value": "81.4", "char_index": [641, 645], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "Ours", "model settings": {"OCR": "false", "ImageNet pretraining": "true"}}, {"value": "83.2", "char_index": [695, 699], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "Ours", "model settings": {"OCR": "true", "ImageNet pretraining": "true"}}, {"value": "82.5", "char_index": [754, 758], "type": "Result", "training data/set": "Cityscapes", "test data/set": "Cityscapes validation", "task": ["semantic segmentation", "scene segmentation"], "metric": ["mIOU", "mean intersection-over-union"], "experimental settings": {"xx": "yy"}, "model": "Ours + RSP", "model settings": {"OCR": "true", "ImageNet pretraining": "true"}}]}, "2210.00705v1_table1": {"table_code": "\\begin{table}[t]\n    \\centering\n    \\caption{\n        Recall scores for image-speech retrieval on Flickr8k and SpokenCOCO testing sets.\n    }\n    \\label{tab:recall}\n    \\small\n    \\begin{tabular}{@{~~}l@{~~}c@{~~}c@{~~}c@{~}c@{~}c@{~~}c@{~~}c@{~~}} \n        \\toprule\n        & \\multicolumn{3}{c}{Speech $\\rightarrow$ Image} & & \\multicolumn{3}{c}{Image $\\rightarrow$ Speech} \\\\ \n        \\cmidrule{2-4}\n        \\cmidrule{6-8}\n        Method & R@1 & R@5 & R@10 & & R@1 & R@5 & R@10 \\\\ \n        \\midrule\n        & \\multicolumn{7}{c}{Flickr8k} \\\\\n        \\cmidrule{2-8}\n        \n        FaST-VGS$_{\\text{CO}}$~\\cite{peng2022fastvgs} & 26.6 & 56.4 & 68.8 & & 36.2 & 66.1 & 76.5 \\\\\n        FaST-VGS$_{\\text{CTF}}$~\\cite{peng2022fastvgs} & 29.3 & 58.6 & 71.0 & & 37.9 & 68.5 & 79.9 \\\\\n        MILAN~\\cite{sanabria2021milan} &\n        33.2 & 62.7 & 73.9 & & 49.6 & 79.2 & 87.5\\\\\n        \\midrule\n        \n        Parallel & 26.7 & 57.1 & 70.0 & & 41.3 & 73.9 & 84.2 \\\\\n        Cascaded & 8.2  & 25.7 & 37.2 & & 14.1 & 34.5 & 49.2 \\\\\n\n        Parallel Large & \\textbf{39.1} & \\textbf{72.0} & \\textbf{83.0} & & \\textbf{54.5} & \\textbf{84.5} & \\textbf{93.2} \\\\\n        Cascaded Large & 14.7 & 41.2 & 55.1 & & 21.8 & 52.0 & 67.7 \\\\\n \n        \\midrule\n        & \\multicolumn{7}{c}{SpokenCOCO} \\\\\n        \\cmidrule{2-8}\n        ResDAVEnet~\\cite{hsu2019transfer} &\n        17.3 & 41.9 & 55.0 & & 22.0 & 50.6 & 65.2 \\\\\n        FaST-VGS$_{\\text{CO}}$~\\cite{peng2022fastvgs} & 31.8 &  62.5 &  75.0 & & 42.5 &  73.7 & 84.9 \\\\\n        FaST-VGS$_{\\text{CTF}}$~\\cite{peng2022fastvgs} & \\textbf{35.9} &  66.3 &  77.9 & & 48.8 &  78.2 & 87.0 \\\\\n        \\midrule\n        \n        Parallel Large & 35.8 & \\textbf{66.5} & \\textbf{78.0} & & \\textbf{50.6} & \\textbf{80.9} & \\textbf{89.1} \\\\\n        Cascaded Large & 6.4 & 20.7 & 31.0 & & 9.6 & 27.7 & 39.7 \\\\\n\n        \\bottomrule\n    \\end{tabular}\n    \\vspace*{-10pt}\n\\end{table}", "table_label": "{tab:recall}", "table_numeric_cells": [["26.6", "26.6", 631, 635, 631, 635], ["56.4", "56.4", 638, 642, 638, 642], ["68.8", "68.8", 645, 649, 645, 649], ["36.2", "36.2", 654, 658, 654, 658], ["66.1", "66.1", 661, 665, 661, 665], ["76.5", "76.5", 668, 672, 668, 672], ["29.3", "29.3", 733, 737, 733, 737], ["58.6", "58.6", 740, 744, 740, 744], ["71.0", "71.0", 747, 751, 747, 751], ["37.9", "37.9", 756, 760, 756, 760], ["68.5", "68.5", 763, 767, 763, 767], ["79.9", "79.9", 770, 774, 770, 774], ["33.2", "33.2", 827, 831, 827, 831], ["62.7", "62.7", 834, 838, 834, 838], ["73.9", "73.9", 841, 845, 841, 845], ["49.6", "49.6", 850, 854, 850, 854], ["79.2", "79.2", 857, 861, 857, 861], ["87.5", "87.5", 864, 868, 864, 868], ["26.7", "26.7", 916, 920, 916, 920], ["57.1", "57.1", 923, 927, 923, 927], ["70.0", "70.0", 930, 934, 930, 934], ["41.3", "41.3", 939, 943, 939, 943], ["73.9", "73.9", 946, 950, 946, 950], ["84.2", "84.2", 953, 957, 953, 957], ["8.2", "8.2", 980, 983, 980, 983], ["25.7", "25.7", 987, 991, 987, 991], ["37.2", "37.2", 994, 998, 994, 998], ["14.1", "14.1", 1003, 1007, 1003, 1007], ["34.5", "34.5", 1010, 1014, 1010, 1014], ["49.2", "49.2", 1017, 1021, 1017, 1021], ["39.1", "\\textbf{39.1}", 1059, 1063, 1051, 1064], ["72.0", "\\textbf{72.0}", 1075, 1079, 1067, 1080], ["83.0", "\\textbf{83.0}", 1091, 1095, 1083, 1096], ["54.5", "\\textbf{54.5}", 1109, 1113, 1101, 1114], ["84.5", "\\textbf{84.5}", 1125, 1129, 1117, 1130], ["93.2", "\\textbf{93.2}", 1141, 1145, 1133, 1146], ["14.7", "14.7", 1175, 1179, 1175, 1179], ["41.2", "41.2", 1182, 1186, 1182, 1186], ["55.1", "55.1", 1189, 1193, 1189, 1193], ["21.8", "21.8", 1198, 1202, 1198, 1202], ["52.0", "52.0", 1205, 1209, 1205, 1209], ["67.7", "67.7", 1212, 1216, 1212, 1216], ["17.3", "17.3", 1358, 1362, 1358, 1362], ["41.9", "41.9", 1365, 1369, 1365, 1369], ["55.0", "55.0", 1372, 1376, 1372, 1376], ["22.0", "22.0", 1381, 1385, 1381, 1385], ["50.6", "50.6", 1388, 1392, 1388, 1392], ["65.2", "65.2", 1395, 1399, 1395, 1399], ["31.8", "31.8", 1459, 1463, 1459, 1463], ["62.5", "62.5", 1467, 1471, 1467, 1471], ["75.0", "75.0", 1475, 1479, 1475, 1479], ["42.5", "42.5", 1484, 1488, 1484, 1488], ["73.7", "73.7", 1492, 1496, 1492, 1496], ["84.9", "84.9", 1499, 1503, 1499, 1503], ["35.9", "\\textbf{35.9}", 1572, 1576, 1564, 1577], ["66.3", "66.3", 1581, 1585, 1581, 1585], ["77.9", "77.9", 1589, 1593, 1589, 1593], ["48.8", "48.8", 1598, 1602, 1598, 1602], ["78.2", "78.2", 1606, 1610, 1606, 1610], ["87.0", "87.0", 1613, 1617, 1613, 1617], ["35.8", "35.8", 1672, 1676, 1672, 1676], ["66.5", "\\textbf{66.5}", 1687, 1691, 1679, 1692], ["78.0", "\\textbf{78.0}", 1703, 1707, 1695, 1708], ["50.6", "\\textbf{50.6}", 1721, 1725, 1713, 1726], ["80.9", "\\textbf{80.9}", 1737, 1741, 1729, 1742], ["89.1", "\\textbf{89.1}", 1753, 1757, 1745, 1758], ["6.4", "6.4", 1787, 1790, 1787, 1790], ["20.7", "20.7", 1793, 1797, 1793, 1797], ["31.0", "31.0", 1800, 1804, 1800, 1804], ["9.6", "9.6", 1809, 1812, 1809, 1812], ["27.7", "27.7", 1815, 1819, 1815, 1819], ["39.7", "39.7", 1822, 1826, 1822, 1826]], "text_chunk_selected": "Much effort was put into using paired images and spoken captions to help speech processing~\\cite{chrupala2022visually}, and they are usually called visually grounded speech models~(VGS).\nVGS models benefit many applications like speech recognition~\\cite{hsu2019transfer}, word discovery~\\cite{harwath2015deep}, speech generation~\\cite{hsu2020text}, cross-modal alignment~\\cite{harwath2018jointly,wang2021align,khorrami2021evaluation}, and multilingual spoken language processing~\\cite{harwath2018vision,kamper2018visually,havard2020catplayinginthesnow,ohishi2020trilingual}.\nMost studies pre-train and evaluate VGS models on image-speech retrieval, showing the capabilities of capturing the correspondence between images and speech~\\cite{ilharco2019large,sanabria2021milan}.\nE.g., the recent Fast-Slow Transformer for Visually Grounding Speech~(FaST-VGS and FaST-VGS+) succeeds in many speech processing tasks by utilizing transformers and cross-modal attention mechanisms to perform image-speech retrieval and semantic tasks~\\cite{peng2022fastvgs,peng2022fastvgsplus}.\nMoreover, VGS models trained with retrieval objectives can extract semantic and word-level information from speech~\\cite{peng2022vghubert}, which is difficult to achieve by training solely with speech~\\cite{pasad2021layer}.\n\nThe VQ process is described as follows.\nWe first compute the cosine similarity between the $k^{\\text{th}}$ normalized CLS embedding ($\\boldsymbol{z}_k$) and the $v^{\\text{th}}$ subword embedding ($\\boldsymbol{e}_v$) as\n\\vspace{-3pt}\n\nwhere each embedding $\\boldsymbol{e}_v$ is a column vector and $\\tau$ is a hyper-parameter ($\\tau=0.1$).\nCombining Eqs. \\ref{eq:hardkw} and \\ref{eq:softkw}, we apply straight-through gradient estimator~\\cite{bengio2013estimating} to obtain quantized keywords\n\\vspace{-3pt}\n\n\\noindent\\textbf{Dataset.}\nSpeechCLIP is pre-trained and evaluated with retrieval on Flickr8k Audio Captions Corpus~\\cite{harwath2015deep} and SpokenCOCO dataset~\\cite{hsu2020text}.\nEach image in both datasets is paired with five spoken captions produced by humans uttering text captions.\nFlickr8k consists of 8k images and 46 hours of speech, while SpokenCOCO has 123k images and 742 hours of speech.\nFollowing FaST-VGS, we use the Karpathy split for SpokenCOCO~\\cite{karpathy2015deep}.\n\n\\noindent\\textbf{Model.}\nWe implemented SpeechCLIP in two sizes: Base and Large, a detailed comparison is shown in Table \\ref{tab:model_compare}.\nNote that we omit the Base notation in the following sections.\nThe hidden dimension of the transformer encoder is the same as that of the audio encoder.\nThe feed-forward network in the cascaded model's transformer encoder is removed for better performance.\nParallel and cascaded models have respectively eight and one attention head.\nWe set $K$ to 8 in all experiments.\nAll models are trained with Adam optimizer with a weight decay of 10$^{-\\text{6}}$, batch size of 256, and 50k steps in total.\nThe learning rate linearly increases to 10$^{-\\text{4}}$ in the first 5k steps and linearly decreases to 10$^{-\\text{8}}$ afterward.\nAll experiments are conducted on a 32GB V100 GPU except for pre-training on SpokenCOCO, which uses two.\nThe largest model's pre-training lasts approximately two days.\n\nIn this section, we evaluate SpeechCLIP on the image-speech retrieval task, showing how well models can align speech with CLIP image embeddings.\nAs shown in Table~\\ref{tab:recall}, parallel SpeechCLIP models surpass almost all baseline methods, especially for the Large SpeechCLIP models.\nThe parallel Base model on Flickr8k also shows competitive performance with the FaST-VGS$_{\\text{CO}}$ model, indicating that utilizing powerful pre-trained models and a small set of learnable parameters is sufficient.\nMoreover, the cascaded models obtain the lowest recall scores because passing encoded speech through VQ and a CLIP text encoder loses information.\nOverall, the results show the benefits of integrating CLIP in VGS models even with minimal fine-tuning.\n\nDue to the unique design of cascaded SpeechCLIP, we investigate what and how well the speech encoder extracts keywords.\nFor each encoded and normalized CLS token $\\boldsymbol{z}_k$, keywords are retrieved by finding subwords with the highest cosine similarities between $\\boldsymbol{z}_k$ and the corresponding subword embeddings.\nNotice that previous works~\\cite{kamper2018semantic,pasad2019contributions} are also capable of retrieving semantically related keywords from speech. Nonetheless, they required pretrained image tagger and the size of keywords set is very limited. For SpeechCLIP, we can apply the same method to other pretrained langange models' vocabulary, technically.\nAlso, our setting is quite different from ~\\cite{olaleye2022keyword}, where the 8 keywords are discovered from speech utterance without any text query in our work. Namely, SpeechCLIP can automatically summarize the speech by selecting 8 keywords.\nWe offer quantitative and qualitative analyses in the following paragraphs.\n\nWe inspect how well keywords are retrieved from speech signals for the quantitative analysis.\nThe evaluation metric is hit rate, which is the percentage of successful top-1 keyword retrieval of any word in the caption averaged over all testing samples.\nIn Table~\\ref{tab:kw_hit}, some CLS tokens frequently retrieve words in the ground truth captions, showing that the cascaded architecture can directly capture words from speech.\nMoreover, the first keyword's hit rate for models trained on Flickr8k is relatively high compared to other keywords.\nProbably because the first word in a sentence has a higher chance to be ``a'', which is also the top-1 commonly retrieved subword from the first keyword in Flickr8k.\nAnother finding is that the Large model obtains a higher averaged keyword hit rate than the Base model on Flickr8k, which is consistent with the trend in Table~\\ref{tab:recall}.\nHence, retrieving correct keywords is related to retrieving between speech and image samples.\nAlthough some CLS tokens obtain reasonable hit rates, one might question whether the retrieved words are meaningful instead of stopwords.\nHence, we next analyze the results qualitatively to address this concern.", "table_source": "\\begin{table}[t]\n    \\centering\n    \\caption{\n        Recall scores for image-speech retrieval on Flickr8k and SpokenCOCO testing sets.\n    }\n    \\label{tab:recall}\n    \\small\n    \\begin{tabular}{@{~~}l@{~~}c@{~~}c@{~~}c@{~}c@{~}c@{~~}c@{~~}c@{~~}} \n        \\toprule\n        & \\multicolumn{3}{c}{Speech $\\rightarrow$ Image} & & \\multicolumn{3}{c}{Image $\\rightarrow$ Speech} \\\\ \n        \\cmidrule{2-4}\n        \\cmidrule{6-8}\n        Method & R@1 & R@5 & R@10 & & R@1 & R@5 & R@10 \\\\ \n        \\midrule\n        & \\multicolumn{7}{c}{Flickr8k} \\\\\n        \\cmidrule{2-8}\n        \n        FaST-VGS$_{\\text{CO}}$~\\cite{peng2022fastvgs} & 26.6 & 56.4 & 68.8 & & 36.2 & 66.1 & 76.5 \\\\\n        FaST-VGS$_{\\text{CTF}}$~\\cite{peng2022fastvgs} & 29.3 & 58.6 & 71.0 & & 37.9 & 68.5 & 79.9 \\\\\n        MILAN~\\cite{sanabria2021milan} &\n        33.2 & 62.7 & 73.9 & & 49.6 & 79.2 & 87.5\\\\\n        \\midrule\n        \n        Parallel & 26.7 & 57.1 & 70.0 & & 41.3 & 73.9 & 84.2 \\\\\n        Cascaded & 8.2  & 25.7 & 37.2 & & 14.1 & 34.5 & 49.2 \\\\\n\n        Parallel Large & \\textbf{39.1} & \\textbf{72.0} & \\textbf{83.0} & & \\textbf{54.5} & \\textbf{84.5} & \\textbf{93.2} \\\\\n        Cascaded Large & 14.7 & 41.2 & 55.1 & & 21.8 & 52.0 & 67.7 \\\\\n \n        \\midrule\n        & \\multicolumn{7}{c}{SpokenCOCO} \\\\\n        \\cmidrule{2-8}\n        ResDAVEnet~\\cite{hsu2019transfer} &\n        17.3 & 41.9 & 55.0 & & 22.0 & 50.6 & 65.2 \\\\\n        FaST-VGS$_{\\text{CO}}$~\\cite{peng2022fastvgs} & 31.8 &  62.5 &  75.0 & & 42.5 &  73.7 & 84.9 \\\\\n        FaST-VGS$_{\\text{CTF}}$~\\cite{peng2022fastvgs} & \\textbf{35.9} &  66.3 &  77.9 & & 48.8 &  78.2 & 87.0 \\\\\n        \\midrule\n        \n        Parallel Large & 35.8 & \\textbf{66.5} & \\textbf{78.0} & & \\textbf{50.6} & \\textbf{80.9} & \\textbf{89.1} \\\\\n        Cascaded Large & 6.4 & 20.7 & 31.0 & & 9.6 & 27.7 & 39.7 \\\\\n\n        \\bottomrule\n    \\end{tabular}\n    \\vspace*{-10pt}\n\\end{table}", "cell_list_gold": [{"value": "26.6", "char_index": [631, 635], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "56.4", "char_index": [638, 642], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "68.8", "char_index": [645, 649], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "36.2", "char_index": [654, 658], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "66.1", "char_index": [661, 665], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "76.5", "char_index": [668, 672], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "29.3", "char_index": [733, 737], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "58.6", "char_index": [740, 744], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "71.0", "char_index": [747, 751], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "37.9", "char_index": [756, 760], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "68.5", "char_index": [763, 767], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "79.9", "char_index": [770, 774], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "33.2", "char_index": [827, 831], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "MILAN", "model settings": {"xx": "yy"}}, {"value": "62.7", "char_index": [834, 838], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "MILAN", "model settings": {"xx": "yy"}}, {"value": "73.9", "char_index": [841, 845], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "MILAN", "model settings": {"xx": "yy"}}, {"value": "49.6", "char_index": [850, 854], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "MILAN", "model settings": {"xx": "yy"}}, {"value": "79.2", "char_index": [857, 861], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "MILAN", "model settings": {"xx": "yy"}}, {"value": "87.5", "char_index": [864, 868], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "MILAN", "model settings": {"xx": "yy"}}, {"value": "26.7", "char_index": [916, 920], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel", "model settings": {"xx": "yy"}}, {"value": "57.1", "char_index": [923, 927], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel", "model settings": {"xx": "yy"}}, {"value": "70.0", "char_index": [930, 934], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel", "model settings": {"xx": "yy"}}, {"value": "41.3", "char_index": [939, 943], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel", "model settings": {"xx": "yy"}}, {"value": "73.9", "char_index": [946, 950], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel", "model settings": {"xx": "yy"}}, {"value": "84.2", "char_index": [953, 957], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel", "model settings": {"xx": "yy"}}, {"value": "8.2", "char_index": [980, 983], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"xx": "yy"}}, {"value": "25.7", "char_index": [987, 991], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"xx": "yy"}}, {"value": "37.2", "char_index": [994, 998], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"xx": "yy"}}, {"value": "14.1", "char_index": [1003, 1007], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"xx": "yy"}}, {"value": "34.5", "char_index": [1010, 1014], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"xx": "yy"}}, {"value": "49.2", "char_index": [1017, 1021], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"xx": "yy"}}, {"value": "39.1", "char_index": [1059, 1063], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "72.0", "char_index": [1075, 1079], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "83.0", "char_index": [1091, 1095], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "54.5", "char_index": [1109, 1113], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "84.5", "char_index": [1125, 1129], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "93.2", "char_index": [1141, 1145], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "14.7", "char_index": [1175, 1179], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "41.2", "char_index": [1182, 1186], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "55.1", "char_index": [1189, 1193], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "21.8", "char_index": [1198, 1202], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "52.0", "char_index": [1205, 1209], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "67.7", "char_index": [1212, 1216], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "17.3", "char_index": [1358, 1362], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "ResDAVEnet", "model settings": {"xx": "yy"}}, {"value": "41.9", "char_index": [1365, 1369], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "ResDAVEnet", "model settings": {"xx": "yy"}}, {"value": "55.0", "char_index": [1372, 1376], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "ResDAVEnet", "model settings": {"xx": "yy"}}, {"value": "22.0", "char_index": [1381, 1385], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "ResDAVEnet", "model settings": {"xx": "yy"}}, {"value": "50.6", "char_index": [1388, 1392], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "ResDAVEnet", "model settings": {"xx": "yy"}}, {"value": "65.2", "char_index": [1395, 1399], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "ResDAVEnet", "model settings": {"xx": "yy"}}, {"value": "31.8", "char_index": [1459, 1463], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "62.5", "char_index": [1467, 1471], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "75.0", "char_index": [1475, 1479], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "42.5", "char_index": [1484, 1488], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "73.7", "char_index": [1492, 1496], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "84.9", "char_index": [1499, 1503], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CO}}$", "model settings": {"xx": "yy"}}, {"value": "35.9", "char_index": [1572, 1576], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "66.3", "char_index": [1581, 1585], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "77.9", "char_index": [1589, 1593], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "48.8", "char_index": [1598, 1602], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "78.2", "char_index": [1606, 1610], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "87.0", "char_index": [1613, 1617], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "FaST-VGS$_{\\text{CTF}}$", "model settings": {"xx": "yy"}}, {"value": "35.8", "char_index": [1672, 1676], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "66.5", "char_index": [1687, 1691], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "78.0", "char_index": [1703, 1707], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "50.6", "char_index": [1721, 1725], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "80.9", "char_index": [1737, 1741], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "89.1", "char_index": [1753, 1757], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "6.4", "char_index": [1787, 1790], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "20.7", "char_index": [1793, 1797], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "31.0", "char_index": [1800, 1804], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "9.6", "char_index": [1809, 1812], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "27.7", "char_index": [1815, 1819], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}, {"value": "39.7", "char_index": [1822, 1826], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "image-speech retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded Large", "model settings": {"xx": "yy"}}]}, "2210.00705v1_table2": {"table_code": "\\begin{table}[t]\n    \\centering\n    \\caption{\n        Recall for speech-text retrieval on Flickr8k and SpokenCOCO.\n        `Sup.'' indicates the supervised version of parallel SpeechCLIP by replacing the image encoder with CLIP text encoder in parallel SpeechCLIP.\n    }\n    \\label{tab:zeroshot_speech_text}\n    \\small\n    \\begin{tabular}{@{~~}l@{~~}c@{~~}c@{~~}c@{~}c@{~}c@{~~}c@{~~}c@{~~}}\n        \\toprule\n        & \\multicolumn{3}{c}{Speech $\\rightarrow$ Text}  & & \\multicolumn{3}{c}{Text $\\rightarrow$ Speech} \\\\ \n        \\cmidrule{2-4}\n        \\cmidrule{6-8}\n        Method & R@1 & R@5 & R@10 & & R@1 & R@5 & R@10 \\\\ \n        \\midrule\n        & \\multicolumn{7}{c}{Flickr8k} \\\\\n        \\cmidrule{2-8}\n        Random & 0.10 &\t0.50 &\t0.99 & & 0.10 &\t0.50 &\t0.99 \\\\\n        Parallel Large & 19.56 & 44.06 & 58.46 & & 22.50 & 44.14 & 54.54 \\\\\n        Parallel Large (Sup.) & 97.06 & 99.24 & 99.46 & & 97.88 & 99.76 & 99.90 \\\\\n        \\midrule\n        & \\multicolumn{7}{c}{SpokenCOCO} \\\\\n        \\cmidrule{2-8}\n        Random & 0.02 &\t0.10 &\t0.20 & & 0.02 &\t0.10 &\t0.20 \\\\\n        Parallel Large & 60.32 & 81.81 & 88.18 & & 65.45 &\t85.82 &\t91.27 \\\\\n        Parallel Large (Sup.) & \n        95.02 &\t99.46 &\t99.78 & & 95.35 &\t99.68 &\t99.93 \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}", "table_label": "{tab:zeroshot_speech_text}", "table_numeric_cells": [["0.10", "0.10", 724, 728, 724, 728], ["0.50", "0.50", 731, 735, 731, 735], ["0.99", "0.99", 738, 742, 738, 742], ["0.10", "0.10", 747, 751, 747, 751], ["0.50", "0.50", 754, 758, 754, 758], ["0.99", "0.99", 761, 765, 761, 765], ["19.56", "19.56", 794, 799, 794, 799], ["44.06", "44.06", 802, 807, 802, 807], ["58.46", "58.46", 810, 815, 810, 815], ["22.50", "22.50", 820, 825, 820, 825], ["44.14", "44.14", 828, 833, 828, 833], ["54.54", "54.54", 836, 841, 836, 841], ["97.06", "97.06", 877, 882, 877, 882], ["99.24", "99.24", 885, 890, 885, 890], ["99.46", "99.46", 893, 898, 893, 898], ["97.88", "97.88", 903, 908, 903, 908], ["99.76", "99.76", 911, 916, 911, 916], ["99.90", "99.90", 919, 924, 919, 924], ["0.02", "0.02", 1029, 1033, 1029, 1033], ["0.10", "0.10", 1036, 1040, 1036, 1040], ["0.20", "0.20", 1043, 1047, 1043, 1047], ["0.02", "0.02", 1052, 1056, 1052, 1056], ["0.10", "0.10", 1059, 1063, 1059, 1063], ["0.20", "0.20", 1066, 1070, 1066, 1070], ["60.32", "60.32", 1099, 1104, 1099, 1104], ["81.81", "81.81", 1107, 1112, 1107, 1112], ["88.18", "88.18", 1115, 1120, 1115, 1120], ["65.45", "65.45", 1125, 1130, 1125, 1130], ["85.82", "85.82", 1133, 1138, 1133, 1138], ["91.27", "91.27", 1141, 1146, 1141, 1146], ["95.02", "95.02", 1191, 1196, 1191, 1196], ["99.46", "99.46", 1199, 1204, 1199, 1204], ["99.78", "99.78", 1207, 1212, 1207, 1212], ["95.35", "95.35", 1217, 1222, 1217, 1222], ["99.68", "99.68", 1225, 1230, 1225, 1230], ["99.93", "99.93", 1233, 1238, 1233, 1238]], "text_chunk_selected": "The VQ process is described as follows.\nWe first compute the cosine similarity between the $k^{\\text{th}}$ normalized CLS embedding ($\\boldsymbol{z}_k$) and the $v^{\\text{th}}$ subword embedding ($\\boldsymbol{e}_v$) as\n\\vspace{-3pt}\n\n\\begin{equation}\n    \\vspace{-3pt}\n    s_{kv} = \\text{cos}\\left( \\boldsymbol{z}_k , \\boldsymbol{e}_v \\right).\n    \\label{eq:cos}\n\\end{equation}\n\n\\begin{equation}\n    \\vspace{-3pt}\n    \\boldsymbol{e}_{v^{\\star}}, \\text{~where~} v^{\\star} = \\underset{1 \\leq v \\leq V}{\\text{argmax}} ~ s_{kv}.\n    \\label{eq:hardkw}\n\\end{equation}\n\n\\begin{equation}\n    \\vspace{-3pt}\n    \\overline{\\boldsymbol{h}}_k = \\left[ \\boldsymbol{e}_1 \\dots \\boldsymbol{e}_V \\right] \\text{softmax}\\left( \\left[ s_{k1} \\dots s_{kV} \\right]^\\top / \\tau \\right),\n    \\label{eq:softkw}\n\\end{equation}\n\n\\begin{equation}\n    \\vspace{-3pt}\n    \\boldsymbol{h}_k = \\boldsymbol{e}_{v^{\\star}} + \\overline{\\boldsymbol{h}}_k - \\text{sg}\\left( \\overline{\\boldsymbol{h}}_k \\right),\n\\end{equation}\n\n\\noindent\\textbf{Model.}\nWe implemented SpeechCLIP in two sizes: Base and Large, a detailed comparison is shown in Table \\ref{tab:model_compare}.\nNote that we omit the Base notation in the following sections.\nThe hidden dimension of the transformer encoder is the same as that of the audio encoder.\nThe feed-forward network in the cascaded model's transformer encoder is removed for better performance.\nParallel and cascaded models have respectively eight and one attention head.\nWe set $K$ to 8 in all experiments.\nAll models are trained with Adam optimizer with a weight decay of 10$^{-\\text{6}}$, batch size of 256, and 50k steps in total.\nThe learning rate linearly increases to 10$^{-\\text{4}}$ in the first 5k steps and linearly decreases to 10$^{-\\text{8}}$ afterward.\nAll experiments are conducted on a 32GB V100 GPU except for pre-training on SpokenCOCO, which uses two.\nThe largest model's pre-training lasts approximately two days.\n\nIn this section, we evaluate SpeechCLIP on the image-speech retrieval task, showing how well models can align speech with CLIP image embeddings.\nAs shown in Table~\\ref{tab:recall}, parallel SpeechCLIP models surpass almost all baseline methods, especially for the Large SpeechCLIP models.\nThe parallel Base model on Flickr8k also shows competitive performance with the FaST-VGS$_{\\text{CO}}$ model, indicating that utilizing powerful pre-trained models and a small set of learnable parameters is sufficient.\nMoreover, the cascaded models obtain the lowest recall scores because passing encoded speech through VQ and a CLIP text encoder loses information.\nOverall, the results show the benefits of integrating CLIP in VGS models even with minimal fine-tuning.\n\nAccording to Table~\\ref{tab:zeroshot_speech_text}, proposed SpeechCLIP models yield considerably better performance than random retrieval, showing that speech and text embedding spaces are well aligned.\nSpecifically, parallel SpeechCLIP performs better on this task when trained on a larger dataset like SpokenCOCO.\nAlthough the performance gap between the proposed methods and the supervised toplines remains, we show that bridging speech and text with image is possible and promising.", "table_source": "\\begin{table}[t]\n    \\centering\n    \\caption{\n        Recall for speech-text retrieval on Flickr8k and SpokenCOCO.\n        `Sup.'' indicates the supervised version of parallel SpeechCLIP by replacing the image encoder with CLIP text encoder in parallel SpeechCLIP.\n    }\n    \\label{tab:zeroshot_speech_text}\n    \\small\n    \\begin{tabular}{@{~~}l@{~~}c@{~~}c@{~~}c@{~}c@{~}c@{~~}c@{~~}c@{~~}}\n        \\toprule\n        & \\multicolumn{3}{c}{Speech $\\rightarrow$ Text}  & & \\multicolumn{3}{c}{Text $\\rightarrow$ Speech} \\\\ \n        \\cmidrule{2-4}\n        \\cmidrule{6-8}\n        Method & R@1 & R@5 & R@10 & & R@1 & R@5 & R@10 \\\\ \n        \\midrule\n        & \\multicolumn{7}{c}{Flickr8k} \\\\\n        \\cmidrule{2-8}\n        Random & 0.10 &\t0.50 &\t0.99 & & 0.10 &\t0.50 &\t0.99 \\\\\n        Parallel Large & 19.56 & 44.06 & 58.46 & & 22.50 & 44.14 & 54.54 \\\\\n        Parallel Large (Sup.) & 97.06 & 99.24 & 99.46 & & 97.88 & 99.76 & 99.90 \\\\\n        \\midrule\n        & \\multicolumn{7}{c}{SpokenCOCO} \\\\\n        \\cmidrule{2-8}\n        Random & 0.02 &\t0.10 &\t0.20 & & 0.02 &\t0.10 &\t0.20 \\\\\n        Parallel Large & 60.32 & 81.81 & 88.18 & & 65.45 &\t85.82 &\t91.27 \\\\\n        Parallel Large (Sup.) & \n        95.02 &\t99.46 &\t99.78 & & 95.35 &\t99.68 &\t99.93 \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}", "cell_list_gold": [{"value": "0.10", "char_index": [724, 728], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.50", "char_index": [731, 735], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [738, 742], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.10", "char_index": [747, 751], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.50", "char_index": [754, 758], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [761, 765], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "19.56", "char_index": [794, 799], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "44.06", "char_index": [802, 807], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "58.46", "char_index": [810, 815], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "22.50", "char_index": [820, 825], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "44.14", "char_index": [828, 833], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "54.54", "char_index": [836, 841], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "97.06", "char_index": [877, 882], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "99.24", "char_index": [885, 890], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "99.46", "char_index": [893, 898], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "97.88", "char_index": [903, 908], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "99.76", "char_index": [911, 916], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "99.90", "char_index": [919, 924], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "0.02", "char_index": [1029, 1033], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.10", "char_index": [1036, 1040], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.20", "char_index": [1043, 1047], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.02", "char_index": [1052, 1056], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.10", "char_index": [1059, 1063], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "0.20", "char_index": [1066, 1070], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Random", "model settings": {"xx": "yy"}}, {"value": "60.32", "char_index": [1099, 1104], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "81.81", "char_index": [1107, 1112], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "88.18", "char_index": [1115, 1120], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "65.45", "char_index": [1125, 1130], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "85.82", "char_index": [1133, 1138], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "91.27", "char_index": [1141, 1146], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large", "model settings": {"xx": "yy"}}, {"value": "95.02", "char_index": [1191, 1196], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "99.46", "char_index": [1199, 1204], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "99.78", "char_index": [1207, 1212], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Text": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "95.35", "char_index": [1217, 1222], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "99.68", "char_index": [1225, 1230], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}, {"value": "99.93", "char_index": [1233, 1238], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "speech-text retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Text $\rightarrow$ Speech": "true"}, "model": "Parallel Large (Sup.)", "model settings": {"xx": "yy"}}]}, "2210.00705v1_table3": {"table_code": "\\begin{table}[t]\n    \\caption{\n        Keyword hit rates for cascaded SpeechCLIP.\n        Avg denotes averaged hit rate.\n        $^\\dagger$ and $^\\ddagger$ respectively denote models trained on Flickr8k and SpokenCOCO.\n    }\n    \\label{tab:kw_hit}\n    \\centering\n    \\small\n    \\begin{tabular}{@{~~}l@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}}\n        \\toprule\n        Model & kw1 & kw2 & kw3 & kw4 & kw5 & kw6 & kw7 & kw8 & Avg \\\\\n        \\midrule\n        Base$^{\\dagger}$ & 57.0 & 25.6 & 20.2 & 5.0 & 20.0 & 26.5 & 10.5 & 16.6 & 22.7 \\\\\n        Large$^{\\dagger}$ & 56.5 & 19.6 & 20.5 & 37.5 & 21.7 & 34.6 & 26.4 & 44.7 & 32.7\\\\\n        Large$^{\\ddagger}$ & 27.5 & 22.4 & 35.8 & 61.0 & 21.6 & 54.2 & 60.1 & 22.9 & 38.2 \\\\\n        \n\n        \\bottomrule\n    \\end{tabular}\n    \\vspace*{-12pt}\n\\end{table}", "table_label": "{tab:kw_hit}", "table_numeric_cells": [["57.0", "57.0", 493, 497, 493, 497], ["25.6", "25.6", 500, 504, 500, 504], ["20.2", "20.2", 507, 511, 507, 511], ["5.0", "5.0", 514, 517, 514, 517], ["20.0", "20.0", 520, 524, 520, 524], ["26.5", "26.5", 527, 531, 527, 531], ["10.5", "10.5", 534, 538, 534, 538], ["16.6", "16.6", 541, 545, 541, 545], ["22.7", "22.7", 548, 552, 548, 552], ["56.5", "56.5", 584, 588, 584, 588], ["19.6", "19.6", 591, 595, 591, 595], ["20.5", "20.5", 598, 602, 598, 602], ["37.5", "37.5", 605, 609, 605, 609], ["21.7", "21.7", 612, 616, 612, 616], ["34.6", "34.6", 619, 623, 619, 623], ["26.4", "26.4", 626, 630, 626, 630], ["44.7", "44.7", 633, 637, 633, 637], ["32.7", "32.7", 640, 644, 640, 644], ["27.5", "27.5", 676, 680, 676, 680], ["22.4", "22.4", 683, 687, 683, 687], ["35.8", "35.8", 690, 694, 690, 694], ["61.0", "61.0", 697, 701, 697, 701], ["21.6", "21.6", 704, 708, 704, 708], ["54.2", "54.2", 711, 715, 711, 715], ["60.1", "60.1", 718, 722, 718, 722], ["22.9", "22.9", 725, 729, 725, 729], ["38.2", "38.2", 732, 736, 732, 736]], "text_chunk_selected": "The VQ process is described as follows.\nWe first compute the cosine similarity between the $k^{\\text{th}}$ normalized CLS embedding ($\\boldsymbol{z}_k$) and the $v^{\\text{th}}$ subword embedding ($\\boldsymbol{e}_v$) as\n\\vspace{-3pt}\n\nSince $\\boldsymbol{e}_{v^{\\star}}$ is not differentiable, we compute another embedding by weighted summing all $V$ subword embeddings as\n\\vspace{-3pt}\n\nwhere each embedding $\\boldsymbol{e}_v$ is a column vector and $\\tau$ is a hyper-parameter ($\\tau=0.1$).\nCombining Eqs. \\ref{eq:hardkw} and \\ref{eq:softkw}, we apply straight-through gradient estimator~\\cite{bengio2013estimating} to obtain quantized keywords\n\\vspace{-3pt}\n\nwhere $\\text{sg}(x) = x$ and $\\frac{d}{dx}\\text{sg}(x)=0$ is the stop gradient operator.\nThe $K$ keywords are then fed into the CLIP text encoder for computing the contrastive objective.\n\n\\noindent\\textbf{Model.}\nWe implemented SpeechCLIP in two sizes: Base and Large, a detailed comparison is shown in Table \\ref{tab:model_compare}.\nNote that we omit the Base notation in the following sections.\nThe hidden dimension of the transformer encoder is the same as that of the audio encoder.\nThe feed-forward network in the cascaded model's transformer encoder is removed for better performance.\nParallel and cascaded models have respectively eight and one attention head.\nWe set $K$ to 8 in all experiments.\nAll models are trained with Adam optimizer with a weight decay of 10$^{-\\text{6}}$, batch size of 256, and 50k steps in total.\nThe learning rate linearly increases to 10$^{-\\text{4}}$ in the first 5k steps and linearly decreases to 10$^{-\\text{8}}$ afterward.\nAll experiments are conducted on a 32GB V100 GPU except for pre-training on SpokenCOCO, which uses two.\nThe largest model's pre-training lasts approximately two days.\n\nIn this section, we evaluate SpeechCLIP on the image-speech retrieval task, showing how well models can align speech with CLIP image embeddings.\nAs shown in Table~\\ref{tab:recall}, parallel SpeechCLIP models surpass almost all baseline methods, especially for the Large SpeechCLIP models.\nThe parallel Base model on Flickr8k also shows competitive performance with the FaST-VGS$_{\\text{CO}}$ model, indicating that utilizing powerful pre-trained models and a small set of learnable parameters is sufficient.\nMoreover, the cascaded models obtain the lowest recall scores because passing encoded speech through VQ and a CLIP text encoder loses information.\nOverall, the results show the benefits of integrating CLIP in VGS models even with minimal fine-tuning.\n\nDue to the unique design of cascaded SpeechCLIP, we investigate what and how well the speech encoder extracts keywords.\nFor each encoded and normalized CLS token $\\boldsymbol{z}_k$, keywords are retrieved by finding subwords with the highest cosine similarities between $\\boldsymbol{z}_k$ and the corresponding subword embeddings.\nNotice that previous works~\\cite{kamper2018semantic,pasad2019contributions} are also capable of retrieving semantically related keywords from speech. Nonetheless, they required pretrained image tagger and the size of keywords set is very limited. For SpeechCLIP, we can apply the same method to other pretrained langange models' vocabulary, technically.\nAlso, our setting is quite different from ~\\cite{olaleye2022keyword}, where the 8 keywords are discovered from speech utterance without any text query in our work. Namely, SpeechCLIP can automatically summarize the speech by selecting 8 keywords.\nWe offer quantitative and qualitative analyses in the following paragraphs.\n\nWe inspect how well keywords are retrieved from speech signals for the quantitative analysis.\nThe evaluation metric is hit rate, which is the percentage of successful top-1 keyword retrieval of any word in the caption averaged over all testing samples.\nIn Table~\\ref{tab:kw_hit}, some CLS tokens frequently retrieve words in the ground truth captions, showing that the cascaded architecture can directly capture words from speech.\nMoreover, the first keyword's hit rate for models trained on Flickr8k is relatively high compared to other keywords.\nProbably because the first word in a sentence has a higher chance to be ``a'', which is also the top-1 commonly retrieved subword from the first keyword in Flickr8k.\nAnother finding is that the Large model obtains a higher averaged keyword hit rate than the Base model on Flickr8k, which is consistent with the trend in Table~\\ref{tab:recall}.\nHence, retrieving correct keywords is related to retrieving between speech and image samples.\nAlthough some CLS tokens obtain reasonable hit rates, one might question whether the retrieved words are meaningful instead of stopwords.\nHence, we next analyze the results qualitatively to address this concern.", "table_source": "\\begin{table}[t]\n    \\caption{\n        Keyword hit rates for cascaded SpeechCLIP.\n        Avg denotes averaged hit rate.\n        $^\\dagger$ and $^\\ddagger$ respectively denote models trained on Flickr8k and SpokenCOCO.\n    }\n    \\label{tab:kw_hit}\n    \\centering\n    \\small\n    \\begin{tabular}{@{~~}l@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}}\n        \\toprule\n        Model & kw1 & kw2 & kw3 & kw4 & kw5 & kw6 & kw7 & kw8 & Avg \\\\\n        \\midrule\n        Base$^{\\dagger}$ & 57.0 & 25.6 & 20.2 & 5.0 & 20.0 & 26.5 & 10.5 & 16.6 & 22.7 \\\\\n        Large$^{\\dagger}$ & 56.5 & 19.6 & 20.5 & 37.5 & 21.7 & 34.6 & 26.4 & 44.7 & 32.7\\\\\n        Large$^{\\ddagger}$ & 27.5 & 22.4 & 35.8 & 61.0 & 21.6 & 54.2 & 60.1 & 22.9 & 38.2 \\\\\n        \n\n        \\bottomrule\n    \\end{tabular}\n    \\vspace*{-12pt}\n\\end{table}", "cell_list_gold": [{"value": "57.0", "char_index": [493, 497], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 1", "kw1"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "25.6", "char_index": [500, 504], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 2", "kw2"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "20.2", "char_index": [507, 511], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 3", "kw3"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "5.0", "char_index": [514, 517], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 4", "kw4"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "20.0", "char_index": [520, 524], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 5", "kw5"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "26.5", "char_index": [527, 531], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 6", "kw6"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "10.5", "char_index": [534, 538], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 7", "kw7"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "16.6", "char_index": [541, 545], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 8", "kw8"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "22.7", "char_index": [548, 552], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["average keyword hit rates", "avg"], "experimental settings": {"xx": "yy"}, "model": "Base", "model settings": {"xx": "yy"}}, {"value": "56.5", "char_index": [584, 588], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 1", "kw1"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "19.6", "char_index": [591, 595], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 2", "kw2"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "20.5", "char_index": [598, 602], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 3", "kw3"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "37.5", "char_index": [605, 609], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 4", "kw4"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "21.7", "char_index": [612, 616], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 5", "kw5"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "34.6", "char_index": [619, 623], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 6", "kw6"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "26.4", "char_index": [626, 630], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 7", "kw7"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "44.7", "char_index": [633, 637], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["keyword hit rates 8", "kw8"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "32.7", "char_index": [640, 644], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "Keyword Retrieval", "metric": ["average keyword hit rates", "avg"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "27.5", "char_index": [676, 680], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["keyword hit rates 1", "kw1"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "22.4", "char_index": [683, 687], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["keyword hit rates 2", "kw2"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "35.8", "char_index": [690, 694], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["keyword hit rates 3", "kw3"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "61.0", "char_index": [697, 701], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["keyword hit rates 4", "kw4"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "21.6", "char_index": [704, 708], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["keyword hit rates 5", "kw5"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "54.2", "char_index": [711, 715], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["keyword hit rates 6", "kw6"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "60.1", "char_index": [718, 722], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["keyword hit rates 7", "kw7"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "22.9", "char_index": [725, 729], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["keyword hit rates 8", "kw8"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}, {"value": "38.2", "char_index": [732, 736], "type": "Result", "training data/set": "SpokenCOCO", "test data/set": "SpokenCOCO", "task": "Keyword Retrieval", "metric": ["average keyword hit rates", "avg"], "experimental settings": {"xx": "yy"}, "model": "Large", "model settings": {"xx": "yy"}}]}, "2210.00705v1_table5": {"table_code": "\\begin{table}[t]\n    \\centering\n    \\caption{\n        Recall scores on Flickr8k for ablation studies.\n    }\n    \\label{tab:ablation}\n    \\small\n    \\vspace{-5pt}\n    \\begin{tabular}{@{~~}l@{~~}c@{~~}c@{~~}c@{~}c@{~}c@{~~}c@{~~}c@{~~}} \n        \\toprule\n        & \\multicolumn{3}{c}{Speech $\\rightarrow$ Image} & & \\multicolumn{3}{c}{Image $\\rightarrow$ Speech} \\\\ \n        \\cmidrule{2-4}\n        \\cmidrule{6-8}\n        Method & R@1 & R@5 & R@10 & & R@1 & R@5 & R@10 \\\\\n        \\midrule\n        & \\multicolumn{7}{c}{Batch Normalization} \\\\\n        \\cmidrule{2-8}\n        Cascaded (w/ BN) & 8.2  & 25.7 & 37.2 & & 14.1 & 34.5 & 49.2 \\\\\n        Cascaded (w/o BN) & 1.1 & 4.7 & 8.4 & & 1.4 & 5.7 & 9.4 \\\\\n        \\midrule\n        & \\multicolumn{7}{c}{Keyword Num} \\\\\n        \\cmidrule{2-8}\n        Cascaded ($K$ $=$ 8) & 8.2  & 25.7 & 37.2 & & 14.1 & 34.5 & 49.2 \\\\\n        Cascaded ($K$ $=$ 4) & 3.5 & 13.2  & 21.1 & & 5.2 & 17.5 & 27.4 \\\\\n        Cascaded ($K$ $=$ 2) & 2.1 & 8.4   & 14.4 & & 2.7 & 10.6 & 17.6 \\\\\n        \n        \n        \n        \n        \\bottomrule\n    \\end{tabular}\n    \\vspace*{-10pt}\n\\end{table}", "table_label": "{tab:ablation}", "table_numeric_cells": [["8.2", "8.2", 589, 592, 589, 592], ["25.7", "25.7", 596, 600, 596, 600], ["37.2", "37.2", 603, 607, 603, 607], ["14.1", "14.1", 612, 616, 612, 616], ["34.5", "34.5", 619, 623, 619, 623], ["49.2", "49.2", 626, 630, 626, 630], ["1.1", "1.1", 662, 665, 662, 665], ["4.7", "4.7", 668, 671, 668, 671], ["8.4", "8.4", 674, 677, 674, 677], ["1.4", "1.4", 682, 685, 682, 685], ["5.7", "5.7", 688, 691, 688, 691], ["9.4", "9.4", 694, 697, 694, 697], ["8.2", "8.2", 817, 820, 817, 820], ["25.7", "25.7", 824, 828, 824, 828], ["37.2", "37.2", 831, 835, 831, 835], ["14.1", "14.1", 840, 844, 840, 844], ["34.5", "34.5", 847, 851, 847, 851], ["49.2", "49.2", 854, 858, 854, 858], ["3.5", "3.5", 893, 896, 893, 896], ["13.2", "13.2", 899, 903, 899, 903], ["21.1", "21.1", 907, 911, 907, 911], ["5.2", "5.2", 916, 919, 916, 919], ["17.5", "17.5", 922, 926, 922, 926], ["27.4", "27.4", 929, 933, 929, 933], ["2.1", "2.1", 968, 971, 968, 971], ["8.4", "8.4", 974, 977, 974, 977], ["14.4", "14.4", 982, 986, 982, 986], ["2.7", "2.7", 991, 994, 991, 994], ["10.6", "10.6", 997, 1001, 997, 1001], ["17.6", "17.6", 1004, 1008, 1004, 1008]], "text_chunk_selected": "The VQ process is described as follows.\nWe first compute the cosine similarity between the $k^{\\text{th}}$ normalized CLS embedding ($\\boldsymbol{z}_k$) and the $v^{\\text{th}}$ subword embedding ($\\boldsymbol{e}_v$) as\n\\vspace{-3pt}\n\nwhere each embedding $\\boldsymbol{e}_v$ is a column vector and $\\tau$ is a hyper-parameter ($\\tau=0.1$).\nCombining Eqs. \\ref{eq:hardkw} and \\ref{eq:softkw}, we apply straight-through gradient estimator~\\cite{bengio2013estimating} to obtain quantized keywords\n\\vspace{-3pt}\n\nwhere $\\text{sg}(x) = x$ and $\\frac{d}{dx}\\text{sg}(x)=0$ is the stop gradient operator.\nThe $K$ keywords are then fed into the CLIP text encoder for computing the contrastive objective.\n\n\\noindent\\textbf{Model.}\nWe implemented SpeechCLIP in two sizes: Base and Large, a detailed comparison is shown in Table \\ref{tab:model_compare}.\nNote that we omit the Base notation in the following sections.\nThe hidden dimension of the transformer encoder is the same as that of the audio encoder.\nThe feed-forward network in the cascaded model's transformer encoder is removed for better performance.\nParallel and cascaded models have respectively eight and one attention head.\nWe set $K$ to 8 in all experiments.\nAll models are trained with Adam optimizer with a weight decay of 10$^{-\\text{6}}$, batch size of 256, and 50k steps in total.\nThe learning rate linearly increases to 10$^{-\\text{4}}$ in the first 5k steps and linearly decreases to 10$^{-\\text{8}}$ afterward.\nAll experiments are conducted on a 32GB V100 GPU except for pre-training on SpokenCOCO, which uses two.\nThe largest model's pre-training lasts approximately two days.\n\nIn this section, we evaluate SpeechCLIP on the image-speech retrieval task, showing how well models can align speech with CLIP image embeddings.\nAs shown in Table~\\ref{tab:recall}, parallel SpeechCLIP models surpass almost all baseline methods, especially for the Large SpeechCLIP models.\nThe parallel Base model on Flickr8k also shows competitive performance with the FaST-VGS$_{\\text{CO}}$ model, indicating that utilizing powerful pre-trained models and a small set of learnable parameters is sufficient.\nMoreover, the cascaded models obtain the lowest recall scores because passing encoded speech through VQ and a CLIP text encoder loses information.\nOverall, the results show the benefits of integrating CLIP in VGS models even with minimal fine-tuning.\n\nIn this section, we show which HuBERT hidden layers are crucial for SpeechCLIP to perform well in various tasks discussed earlier.\nHence, we visualize the learned weights in the weighted sum mechanism mentioned in Sec.~\\ref{subsec:prelim} in Fig.~\\ref{fig:speechCLIP_weight}.\nBoth parallel and cascaded SpeechCLIP utilize the roughly the 8$^{\\text{th}}$ to the 10$^{\\text{th}}$ layers in HuBERT, inferring that HuBERT's top layers capture rich content and semantic information.\nThis result is consistent with prior works investigating the importance of different hidden layers in speech SSL models~\\cite{baevski2021wav2vec-u,pasad2021layer,chang2022distilhubert}, i.e., the top hidden layers contain word meaning and content information.\nHowever, the cascaded model's weights distribute more evenly over the layers than parallel SpeechCLIP, showing that the model architecture design affects the utilization of HuBERT's layers.\n\nHere, we demonstrate the importance of batch normalization in the cascaded SpeechCLIP.\nWe compare cascaded SpeechCLIP with its variant without using batch normalization, as shown in the first two rows of Table~\\ref{tab:ablation}.\nRemoving batch normalization degrades retrieval performance significantly, showing the significance of mean and variance matching described in Sec.~\\ref{subsec:cascaded}.\n\nThis section discusses the impact of the number of keywords in cascaded SpeechCLIP.\nWe report retrieval results on Flickr8k using different amounts of keywords in Table~\\ref{tab:ablation}.\nResults show that reducing keywords degrades retrieval performance, indicating that using fewer keywords is incapable of passing information from the speech encoder to the CLIP text encoder.\nFurthermore, the number of subword tokens in a Flickr8k utterance is 11.3 $\\pm$ 4.1, and some tokens carry less information like stopwords.\nTherefore, we suggest 8 is a reasonable number for $K$ to obtain good performance with cascaded SpeechCLIP.\nAlthough dynamically assigning $K$ for utterances of different lengths is more appropriate, we leave this approach for future investigation.", "table_source": "\\begin{table}[t]\n    \\centering\n    \\caption{\n        Recall scores on Flickr8k for ablation studies.\n    }\n    \\label{tab:ablation}\n    \\small\n    \\vspace{-5pt}\n    \\begin{tabular}{@{~~}l@{~~}c@{~~}c@{~~}c@{~}c@{~}c@{~~}c@{~~}c@{~~}} \n        \\toprule\n        & \\multicolumn{3}{c}{Speech $\\rightarrow$ Image} & & \\multicolumn{3}{c}{Image $\\rightarrow$ Speech} \\\\ \n        \\cmidrule{2-4}\n        \\cmidrule{6-8}\n        Method & R@1 & R@5 & R@10 & & R@1 & R@5 & R@10 \\\\\n        \\midrule\n        & \\multicolumn{7}{c}{Batch Normalization} \\\\\n        \\cmidrule{2-8}\n        Cascaded (w/ BN) & 8.2  & 25.7 & 37.2 & & 14.1 & 34.5 & 49.2 \\\\\n        Cascaded (w/o BN) & 1.1 & 4.7 & 8.4 & & 1.4 & 5.7 & 9.4 \\\\\n        \\midrule\n        & \\multicolumn{7}{c}{Keyword Num} \\\\\n        \\cmidrule{2-8}\n        Cascaded ($K$ $=$ 8) & 8.2  & 25.7 & 37.2 & & 14.1 & 34.5 & 49.2 \\\\\n        Cascaded ($K$ $=$ 4) & 3.5 & 13.2  & 21.1 & & 5.2 & 17.5 & 27.4 \\\\\n        Cascaded ($K$ $=$ 2) & 2.1 & 8.4   & 14.4 & & 2.7 & 10.6 & 17.6 \\\\\n        \n        \n        \n        \n        \\bottomrule\n    \\end{tabular}\n    \\vspace*{-10pt}\n\\end{table}", "cell_list_gold": [{"value": "8.2", "char_index": [589, 592], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"w/ BN": "true"}}, {"value": "25.7", "char_index": [596, 600], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"w/ BN": "true"}}, {"value": "37.2", "char_index": [603, 607], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"w/ BN": "true"}}, {"value": "14.1", "char_index": [612, 616], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"w/ BN": "true"}}, {"value": "34.5", "char_index": [619, 623], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"w/ BN": "true"}}, {"value": "49.2", "char_index": [626, 630], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"w/ BN": "true"}}, {"value": "1.1", "char_index": [662, 665], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"w/o BN": "true"}}, {"value": "4.7", "char_index": [668, 671], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"w/o BN": "true"}}, {"value": "8.4", "char_index": [674, 677], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"w/o BN": "true"}}, {"value": "1.4", "char_index": [682, 685], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"w/o BN": "true"}}, {"value": "5.7", "char_index": [688, 691], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"w/o BN": "true"}}, {"value": "9.4", "char_index": [694, 697], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"w/o BN": "true"}}, {"value": "8.2", "char_index": [817, 820], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "8"}}, {"value": "25.7", "char_index": [824, 828], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "8"}}, {"value": "37.2", "char_index": [831, 835], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "8"}}, {"value": "14.1", "char_index": [840, 844], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "8"}}, {"value": "34.5", "char_index": [847, 851], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "8"}}, {"value": "49.2", "char_index": [854, 858], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "8"}}, {"value": "3.5", "char_index": [893, 896], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "4"}}, {"value": "13.2", "char_index": [899, 903], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "4"}}, {"value": "21.1", "char_index": [907, 911], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "4"}}, {"value": "5.2", "char_index": [916, 919], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "4"}}, {"value": "17.5", "char_index": [922, 926], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "4"}}, {"value": "27.4", "char_index": [929, 933], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "4"}}, {"value": "2.1", "char_index": [968, 971], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "2"}}, {"value": "8.4", "char_index": [974, 977], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "2"}}, {"value": "14.4", "char_index": [982, 986], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Speech $\rightarrow$ Image": "true"}, "model": "Cascaded", "model settings": {"K": "2"}}, {"value": "2.7", "char_index": [991, 994], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@1", "R@1"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "2"}}, {"value": "10.6", "char_index": [997, 1001], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@5", "R@5"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "2"}}, {"value": "17.6", "char_index": [1004, 1008], "type": "Result", "training data/set": "Flickr8k", "test data/set": "Flickr8k", "task": "speech-image retrieval", "metric": ["Recall@10", "R@10"], "experimental settings": {"Image $\rightarrow$ Speech": "true"}, "model": "Cascaded", "model settings": {"K": "2"}}]}, "2210.00740v1_table0": {"table_code": "\\begin{table}\n\\caption{The improvement of AP on COCO validation set when our proposed method is applied to various baselines.}\n\\footnotesize\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|l|c|lccccc}\n\\hline\nMethod & Venue & Backbone &Input size & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nHourglass\\cite{newell2016stacked}& ECCV~2016 & 8-Stage Hourglass &$256\\times192$ &66.9 & - & - & - & - & -\\\\\nCPN\\cite{chen2018cascaded}& CVPR~2018 & ResNet-50 &$256\\times192$ &69.4 & - & - & - & - & -\\\\\nCPN\\cite{chen2018cascaded}& CVPR~2018 & ResNet-50 &$384\\times288$ &71.6 & - & - & - & - & -\\\\\nDarkPose\\cite{zhang2020distribution}& CVPR~2020 & HRNet-W32   &$384\\times288$ &76.6&90.7&82.8&72.7&83.9&81.5\\\\\nUDP\\cite{huang2020devil}& CVPR~2020 & HRNet-W32 &$384\\times288$ &77.8&91.7&84.5&74.2&84.3&82.4\\\\\nUDP\\cite{huang2020devil}& CVPR~2020 & HRNet-W48 &$384\\times288$ &77.8&92.0&84.3&74.2&84.5&82.5\\\\\nTokenPose\\cite{li2021tokenpose}& ICCV~2021 & TokenPose-L/D24 & $256\\times192$ & 75.8 &90.3 &82.5 &72.3 &82.7 &80.9\\\\\nRemoving Bias\\cite{gu2021removing}& ICCV~2021 & ResNet-152   &$384\\times288$ & 74.4 & - & - & - & - & -\\\\\nRemoving Bias\\cite{gu2021removing}& ICCV~2021 & HRNet-W32   &$256\\times192$ & 75.8 & - & - & - & - & -\\\\\n\\hline\nSimple Baseline\\cite{xiao2018simple} & ECCV~2018 & ResNet-152   &$384\\times288$ & 75.0 & 90.8 & 82.1 & 67.8 & 78.3 & 80.0 \\\\\n\\textbf{+ Ours} & & ResNet-152   &$384\\times288$ & \\textbf{76.7(\\textuparrow 1.7)} & \\textbf{92.1} & \\textbf{83.6} & \\textbf{69.7} & \\textbf{80.0} & \\textbf{81.3}\\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019 & HRNet-W32 &$384\\times288$ &76.7&91.9&83.6&73.2&83.2&81.6 \\\\\n\\textbf{+ Ours} & & HRNet-W32 &$384\\times288$ & \\textbf{78.2(\\textuparrow1.5)} & \\textbf{92.2} & \\textbf{84.5} & \\textbf{74.3} & \\textbf{84.7} & \\textbf{82.5}\\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019 & HRNet-W48 &$384\\times288$ &77.1&91.8&83.8&73.5&83.5&81.8\\\\\n\\textbf{+ Ours} & & HRNet-W48 &$384\\times288$ & \\textbf{78.8(\\textuparrow1.7)} & \\textbf{92.5} & \\textbf{85.1} & \\textbf{75.0} & \\textbf{85.3} & \\textbf{83.1}\\\\\n\\hline\nHRFormer\\cite{YuanFHLZCW21} & NIPS~2021 & HRFormer-Base &$384\\times288$ &78.0&92.2&84.8& 74.3 & 84.6 &82.6\\\\\n\\textbf{+ Ours} & & HRFormer-Base &$384\\times288$ & \\textbf{78.9(\\textuparrow0.9)} & \\textbf{92.6} & \\textbf{85.4} & \\textbf{75.3} & \\textbf{85.3} & \\textbf{83.3}\\\\\n\\hline\n\\end{tabular}}\n\\label{Tab:coco_val}\n\\end{table}", "table_label": "{Tab:coco_val}", "table_numeric_cells": [["66.9", "66.9", 445, 449, 445, 449], ["69.4", "69.4", 539, 543, 539, 543], ["71.6", "71.6", 633, 637, 633, 637], ["76.6", "76.6", 739, 743, 739, 743], ["90.7", "90.7", 744, 748, 744, 748], ["82.8", "82.8", 749, 753, 749, 753], ["72.7", "72.7", 754, 758, 754, 758], ["83.9", "83.9", 759, 763, 759, 763], ["81.5", "81.5", 764, 768, 764, 768], ["77.8", "77.8", 836, 840, 836, 840], ["91.7", "91.7", 841, 845, 841, 845], ["84.5", "84.5", 846, 850, 846, 850], ["74.2", "74.2", 851, 855, 851, 855], ["84.3", "84.3", 856, 860, 856, 860], ["82.4", "82.4", 861, 865, 861, 865], ["77.8", "77.8", 933, 937, 933, 937], ["92.0", "92.0", 938, 942, 938, 942], ["84.3", "84.3", 943, 947, 943, 947], ["74.2", "74.2", 948, 952, 948, 952], ["84.5", "84.5", 953, 957, 953, 957], ["82.5", "82.5", 958, 962, 958, 962], ["75.8", "75.8", 1045, 1049, 1045, 1049], ["90.3", "90.3", 1051, 1055, 1051, 1055], ["82.5", "82.5", 1057, 1061, 1057, 1061], ["72.3", "72.3", 1063, 1067, 1063, 1067], ["82.7", "82.7", 1069, 1073, 1069, 1073], ["80.9", "80.9", 1075, 1079, 1075, 1079], ["74.4", "74.4", 1161, 1165, 1161, 1165], ["75.8", "75.8", 1266, 1270, 1266, 1270], ["75.0", "75.0", 1382, 1386, 1382, 1386], ["90.8", "90.8", 1389, 1393, 1389, 1393], ["82.1", "82.1", 1396, 1400, 1396, 1400], ["67.8", "67.8", 1403, 1407, 1403, 1407], ["78.3", "78.3", 1410, 1414, 1410, 1414], ["80.0", "80.0", 1417, 1421, 1417, 1421], ["76.7", "\\textbf{76.7(\\textuparrow 1.7)}", 1484, 1488, 1476, 1507], ["92.1", "\\textbf{92.1}", 1518, 1522, 1510, 1523], ["83.6", "\\textbf{83.6}", 1534, 1538, 1526, 1539], ["69.7", "\\textbf{69.7}", 1550, 1554, 1542, 1555], ["80.0", "\\textbf{80.0}", 1566, 1570, 1558, 1571], ["81.3", "\\textbf{81.3}", 1582, 1586, 1574, 1587], ["76.7", "76.7", 1662, 1666, 1662, 1666], ["91.9", "91.9", 1667, 1671, 1667, 1671], ["83.6", "83.6", 1672, 1676, 1672, 1676], ["73.2", "73.2", 1677, 1681, 1677, 1681], ["83.2", "83.2", 1682, 1686, 1682, 1686], ["81.6", "81.6", 1687, 1691, 1687, 1691], ["78.2", "\\textbf{78.2(\\textuparrow1.5)}", 1751, 1755, 1743, 1773], ["92.2", "\\textbf{92.2}", 1784, 1788, 1776, 1789], ["84.5", "\\textbf{84.5}", 1800, 1804, 1792, 1805], ["74.3", "\\textbf{74.3}", 1816, 1820, 1808, 1821], ["84.7", "\\textbf{84.7}", 1832, 1836, 1824, 1837], ["82.5", "\\textbf{82.5}", 1848, 1852, 1840, 1853], ["77.1", "77.1", 1928, 1932, 1928, 1932], ["91.8", "91.8", 1933, 1937, 1933, 1937], ["83.8", "83.8", 1938, 1942, 1938, 1942], ["73.5", "73.5", 1943, 1947, 1943, 1947], ["83.5", "83.5", 1948, 1952, 1948, 1952], ["81.8", "81.8", 1953, 1957, 1953, 1957], ["78.8", "\\textbf{78.8(\\textuparrow1.7)}", 2016, 2020, 2008, 2038], ["92.5", "\\textbf{92.5}", 2049, 2053, 2041, 2054], ["85.1", "\\textbf{85.1}", 2065, 2069, 2057, 2070], ["75.0", "\\textbf{75.0}", 2081, 2085, 2073, 2086], ["85.3", "\\textbf{85.3}", 2097, 2101, 2089, 2102], ["83.1", "\\textbf{83.1}", 2113, 2117, 2105, 2118], ["78.0", "78.0", 2201, 2205, 2201, 2205], ["92.2", "92.2", 2206, 2210, 2206, 2210], ["84.8", "84.8", 2211, 2215, 2211, 2215], ["74.3", "74.3", 2217, 2221, 2217, 2221], ["84.6", "84.6", 2224, 2228, 2224, 2228], ["82.6", "82.6", 2230, 2234, 2230, 2234], ["78.9", "\\textbf{78.9(\\textuparrow0.9)}", 2297, 2301, 2289, 2319], ["92.6", "\\textbf{92.6}", 2330, 2334, 2322, 2335], ["85.4", "\\textbf{85.4}", 2346, 2350, 2338, 2351], ["75.3", "\\textbf{75.3}", 2362, 2366, 2354, 2367], ["85.3", "\\textbf{85.3}", 2378, 2382, 2370, 2383], ["83.3", "\\textbf{83.3}", 2394, 2398, 2386, 2399]], "text_chunk_selected": "The Earth Mover's Distance is a \na technique used for measuring the difference between two probability distributions, which can be understood as the optimal cost needed to transport the mass from one distribution to another. Specifically, for calculating the Earth Mover's Distance, we regard the source distribution as a set of ($N$) suppliers $S = (s_1, ..., s_N)^\\top$, where $s_n$ represents the total units of mass that the $n$-th supplier has, and we regard the target distribution as a set of ($M$) demanders $D = (d_1, ..., d_M)^\\top$, where $d_m$ represents the total units of mass that the $m$-th demander requires. Besides, we also denote $C \\in  R^{N \\times M}_{\\geq 0}$ as the cost function between the source and target distributions, where $C_{n,m}$ represents the cost for transporting a unit of mass from the $n$-th supplier to the $m$-th demander. Then we aim to find a least-cost transportation plan from the set of possible plans $P = \\{p \\in R^{N \\times M}_{\\geq 0}:~p\\textbf{1} = S~\\&~p^\\top\\textbf{1} = D\\}$ to transport all mass from the $N$ suppliers to the $M$ demanders, where \\textbf{1} represents a vector of ones. The Earth Mover's Distance $E_C(S, D)$ denotes the cost of the least-cost transportation plan, which can be formulated as:\n\n\\begin{equation}\\label{eq:pre_2}\n\\begin{aligned}\nE_C^{reg}(S, D)~=~\\langle C,p^{reg} \\rangle\n~\\textbf{where}~ p^{reg}~=~\\mathop{\\arg\\min}_{p \\in P}\\quad  \\left[\\langle C,p \\rangle - \\frac{1}{\\lambda} h(p)\\right]\n\\end{aligned}\n\\end{equation}\n\n\\noindent\\textbf{The demanders $D$.} As for the demanders, we aim to construct $D$ w.r.t. each body joint to represent its corresponding GT dot annotation.\nTo achieve this goal, for each body joint, a naive formulation of $D$ is to identify the pixel containing the GT dot annotation (i.e., the upper left pixel in Fig.~\\ref{fig:demander}(a)) and construct a single demander (i.e., the yellow dot in Fig.~\\ref{fig:demander}(a)) at the center of this pixel.\nHowever, this naive formulation can result in a suboptimal model performance, as the demander formulated in this way can be noticeably different from what the GT dot annotation might suggest.\nGenerally, the predicted heatmaps outputted by most of the existing \\textit{heatmap-based methods} \\cite{tompson2014joint,newell2016stacked,xiao2018simple,sun2019deep,li2019rethinking,cheng2020higherhrnet,zhang2020distribution,yang2020transpose,li2021tokenpose,luo2021rethinking,YuanFHLZCW21} have a lower resolution compared to the input image. For example, for the method HRNet \\cite{sun2019deep}, when the size of the input image is $384\\times288$, the size of the predicted heatmap is $96\\times72$ only. Due to such a resolution gap, as shown in  Fig.~\\ref{fig:demander}(a), there can exist a non-negligible distance between the location of the demander and the location of the dot annotation, which can affect the performance of the pose estimation model.\n\n\\begin{equation}\\label{eq:demander}\n\\begin{aligned}\nD = (d_1, d_2, d_3, d_4) ,~\\textbf{where}~ d_i = \\frac{(g-|x_{dot} - x_i|) \\times (g-|y_{dot} - y_i|)}{g^2}\n\\end{aligned}\n\\end{equation}\n\n\\noindent\\textbf{The cost function $C$.} \nTo measure the distribution difference between the $H \\times W$ suppliers ($S$) constructed from the predicted heatmap and the $4$ demanders ($D$) constructed from the GT dot annotation via calculating their Earth Mover's Distance, we also need to formulate a cost function $C \\in  R^{HW \\times 4}_{\\geq 0}$. \nDenote the coordinates of the $n$-th supplier as $(x_{s_n}, y_{s_n})$, where $n \\in \\{1, ..., H \\times W\\}$, and the coordinates of the $m$-th demander as $(x_{d_m}, y_{d_m})$, where $m \\in \\{1, 2, 3, 4\\}$. We here simply formulate $C_{n,m}$, the cost per unit transported from the $n$-th supplier to the $m$-th demander, as the L2 distance between the $n$-th supplier and the $m$-th demander, i.e., $C_{n,m} = \\sqrt{(x_{d_m} - x_{s_n})^2 + (y_{d_m} - y_{s_n})^2}$. Using such a cost function, our method can optimize the model directly towards accurately localizing the dot annotation of the body joint via minimizing the distribution difference between the suppliers $S$ and the demanders $D$.\n\nWe denote $K$ the number of joints per input image $I$. Then we denote $\\mathbf{H_{dot}} = \\{H_{dot}^1, ..., H_{dot}^K\\}$, $\\mathbf{H_{Gau}} = \\{H_{Gau}^1, ..., H_{Gau}^K\\}$,  and $\\mathbf{H_{pred}} = \\{H_{pred}^1, ..., H_{pred}^K\\}$ respectively the corresponding $K$ GT dot-annotated heatmaps, GT Gaussian-smoothed heatmaps, and predicted heatmaps of the input image $I$. We denote $\\mathcal D_{dot} = \\{(I, \\mathbf{H_{dot}})\\}$ the joint distribution of the input image and the corresponding $K$ dot-annotated heatmaps, and $\\mathcal D_{Gau} = \\{(I, \\mathbf{H_{Gau}})\\}$ the joint distribution of the input image and the corresponding $K$ Gaussian-smoothed heatmaps. Besides, we denote $l_{MSE}(a, b) = \\left\\| a - b\\right\\|_2^2$ the pixel-wise MSE loss, and $\\phi$ the model parameters where $\\phi(I) = \\mathbf{H_{pred}}$. After that, we denote $R(\\mathcal D_{dot}, \\phi, l_{MSE}) = \\mathbb{E}_{(I, \\mathbf{H_{dot}}) \\sim \\mathcal D_{dot}}[l_{MSE}(\\phi(I), \\mathbf{H_{dot}})]$ as the expected risk calculated between the predicted heatmaps and the GT dot-annotated heatmaps, and $R(\\mathcal D_{Gau}, \\phi, l_{MSE}) = \\mathbb{E}_{(I, \\mathbf{H_{Gau}}) \\sim \\mathcal D_{Gau}}[l_{MSE}(\\phi(I), \\mathbf{H_{Gau}})]$ as the expected risk calculated between the predicted heatmaps and the Gaussian-smoothed heatmaps.\n\n\\subsection{COCO Keypoint Detection}\n\\noindent\\textbf{Dataset \\& evaluation metric.} The COCO dataset \\cite{lin2014microsoft} contains more than 200k images and 250k person instances, which are annotated with 17 body joints. This dataset has three subsets including COCO training set, COCO validation set, and COCO test-dev set, which have 57k, 5k and 20k images, respectively. We conduct experiments on this dataset via first training the model on the train2017 set, and then evaluating the model on the val2017 set and test-dev2017 set. Following \\cite{xiao2018simple,sun2019deep,YuanFHLZCW21}, we use standard average precision (AP) calculated based on Object Keypoint Similarity (OKS) to evaluate model performance.\n\n\\noindent\\textbf{Results.} In Tab.~\\ref{Tab:coco_val} and Tab.~\\ref{Tab:coco_test_dev}, we report results on the COCO validation and test-dev sets. We observe that after applying our method on various baselines, a significant performance enhancement is achieved, which shows effectiveness of our proposed method. Moreover, we compare our method with other state-of-the-art 2D human pose estimation methods. Our method achieves superior performance compared to these methods, further demonstrating the effectiveness of our  method.", "table_source": "\\begin{table}\n\\caption{The improvement of AP on COCO validation set when our proposed method is applied to various baselines.}\n\\footnotesize\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|l|c|lccccc}\n\\hline\nMethod & Venue & Backbone &Input size & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nHourglass\\cite{newell2016stacked}& ECCV~2016 & 8-Stage Hourglass &$256\\times192$ &66.9 & - & - & - & - & -\\\\\nCPN\\cite{chen2018cascaded}& CVPR~2018 & ResNet-50 &$256\\times192$ &69.4 & - & - & - & - & -\\\\\nCPN\\cite{chen2018cascaded}& CVPR~2018 & ResNet-50 &$384\\times288$ &71.6 & - & - & - & - & -\\\\\nDarkPose\\cite{zhang2020distribution}& CVPR~2020 & HRNet-W32   &$384\\times288$ &76.6&90.7&82.8&72.7&83.9&81.5\\\\\nUDP\\cite{huang2020devil}& CVPR~2020 & HRNet-W32 &$384\\times288$ &77.8&91.7&84.5&74.2&84.3&82.4\\\\\nUDP\\cite{huang2020devil}& CVPR~2020 & HRNet-W48 &$384\\times288$ &77.8&92.0&84.3&74.2&84.5&82.5\\\\\nTokenPose\\cite{li2021tokenpose}& ICCV~2021 & TokenPose-L/D24 & $256\\times192$ & 75.8 &90.3 &82.5 &72.3 &82.7 &80.9\\\\\nRemoving Bias\\cite{gu2021removing}& ICCV~2021 & ResNet-152   &$384\\times288$ & 74.4 & - & - & - & - & -\\\\\nRemoving Bias\\cite{gu2021removing}& ICCV~2021 & HRNet-W32   &$256\\times192$ & 75.8 & - & - & - & - & -\\\\\n\\hline\nSimple Baseline\\cite{xiao2018simple} & ECCV~2018 & ResNet-152   &$384\\times288$ & 75.0 & 90.8 & 82.1 & 67.8 & 78.3 & 80.0 \\\\\n\\textbf{+ Ours} & & ResNet-152   &$384\\times288$ & \\textbf{76.7(\\textuparrow 1.7)} & \\textbf{92.1} & \\textbf{83.6} & \\textbf{69.7} & \\textbf{80.0} & \\textbf{81.3}\\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019 & HRNet-W32 &$384\\times288$ &76.7&91.9&83.6&73.2&83.2&81.6 \\\\\n\\textbf{+ Ours} & & HRNet-W32 &$384\\times288$ & \\textbf{78.2(\\textuparrow1.5)} & \\textbf{92.2} & \\textbf{84.5} & \\textbf{74.3} & \\textbf{84.7} & \\textbf{82.5}\\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019 & HRNet-W48 &$384\\times288$ &77.1&91.8&83.8&73.5&83.5&81.8\\\\\n\\textbf{+ Ours} & & HRNet-W48 &$384\\times288$ & \\textbf{78.8(\\textuparrow1.7)} & \\textbf{92.5} & \\textbf{85.1} & \\textbf{75.0} & \\textbf{85.3} & \\textbf{83.1}\\\\\n\\hline\nHRFormer\\cite{YuanFHLZCW21} & NIPS~2021 & HRFormer-Base &$384\\times288$ &78.0&92.2&84.8& 74.3 & 84.6 &82.6\\\\\n\\textbf{+ Ours} & & HRFormer-Base &$384\\times288$ & \\textbf{78.9(\\textuparrow0.9)} & \\textbf{92.6} & \\textbf{85.4} & \\textbf{75.3} & \\textbf{85.3} & \\textbf{83.3}\\\\\n\\hline\n\\end{tabular}}\n\\label{Tab:coco_val}\n\\end{table}", "cell_list_gold": [{"value": "66.9", "char_index": [445, 449], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Hourglass", "model settings": {"Backbone": "8-Stage Hourglass", "Input size": "256\\times192"}}, {"value": "69.4", "char_index": [539, 543], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "CPN", "model settings": {"Backbone": "ResNet-50", "Input size": "256\\times192"}}, {"value": "71.6", "char_index": [633, 637], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "CPN", "model settings": {"Backbone": "ResNet-50", "Input size": "384\\times288"}}, {"value": "76.6", "char_index": [739, 743], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "90.7", "char_index": [744, 748], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "82.8", "char_index": [749, 753], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "72.7", "char_index": [754, 758], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "83.9", "char_index": [759, 763], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "81.5", "char_index": [764, 768], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "77.8", "char_index": [836, 840], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "91.7", "char_index": [841, 845], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "84.5", "char_index": [846, 850], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "74.2", "char_index": [851, 855], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "84.3", "char_index": [856, 860], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "82.4", "char_index": [861, 865], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "77.8", "char_index": [933, 937], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "92.0", "char_index": [938, 942], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "84.3", "char_index": [943, 947], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "74.2", "char_index": [948, 952], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "84.5", "char_index": [953, 957], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "82.5", "char_index": [958, 962], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "75.8", "char_index": [1045, 1049], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times192"}}, {"value": "90.3", "char_index": [1051, 1055], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times192"}}, {"value": "82.5", "char_index": [1057, 1061], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times192"}}, {"value": "72.3", "char_index": [1063, 1067], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times192"}}, {"value": "82.7", "char_index": [1069, 1073], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times192"}}, {"value": "80.9", "char_index": [1075, 1079], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times192"}}, {"value": "74.4", "char_index": [1161, 1165], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Removing Bias", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "75.8", "char_index": [1266, 1270], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Removing Bias", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times192"}}, {"value": "75.0", "char_index": [1382, 1386], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "90.8", "char_index": [1389, 1393], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "82.1", "char_index": [1396, 1400], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "67.8", "char_index": [1403, 1407], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "78.3", "char_index": [1410, 1414], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "80.0", "char_index": [1417, 1421], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "76.7", "char_index": [1484, 1488], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "92.1", "char_index": [1518, 1522], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "83.6", "char_index": [1534, 1538], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "69.7", "char_index": [1550, 1554], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "80.0", "char_index": [1566, 1570], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "81.3", "char_index": [1582, 1586], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "76.7", "char_index": [1662, 1666], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "91.9", "char_index": [1667, 1671], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "83.6", "char_index": [1672, 1676], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "73.2", "char_index": [1677, 1681], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "83.2", "char_index": [1682, 1686], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "81.6", "char_index": [1687, 1691], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "78.2", "char_index": [1751, 1755], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "92.2", "char_index": [1784, 1788], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "84.5", "char_index": [1800, 1804], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "74.3", "char_index": [1816, 1820], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "84.7", "char_index": [1832, 1836], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "82.5", "char_index": [1848, 1852], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "77.1", "char_index": [1928, 1932], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "91.8", "char_index": [1933, 1937], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "83.8", "char_index": [1938, 1942], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "73.5", "char_index": [1943, 1947], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "83.5", "char_index": [1948, 1952], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "81.8", "char_index": [1953, 1957], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "78.8", "char_index": [2016, 2020], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "92.5", "char_index": [2049, 2053], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "85.1", "char_index": [2065, 2069], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "75.0", "char_index": [2081, 2085], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "85.3", "char_index": [2097, 2101], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "83.1", "char_index": [2113, 2117], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "78.0", "char_index": [2201, 2205], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "92.2", "char_index": [2206, 2210], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "84.8", "char_index": [2211, 2215], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "74.3", "char_index": [2217, 2221], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "84.6", "char_index": [2224, 2228], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "82.6", "char_index": [2230, 2234], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "78.9", "char_index": [2297, 2301], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "92.6", "char_index": [2330, 2334], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "85.4", "char_index": [2346, 2350], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "75.3", "char_index": [2362, 2366], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "85.3", "char_index": [2378, 2382], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "83.3", "char_index": [2394, 2398], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}]}, "2210.00740v1_table1": {"table_code": "\\begin{table}\n\\caption{The improvement of AP on COCO test-dev set when our proposed method is applied to various baselines.}\n\\footnotesize\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|l|c|lccccc}\n\\hline\nMethod & Venue & Backbone &Input size & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nG-RMI\\cite{papandreou2017towards} & CVPR~2017 & ResNet-101 &$353\\times 257$ &64.9 & 85.5 & 71.3   & 62.3 &70.0 &69.7\\\\\nMask-RCNN\\cite{he2017mask} & ICCV~2017  & ResNet-50-FPN  &- &63.1 & 87.3 & 68.7 & 57.8 &71.4 &- \\\\\nRMPE\\cite{fang2017rmpe}  & ICCV~2017 & PyraNet\\cite{yang2017learning}&$320\\times 256$  &72.3 & 89.2   & 79.1   & 68.0 &78.6 &-   \\\\\nCFN\\cite{huang2017coarse} & ICCV~2017  & -      &-  &72.6 & 86.1   & 69.7   & 78.3 &64.1 &-   \\\\\nCPN\\cite{chen2018cascaded} & CVPR~2018 & ResNet-Inception &$384\\times288$ &72.1 & 91.4   & 80.0   & 68.7 &77.2 &78.5\\\\\nCPN(ensemble)\\cite{chen2018cascaded} & CVPR~2018 & ResNet-Inception &$384\\times 288$       &73.0 & 91.7   & 80.9   & 69.5 &78.1 &79.0\\\\\nIntegral Pose Regression\\cite{sun2018integral} & ECCV~2018  & ResNet-101 &$256\\times 256$ &67.8 & 88.2 & 74.8 & 63.9 &74.0 &-   \\\\\nPosefix\\cite{moon2019posefix}  & CVPR~2019  & ResNet-152       &$384\\times288$      &73.6 & 90.8   & 81.0   & 70.3 &79.8 &79.0\\\\\nDarkPose\\cite{zhang2020distribution}& CVPR~2020& HRNet-W48 &$384\\times288$& 76.2 & 92.5 & 83.6 & 72.5 & 82.4 & 81.1\\\\\nUDP\\cite{huang2020devil} & CVPR~2020 & HRNet-W48 &$384\\times288$ &76.5   & 92.7 & 84.0 & 73.0&82.4 &81.6\\\\\nTokenPose\\cite{li2021tokenpose}& ICCV~2021 & TokenPose-L/D24 & $384\\times288$ & 75.9 & 92.3 & 83.4 & 72.2 & 82.1 & 80.8\\\\\nRemoving Bias\\cite{gu2021removing}& ICCV~2021 & HRNet-W48 &$384\\times288$ & 76.1 & - & - & - & - & 81.0\\\\\n\\hline\nSimple Baseline\\cite{xiao2018simple}& ECCV~2018 & ResNet-152   &$384\\times288$ &73.8 & 91.7 & 81.2 & 70.3 &80.0 &79.1\\\\\n\\textbf{+ Ours} && ResNet-152   &$384\\times288$ & \\textbf{75.3(\\textuparrow1.5)} & \\textbf{92.6} & \\textbf{83.1} & \\textbf{71.7} & \\textbf{81.1} & \\textbf{80.3}\\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019 & HRNet-W32 &$384\\times288$ &74.9 & 92.5 & 82.8 & 71.3 &80.9 &80.1\\\\\n\\textbf{+ Ours} && HRNet-W32 &$384\\times288$ & \\textbf{76.7(\\textuparrow1.8)} & \\textbf{92.6} & \\textbf{84.0} & \\textbf{73.0} & \\textbf{82.8} & \\textbf{81.5}\\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019& HRNet-W48 &$384\\times288$ &75.5 & 92.5  & 83.3& 71.9  &81.5 &80.5\\\\\n\\textbf{+ Ours} && HRNet-W48 &$384\\times288$ & \\textbf{77.2(\\textuparrow1.7)} & \\textbf{93.0} & \\textbf{84.4} & \\textbf{73.4} & \\textbf{83.3} & \\textbf{82.0}\\\\\n\\hline\nHRFormer\\cite{YuanFHLZCW21} & NIPS~2021& HRFormer-Base &$384\\times288$ & 76.2 & 92.7 & 83.8 & 72.5 & 82.3 & 81.2\\\\\n\\textbf{+ Ours} & & HRFormer-Base &$384\\times288$ & \\textbf{77.2(\\textuparrow1.0)} & \\textbf{93.1} & \\textbf{84.7} & \\textbf{73.8} & \\textbf{83.0} & \\textbf{82.1}\\\\\n\\hline\n\\end{tabular}}\n\\label{Tab:coco_test_dev}\n\\end{table}", "table_label": "{Tab:coco_test_dev}", "table_numeric_cells": [["64.9", "64.9", 438, 442, 438, 442], ["85.5", "85.5", 445, 449, 445, 449], ["71.3", "71.3", 452, 456, 452, 456], ["62.3", "62.3", 461, 465, 461, 465], ["70.0", "70.0", 467, 471, 467, 471], ["69.7", "69.7", 473, 477, 473, 477], ["63.1", "63.1", 541, 545, 541, 545], ["87.3", "87.3", 548, 552, 548, 552], ["68.7", "68.7", 555, 559, 555, 559], ["57.8", "57.8", 562, 566, 562, 566], ["71.4", "71.4", 568, 572, 568, 572], ["72.3", "72.3", 667, 671, 667, 671], ["89.2", "89.2", 674, 678, 674, 678], ["79.1", "79.1", 683, 687, 683, 687], ["68.0", "68.0", 692, 696, 692, 696], ["78.6", "78.6", 698, 702, 698, 702], ["72.6", "72.6", 764, 768, 764, 768], ["86.1", "86.1", 771, 775, 771, 775], ["69.7", "69.7", 780, 784, 780, 784], ["78.3", "78.3", 789, 793, 789, 793], ["64.1", "64.1", 795, 799, 795, 799], ["72.1", "72.1", 883, 887, 883, 887], ["91.4", "91.4", 890, 894, 890, 894], ["80.0", "80.0", 899, 903, 899, 903], ["68.7", "68.7", 908, 912, 908, 912], ["77.2", "77.2", 914, 918, 914, 918], ["78.5", "78.5", 920, 924, 920, 924], ["73.0", "73.0", 1019, 1023, 1019, 1023], ["91.7", "91.7", 1026, 1030, 1026, 1030], ["80.9", "80.9", 1035, 1039, 1035, 1039], ["69.5", "69.5", 1044, 1048, 1044, 1048], ["78.1", "78.1", 1050, 1054, 1050, 1054], ["79.0", "79.0", 1056, 1060, 1056, 1060], ["67.8", "67.8", 1154, 1158, 1154, 1158], ["88.2", "88.2", 1161, 1165, 1161, 1165], ["74.8", "74.8", 1168, 1172, 1168, 1172], ["63.9", "63.9", 1175, 1179, 1175, 1179], ["74.0", "74.0", 1181, 1185, 1181, 1185], ["73.6", "73.6", 1279, 1283, 1279, 1283], ["90.8", "90.8", 1286, 1290, 1286, 1290], ["81.0", "81.0", 1295, 1299, 1295, 1299], ["70.3", "70.3", 1304, 1308, 1304, 1308], ["79.8", "79.8", 1310, 1314, 1310, 1314], ["79.0", "79.0", 1316, 1320, 1316, 1320], ["76.2", "76.2", 1399, 1403, 1399, 1403], ["92.5", "92.5", 1406, 1410, 1406, 1410], ["83.6", "83.6", 1413, 1417, 1413, 1417], ["72.5", "72.5", 1420, 1424, 1420, 1424], ["82.4", "82.4", 1427, 1431, 1427, 1431], ["81.1", "81.1", 1434, 1438, 1434, 1438], ["76.5", "76.5", 1507, 1511, 1507, 1511], ["92.7", "92.7", 1516, 1520, 1516, 1520], ["84.0", "84.0", 1523, 1527, 1523, 1527], ["73.0", "73.0", 1530, 1534, 1530, 1534], ["82.4", "82.4", 1535, 1539, 1535, 1539], ["81.6", "81.6", 1541, 1545, 1541, 1545], ["75.9", "75.9", 1628, 1632, 1628, 1632], ["92.3", "92.3", 1635, 1639, 1635, 1639], ["83.4", "83.4", 1642, 1646, 1642, 1646], ["72.2", "72.2", 1649, 1653, 1649, 1653], ["82.1", "82.1", 1656, 1660, 1656, 1660], ["80.8", "80.8", 1663, 1667, 1663, 1667], ["76.1", "76.1", 1746, 1750, 1746, 1750], ["81.0", "81.0", 1769, 1773, 1769, 1773], ["73.8", "73.8", 1863, 1867, 1863, 1867], ["91.7", "91.7", 1870, 1874, 1870, 1874], ["81.2", "81.2", 1877, 1881, 1877, 1881], ["70.3", "70.3", 1884, 1888, 1884, 1888], ["80.0", "80.0", 1890, 1894, 1890, 1894], ["79.1", "79.1", 1896, 1900, 1896, 1900], ["75.3", "\\textbf{75.3(\\textuparrow1.5)}", 1961, 1965, 1953, 1983], ["92.6", "\\textbf{92.6}", 1994, 1998, 1986, 1999], ["83.1", "\\textbf{83.1}", 2010, 2014, 2002, 2015], ["71.7", "\\textbf{71.7}", 2026, 2030, 2018, 2031], ["81.1", "\\textbf{81.1}", 2042, 2046, 2034, 2047], ["80.3", "\\textbf{80.3}", 2058, 2062, 2050, 2063], ["74.9", "74.9", 2138, 2142, 2138, 2142], ["92.5", "92.5", 2145, 2149, 2145, 2149], ["82.8", "82.8", 2152, 2156, 2152, 2156], ["71.3", "71.3", 2159, 2163, 2159, 2163], ["80.9", "80.9", 2165, 2169, 2165, 2169], ["80.1", "80.1", 2171, 2175, 2171, 2175], ["76.7", "\\textbf{76.7(\\textuparrow1.8)}", 2233, 2237, 2225, 2255], ["92.6", "\\textbf{92.6}", 2266, 2270, 2258, 2271], ["84.0", "\\textbf{84.0}", 2282, 2286, 2274, 2287], ["73.0", "\\textbf{73.0}", 2298, 2302, 2290, 2303], ["82.8", "\\textbf{82.8}", 2314, 2318, 2306, 2319], ["81.5", "\\textbf{81.5}", 2330, 2334, 2322, 2335], ["75.5", "75.5", 2409, 2413, 2409, 2413], ["92.5", "92.5", 2416, 2420, 2416, 2420], ["83.3", "83.3", 2424, 2428, 2424, 2428], ["71.9", "71.9", 2430, 2434, 2430, 2434], ["81.5", "81.5", 2437, 2441, 2437, 2441], ["80.5", "80.5", 2443, 2447, 2443, 2447], ["77.2", "\\textbf{77.2(\\textuparrow1.7)}", 2505, 2509, 2497, 2527], ["93.0", "\\textbf{93.0}", 2538, 2542, 2530, 2543], ["84.4", "\\textbf{84.4}", 2554, 2558, 2546, 2559], ["73.4", "\\textbf{73.4}", 2570, 2574, 2562, 2575], ["83.3", "\\textbf{83.3}", 2586, 2590, 2578, 2591], ["82.0", "\\textbf{82.0}", 2602, 2606, 2594, 2607], ["76.2", "76.2", 2690, 2694, 2690, 2694], ["92.7", "92.7", 2697, 2701, 2697, 2701], ["83.8", "83.8", 2704, 2708, 2704, 2708], ["72.5", "72.5", 2711, 2715, 2711, 2715], ["82.3", "82.3", 2718, 2722, 2718, 2722], ["81.2", "81.2", 2725, 2729, 2725, 2729], ["77.2", "\\textbf{77.2(\\textuparrow1.0)}", 2792, 2796, 2784, 2814], ["93.1", "\\textbf{93.1}", 2825, 2829, 2817, 2830], ["84.7", "\\textbf{84.7}", 2841, 2845, 2833, 2846], ["73.8", "\\textbf{73.8}", 2857, 2861, 2849, 2862], ["83.0", "\\textbf{83.0}", 2873, 2877, 2865, 2878], ["82.1", "\\textbf{82.1}", 2889, 2893, 2881, 2894]], "text_chunk_selected": "The Earth Mover's Distance is a \na technique used for measuring the difference between two probability distributions, which can be understood as the optimal cost needed to transport the mass from one distribution to another. Specifically, for calculating the Earth Mover's Distance, we regard the source distribution as a set of ($N$) suppliers $S = (s_1, ..., s_N)^\\top$, where $s_n$ represents the total units of mass that the $n$-th supplier has, and we regard the target distribution as a set of ($M$) demanders $D = (d_1, ..., d_M)^\\top$, where $d_m$ represents the total units of mass that the $m$-th demander requires. Besides, we also denote $C \\in  R^{N \\times M}_{\\geq 0}$ as the cost function between the source and target distributions, where $C_{n,m}$ represents the cost for transporting a unit of mass from the $n$-th supplier to the $m$-th demander. Then we aim to find a least-cost transportation plan from the set of possible plans $P = \\{p \\in R^{N \\times M}_{\\geq 0}:~p\\textbf{1} = S~\\&~p^\\top\\textbf{1} = D\\}$ to transport all mass from the $N$ suppliers to the $M$ demanders, where \\textbf{1} represents a vector of ones. The Earth Mover's Distance $E_C(S, D)$ denotes the cost of the least-cost transportation plan, which can be formulated as:\n\n\\begin{equation}\\label{eq:pre_2}\n\\begin{aligned}\nE_C^{reg}(S, D)~=~\\langle C,p^{reg} \\rangle\n~\\textbf{where}~ p^{reg}~=~\\mathop{\\arg\\min}_{p \\in P}\\quad  \\left[\\langle C,p \\rangle - \\frac{1}{\\lambda} h(p)\\right]\n\\end{aligned}\n\\end{equation}\n\n\\noindent\\textbf{The demanders $D$.} As for the demanders, we aim to construct $D$ w.r.t. each body joint to represent its corresponding GT dot annotation.\nTo achieve this goal, for each body joint, a naive formulation of $D$ is to identify the pixel containing the GT dot annotation (i.e., the upper left pixel in Fig.~\\ref{fig:demander}(a)) and construct a single demander (i.e., the yellow dot in Fig.~\\ref{fig:demander}(a)) at the center of this pixel.\nHowever, this naive formulation can result in a suboptimal model performance, as the demander formulated in this way can be noticeably different from what the GT dot annotation might suggest.\nGenerally, the predicted heatmaps outputted by most of the existing \\textit{heatmap-based methods} \\cite{tompson2014joint,newell2016stacked,xiao2018simple,sun2019deep,li2019rethinking,cheng2020higherhrnet,zhang2020distribution,yang2020transpose,li2021tokenpose,luo2021rethinking,YuanFHLZCW21} have a lower resolution compared to the input image. For example, for the method HRNet \\cite{sun2019deep}, when the size of the input image is $384\\times288$, the size of the predicted heatmap is $96\\times72$ only. Due to such a resolution gap, as shown in  Fig.~\\ref{fig:demander}(a), there can exist a non-negligible distance between the location of the demander and the location of the dot annotation, which can affect the performance of the pose estimation model.\n\n\\begin{equation}\\label{eq:demander}\n\\begin{aligned}\nD = (d_1, d_2, d_3, d_4) ,~\\textbf{where}~ d_i = \\frac{(g-|x_{dot} - x_i|) \\times (g-|y_{dot} - y_i|)}{g^2}\n\\end{aligned}\n\\end{equation}\n\n\\noindent\\textbf{The cost function $C$.} \nTo measure the distribution difference between the $H \\times W$ suppliers ($S$) constructed from the predicted heatmap and the $4$ demanders ($D$) constructed from the GT dot annotation via calculating their Earth Mover's Distance, we also need to formulate a cost function $C \\in  R^{HW \\times 4}_{\\geq 0}$. \nDenote the coordinates of the $n$-th supplier as $(x_{s_n}, y_{s_n})$, where $n \\in \\{1, ..., H \\times W\\}$, and the coordinates of the $m$-th demander as $(x_{d_m}, y_{d_m})$, where $m \\in \\{1, 2, 3, 4\\}$. We here simply formulate $C_{n,m}$, the cost per unit transported from the $n$-th supplier to the $m$-th demander, as the L2 distance between the $n$-th supplier and the $m$-th demander, i.e., $C_{n,m} = \\sqrt{(x_{d_m} - x_{s_n})^2 + (y_{d_m} - y_{s_n})^2}$. Using such a cost function, our method can optimize the model directly towards accurately localizing the dot annotation of the body joint via minimizing the distribution difference between the suppliers $S$ and the demanders $D$.\n\nWe denote $K$ the number of joints per input image $I$. Then we denote $\\mathbf{H_{dot}} = \\{H_{dot}^1, ..., H_{dot}^K\\}$, $\\mathbf{H_{Gau}} = \\{H_{Gau}^1, ..., H_{Gau}^K\\}$,  and $\\mathbf{H_{pred}} = \\{H_{pred}^1, ..., H_{pred}^K\\}$ respectively the corresponding $K$ GT dot-annotated heatmaps, GT Gaussian-smoothed heatmaps, and predicted heatmaps of the input image $I$. We denote $\\mathcal D_{dot} = \\{(I, \\mathbf{H_{dot}})\\}$ the joint distribution of the input image and the corresponding $K$ dot-annotated heatmaps, and $\\mathcal D_{Gau} = \\{(I, \\mathbf{H_{Gau}})\\}$ the joint distribution of the input image and the corresponding $K$ Gaussian-smoothed heatmaps. Besides, we denote $l_{MSE}(a, b) = \\left\\| a - b\\right\\|_2^2$ the pixel-wise MSE loss, and $\\phi$ the model parameters where $\\phi(I) = \\mathbf{H_{pred}}$. After that, we denote $R(\\mathcal D_{dot}, \\phi, l_{MSE}) = \\mathbb{E}_{(I, \\mathbf{H_{dot}}) \\sim \\mathcal D_{dot}}[l_{MSE}(\\phi(I), \\mathbf{H_{dot}})]$ as the expected risk calculated between the predicted heatmaps and the GT dot-annotated heatmaps, and $R(\\mathcal D_{Gau}, \\phi, l_{MSE}) = \\mathbb{E}_{(I, \\mathbf{H_{Gau}}) \\sim \\mathcal D_{Gau}}[l_{MSE}(\\phi(I), \\mathbf{H_{Gau}})]$ as the expected risk calculated between the predicted heatmaps and the Gaussian-smoothed heatmaps.\n\n\\subsection{COCO Keypoint Detection}\n\\noindent\\textbf{Dataset \\& evaluation metric.} The COCO dataset \\cite{lin2014microsoft} contains more than 200k images and 250k person instances, which are annotated with 17 body joints. This dataset has three subsets including COCO training set, COCO validation set, and COCO test-dev set, which have 57k, 5k and 20k images, respectively. We conduct experiments on this dataset via first training the model on the train2017 set, and then evaluating the model on the val2017 set and test-dev2017 set. Following \\cite{xiao2018simple,sun2019deep,YuanFHLZCW21}, we use standard average precision (AP) calculated based on Object Keypoint Similarity (OKS) to evaluate model performance.\n\n\\noindent\\textbf{Results.} In Tab.~\\ref{Tab:coco_val} and Tab.~\\ref{Tab:coco_test_dev}, we report results on the COCO validation and test-dev sets. We observe that after applying our method on various baselines, a significant performance enhancement is achieved, which shows effectiveness of our proposed method. Moreover, we compare our method with other state-of-the-art 2D human pose estimation methods. Our method achieves superior performance compared to these methods, further demonstrating the effectiveness of our  method.", "table_source": "\\begin{table}\n\\caption{The improvement of AP on COCO test-dev set when our proposed method is applied to various baselines.}\n\\footnotesize\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|l|c|lccccc}\n\\hline\nMethod & Venue & Backbone &Input size & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nG-RMI\\cite{papandreou2017towards} & CVPR~2017 & ResNet-101 &$353\\times 257$ &64.9 & 85.5 & 71.3   & 62.3 &70.0 &69.7\\\\\nMask-RCNN\\cite{he2017mask} & ICCV~2017  & ResNet-50-FPN  &- &63.1 & 87.3 & 68.7 & 57.8 &71.4 &- \\\\\nRMPE\\cite{fang2017rmpe}  & ICCV~2017 & PyraNet\\cite{yang2017learning}&$320\\times 256$  &72.3 & 89.2   & 79.1   & 68.0 &78.6 &-   \\\\\nCFN\\cite{huang2017coarse} & ICCV~2017  & -      &-  &72.6 & 86.1   & 69.7   & 78.3 &64.1 &-   \\\\\nCPN\\cite{chen2018cascaded} & CVPR~2018 & ResNet-Inception &$384\\times288$ &72.1 & 91.4   & 80.0   & 68.7 &77.2 &78.5\\\\\nCPN(ensemble)\\cite{chen2018cascaded} & CVPR~2018 & ResNet-Inception &$384\\times 288$       &73.0 & 91.7   & 80.9   & 69.5 &78.1 &79.0\\\\\nIntegral Pose Regression\\cite{sun2018integral} & ECCV~2018  & ResNet-101 &$256\\times 256$ &67.8 & 88.2 & 74.8 & 63.9 &74.0 &-   \\\\\nPosefix\\cite{moon2019posefix}  & CVPR~2019  & ResNet-152       &$384\\times288$      &73.6 & 90.8   & 81.0   & 70.3 &79.8 &79.0\\\\\nDarkPose\\cite{zhang2020distribution}& CVPR~2020& HRNet-W48 &$384\\times288$& 76.2 & 92.5 & 83.6 & 72.5 & 82.4 & 81.1\\\\\nUDP\\cite{huang2020devil} & CVPR~2020 & HRNet-W48 &$384\\times288$ &76.5   & 92.7 & 84.0 & 73.0&82.4 &81.6\\\\\nTokenPose\\cite{li2021tokenpose}& ICCV~2021 & TokenPose-L/D24 & $384\\times288$ & 75.9 & 92.3 & 83.4 & 72.2 & 82.1 & 80.8\\\\\nRemoving Bias\\cite{gu2021removing}& ICCV~2021 & HRNet-W48 &$384\\times288$ & 76.1 & - & - & - & - & 81.0\\\\\n\\hline\nSimple Baseline\\cite{xiao2018simple}& ECCV~2018 & ResNet-152   &$384\\times288$ &73.8 & 91.7 & 81.2 & 70.3 &80.0 &79.1\\\\\n\\textbf{+ Ours} && ResNet-152   &$384\\times288$ & \\textbf{75.3(\\textuparrow1.5)} & \\textbf{92.6} & \\textbf{83.1} & \\textbf{71.7} & \\textbf{81.1} & \\textbf{80.3}\\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019 & HRNet-W32 &$384\\times288$ &74.9 & 92.5 & 82.8 & 71.3 &80.9 &80.1\\\\\n\\textbf{+ Ours} && HRNet-W32 &$384\\times288$ & \\textbf{76.7(\\textuparrow1.8)} & \\textbf{92.6} & \\textbf{84.0} & \\textbf{73.0} & \\textbf{82.8} & \\textbf{81.5}\\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019& HRNet-W48 &$384\\times288$ &75.5 & 92.5  & 83.3& 71.9  &81.5 &80.5\\\\\n\\textbf{+ Ours} && HRNet-W48 &$384\\times288$ & \\textbf{77.2(\\textuparrow1.7)} & \\textbf{93.0} & \\textbf{84.4} & \\textbf{73.4} & \\textbf{83.3} & \\textbf{82.0}\\\\\n\\hline\nHRFormer\\cite{YuanFHLZCW21} & NIPS~2021& HRFormer-Base &$384\\times288$ & 76.2 & 92.7 & 83.8 & 72.5 & 82.3 & 81.2\\\\\n\\textbf{+ Ours} & & HRFormer-Base &$384\\times288$ & \\textbf{77.2(\\textuparrow1.0)} & \\textbf{93.1} & \\textbf{84.7} & \\textbf{73.8} & \\textbf{83.0} & \\textbf{82.1}\\\\\n\\hline\n\\end{tabular}}\n\\label{Tab:coco_test_dev}\n\\end{table}", "cell_list_gold": [{"value": "64.9", "char_index": [438, 442], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "G-RMI", "model settings": {"Backbone": "ResNet-101", "Input size": "353\\times 257"}}, {"value": "85.5", "char_index": [445, 449], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "G-RMI", "model settings": {"Backbone": "ResNet-101", "Input size": "353\\times 257"}}, {"value": "71.3", "char_index": [452, 456], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "G-RMI", "model settings": {"Backbone": "ResNet-101", "Input size": "353\\times 257"}}, {"value": "62.3", "char_index": [461, 465], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "G-RMI", "model settings": {"Backbone": "ResNet-101", "Input size": "353\\times 257"}}, {"value": "70.0", "char_index": [467, 471], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "G-RMI", "model settings": {"Backbone": "ResNet-101", "Input size": "353\\times 257"}}, {"value": "69.7", "char_index": [473, 477], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "G-RMI", "model settings": {"Backbone": "ResNet-101", "Input size": "353\\times 257"}}, {"value": "63.1", "char_index": [541, 545], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"Backbone": "ResNet-50-FPN"}}, {"value": "87.3", "char_index": [548, 552], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"Backbone": "ResNet-50-FPN"}}, {"value": "68.7", "char_index": [555, 559], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"Backbone": "ResNet-50-FPN"}}, {"value": "57.8", "char_index": [562, 566], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"Backbone": "ResNet-50-FPN"}}, {"value": "71.4", "char_index": [568, 572], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"Backbone": "ResNet-50-FPN"}}, {"value": "72.3", "char_index": [667, 671], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "RMPE", "model settings": {"Backbone": "PyraNet", "Input size": "320\\times 256"}}, {"value": "89.2", "char_index": [674, 678], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "RMPE", "model settings": {"Backbone": "PyraNet", "Input size": "320\\times 256"}}, {"value": "79.1", "char_index": [683, 687], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "RMPE", "model settings": {"Backbone": "PyraNet", "Input size": "320\\times 256"}}, {"value": "68.0", "char_index": [692, 696], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "RMPE", "model settings": {"Backbone": "PyraNet", "Input size": "320\\times 256"}}, {"value": "78.6", "char_index": [698, 702], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "RMPE", "model settings": {"Backbone": "PyraNet", "Input size": "320\\times 256"}}, {"value": "72.6", "char_index": [764, 768], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "CFN", "model settings": {"xx": "yy"}}, {"value": "86.1", "char_index": [771, 775], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "CFN", "model settings": {"xx": "yy"}}, {"value": "69.7", "char_index": [780, 784], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "CFN", "model settings": {"xx": "yy"}}, {"value": "78.3", "char_index": [789, 793], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "CFN", "model settings": {"xx": "yy"}}, {"value": "64.1", "char_index": [795, 799], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "CFN", "model settings": {"xx": "yy"}}, {"value": "72.1", "char_index": [883, 887], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "CPN", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "91.4", "char_index": [890, 894], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "CPN", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "80.0", "char_index": [899, 903], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "CPN", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "68.7", "char_index": [908, 912], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "CPN", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "77.2", "char_index": [914, 918], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "CPN", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "78.5", "char_index": [920, 924], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "CPN", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "73.0", "char_index": [1019, 1023], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "CPN(ensemble)", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "91.7", "char_index": [1026, 1030], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "CPN(ensemble)", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "80.9", "char_index": [1035, 1039], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "CPN(ensemble)", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "69.5", "char_index": [1044, 1048], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "CPN(ensemble)", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "78.1", "char_index": [1050, 1054], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "CPN(ensemble)", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "79.0", "char_index": [1056, 1060], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "CPN(ensemble)", "model settings": {"Backbone": "ResNet-Inception", "Input size": "384\\times288"}}, {"value": "67.8", "char_index": [1154, 1158], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Integral Pose Regression", "model settings": {"Backbone": "ResNet-101", "Input size": "256\\times 256"}}, {"value": "88.2", "char_index": [1161, 1165], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Integral Pose Regression", "model settings": {"Backbone": "ResNet-101", "Input size": "256\\times 256"}}, {"value": "74.8", "char_index": [1168, 1172], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Integral Pose Regression", "model settings": {"Backbone": "ResNet-101", "Input size": "256\\times 256"}}, {"value": "63.9", "char_index": [1175, 1179], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Integral Pose Regression", "model settings": {"Backbone": "ResNet-101", "Input size": "256\\times 256"}}, {"value": "74.0", "char_index": [1181, 1185], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Integral Pose Regression", "model settings": {"Backbone": "ResNet-101", "Input size": "256\\times 256"}}, {"value": "73.6", "char_index": [1279, 1283], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Posefix", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "90.8", "char_index": [1286, 1290], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Posefix", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "81.0", "char_index": [1295, 1299], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Posefix", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "70.3", "char_index": [1304, 1308], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Posefix", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "79.8", "char_index": [1310, 1314], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Posefix", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "79.0", "char_index": [1316, 1320], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Posefix", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "76.2", "char_index": [1399, 1403], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "92.5", "char_index": [1406, 1410], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "83.6", "char_index": [1413, 1417], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "72.5", "char_index": [1420, 1424], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "82.4", "char_index": [1427, 1431], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "81.1", "char_index": [1434, 1438], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "76.5", "char_index": [1507, 1511], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "92.7", "char_index": [1516, 1520], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "84.0", "char_index": [1523, 1527], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "73.0", "char_index": [1530, 1534], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "82.4", "char_index": [1535, 1539], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "81.6", "char_index": [1541, 1545], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "75.9", "char_index": [1628, 1632], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "384\\times288"}}, {"value": "92.3", "char_index": [1635, 1639], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "384\\times288"}}, {"value": "83.4", "char_index": [1642, 1646], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "384\\times288"}}, {"value": "72.2", "char_index": [1649, 1653], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "384\\times288"}}, {"value": "82.1", "char_index": [1656, 1660], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "384\\times288"}}, {"value": "80.8", "char_index": [1663, 1667], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "384\\times288"}}, {"value": "76.1", "char_index": [1746, 1750], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Removing Bias", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "81.0", "char_index": [1769, 1773], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Removing Bias", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "73.8", "char_index": [1863, 1867], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "91.7", "char_index": [1870, 1874], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "81.2", "char_index": [1877, 1881], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "70.3", "char_index": [1884, 1888], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "80.0", "char_index": [1890, 1894], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "79.1", "char_index": [1896, 1900], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "75.3", "char_index": [1961, 1965], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "92.6", "char_index": [1994, 1998], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "83.1", "char_index": [2010, 2014], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "71.7", "char_index": [2026, 2030], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "81.1", "char_index": [2042, 2046], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "80.3", "char_index": [2058, 2062], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "384\\times288"}}, {"value": "74.9", "char_index": [2138, 2142], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "92.5", "char_index": [2145, 2149], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "82.8", "char_index": [2152, 2156], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "71.3", "char_index": [2159, 2163], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "80.9", "char_index": [2165, 2169], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "80.1", "char_index": [2171, 2175], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "76.7", "char_index": [2233, 2237], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "92.6", "char_index": [2266, 2270], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "84.0", "char_index": [2282, 2286], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "73.0", "char_index": [2298, 2302], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "82.8", "char_index": [2314, 2318], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "81.5", "char_index": [2330, 2334], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "384\\times288"}}, {"value": "75.5", "char_index": [2409, 2413], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "92.5", "char_index": [2416, 2420], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "83.3", "char_index": [2424, 2428], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "71.9", "char_index": [2430, 2434], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "81.5", "char_index": [2437, 2441], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "80.5", "char_index": [2443, 2447], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "77.2", "char_index": [2505, 2509], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "93.0", "char_index": [2538, 2542], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "84.4", "char_index": [2554, 2558], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "73.4", "char_index": [2570, 2574], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "83.3", "char_index": [2586, 2590], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "82.0", "char_index": [2602, 2606], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "384\\times288"}}, {"value": "76.2", "char_index": [2690, 2694], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "92.7", "char_index": [2697, 2701], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "83.8", "char_index": [2704, 2708], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "72.5", "char_index": [2711, 2715], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "82.3", "char_index": [2718, 2722], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "81.2", "char_index": [2725, 2729], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRFormer", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "77.2", "char_index": [2792, 2796], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "93.1", "char_index": [2825, 2829], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "84.7", "char_index": [2841, 2845], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "73.8", "char_index": [2857, 2861], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "83.0", "char_index": [2873, 2877], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}, {"value": "82.1", "char_index": [2889, 2893], "type": "Result", "training data/set": "COCO", "test data/set": "COCO test-dev", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "HRFormer + Ours", "model settings": {"Backbone": "HRFormer-Base", "Input size": "384\\times288"}}]}, "2210.00740v1_table2": {"table_code": "\\begin{table}[t]\n\\caption{The improvement of AP on MPII validation set when our proposed method is applied to various baselines.}\n\\footnotesize\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|l|c|lccccccc}\n\\hline\nMethod & Venue & Backbone &Input size & Mean & Hea & Sho & Elb & Wri & Hip & Kne & Ank \\\\\n\\hline\nIntegral Pose Regression\\cite{sun2018integral} & ECCV~2018  & ResNet-101 &$256\\times 256$ &87.9&-&-&-&-&-&-&-\\\\\nUDP\\cite{huang2020devil} & CVPR~2020& HRNet-W32 &$256\\times256$ &90.4&97.4&96.0&91.0&86.5&89.1&86.6&83.3\\\\\nDarkPose\\cite{zhang2020distribution} & CVPR~2020 & HRNet-W32   &$256\\times256$ &90.6&97.2&95.9&91.2&86.7&89.7&86.7&84.0\\\\\nTokenPose\\cite{li2021tokenpose} & ICCV~2021& TokenPose-L/D6 & $256\\times256$  &90.1&97.1&95.9&91.0&85.8&89.5&86.1&82.7 \\\\\nTokenPose\\cite{li2021tokenpose} & ICCV~2021& TokenPose-L/D12 & $256\\times256$  &90.1&97.2&95.8&90.7&85.9&89.2&86.2&82.3 \\\\\nTokenPose\\cite{li2021tokenpose} & ICCV~2021& TokenPose-L/D24 & $256\\times256$  &90.2&97.1&95.9&90.4&86.0&89.3&87.1&82.5 \\\\\nRemoving Bias\\cite{gu2021removing} & ICCV~2021& ResNet-152   &$256\\times256$ & 89.9 & - & - & - & - & -& - & -\\\\\nRemoving Bias\\cite{gu2021removing} & ICCV~2021& HRNet-W32   &$256\\times256$ & 90.6 & - & - & - & - & -& - & -\\\\\n\\hline\nSimple Baseline\\cite{xiao2018simple}& ECCV~2018 & ResNet-152   &$256\\times256$ & 89.6 & \\textbf{97.0} & 95.9 & 90.0 & 85.0 & \\textbf{89.2} & 85.3 & 81.3  \\\\\n\\textbf{+ Ours} && ResNet-152   &$256\\times256$ & \\textbf{90.3(\\textuparrow0.7)} & \\textbf{97.0} & \\textbf{96.1} & \\textbf{90.6} & \\textbf{86.1} & \\textbf{89.2} & \\textbf{86.7} & \\textbf{83.1} \\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019& HRNet-W32 &$256\\times256$ & 90.4 & 97.1 & 95.9 & 90.7 & 86.1 & 89.4 & 86.9 & 83.2 \\\\\n\\textbf{+ Ours}& & HRNet-W32 &$256\\times256$ & \\textbf{90.9(\\textuparrow0.5)} & \\textbf{97.3} & \\textbf{96.2} & \\textbf{91.2} & \\textbf{86.8} & \\textbf{90.1} & \\textbf{87.4} & \\textbf{84.1} \\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019& HRNet-W48 &$256\\times256$ & 90.5 & 96.9 & 96.0 & 90.9 & 86.2 & 89.6 & 87.1 & 83.5 \\\\\n\\textbf{+ Ours} && HRNet-W48 &$256\\times256$ & \\textbf{90.9(\\textuparrow0.4)} & \\textbf{97.1} & \\textbf{96.3} & \\textbf{91.2} & \\textbf{87.0} & \\textbf{90.2} & \\textbf{87.5} & \\textbf{84.2}\\\\\n\\hline\n\\end{tabular}}\n\\label{Tab:mpii}\n\\end{table}", "table_label": "{Tab:mpii}", "table_numeric_cells": [["87.9", "87.9", 411, 415, 411, 415], ["90.4", "90.4", 497, 501, 497, 501], ["97.4", "97.4", 502, 506, 502, 506], ["96.0", "96.0", 507, 511, 507, 511], ["91.0", "91.0", 512, 516, 512, 516], ["86.5", "86.5", 517, 521, 517, 521], ["89.1", "89.1", 522, 526, 522, 526], ["86.6", "86.6", 527, 531, 527, 531], ["83.3", "83.3", 532, 536, 532, 536], ["90.6", "90.6", 619, 623, 619, 623], ["97.2", "97.2", 624, 628, 624, 628], ["95.9", "95.9", 629, 633, 629, 633], ["91.2", "91.2", 634, 638, 634, 638], ["86.7", "86.7", 639, 643, 639, 643], ["89.7", "89.7", 644, 648, 644, 648], ["86.7", "86.7", 649, 653, 649, 653], ["84.0", "84.0", 654, 658, 654, 658], ["90.1", "90.1", 740, 744, 740, 744], ["97.1", "97.1", 745, 749, 745, 749], ["95.9", "95.9", 750, 754, 750, 754], ["91.0", "91.0", 755, 759, 755, 759], ["85.8", "85.8", 760, 764, 760, 764], ["89.5", "89.5", 765, 769, 765, 769], ["86.1", "86.1", 770, 774, 770, 774], ["82.7", "82.7", 775, 779, 775, 779], ["90.1", "90.1", 863, 867, 863, 867], ["97.2", "97.2", 868, 872, 868, 872], ["95.8", "95.8", 873, 877, 873, 877], ["90.7", "90.7", 878, 882, 878, 882], ["85.9", "85.9", 883, 887, 883, 887], ["89.2", "89.2", 888, 892, 888, 892], ["86.2", "86.2", 893, 897, 893, 897], ["82.3", "82.3", 898, 902, 898, 902], ["90.2", "90.2", 986, 990, 986, 990], ["97.1", "97.1", 991, 995, 991, 995], ["95.9", "95.9", 996, 1000, 996, 1000], ["90.4", "90.4", 1001, 1005, 1001, 1005], ["86.0", "86.0", 1006, 1010, 1006, 1010], ["89.3", "89.3", 1011, 1015, 1011, 1015], ["87.1", "87.1", 1016, 1020, 1016, 1020], ["82.5", "82.5", 1021, 1025, 1021, 1025], ["89.9", "89.9", 1108, 1112, 1108, 1112], ["90.6", "90.6", 1220, 1224, 1220, 1224], ["89.6", "89.6", 1342, 1346, 1342, 1346], ["97.0", "\\textbf{97.0}", 1357, 1361, 1349, 1362], ["95.9", "95.9", 1365, 1369, 1365, 1369], ["90.0", "90.0", 1372, 1376, 1372, 1376], ["85.0", "85.0", 1379, 1383, 1379, 1383], ["89.2", "\\textbf{89.2}", 1394, 1398, 1386, 1399], ["85.3", "85.3", 1402, 1406, 1402, 1406], ["81.3", "81.3", 1409, 1413, 1409, 1413], ["90.3", "\\textbf{90.3(\\textuparrow0.7)}", 1476, 1480, 1468, 1498], ["97.0", "\\textbf{97.0}", 1509, 1513, 1501, 1514], ["96.1", "\\textbf{96.1}", 1525, 1529, 1517, 1530], ["90.6", "\\textbf{90.6}", 1541, 1545, 1533, 1546], ["86.1", "\\textbf{86.1}", 1557, 1561, 1549, 1562], ["89.2", "\\textbf{89.2}", 1573, 1577, 1565, 1578], ["86.7", "\\textbf{86.7}", 1589, 1593, 1581, 1594], ["83.1", "\\textbf{83.1}", 1605, 1609, 1597, 1610], ["90.4", "90.4", 1686, 1690, 1686, 1690], ["97.1", "97.1", 1693, 1697, 1693, 1697], ["95.9", "95.9", 1700, 1704, 1700, 1704], ["90.7", "90.7", 1707, 1711, 1707, 1711], ["86.1", "86.1", 1714, 1718, 1714, 1718], ["89.4", "89.4", 1721, 1725, 1721, 1725], ["86.9", "86.9", 1728, 1732, 1728, 1732], ["83.2", "83.2", 1735, 1739, 1735, 1739], ["90.9", "\\textbf{90.9(\\textuparrow0.5)}", 1798, 1802, 1790, 1820], ["97.3", "\\textbf{97.3}", 1831, 1835, 1823, 1836], ["96.2", "\\textbf{96.2}", 1847, 1851, 1839, 1852], ["91.2", "\\textbf{91.2}", 1863, 1867, 1855, 1868], ["86.8", "\\textbf{86.8}", 1879, 1883, 1871, 1884], ["90.1", "\\textbf{90.1}", 1895, 1899, 1887, 1900], ["87.4", "\\textbf{87.4}", 1911, 1915, 1903, 1916], ["84.1", "\\textbf{84.1}", 1927, 1931, 1919, 1932], ["90.5", "90.5", 2008, 2012, 2008, 2012], ["96.9", "96.9", 2015, 2019, 2015, 2019], ["96.0", "96.0", 2022, 2026, 2022, 2026], ["90.9", "90.9", 2029, 2033, 2029, 2033], ["86.2", "86.2", 2036, 2040, 2036, 2040], ["89.6", "89.6", 2043, 2047, 2043, 2047], ["87.1", "87.1", 2050, 2054, 2050, 2054], ["83.5", "83.5", 2057, 2061, 2057, 2061], ["90.9", "\\textbf{90.9(\\textuparrow0.4)}", 2120, 2124, 2112, 2142], ["97.1", "\\textbf{97.1}", 2153, 2157, 2145, 2158], ["96.3", "\\textbf{96.3}", 2169, 2173, 2161, 2174], ["91.2", "\\textbf{91.2}", 2185, 2189, 2177, 2190], ["87.0", "\\textbf{87.0}", 2201, 2205, 2193, 2206], ["90.2", "\\textbf{90.2}", 2217, 2221, 2209, 2222], ["87.5", "\\textbf{87.5}", 2233, 2237, 2225, 2238], ["84.2", "\\textbf{84.2}", 2249, 2253, 2241, 2254]], "text_chunk_selected": "The Earth Mover's Distance is a \na technique used for measuring the difference between two probability distributions, which can be understood as the optimal cost needed to transport the mass from one distribution to another. Specifically, for calculating the Earth Mover's Distance, we regard the source distribution as a set of ($N$) suppliers $S = (s_1, ..., s_N)^\\top$, where $s_n$ represents the total units of mass that the $n$-th supplier has, and we regard the target distribution as a set of ($M$) demanders $D = (d_1, ..., d_M)^\\top$, where $d_m$ represents the total units of mass that the $m$-th demander requires. Besides, we also denote $C \\in  R^{N \\times M}_{\\geq 0}$ as the cost function between the source and target distributions, where $C_{n,m}$ represents the cost for transporting a unit of mass from the $n$-th supplier to the $m$-th demander. Then we aim to find a least-cost transportation plan from the set of possible plans $P = \\{p \\in R^{N \\times M}_{\\geq 0}:~p\\textbf{1} = S~\\&~p^\\top\\textbf{1} = D\\}$ to transport all mass from the $N$ suppliers to the $M$ demanders, where \\textbf{1} represents a vector of ones. The Earth Mover's Distance $E_C(S, D)$ denotes the cost of the least-cost transportation plan, which can be formulated as:\n\n\\begin{equation}\\label{eq:pre_2}\n\\begin{aligned}\nE_C^{reg}(S, D)~=~\\langle C,p^{reg} \\rangle\n~\\textbf{where}~ p^{reg}~=~\\mathop{\\arg\\min}_{p \\in P}\\quad  \\left[\\langle C,p \\rangle - \\frac{1}{\\lambda} h(p)\\right]\n\\end{aligned}\n\\end{equation}\n\n\\begin{equation}\\label{eq:demander}\n\\begin{aligned}\nD = (d_1, d_2, d_3, d_4) ,~\\textbf{where}~ d_i = \\frac{(g-|x_{dot} - x_i|) \\times (g-|y_{dot} - y_i|)}{g^2}\n\\end{aligned}\n\\end{equation}\n\n\\noindent\\textbf{The cost function $C$.} \nTo measure the distribution difference between the $H \\times W$ suppliers ($S$) constructed from the predicted heatmap and the $4$ demanders ($D$) constructed from the GT dot annotation via calculating their Earth Mover's Distance, we also need to formulate a cost function $C \\in  R^{HW \\times 4}_{\\geq 0}$. \nDenote the coordinates of the $n$-th supplier as $(x_{s_n}, y_{s_n})$, where $n \\in \\{1, ..., H \\times W\\}$, and the coordinates of the $m$-th demander as $(x_{d_m}, y_{d_m})$, where $m \\in \\{1, 2, 3, 4\\}$. We here simply formulate $C_{n,m}$, the cost per unit transported from the $n$-th supplier to the $m$-th demander, as the L2 distance between the $n$-th supplier and the $m$-th demander, i.e., $C_{n,m} = \\sqrt{(x_{d_m} - x_{s_n})^2 + (y_{d_m} - y_{s_n})^2}$. Using such a cost function, our method can optimize the model directly towards accurately localizing the dot annotation of the body joint via minimizing the distribution difference between the suppliers $S$ and the demanders $D$.\n\nWe denote $K$ the number of joints per input image $I$. Then we denote $\\mathbf{H_{dot}} = \\{H_{dot}^1, ..., H_{dot}^K\\}$, $\\mathbf{H_{Gau}} = \\{H_{Gau}^1, ..., H_{Gau}^K\\}$,  and $\\mathbf{H_{pred}} = \\{H_{pred}^1, ..., H_{pred}^K\\}$ respectively the corresponding $K$ GT dot-annotated heatmaps, GT Gaussian-smoothed heatmaps, and predicted heatmaps of the input image $I$. We denote $\\mathcal D_{dot} = \\{(I, \\mathbf{H_{dot}})\\}$ the joint distribution of the input image and the corresponding $K$ dot-annotated heatmaps, and $\\mathcal D_{Gau} = \\{(I, \\mathbf{H_{Gau}})\\}$ the joint distribution of the input image and the corresponding $K$ Gaussian-smoothed heatmaps. Besides, we denote $l_{MSE}(a, b) = \\left\\| a - b\\right\\|_2^2$ the pixel-wise MSE loss, and $\\phi$ the model parameters where $\\phi(I) = \\mathbf{H_{pred}}$. After that, we denote $R(\\mathcal D_{dot}, \\phi, l_{MSE}) = \\mathbb{E}_{(I, \\mathbf{H_{dot}}) \\sim \\mathcal D_{dot}}[l_{MSE}(\\phi(I), \\mathbf{H_{dot}})]$ as the expected risk calculated between the predicted heatmaps and the GT dot-annotated heatmaps, and $R(\\mathcal D_{Gau}, \\phi, l_{MSE}) = \\mathbb{E}_{(I, \\mathbf{H_{Gau}}) \\sim \\mathcal D_{Gau}}[l_{MSE}(\\phi(I), \\mathbf{H_{Gau}})]$ as the expected risk calculated between the predicted heatmaps and the Gaussian-smoothed heatmaps.\n\n\\subsection{COCO Keypoint Detection}\n\\noindent\\textbf{Dataset \\& evaluation metric.} The COCO dataset \\cite{lin2014microsoft} contains more than 200k images and 250k person instances, which are annotated with 17 body joints. This dataset has three subsets including COCO training set, COCO validation set, and COCO test-dev set, which have 57k, 5k and 20k images, respectively. We conduct experiments on this dataset via first training the model on the train2017 set, and then evaluating the model on the val2017 set and test-dev2017 set. Following \\cite{xiao2018simple,sun2019deep,YuanFHLZCW21}, we use standard average precision (AP) calculated based on Object Keypoint Similarity (OKS) to evaluate model performance.\n\n\\subsection{MPII Human Pose Estimation}\n\\noindent\\textbf{Dataset \\& evaluation metric.} The MPII dataset \\cite{andriluka20142d} contains around 25K images and more than 40k person instances, which are annotated with 16 body joints. We adopt the standard train/val split in \\cite{andriluka20142d} to build the MPII training set and validation set, and conduct all the experiments on this dataset via first training the model on the MPII training set, and then evaluating it on the MPII validation set. Following \\cite{sun2019deep}, we use the head-normalized probability of correct keypoint (PCKh) \\cite{andriluka20142d} score as the evaluation metric on this dataset and report the PCKh@0.5 score.\n\n\\noindent\\textbf{Results on the MPII validation set.} In Tab.~\\ref{Tab:mpii}, we report the results on the MPII validation set. As shown, applying our proposed method on various baselines results in a consistent performance improvement, which demonstrates the effectiveness of our proposed method.", "table_source": "\\begin{table}[t]\n\\caption{The improvement of AP on MPII validation set when our proposed method is applied to various baselines.}\n\\footnotesize\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|l|c|lccccccc}\n\\hline\nMethod & Venue & Backbone &Input size & Mean & Hea & Sho & Elb & Wri & Hip & Kne & Ank \\\\\n\\hline\nIntegral Pose Regression\\cite{sun2018integral} & ECCV~2018  & ResNet-101 &$256\\times 256$ &87.9&-&-&-&-&-&-&-\\\\\nUDP\\cite{huang2020devil} & CVPR~2020& HRNet-W32 &$256\\times256$ &90.4&97.4&96.0&91.0&86.5&89.1&86.6&83.3\\\\\nDarkPose\\cite{zhang2020distribution} & CVPR~2020 & HRNet-W32   &$256\\times256$ &90.6&97.2&95.9&91.2&86.7&89.7&86.7&84.0\\\\\nTokenPose\\cite{li2021tokenpose} & ICCV~2021& TokenPose-L/D6 & $256\\times256$  &90.1&97.1&95.9&91.0&85.8&89.5&86.1&82.7 \\\\\nTokenPose\\cite{li2021tokenpose} & ICCV~2021& TokenPose-L/D12 & $256\\times256$  &90.1&97.2&95.8&90.7&85.9&89.2&86.2&82.3 \\\\\nTokenPose\\cite{li2021tokenpose} & ICCV~2021& TokenPose-L/D24 & $256\\times256$  &90.2&97.1&95.9&90.4&86.0&89.3&87.1&82.5 \\\\\nRemoving Bias\\cite{gu2021removing} & ICCV~2021& ResNet-152   &$256\\times256$ & 89.9 & - & - & - & - & -& - & -\\\\\nRemoving Bias\\cite{gu2021removing} & ICCV~2021& HRNet-W32   &$256\\times256$ & 90.6 & - & - & - & - & -& - & -\\\\\n\\hline\nSimple Baseline\\cite{xiao2018simple}& ECCV~2018 & ResNet-152   &$256\\times256$ & 89.6 & \\textbf{97.0} & 95.9 & 90.0 & 85.0 & \\textbf{89.2} & 85.3 & 81.3  \\\\\n\\textbf{+ Ours} && ResNet-152   &$256\\times256$ & \\textbf{90.3(\\textuparrow0.7)} & \\textbf{97.0} & \\textbf{96.1} & \\textbf{90.6} & \\textbf{86.1} & \\textbf{89.2} & \\textbf{86.7} & \\textbf{83.1} \\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019& HRNet-W32 &$256\\times256$ & 90.4 & 97.1 & 95.9 & 90.7 & 86.1 & 89.4 & 86.9 & 83.2 \\\\\n\\textbf{+ Ours}& & HRNet-W32 &$256\\times256$ & \\textbf{90.9(\\textuparrow0.5)} & \\textbf{97.3} & \\textbf{96.2} & \\textbf{91.2} & \\textbf{86.8} & \\textbf{90.1} & \\textbf{87.4} & \\textbf{84.1} \\\\\n\\hline\nHRNet\\cite{sun2019deep} & CVPR~2019& HRNet-W48 &$256\\times256$ & 90.5 & 96.9 & 96.0 & 90.9 & 86.2 & 89.6 & 87.1 & 83.5 \\\\\n\\textbf{+ Ours} && HRNet-W48 &$256\\times256$ & \\textbf{90.9(\\textuparrow0.4)} & \\textbf{97.1} & \\textbf{96.3} & \\textbf{91.2} & \\textbf{87.0} & \\textbf{90.2} & \\textbf{87.5} & \\textbf{84.2}\\\\\n\\hline\n\\end{tabular}}\n\\label{Tab:mpii}\n\\end{table}", "cell_list_gold": [{"value": "87.9", "char_index": [411, 415], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "Integral Pose Regression", "model settings": {"Backbone": "ResNet-101", "Input size": "256\\times 256"}}, {"value": "90.4", "char_index": [497, 501], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "97.4", "char_index": [502, 506], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "96.0", "char_index": [507, 511], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "91.0", "char_index": [512, 516], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "86.5", "char_index": [517, 521], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "89.1", "char_index": [522, 526], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "86.6", "char_index": [527, 531], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "83.3", "char_index": [532, 536], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "UDP", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "90.6", "char_index": [619, 623], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "97.2", "char_index": [624, 628], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "95.9", "char_index": [629, 633], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "91.2", "char_index": [634, 638], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "86.7", "char_index": [639, 643], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "89.7", "char_index": [644, 648], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "86.7", "char_index": [649, 653], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "84.0", "char_index": [654, 658], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "DarkPose", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "90.1", "char_index": [740, 744], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D6", "Input size": "256\\times256"}}, {"value": "97.1", "char_index": [745, 749], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D6", "Input size": "256\\times256"}}, {"value": "95.9", "char_index": [750, 754], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D6", "Input size": "256\\times256"}}, {"value": "91.0", "char_index": [755, 759], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D6", "Input size": "256\\times256"}}, {"value": "85.8", "char_index": [760, 764], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D6", "Input size": "256\\times256"}}, {"value": "89.5", "char_index": [765, 769], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D6", "Input size": "256\\times256"}}, {"value": "86.1", "char_index": [770, 774], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D6", "Input size": "256\\times256"}}, {"value": "82.7", "char_index": [775, 779], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D6", "Input size": "256\\times256"}}, {"value": "90.1", "char_index": [863, 867], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D12", "Input size": "256\\times256"}}, {"value": "97.2", "char_index": [868, 872], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D12", "Input size": "256\\times256"}}, {"value": "95.8", "char_index": [873, 877], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D12", "Input size": "256\\times256"}}, {"value": "90.7", "char_index": [878, 882], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D12", "Input size": "256\\times256"}}, {"value": "85.9", "char_index": [883, 887], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D12", "Input size": "256\\times256"}}, {"value": "89.2", "char_index": [888, 892], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D12", "Input size": "256\\times256"}}, {"value": "86.2", "char_index": [893, 897], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D12", "Input size": "256\\times256"}}, {"value": "82.3", "char_index": [898, 902], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D12", "Input size": "256\\times256"}}, {"value": "90.2", "char_index": [986, 990], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times256"}}, {"value": "97.1", "char_index": [991, 995], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times256"}}, {"value": "95.9", "char_index": [996, 1000], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times256"}}, {"value": "90.4", "char_index": [1001, 1005], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times256"}}, {"value": "86.0", "char_index": [1006, 1010], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times256"}}, {"value": "89.3", "char_index": [1011, 1015], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times256"}}, {"value": "87.1", "char_index": [1016, 1020], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times256"}}, {"value": "82.5", "char_index": [1021, 1025], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "TokenPose", "model settings": {"Backbone": "TokenPose-L/D24", "Input size": "256\\times256"}}, {"value": "89.9", "char_index": [1108, 1112], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "Removing Bias", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "90.6", "char_index": [1220, 1224], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "Removing Bias", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "89.6", "char_index": [1342, 1346], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "97.0", "char_index": [1357, 1361], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "95.9", "char_index": [1365, 1369], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "90.0", "char_index": [1372, 1376], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "85.0", "char_index": [1379, 1383], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "89.2", "char_index": [1394, 1398], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "85.3", "char_index": [1402, 1406], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "81.3", "char_index": [1409, 1413], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "90.3", "char_index": [1476, 1480], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "97.0", "char_index": [1509, 1513], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "96.1", "char_index": [1525, 1529], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "90.6", "char_index": [1541, 1545], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "86.1", "char_index": [1557, 1561], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "89.2", "char_index": [1573, 1577], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "86.7", "char_index": [1589, 1593], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "83.1", "char_index": [1605, 1609], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Simple Baseline + Ours", "model settings": {"Backbone": "ResNet-152", "Input size": "256\\times256"}}, {"value": "90.4", "char_index": [1686, 1690], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "97.1", "char_index": [1693, 1697], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "95.9", "char_index": [1700, 1704], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "90.7", "char_index": [1707, 1711], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "86.1", "char_index": [1714, 1718], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "89.4", "char_index": [1721, 1725], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "86.9", "char_index": [1728, 1732], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "83.2", "char_index": [1735, 1739], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "90.9", "char_index": [1798, 1802], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "97.3", "char_index": [1831, 1835], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "96.2", "char_index": [1847, 1851], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "91.2", "char_index": [1863, 1867], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "86.8", "char_index": [1879, 1883], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "90.1", "char_index": [1895, 1899], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "87.4", "char_index": [1911, 1915], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "84.1", "char_index": [1927, 1931], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W32", "Input size": "256\\times256"}}, {"value": "90.5", "char_index": [2008, 2012], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "96.9", "char_index": [2015, 2019], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "96.0", "char_index": [2022, 2026], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "90.9", "char_index": [2029, 2033], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "86.2", "char_index": [2036, 2040], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "89.6", "char_index": [2043, 2047], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "87.1", "char_index": [2050, 2054], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "83.5", "char_index": [2057, 2061], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "90.9", "char_index": [2120, 2124], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP Mean", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "97.1", "char_index": [2153, 2157], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hea", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "96.3", "char_index": [2169, 2173], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Sho", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "91.2", "char_index": [2185, 2189], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Elb", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "87.0", "char_index": [2201, 2205], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Wri", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "90.2", "char_index": [2217, 2221], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Hip", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "87.5", "char_index": [2233, 2237], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Kne", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}, {"value": "84.2", "char_index": [2249, 2253], "type": "Result", "training data/set": "MPII", "test data/set": "MPII validation Ank", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "HRNet + Ours", "model settings": {"Backbone": "HRNet-W48", "Input size": "256\\times256"}}]}, "2210.00740v1_table3": {"table_code": "\\parbox{0.49\\textwidth}{\n\\caption{Evaluation on the effectiveness of formulating the demanders $D$ involving the idea of sub-pixel resolution.}\n\\label{Tab:ablation_1}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{l|cccccc}\n\\hline\nMethod & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nBaseline(HRNet-W48) &77.1 &91.8 &83.8 &73.5 &83.5 &81.8\\\\\n\\hline\n\\textbf{Naive demanders formulation} & 77.9 & 92.5 & 84.8 & 74.5 & 83.9 & 82.4 \\\\\n\\textbf{Sub-pixel demanders formulation} & 78.8 & 92.5 & 85.1 & 75.0 & 85.3 & 83.1\\\\\n\\hline\n\\end{tabular}}}", "table_label": "{Tab:ablation_1}", "table_numeric_cells": [["77.1", "77.1", 366, 370, 366, 370], ["91.8", "91.8", 372, 376, 372, 376], ["83.8", "83.8", 378, 382, 378, 382], ["73.5", "73.5", 384, 388, 384, 388], ["83.5", "83.5", 390, 394, 390, 394], ["81.8", "81.8", 396, 400, 396, 400], ["77.9", "77.9", 449, 453, 449, 453], ["92.5", "92.5", 456, 460, 456, 460], ["84.8", "84.8", 463, 467, 463, 467], ["74.5", "74.5", 470, 474, 470, 474], ["83.9", "83.9", 477, 481, 477, 481], ["82.4", "82.4", 484, 488, 484, 488], ["78.8", "78.8", 535, 539, 535, 539], ["92.5", "92.5", 542, 546, 542, 546], ["85.1", "85.1", 549, 553, 549, 553], ["75.0", "75.0", 556, 560, 556, 560], ["85.3", "85.3", 563, 567, 563, 567], ["83.1", "83.1", 570, 574, 570, 574]], "text_chunk_selected": "The Earth Mover's Distance is a \na technique used for measuring the difference between two probability distributions, which can be understood as the optimal cost needed to transport the mass from one distribution to another. Specifically, for calculating the Earth Mover's Distance, we regard the source distribution as a set of ($N$) suppliers $S = (s_1, ..., s_N)^\\top$, where $s_n$ represents the total units of mass that the $n$-th supplier has, and we regard the target distribution as a set of ($M$) demanders $D = (d_1, ..., d_M)^\\top$, where $d_m$ represents the total units of mass that the $m$-th demander requires. Besides, we also denote $C \\in  R^{N \\times M}_{\\geq 0}$ as the cost function between the source and target distributions, where $C_{n,m}$ represents the cost for transporting a unit of mass from the $n$-th supplier to the $m$-th demander. Then we aim to find a least-cost transportation plan from the set of possible plans $P = \\{p \\in R^{N \\times M}_{\\geq 0}:~p\\textbf{1} = S~\\&~p^\\top\\textbf{1} = D\\}$ to transport all mass from the $N$ suppliers to the $M$ demanders, where \\textbf{1} represents a vector of ones. The Earth Mover's Distance $E_C(S, D)$ denotes the cost of the least-cost transportation plan, which can be formulated as:\n\n\\noindent\\textbf{The demanders $D$.} As for the demanders, we aim to construct $D$ w.r.t. each body joint to represent its corresponding GT dot annotation.\nTo achieve this goal, for each body joint, a naive formulation of $D$ is to identify the pixel containing the GT dot annotation (i.e., the upper left pixel in Fig.~\\ref{fig:demander}(a)) and construct a single demander (i.e., the yellow dot in Fig.~\\ref{fig:demander}(a)) at the center of this pixel.\nHowever, this naive formulation can result in a suboptimal model performance, as the demander formulated in this way can be noticeably different from what the GT dot annotation might suggest.\nGenerally, the predicted heatmaps outputted by most of the existing \\textit{heatmap-based methods} \\cite{tompson2014joint,newell2016stacked,xiao2018simple,sun2019deep,li2019rethinking,cheng2020higherhrnet,zhang2020distribution,yang2020transpose,li2021tokenpose,luo2021rethinking,YuanFHLZCW21} have a lower resolution compared to the input image. For example, for the method HRNet \\cite{sun2019deep}, when the size of the input image is $384\\times288$, the size of the predicted heatmap is $96\\times72$ only. Due to such a resolution gap, as shown in  Fig.~\\ref{fig:demander}(a), there can exist a non-negligible distance between the location of the demander and the location of the dot annotation, which can affect the performance of the pose estimation model.\n\nTo address this problem, we aim to make the formulated demanders $D$ a more accurate representation of the dot annotation by overcoming the resolution gap. To achieve this, inspired by the fact that the more accurate location information of an object can be deduced using sub-pixel resolutions \\cite{khademi2012sub}, we propose to formulate the demanders $D$ with the following four steps. (1) As illustrated in the upper left of Fig.~\\ref{fig:demander}(b), we first split the pixel containing the dot annotation into four \"sub-pixels\" (four squares separated from each other by dashed lines). (2) After that, we identify the \"sub-pixel\" that the dot annotation lies in (i.e., the green \"sub-pixel\" shown in Fig.~\\ref{fig:demander}(b)). (3) Then, as shown by the four yellow dots in Fig.~\\ref{fig:demander}(b), we localize a set of four demanders at the centers of both the pixel containing the identified \"sub-pixel\" (i.e., the upper left pixel in Fig.~\\ref{fig:demander}(b)), and the three pixels adjacent to the identified \"sub-pixel\" (i.e., the three blue pixels in Fig.~\\ref{fig:demander}(b)). (4) Finally, \nwe determine the units of mass each demander requires in the following way so that the dot annotation can be precisely represented using the set of \nthe four demanders.\nSpecifically, \ndenote the 2D Euclidean distance between the centers of two adjacent pixels as $g$ (which is also the pixel size), the coordinates of the GT dot annotation as $(x_{dot}, y_{dot})$, and the coordinates of the $i$-th demander as $(x_i, y_i)$, where $i \\in \\{1, 2, 3, 4\\}$. Then we construct the demanders $D$ as:\n\n\\noindent\\textbf{The cost function $C$.} \nTo measure the distribution difference between the $H \\times W$ suppliers ($S$) constructed from the predicted heatmap and the $4$ demanders ($D$) constructed from the GT dot annotation via calculating their Earth Mover's Distance, we also need to formulate a cost function $C \\in  R^{HW \\times 4}_{\\geq 0}$. \nDenote the coordinates of the $n$-th supplier as $(x_{s_n}, y_{s_n})$, where $n \\in \\{1, ..., H \\times W\\}$, and the coordinates of the $m$-th demander as $(x_{d_m}, y_{d_m})$, where $m \\in \\{1, 2, 3, 4\\}$. We here simply formulate $C_{n,m}$, the cost per unit transported from the $n$-th supplier to the $m$-th demander, as the L2 distance between the $n$-th supplier and the $m$-th demander, i.e., $C_{n,m} = \\sqrt{(x_{d_m} - x_{s_n})^2 + (y_{d_m} - y_{s_n})^2}$. Using such a cost function, our method can optimize the model directly towards accurately localizing the dot annotation of the body joint via minimizing the distribution difference between the suppliers $S$ and the demanders $D$.\n\nThe proof of Theorem~\\ref{thm:theorem_1} is provided in the supplementary.\nAs shown in Theorem~\\ref{thm:theorem_1}, when we minimize the pixel-wise loss between $\\mathbf{H_{pred}}$ and $\\mathbf{H_{Gau}}$, while $R(\\mathcal D_{Gau}, \\phi, l_{MSE})$ decreases, the second term $2 \\times \\mathbb{E}_{(I, \\mathbf{H_{Gau}}) \\sim \\mathcal D_{Gau}}[\\langle\\mathbf{H_{pred}},\\mathbf{H_{Gau}}\\rangle]$ can increase and the third term $2 \\times \\mathbb{E}_{(I, \\mathbf{H_{dot}}) \\sim \\mathcal D_{dot}} [\\langle\\mathbf{H_{pred}},\\mathbf{H_{dot}}\\rangle]$ cannot be guaranteed to increase or decrease. Because of this, $R(\\mathcal D_{dot}, \\phi, l_{MSE})$ cannot be guaranteed to decrease, and the model performance of body joint localization may not be consistently improved during such optimization of heatmap prediction. A more intuitive analysis is as follows. In the optimization process of most of the \\textit{heatmap-based methods}, since all the $H \\times W$ pixels from the predicted heatmap contribute to the overall pixel-wise loss, learning to fit the other pixels better instead of the pixel representing the dot annotation can also lead to a smaller overall loss, as shown in Fig.~\\ref{fig:example}. Hence, when minimizing the overall pixel-wise loss, the model performance of body joint localization cannot be guaranteed to improve consistently. Besides, during training, the GT Gaussian-smoothed heatmap is constructed via using the Gaussian blob. Therefore, during testing, the predicted heatmap often has a relatively large area of pixels with large values around the dot annotation, as shown in Fig.~\\ref{fig:visual}, which can make it difficult to accurately locate the body joint.\n\n\\subsection{COCO Keypoint Detection}\n\\noindent\\textbf{Dataset \\& evaluation metric.} The COCO dataset \\cite{lin2014microsoft} contains more than 200k images and 250k person instances, which are annotated with 17 body joints. This dataset has three subsets including COCO training set, COCO validation set, and COCO test-dev set, which have 57k, 5k and 20k images, respectively. We conduct experiments on this dataset via first training the model on the train2017 set, and then evaluating the model on the val2017 set and test-dev2017 set. Following \\cite{xiao2018simple,sun2019deep,YuanFHLZCW21}, we use standard average precision (AP) calculated based on Object Keypoint Similarity (OKS) to evaluate model performance.\n\n\\subsection{MPII Human Pose Estimation}\n\\noindent\\textbf{Dataset \\& evaluation metric.} The MPII dataset \\cite{andriluka20142d} contains around 25K images and more than 40k person instances, which are annotated with 16 body joints. We adopt the standard train/val split in \\cite{andriluka20142d} to build the MPII training set and validation set, and conduct all the experiments on this dataset via first training the model on the MPII training set, and then evaluating it on the MPII validation set. Following \\cite{sun2019deep}, we use the head-normalized probability of correct keypoint (PCKh) \\cite{andriluka20142d} score as the evaluation metric on this dataset and report the PCKh@0.5 score.\n\n\\noindent\\textbf{Impact of involving the idea of sub-pixels in formulating $D$.} In our proposed method, we formulate the demanders $D$ by involving the idea of sub-pixel resolution. To investigate the impact of formulating the demanders $D$ in such a way, we compare our proposed method (\\textbf{sub-pixel demanders formulation}) with a variant (\\textbf{naive demanders formulation}). This variant still formulates the suppliers $S$ and the cost function $C$ in the same way, but formulates the demanders $D$ naively as a single demander at the center of the pixel containing the dot annotation, as shown in Fig.~\\ref{fig:demander}(a). As shown in Tab.~\\ref{Tab:ablation_1}, our proposed method consistently outperforms this variant, which shows effectiveness of our \\textbf{sub-pixel demanders formulation}.", "table_source": "\\begin{table}[t]\n\\parbox{0.49\\textwidth}{\n\\caption{Evaluation on the effectiveness of formulating the demanders $D$ involving the idea of sub-pixel resolution.}\n\\label{Tab:ablation_1}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{l|cccccc}\n\\hline\nMethod & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nBaseline(HRNet-W48) &77.1 &91.8 &83.8 &73.5 &83.5 &81.8\\\\\n\\hline\n\\textbf{Naive demanders formulation} & 77.9 & 92.5 & 84.8 & 74.5 & 83.9 & 82.4 \\\\\n\\textbf{Sub-pixel demanders formulation} & 78.8 & 92.5 & 85.1 & 75.0 & 85.3 & 83.1\\\\\n\\hline\n\\end{tabular}}}\n\\hspace{0.01\\textwidth}\n\\parbox{0.49\\textwidth}{\n\\caption{Evaluation on the number of Sinkhorn iterations.}\n\\label{Tab:ablation_2}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{l|cccccc}\n\\hline\nMethod & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nBaseline(HRNet-W48) &77.1 &91.8 &83.8 &73.5 &83.5 &81.8\\\\\n\\hline\n500 Sinkhorn iterations & 78.3 & 92.4 & 84.9 & 74.8 & 84.4 & 82.7 \\\\\n1000 Sinkhorn iterations & 78.8 & 92.5 & 85.1 & 75.0 & 85.3 & 83.1\\\\\n1500 Sinkhorn iterations & 78.7 & 92.4 & 85.2 & 74.8 & 85.4 & 83.0 \\\\\n\\hline\n\\end{tabular}}}\n\\end{table}", "cell_list_gold": [{"value": "77.1", "char_index": [366, 370], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "91.8", "char_index": [372, 376], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "83.8", "char_index": [378, 382], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "73.5", "char_index": [384, 388], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "83.5", "char_index": [390, 394], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "81.8", "char_index": [396, 400], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "77.9", "char_index": [449, 453], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Naive demanders formulation", "model settings": {"xx": "yy"}}, {"value": "92.5", "char_index": [456, 460], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Naive demanders formulation", "model settings": {"xx": "yy"}}, {"value": "84.8", "char_index": [463, 467], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Naive demanders formulation", "model settings": {"xx": "yy"}}, {"value": "74.5", "char_index": [470, 474], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Naive demanders formulation", "model settings": {"xx": "yy"}}, {"value": "83.9", "char_index": [477, 481], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Naive demanders formulation", "model settings": {"xx": "yy"}}, {"value": "82.4", "char_index": [484, 488], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Naive demanders formulation", "model settings": {"xx": "yy"}}, {"value": "78.8", "char_index": [535, 539], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Sub-pixel demanders formulation", "model settings": {"xx": "yy"}}, {"value": "92.5", "char_index": [542, 546], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Sub-pixel demanders formulation", "model settings": {"xx": "yy"}}, {"value": "85.1", "char_index": [549, 553], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Sub-pixel demanders formulation", "model settings": {"xx": "yy"}}, {"value": "75.0", "char_index": [556, 560], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Sub-pixel demanders formulation", "model settings": {"xx": "yy"}}, {"value": "85.3", "char_index": [563, 567], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Sub-pixel demanders formulation", "model settings": {"xx": "yy"}}, {"value": "83.1", "char_index": [570, 574], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Sub-pixel demanders formulation", "model settings": {"xx": "yy"}}]}, "2210.00740v1_table4": {"table_code": "\\parbox{0.49\\textwidth}{\n\\caption{Evaluation on the number of Sinkhorn iterations.}\n\\label{Tab:ablation_2}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{l|cccccc}\n\\hline\nMethod & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nBaseline(HRNet-W48) &77.1 &91.8 &83.8 &73.5 &83.5 &81.8\\\\\n\\hline\n500 Sinkhorn iterations & 78.3 & 92.4 & 84.9 & 74.8 & 84.4 & 82.7 \\\\\n1000 Sinkhorn iterations & 78.8 & 92.5 & 85.1 & 75.0 & 85.3 & 83.1\\\\\n1500 Sinkhorn iterations & 78.7 & 92.4 & 85.2 & 74.8 & 85.4 & 83.0 \\\\\n\\hline\n\\end{tabular}}}", "table_label": "{Tab:ablation_2}", "table_numeric_cells": [["77.1", "77.1", 306, 310, 306, 310], ["91.8", "91.8", 312, 316, 312, 316], ["83.8", "83.8", 318, 322, 318, 322], ["73.5", "73.5", 324, 328, 324, 328], ["83.5", "83.5", 330, 334, 330, 334], ["81.8", "81.8", 336, 340, 336, 340], ["78.3", "78.3", 376, 380, 376, 380], ["92.4", "92.4", 383, 387, 383, 387], ["84.9", "84.9", 390, 394, 390, 394], ["74.8", "74.8", 397, 401, 397, 401], ["84.4", "84.4", 404, 408, 404, 408], ["82.7", "82.7", 411, 415, 411, 415], ["78.8", "78.8", 446, 450, 446, 450], ["92.5", "92.5", 453, 457, 453, 457], ["85.1", "85.1", 460, 464, 460, 464], ["75.0", "75.0", 467, 471, 467, 471], ["85.3", "85.3", 474, 478, 474, 478], ["83.1", "83.1", 481, 485, 481, 485], ["78.7", "78.7", 515, 519, 515, 519], ["92.4", "92.4", 522, 526, 522, 526], ["85.2", "85.2", 529, 533, 529, 533], ["74.8", "74.8", 536, 540, 536, 540], ["85.4", "85.4", 543, 547, 543, 547], ["83.0", "83.0", 550, 554, 550, 554]], "text_chunk_selected": "The Earth Mover's Distance is a \na technique used for measuring the difference between two probability distributions, which can be understood as the optimal cost needed to transport the mass from one distribution to another. Specifically, for calculating the Earth Mover's Distance, we regard the source distribution as a set of ($N$) suppliers $S = (s_1, ..., s_N)^\\top$, where $s_n$ represents the total units of mass that the $n$-th supplier has, and we regard the target distribution as a set of ($M$) demanders $D = (d_1, ..., d_M)^\\top$, where $d_m$ represents the total units of mass that the $m$-th demander requires. Besides, we also denote $C \\in  R^{N \\times M}_{\\geq 0}$ as the cost function between the source and target distributions, where $C_{n,m}$ represents the cost for transporting a unit of mass from the $n$-th supplier to the $m$-th demander. Then we aim to find a least-cost transportation plan from the set of possible plans $P = \\{p \\in R^{N \\times M}_{\\geq 0}:~p\\textbf{1} = S~\\&~p^\\top\\textbf{1} = D\\}$ to transport all mass from the $N$ suppliers to the $M$ demanders, where \\textbf{1} represents a vector of ones. The Earth Mover's Distance $E_C(S, D)$ denotes the cost of the least-cost transportation plan, which can be formulated as:\n\nwhere $\\lambda > 0$, and $h(p) = -\\sum_{n=1}^{N} \\sum_{m=1}^{M} C_{n,m} \\log C_{n,m}$. Optimizing Eq.~\\ref{eq:pre_2} is computationally cheaper than optimizing Eq.~\\ref{eq:pre_1}, since Eq.~\\ref{eq:pre_2} can be optimized with matrix scaling through the Sinkhorn algorithm.\n\nWe denote $K$ the number of joints per input image $I$. Then we denote $\\mathbf{H_{dot}} = \\{H_{dot}^1, ..., H_{dot}^K\\}$, $\\mathbf{H_{Gau}} = \\{H_{Gau}^1, ..., H_{Gau}^K\\}$,  and $\\mathbf{H_{pred}} = \\{H_{pred}^1, ..., H_{pred}^K\\}$ respectively the corresponding $K$ GT dot-annotated heatmaps, GT Gaussian-smoothed heatmaps, and predicted heatmaps of the input image $I$. We denote $\\mathcal D_{dot} = \\{(I, \\mathbf{H_{dot}})\\}$ the joint distribution of the input image and the corresponding $K$ dot-annotated heatmaps, and $\\mathcal D_{Gau} = \\{(I, \\mathbf{H_{Gau}})\\}$ the joint distribution of the input image and the corresponding $K$ Gaussian-smoothed heatmaps. Besides, we denote $l_{MSE}(a, b) = \\left\\| a - b\\right\\|_2^2$ the pixel-wise MSE loss, and $\\phi$ the model parameters where $\\phi(I) = \\mathbf{H_{pred}}$. After that, we denote $R(\\mathcal D_{dot}, \\phi, l_{MSE}) = \\mathbb{E}_{(I, \\mathbf{H_{dot}}) \\sim \\mathcal D_{dot}}[l_{MSE}(\\phi(I), \\mathbf{H_{dot}})]$ as the expected risk calculated between the predicted heatmaps and the GT dot-annotated heatmaps, and $R(\\mathcal D_{Gau}, \\phi, l_{MSE}) = \\mathbb{E}_{(I, \\mathbf{H_{Gau}}) \\sim \\mathcal D_{Gau}}[l_{MSE}(\\phi(I), \\mathbf{H_{Gau}})]$ as the expected risk calculated between the predicted heatmaps and the Gaussian-smoothed heatmaps.\n\n\\subsection{COCO Keypoint Detection}\n\\noindent\\textbf{Dataset \\& evaluation metric.} The COCO dataset \\cite{lin2014microsoft} contains more than 200k images and 250k person instances, which are annotated with 17 body joints. This dataset has three subsets including COCO training set, COCO validation set, and COCO test-dev set, which have 57k, 5k and 20k images, respectively. We conduct experiments on this dataset via first training the model on the train2017 set, and then evaluating the model on the val2017 set and test-dev2017 set. Following \\cite{xiao2018simple,sun2019deep,YuanFHLZCW21}, we use standard average precision (AP) calculated based on Object Keypoint Similarity (OKS) to evaluate model performance.\n\n\\noindent\\textbf{Implementation details.} We apply our method to various baselines including Simple Baseline \\cite{xiao2018simple}, HRNet \\cite{sun2019deep}, and HRFormer \\cite{YuanFHLZCW21}, with their respective backbones including ResNet-152, HRNet-W32, HRNet-W48, and HRFormer-Base. For these baselines, we follow their original learning and optimization configurations for model training. \nTo calculate the Earth Mover's Distance using the Sinkhorn algorithm, we set the Sinkhorn entropic regularization parameter to 1 and the number of Sinkhorn iterations to 1000 in our experiments.\n\n\\subsection{MPII Human Pose Estimation}\n\\noindent\\textbf{Dataset \\& evaluation metric.} The MPII dataset \\cite{andriluka20142d} contains around 25K images and more than 40k person instances, which are annotated with 16 body joints. We adopt the standard train/val split in \\cite{andriluka20142d} to build the MPII training set and validation set, and conduct all the experiments on this dataset via first training the model on the MPII training set, and then evaluating it on the MPII validation set. Following \\cite{sun2019deep}, we use the head-normalized probability of correct keypoint (PCKh) \\cite{andriluka20142d} score as the evaluation metric on this dataset and report the PCKh@0.5 score.\n\n\\noindent\\textbf{Implementation details.} On the MPII dataset, we also apply our method to various methods as our baselines, including Simple Baseline \\cite{xiao2018simple} and HRNet \\cite{sun2019deep}, with their respective backbones including ResNet-152, HRNet-W32, and HRNet-W48. We follow the original learning and optimization configurations for model training for both Simple Baseline \\cite{xiao2018simple} and HRNet \\cite{sun2019deep}. Besides, same as the experiments on the COCO dataset, we also set the Sinkhorn entropic regularization parameter to 1 and the number of Sinkhorn iterations to 1000 in our experiments on the MPII dataset.\n\n\\noindent\\textbf{Impact of the number of Sinkhorn iterations.} For measuring the Earth Mover's Distance utilizing the Sinkhorn algorithm, we need to set the number of Sinkhorn iterations, which we set to 1000 in our experiments. We evaluate other choices of the number of Sinkhorn iterations in Tab.~\\ref{Tab:ablation_2}. As shown, all variants outperform the baseline method, and after the number of Sinkhorn iterations becomes larger than 1000, the model performance becomes stabilized. Hence, we set the number of Sinkhorn iterations to be 1000 in all our experiments.", "table_source": "\\begin{table}[t]\n\\parbox{0.49\\textwidth}{\n\\caption{Evaluation on the effectiveness of formulating the demanders $D$ involving the idea of sub-pixel resolution.}\n\\label{Tab:ablation_1}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{l|cccccc}\n\\hline\nMethod & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nBaseline(HRNet-W48) &77.1 &91.8 &83.8 &73.5 &83.5 &81.8\\\\\n\\hline\n\\textbf{Naive demanders formulation} & 77.9 & 92.5 & 84.8 & 74.5 & 83.9 & 82.4 \\\\\n\\textbf{Sub-pixel demanders formulation} & 78.8 & 92.5 & 85.1 & 75.0 & 85.3 & 83.1\\\\\n\\hline\n\\end{tabular}}}\n\\hspace{0.01\\textwidth}\n\\parbox{0.49\\textwidth}{\n\\caption{Evaluation on the number of Sinkhorn iterations.}\n\\label{Tab:ablation_2}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{l|cccccc}\n\\hline\nMethod & AP & $\\text{AP}^{50}$ & $\\text{AP}^{75}$ & $\\text{AP}^{\\text{M}}$ &$\\text{AP}^{\\text{L}}$ &AR  \\\\\n\\hline\nBaseline(HRNet-W48) &77.1 &91.8 &83.8 &73.5 &83.5 &81.8\\\\\n\\hline\n500 Sinkhorn iterations & 78.3 & 92.4 & 84.9 & 74.8 & 84.4 & 82.7 \\\\\n1000 Sinkhorn iterations & 78.8 & 92.5 & 85.1 & 75.0 & 85.3 & 83.1\\\\\n1500 Sinkhorn iterations & 78.7 & 92.4 & 85.2 & 74.8 & 85.4 & 83.0 \\\\\n\\hline\n\\end{tabular}}}\n\\end{table}", "cell_list_gold": [{"value": "77.1", "char_index": [306, 310], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "91.8", "char_index": [312, 316], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "83.8", "char_index": [318, 322], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "73.5", "char_index": [324, 328], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "83.5", "char_index": [330, 334], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "81.8", "char_index": [336, 340], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "Baseline(HRNet-W48)", "model settings": {"xx": "yy"}}, {"value": "78.3", "char_index": [376, 380], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "92.4", "char_index": [383, 387], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "84.9", "char_index": [390, 394], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "74.8", "char_index": [397, 401], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "84.4", "char_index": [404, 408], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "82.7", "char_index": [411, 415], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "78.8", "char_index": [446, 450], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "1000 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "92.5", "char_index": [453, 457], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "1000 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "85.1", "char_index": [460, 464], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "1000 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "75.0", "char_index": [467, 471], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "1000 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "85.3", "char_index": [474, 478], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "1000 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "83.1", "char_index": [481, 485], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "1000 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "78.7", "char_index": [515, 519], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "1500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "92.4", "char_index": [522, 526], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{50}", "experimental settings": {"xx": "yy"}, "model": "1500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "85.2", "char_index": [529, 533], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{75}", "experimental settings": {"xx": "yy"}, "model": "1500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "74.8", "char_index": [536, 540], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{M}", "experimental settings": {"xx": "yy"}, "model": "1500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "85.4", "char_index": [543, 547], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "\\text{AP}^{\\text{L}}", "experimental settings": {"xx": "yy"}, "model": "1500 Sinkhorn iterations", "model settings": {"xx": "yy"}}, {"value": "83.0", "char_index": [550, 554], "type": "Result", "training data/set": "COCO", "test data/set": "COCO validation", "task": ["human pose estimation", "2D human pose estimation"], "metric": "AR", "experimental settings": {"xx": "yy"}, "model": "1500 Sinkhorn iterations", "model settings": {"xx": "yy"}}]}, "2210.00910v1_table0": {"table_code": "\\begin{table}[t]\n\\footnotesize\n\\begin{tabular}{lrl}\n\\hline\nname & \\multicolumn{1}{l}{\\# examples} & classes \\\\ \n\\hdashline\nHateCheck & 3,728 & \\begin{tabular}[c]{@{}l@{}}hateful (68.8\\%), \\\\ non-hate (31.2\\%)\\end{tabular} \\\\\nETHOS (binary) & 997 & \\begin{tabular}[c]{@{}l@{}}hate speech (64.1\\%), \\\\ not-hate speech (25.9\\%)\\end{tabular} \\\\ \\hline\n\\end{tabular}\n\\caption{The number of examples and the class balance of the datasets.}\n\\label{tab:datasets}\n\\end{table}", "table_label": "{tab:datasets}", "table_numeric_cells": [["3,728", "3,728", 135, 140, 135, 140], ["997", "997", 242, 245, 242, 245]], "text_chunk_selected": "\\begin{abstract}\nStandard approaches to hate speech detection rely on sufficient available hate speech annotations. Extending previous work that repurposes natural language inference (NLI) models for zero-shot text classification, we propose a simple approach that combines multiple hypotheses to improve English NLI-based zero-shot hate speech detection. \nWe first conduct an error analysis for vanilla NLI-based zero-shot hate speech detection and then develop four strategies based on this analysis. The strategies use multiple hypotheses to predict various aspects of an input text and combine these predictions into a final verdict. We find that the zero-shot baseline used for the initial error analysis already outperforms commercial systems and fine-tuned BERT-based hate speech detection models on HateCheck. The combination of the proposed strategies further increases the zero-shot accuracy of 79.4\\% on HateCheck by 7.9 percentage points (pp), and the accuracy of 69.6\\% on ETHOS by 10.0pp.\\footnote{The code and instructions to reproduce the experiments are available at \\url{https://github.com/jagol/nli-for-hate-speech-detection}.}\n\\end{abstract}\n\n\\section{Introduction}\nWith the increasing popularity of social media and online forums, phenomena such as hate speech, offensive and abusive language, and personal attacks have gained a powerful medium through which they can propagate fast. \nDue to the sheer number of posts and comments on social media, manual content moderation has become unfeasible, thus\nthe automatic detection of harmful content becomes essential.\nIn natural language processing, there now exist established tasks with the goal of detecting offensive language \\cite{pradhan_review_2020}, abusive language \\cite{nakov_detecting_2021}, hate speech \\cite{fortuna_survey_2018} and other related types of harmful content \\cite{poletto_resources_2021}. \nIn this work, we focus on the detection of hate speech, which is typically defined as attacking, abusive or discriminatory language that targets people on the basis of identity defining group characteristics such as gender, sexual orientation, disability, race, religion, national origin etc. \\cite{fortuna_survey_2018, poletto_resources_2021, yin_towards_2021}.\nMost current hate speech detection approaches rely on either training models from scratch or fine-tuning pre-trained language models \\cite{jahan_systematic_2021}.\nBoth types of approaches need large amounts of labeled data which are only available for a few high-resource languages \\cite{poletto_resources_2021} and costly to create.\nTherefore, exploring data-efficient methods for hate speech detection is an attractive alternative. \n\n\\section{Background and Related Work}\nEarly approaches to hate speech detection have focused on English social media posts, especially Twitter, and treated the task as binary or ternary text classification \\cite{waseem_hateful_2016, davidson_automated_2017, founta_large_2018}. \nIn more recent work, additional labels have been introduced that indicate whether the post is group-directed or not, who the targeted group is, if the post calls for violence, is aggressive, contains stereotypes, if the hate is expressed implicitly, or if sarcasm or irony is used \\cite{mandl_overview_2019, mandl_overview_2020, sap_social_2020, elsherief_latent_2021, rottger_hatecheck_2021, mollas_ethos_2022}. \nSometimes hate speech is not directly annotated but instead labels, such as \\textit{racism}, \\textit{sexism}, \\textit{homophobia} that already combine hostility with a specific target are annotated and predicted \\cite{waseem_hateful_2016, waseem_are_2016, saha_hateminers_2018, basile_thenorth_2020}.\n\nThe advent of large language models has enabled zero-shot and few-shot text classification approaches such as prompting \\cite{liu_pre-train_2021}, and task descriptions \\cite{raffel_exploring_2020}, which convert the target task to the pre-training objective\nand are usually only used in combination with large language models. \n\\citet{chiu_detecting_2021} use the prompts \\textit{``Is this text racist?''} and \\textit{``Is this text sexist?''} to detect hate speech with GPT-3. \n\\citet{schick_self-diagnosis_2021} show that toxicity in large generative language models can be avoided by using similar prompts to self-diagnose toxicity during the decoding.\n\nIn contrast, NLI-based prediction in which a target task is converted to an NLI-task and fed into an NLI model converts the target task to the fine-tuning task. \nHere, a model is given a premise and a hypothesis and tasked to predict if the premise entails the hypothesis, contradicts it, or is neutral towards it.  \n\\citet{yin_benchmarking_2019} proposed to use an NLI model for zero-shot topic classification, by inputting the text to classify as the premise and constructing for each topic a hypothesis of the form \\enquote{This text is about $<$topic$>$}. \nThey map the labels \\textit{neutral} and \\textit{contradiction} to \\textit{not-entailment}. We can then interpret a prediction of entailment as predicting that the input text belongs to the topic in the given hypothesis.\nConversely, \\textit{not-entailment} implies that the text is not about the topic.\n\\citet{wang_entailment_2021} show for a range of tasks, including offensive language identification, that this task re-formulation also benefits few-shot learning scenarios. \nRecently, \\citet{alkhamissi_token_2022} obtained large performance improvements in few-shot learning for hate speech detection by (1) decomposing the task into four subtasks and (2) additionally training the few-shot model on a knowledge base.\n\n\\section{Data}\n\\paragraph{HateCheck} \\citet{rottger_hatecheck_2021} introduce this English, synthetic, evaluation-only dataset, annotated for a binary decision between hate speech and not-hate speech. It covers 29 functionalities that are either a type of hate speech or challenging types of non-hate speech that could be mistaken for hate speech by a classifier. The examples for each of these functionalities have been constructed on the basis of conversations with NGO workers. \nEach of these templates contains one blank space to be filled with a protected group. The authors fill these templates with seven protected groups, namely: women, gay people, transgender people, black people, Muslims, immigrants, and disabled people. Overall the dataset contains 3,728 examples. \n\n\\paragraph{ETHOS} The ETHOS dataset \\cite{mollas_ethos_2022} is split into two parts: one part is annotated for the presence of hate speech. The other part contains fine-grained annotations that indicate which characteristics have been targeted (gender, sexual orientation, race, ethnicity, religion, national origin, disability), whether the utterance calls for violence, and whether it is directed at an individual or a general statement about a group. The dataset is based on English comments from Youtube and Reddit. For this work, we will only make use of the binary hate speech annotations. These annotations are continuous values between $0$ (indicating no hate speech at all) and $1$ indicating clear hate speech. We rounded all annotations to either $0$ or $1$ using a threshold of $0.5$.\n\nTable \\ref{tab:datasets} displays the class balances of the two datasets.", "table_source": "\\begin{table}[t]\n\\footnotesize\n\\begin{tabular}{lrl}\n\\hline\nname & \\multicolumn{1}{l}{\\# examples} & classes \\\\ \n\\hdashline\nHateCheck & 3,728 & \\begin{tabular}[c]{@{}l@{}}hateful (68.8\\%), \\\\ non-hate (31.2\\%)\\end{tabular} \\\\\nETHOS (binary) & 997 & \\begin{tabular}[c]{@{}l@{}}hate speech (64.1\\%), \\\\ not-hate speech (25.9\\%)\\end{tabular} \\\\ \\hline\n\\end{tabular}\n\\caption{The number of examples and the class balance of the datasets.}\n\\label{tab:datasets}\n\\end{table}", "cell_list_gold": [{"value": "3,728", "char_index": [135, 140], "type": "Data Stat.", "dataset": "HateCheck", "attribute name": ["number of examples", "# examples"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "997", "char_index": [242, 245], "type": "Data Stat.", "dataset": "ETHOS (binary)", "attribute name": ["number of examples", "# examples"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}]}, "2210.00910v1_table11": {"table_code": "\\begin{table*}[t]\n\\small\n\\centering\n\\begin{tabular}{l|rrrr}\nhypothesis & \\multicolumn{1}{l}{accuracy (\\%)} & \\multicolumn{1}{l}{$\\downarrow$ $F_1$ (\\%)} & \\multicolumn{1}{l}{recall (\\%)} & \\multicolumn{1}{l}{precision (\\%)} \\\\ \\hline\nThat example is about gay people. & 99.1 & 96.7 & 94.2 & 99.4 \\\\\nThis example is about gay people. & 99.0 & 96.6 & 94.0 & 99.2 \\\\\nThis text is about gay people.    & 98.8 & 95.9 & 92.9 & 99.0 \\\\\nThis is about gay people.         & 98.8 & 95.8 & 92.6 & 99.2 \\\\\nThat text is about gay people.    & 98.5 & 94.6 & 90.0 & 99.6 \\\\\nThat is about gay people.         & 98.4 & 94.4 & 90.4 & 98.8\n\\end{tabular}\n\\caption{Results for supporting hypotheses aimed at detecting mentions of gay people. }\n\\label{tab:results-gay-people}\n\\end{table*}", "table_label": "{tab:results-gay-people}", "table_numeric_cells": [["99.1", "99.1", 270, 274, 270, 274], ["96.7", "96.7", 277, 281, 277, 281], ["94.2", "94.2", 284, 288, 284, 288], ["99.4", "99.4", 291, 295, 291, 295], ["99.0", "99.0", 335, 339, 335, 339], ["96.6", "96.6", 342, 346, 342, 346], ["94.0", "94.0", 349, 353, 349, 353], ["99.2", "99.2", 356, 360, 356, 360], ["98.8", "98.8", 400, 404, 400, 404], ["95.9", "95.9", 407, 411, 407, 411], ["92.9", "92.9", 414, 418, 414, 418], ["99.0", "99.0", 421, 425, 421, 425], ["98.8", "98.8", 465, 469, 465, 469], ["95.8", "95.8", 472, 476, 472, 476], ["92.6", "92.6", 479, 483, 479, 483], ["99.2", "99.2", 486, 490, 486, 490], ["98.5", "98.5", 530, 534, 530, 534], ["94.6", "94.6", 537, 541, 537, 541], ["90.0", "90.0", 544, 548, 544, 548], ["99.6", "99.6", 551, 555, 551, 555], ["98.4", "98.4", 595, 599, 595, 599], ["94.4", "94.4", 602, 606, 602, 606], ["90.4", "90.4", 609, 613, 609, 613], ["98.8", "98.8", 616, 620, 616, 620]], "text_chunk_selected": "In contrast, NLI-based prediction in which a target task is converted to an NLI-task and fed into an NLI model converts the target task to the fine-tuning task. \nHere, a model is given a premise and a hypothesis and tasked to predict if the premise entails the hypothesis, contradicts it, or is neutral towards it.  \n\\citet{yin_benchmarking_2019} proposed to use an NLI model for zero-shot topic classification, by inputting the text to classify as the premise and constructing for each topic a hypothesis of the form \\enquote{This text is about $<$topic$>$}. \nThey map the labels \\textit{neutral} and \\textit{contradiction} to \\textit{not-entailment}. We can then interpret a prediction of entailment as predicting that the input text belongs to the topic in the given hypothesis.\nConversely, \\textit{not-entailment} implies that the text is not about the topic.\n\\citet{wang_entailment_2021} show for a range of tasks, including offensive language identification, that this task re-formulation also benefits few-shot learning scenarios. \nRecently, \\citet{alkhamissi_token_2022} obtained large performance improvements in few-shot learning for hate speech detection by (1) decomposing the task into four subtasks and (2) additionally training the few-shot model on a knowledge base.\n\n\\section{Data}\n\\paragraph{HateCheck} \\citet{rottger_hatecheck_2021} introduce this English, synthetic, evaluation-only dataset, annotated for a binary decision between hate speech and not-hate speech. It covers 29 functionalities that are either a type of hate speech or challenging types of non-hate speech that could be mistaken for hate speech by a classifier. The examples for each of these functionalities have been constructed on the basis of conversations with NGO workers. \nEach of these templates contains one blank space to be filled with a protected group. The authors fill these templates with seven protected groups, namely: women, gay people, transgender people, black people, Muslims, immigrants, and disabled people. Overall the dataset contains 3,728 examples. \n\n\\paragraph{Experiment setup} To test if an input text contains hate speech, we need a hypothesis expressing that claim. \nHowever, there are many ways how the claim, that a given text contains hate speech, can be expressed. Choosing a sub-optimal way to express this claim will result in lower accuracy. \\citet{wang_entailment_2021} already tested four different hypotheses for hate speech or offensive language. We conduct an extensive evaluation by constructing and testing all grammatically correct sentences built with the following building blocks: \\textit{It/That/This + example/text + contains/is + hate speech/hateful/hateful content}. \nWe conduct all experiments with a BART-large model \\cite{lewis_bart_2020} that was fine-tuned on the \\textbf{M}ulti-Genre \\textbf{N}atural \\textbf{L}anguage \\textbf{I}nference dataset (MNLI) \\cite{williams_broad-coverage_2018} and has been made available via the Huggingface transformers library \\cite{wolf_transformers_2020} as \\texttt{bart-large-mnli}. \nThis model predicts either \\textit{contradiction}, \\textit{neutral}, or \\textit{entailment}. \nWe follow the recommendation of the model creators to ignore the logits for \\textit{neutral} and perform a softmax over the logits of \\textit{contradiction} and \\textit{entailment}. If the probability for entailment is equal or higher than 0.5 we consider this a prediction of \\textit{entailment} and thus \\textit{hate speech}.\\footnote{This procedure is equal to taking the argmax over \\textit{contradiction} and \\textit{entailment}.}\nWe evaluate on HateCheck since the functionalities in this dataset allow for an automatic in-depth error analysis and compare our results to the baselines provided by \\citet{rottger_hatecheck_2021}.\n\nIn this section, we present four methods, which we call strategies, that aim to improve zero-shot hate speech detection. A strategy has the following components and structure: The aim is to assign a label $y=\\{0, 1\\}$ to input text $t$, where $1$ corresponds to the class \\textit{hate speech} and $0$ corresponds to the class \\textit{not-hate speech}.\nThe input text $t$ can be used in one or multiple a premises $p_0$ to $p_m$, that are used in conjunction with the main hypothesis $h_0$ and one or multiple supporting hypotheses $\\left[h_1, ..., h_n\\right]$ to obtain  NLI model predictions $m(p_i, h_j) \\in \\{0,1\\}$ where 0 corresponds to contradiction and $1$ corresponds to entailment.\nThe variables $i$ and $j$ are defined as: $i \\in \\left[0, ..., m\\right]$ and $j \\in \\left[0, ..., n\\right]$. \nThe rules for how to combine model predictions to obtain the final label $y$ are given by the individual strategies.\nAs the main hypothesis we use \\enquote{That contains hate speech.}, since it lead to the highest accuracy on HateCheck in Section \\ref{sec:evaluating-standard-0shot}.\nThe supporting hypotheses used to implement the strategies are listed in Table \\ref{tab:list-of-hypotheses}.\n\n\\subsection{Filtering By Target (FBT)}\n\\label{subsec:filtering-by-target}\nThe error analysis showed that we can improve zero-shot classification accuracy significantly by avoiding predictions of hate speech where no relevant target group occurs. We thus propose to avoid false positives by constructing a set of supporting hypotheses $\\left[h_1, ..., h_n\\right]$ to predict if text $t$ actually targets or mentions a protected group or characteristic. If no protected group or characteristic is predicted to occur in $t$, a potential prediction of \\textit{hate speech} is overridden to \\textit{not-hate speech}.\nFigure \\ref{fig:filtering-by-target} illustrates the method. \n\nTable \\ref{tab:compare-hypotheses-full}, the extended version of Table \\ref{tab:compare-hypotheses}, contains all results for comparing hypotheses for zero-shot hate speech detection on HateCheck. \\enquote{average:$<$ \\texttt{expression}$>$} refers to the average accuracy of all hypotheses containing \\texttt{expression}. The highest accuracy is in bold. \n\nWe use the same model and as in the previous zero-shot experiments for evaluation and test the performance for detecting mentions for all protected groups in HateCheck. We additionally test the detection of the supercategory \\textit{queer people} covering the two protected groups \\textit{gay people} and \\textit{transgender people} in HateCheck. When testing if a text revolves around gender, we treat both \\textit{women} and \\textit{transgender people} as positive classes and all other protected groups as negative classes. While this mapping obviously can result in incorrect labels (a text can be about gender even if another group is targeted), we assume that it holds true for examples in the HateCheck dataset.\n\nTable \\ref{tab:results-black-people} shows the results for detecting if \\textit{black people} are mentioned, Table \\ref{tab:results-muslim-people} for mentions of \\textit{Muslims}, Table \\ref{tab:results-immigrants} for mentions of immigrants, Table \\ref{tab:results-disabled-people} for mentions of \\textit{disabled people}, Table \\ref{tab:results-gay-people} for mentions of \\textit{gay people}, Table \\ref{tab:results-transgender-people} for mentions of \\textit{transgender people}, Table \\ref{tab:results-queer-people} for mentions of \\textit{queer people}, and Table \\ref{tab:results-gender} for detecting if a text is about \\textit{gender}.", "table_source": "\\begin{table*}[t]\n\\small\n\\centering\n\\begin{tabular}{l|rrrr}\nhypothesis & \\multicolumn{1}{l}{accuracy (\\%)} & \\multicolumn{1}{l}{$\\downarrow$ $F_1$ (\\%)} & \\multicolumn{1}{l}{recall (\\%)} & \\multicolumn{1}{l}{precision (\\%)} \\\\ \\hline\nThat example is about gay people. & 99.1 & 96.7 & 94.2 & 99.4 \\\\\nThis example is about gay people. & 99.0 & 96.6 & 94.0 & 99.2 \\\\\nThis text is about gay people.    & 98.8 & 95.9 & 92.9 & 99.0 \\\\\nThis is about gay people.         & 98.8 & 95.8 & 92.6 & 99.2 \\\\\nThat text is about gay people.    & 98.5 & 94.6 & 90.0 & 99.6 \\\\\nThat is about gay people.         & 98.4 & 94.4 & 90.4 & 98.8\n\\end{tabular}\n\\caption{Results for supporting hypotheses aimed at detecting mentions of gay people. }\n\\label{tab:results-gay-people}\n\\end{table*}", "cell_list_gold": [{"value": "99.1", "char_index": [270, 274], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That example is about gay people.", "model settings": {"xx": "yy"}}, {"value": "96.7", "char_index": [277, 281], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That example is about gay people.", "model settings": {"xx": "yy"}}, {"value": "94.2", "char_index": [284, 288], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That example is about gay people.", "model settings": {"xx": "yy"}}, {"value": "99.4", "char_index": [291, 295], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That example is about gay people.", "model settings": {"xx": "yy"}}, {"value": "99.0", "char_index": [335, 339], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This example is about gay people.", "model settings": {"xx": "yy"}}, {"value": "96.6", "char_index": [342, 346], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This example is about gay people.", "model settings": {"xx": "yy"}}, {"value": "94.0", "char_index": [349, 353], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This example is about gay people.", "model settings": {"xx": "yy"}}, {"value": "99.2", "char_index": [356, 360], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This example is about gay people.", "model settings": {"xx": "yy"}}, {"value": "98.8", "char_index": [400, 404], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This text is about gay people.", "model settings": {"xx": "yy"}}, {"value": "95.9", "char_index": [407, 411], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This text is about gay people.", "model settings": {"xx": "yy"}}, {"value": "92.9", "char_index": [414, 418], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This text is about gay people.", "model settings": {"xx": "yy"}}, {"value": "99.0", "char_index": [421, 425], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This text is about gay people.", "model settings": {"xx": "yy"}}, {"value": "98.8", "char_index": [465, 469], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This is about gay people.", "model settings": {"xx": "yy"}}, {"value": "95.8", "char_index": [472, 476], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This is about gay people.", "model settings": {"xx": "yy"}}, {"value": "92.6", "char_index": [479, 483], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This is about gay people.", "model settings": {"xx": "yy"}}, {"value": "99.2", "char_index": [486, 490], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This is about gay people.", "model settings": {"xx": "yy"}}, {"value": "98.5", "char_index": [530, 534], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That text is about gay people.", "model settings": {"xx": "yy"}}, {"value": "94.6", "char_index": [537, 541], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That text is about gay people.", "model settings": {"xx": "yy"}}, {"value": "90.0", "char_index": [544, 548], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That text is about gay people.", "model settings": {"xx": "yy"}}, {"value": "99.6", "char_index": [551, 555], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That text is about gay people.", "model settings": {"xx": "yy"}}, {"value": "98.4", "char_index": [595, 599], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That is about gay people.", "model settings": {"xx": "yy"}}, {"value": "94.4", "char_index": [602, 606], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That is about gay people.", "model settings": {"xx": "yy"}}, {"value": "90.4", "char_index": [609, 613], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That is about gay people.", "model settings": {"xx": "yy"}}, {"value": "98.8", "char_index": [616, 620], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck gay people", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That is about gay people.", "model settings": {"xx": "yy"}}]}, "2210.00910v1_table5": {"table_code": "\\begin{table}[ht]\n\\centering\n\\small\n\\begin{tabular}{lrr}\n\\hline\nstrategy & F20 & overall \\\\ \n\\hdashline\nNo strategy & 0.0 & 79.4 \\\\\nFCS & +100 & +4.6 \\\\\n\\hdashline\nFCS$_{p_1}$ & +0.0 & +0.0 \\\\\nFCS$_{p_1 FBT}$ & +6.9 & +0.3 \\\\\n\\hline\n\\end{tabular}\n\\caption{Evaluation of FCS variants. The two bottom rows display the variants adjusted for detecting Hate Speech in $p_1$. The functionality F20 contains \\textit{Denouncements of hate that quote it}. The scores are given in accuracy (\\%) and change in accuracy compared to \\textit{No strategy}.}\n\\label{tab:FCS-follow-up}\n\\end{table}", "table_label": "{tab:FCS-follow-up}", "table_numeric_cells": [["0.0", "0.0", 118, 121, 118, 121], ["79.4", "79.4", 124, 128, 124, 128], ["+100", "+100", 138, 142, 138, 142], ["+4.6", "+4.6", 145, 149, 145, 149], ["+0.0", "+0.0", 178, 182, 178, 182], ["+0.0", "+0.0", 185, 189, 185, 189], ["+6.9", "+6.9", 211, 215, 211, 215], ["+0.3", "+0.3", 218, 222, 218, 222]], "text_chunk_selected": "In contrast, NLI-based prediction in which a target task is converted to an NLI-task and fed into an NLI model converts the target task to the fine-tuning task. \nHere, a model is given a premise and a hypothesis and tasked to predict if the premise entails the hypothesis, contradicts it, or is neutral towards it.  \n\\citet{yin_benchmarking_2019} proposed to use an NLI model for zero-shot topic classification, by inputting the text to classify as the premise and constructing for each topic a hypothesis of the form \\enquote{This text is about $<$topic$>$}. \nThey map the labels \\textit{neutral} and \\textit{contradiction} to \\textit{not-entailment}. We can then interpret a prediction of entailment as predicting that the input text belongs to the topic in the given hypothesis.\nConversely, \\textit{not-entailment} implies that the text is not about the topic.\n\\citet{wang_entailment_2021} show for a range of tasks, including offensive language identification, that this task re-formulation also benefits few-shot learning scenarios. \nRecently, \\citet{alkhamissi_token_2022} obtained large performance improvements in few-shot learning for hate speech detection by (1) decomposing the task into four subtasks and (2) additionally training the few-shot model on a knowledge base.\n\n\\paragraph{Error Analysis} Column \\enquote{No Strat.} in Table \\ref{tab:full-error-analysis} shows the accuracy per HateCheck functionality for the hypothesis \\enquote{That contains hate speech.}. Most notably, the model wrongly predicted all denouncements of hate (F20 and F21) as hate speech. In four functionalities (F22, F11, F23, F20) the model predicted hate speech even though no one or no relevant group was targeted. Finally, we see that the model often fails at analyzing sentences with negations (F15) and that it fails at recognizing when slurs are reclaimed and used in a positive way (F9). \nIn what follows, we will present and evaluate strategies to avoid these errors.\n\nIn this section, we present four methods, which we call strategies, that aim to improve zero-shot hate speech detection. A strategy has the following components and structure: The aim is to assign a label $y=\\{0, 1\\}$ to input text $t$, where $1$ corresponds to the class \\textit{hate speech} and $0$ corresponds to the class \\textit{not-hate speech}.\nThe input text $t$ can be used in one or multiple a premises $p_0$ to $p_m$, that are used in conjunction with the main hypothesis $h_0$ and one or multiple supporting hypotheses $\\left[h_1, ..., h_n\\right]$ to obtain  NLI model predictions $m(p_i, h_j) \\in \\{0,1\\}$ where 0 corresponds to contradiction and $1$ corresponds to entailment.\nThe variables $i$ and $j$ are defined as: $i \\in \\left[0, ..., m\\right]$ and $j \\in \\left[0, ..., n\\right]$. \nThe rules for how to combine model predictions to obtain the final label $y$ are given by the individual strategies.\nAs the main hypothesis we use \\enquote{That contains hate speech.}, since it lead to the highest accuracy on HateCheck in Section \\ref{sec:evaluating-standard-0shot}.\nThe supporting hypotheses used to implement the strategies are listed in Table \\ref{tab:list-of-hypotheses}.\n\n\\subsection{Filtering By Target (FBT)}\n\\label{subsec:filtering-by-target}\nThe error analysis showed that we can improve zero-shot classification accuracy significantly by avoiding predictions of hate speech where no relevant target group occurs. We thus propose to avoid false positives by constructing a set of supporting hypotheses $\\left[h_1, ..., h_n\\right]$ to predict if text $t$ actually targets or mentions a protected group or characteristic. If no protected group or characteristic is predicted to occur in $t$, a potential prediction of \\textit{hate speech} is overridden to \\textit{not-hate speech}.\nFigure \\ref{fig:filtering-by-target} illustrates the method. \n\nOur zero-shot model wrongly classifies all examples of counterspeech that quote or reference hate speech as actual hate speech. \nReferences to hate speech without quotation marks are hard to identify.\nThus, for this work, we limit ourselves to counterspeech that quotes hate speech explicitly. \nWe propose a three-stage strategy to this phenomenon: (1) quotation identification, (2) hate speech classification of the quoted content, (3) detecting the stance of the post towards the quoted content. \nFormally, the input text $t$ is divided into premise $p_0$ which contains the quoted text and premise $p_1$ which contains the text around the quotes. The quoted text is represented as ``$\\left[X\\right]$'' in $p_1$. \nUsing the main hypothesis $h_0$ we predict if $p_0$ contains hate speech or not. We use the supporting hypothesis \\enquote{This text supports $\\left[X\\right].$} ($h_1$) to predict the stance of $p_1$ towards $p_0$. \nIf $p_0$ contains hate speech and $p_1$ has a supportive stance towards $p_0$, $t$ is classified as \\textit{hate speech}, otherwise it is classified as \\textit{not-hate speech}.\nThe strategy is depicted in Figure \\ref{fig:counterspeech-filter}.\n\nWhile our experiments did not show problems in generalization, we can imagine the following weakness for the FCS strategy: \nGiven an input text $t$ that contains a quote and hate speech, where the hate speech does not occur inside of the quotes, the current FCS strategy would fail, since it only detects hate speech in $p_0$, that is inside the quotes. Such an example is given in Figure \\ref{fig:counterspeech-filter-fbt}.\n\nThe results, displayed in Table \\ref{tab:FCS-follow-up} show that this modification removes all the gains obtained through FBT. \nWe assume that this is due to the fact that the counterspeech often also conveys strong negative emotions that are mistaken by the model for hate speech.\n\nWe further test if this problem can be alleviated by applying the FBT strategy if hate speech is detected in $p_1$ (i.e. outside of the quotes) as depicted in Figure \\ref{fig:counterspeech-filter-fbt}. \nThe results in Table \\ref{tab:FCS-follow-up} (row FCS$_{p_1FBT}$) show that additionally applying FBT only recovers a fraction of the positive effect of FCS. \nWe assume that this is due to counterspeech including or being associated with target groups. \nThus, further research that investigates how the problem can be alleviated is needed.", "table_source": "\\begin{table}[ht]\n\\centering\n\\small\n\\begin{tabular}{lrr}\n\\hline\nstrategy & F20 & overall \\\\ \n\\hdashline\nNo strategy & 0.0 & 79.4 \\\\\nFCS & +100 & +4.6 \\\\\n\\hdashline\nFCS$_{p_1}$ & +0.0 & +0.0 \\\\\nFCS$_{p_1 FBT}$ & +6.9 & +0.3 \\\\\n\\hline\n\\end{tabular}\n\\caption{Evaluation of FCS variants. The two bottom rows display the variants adjusted for detecting Hate Speech in $p_1$. The functionality F20 contains \\textit{Denouncements of hate that quote it}. The scores are given in accuracy (\\%) and change in accuracy compared to \\textit{No strategy}.}\n\\label{tab:FCS-follow-up}\n\\end{table}", "cell_list_gold": [{"value": "0.0", "char_index": [118, 121], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck F20", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "No strategy", "model settings": {"xx": "yy"}}, {"value": "79.4", "char_index": [124, 128], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "No strategy", "model settings": {"xx": "yy"}}, {"value": "+100", "char_index": [138, 142], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck F20", "task": "zero-shot hate speech detection", "metric": ["accuracy boost", "accuracy increase"], "experimental settings": {"xx": "yy"}, "model": "FCS", "model settings": {"xx": "yy"}}, {"value": "+4.6", "char_index": [145, 149], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": ["accuracy boost", "accuracy increase"], "experimental settings": {"xx": "yy"}, "model": "FCS", "model settings": {"xx": "yy"}}, {"value": "+0.0", "char_index": [178, 182], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck F20", "task": "zero-shot hate speech detection", "metric": ["accuracy boost", "accuracy increase"], "experimental settings": {"xx": "yy"}, "model": "FCS$_{p_1}$", "model settings": {"xx": "yy"}}, {"value": "+0.0", "char_index": [185, 189], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": ["accuracy boost", "accuracy increase"], "experimental settings": {"xx": "yy"}, "model": "FCS$_{p_1}$", "model settings": {"xx": "yy"}}, {"value": "+6.9", "char_index": [211, 215], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck F20", "task": "zero-shot hate speech detection", "metric": ["accuracy boost", "accuracy increase"], "experimental settings": {"xx": "yy"}, "model": "FCS$_{p_1 FBT}$", "model settings": {"xx": "yy"}}, {"value": "+0.3", "char_index": [218, 222], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": ["accuracy boost", "accuracy increase"], "experimental settings": {"xx": "yy"}, "model": "FCS$_{p_1 FBT}$", "model settings": {"xx": "yy"}}]}, "2210.00910v1_table6": {"table_code": "\\begin{table}[h!]\n\\centering\n\\small\n\\begin{tabular}{lr}\n\\hline\nhypothesis & accuracy (\\%) \\\\ \n\\hdashline\nContaining hate speech. & 74.7 \\\\\nContains hate speech. & 78.6 \\\\\nHate speech. & 72.9 \\\\\nHateful. & 71.8 \\\\\n\\hdashline\nIt contains hate speech. & 78.7 \\\\\nIt is hateful. & 75.0 \\\\\nIt contains hate speech. & 78.7 \\\\\nIt is hate speech. & 70.8 \\\\\nIt is hateful. & 75.0 \\\\\nThat contains hate speech. & \\textbf{79.4} \\\\\nThat contains hateful content. & 78.0 \\\\\nThat example contains hateful content. & 77.8 \\\\\nThat example is hate speech. & 66.6 \\\\\nThat example is hateful. & 76.8 \\\\\nThat is hateful. & 66.6 \\\\\nThat text contains hate speech. & 78.8 \\\\\nThat text contains hateful content. & 78.6 \\\\\nThat text is hate speech. & 69.2 \\\\\nThat text is hateful. & 77.2 \\\\\nThis contains hate speech. & 79.1 \\\\\nThis contains hateful content. & 78.2 \\\\\nThis example contains hate speech. & 77.3 \\\\\nThis example contains hateful content. & 77.8 \\\\\nThis example is hate speech. & 67.2 \\\\\nThis example is hateful. & 77.4 \\\\\nThis is hateful. & 70.6 \\\\\nThis text contains hate speech. & 78.8 \\\\\nThis text contains hateful content. & 78.3 \\\\\nThis text is hate speech. & 69.5 \\\\\nThis text is hateful. & 78.7 \\\\\n\\hdashline\naverage: It & 74.8 \\\\\naverage: This & 75.7 \\\\\naverage: That & 74.9 \\\\\naverage: hateful & 75.8 \\\\\naverage: hateful content & 78.1 \\\\\naverage: hate speech & 74.5 \\\\\naverage: example & 74.4 \\\\\naverage: text & 76.1 \\\\\naverage: is & 73.9 \\\\\naverage: contain & 78.2 \\\\\n\\hdashline\nSiftNinja & 33.2 \\\\\nBERT fine-tuned on \\citet{davidson_automated_2017} & 60.2 \\\\\nBERT fine-tuned on \\citet{founta_large_2018} & 63.2 \\\\\nGoogle-Jigsaw & 76.6 \\\\\n\\hline\n\\end{tabular}\n\\caption{Full evaluation of hypotheses, that claim hate speech exists in the input text, on HateCheck.}\n\\label{tab:compare-hypotheses-full}\n\\end{table}", "table_label": "{tab:compare-hypotheses-full}", "table_numeric_cells": [["74.7", "74.7", 131, 135, 131, 135], ["78.6", "78.6", 163, 167, 163, 167], ["72.9", "72.9", 186, 190, 186, 190], ["71.8", "71.8", 205, 209, 205, 209], ["78.7", "78.7", 251, 255, 251, 255], ["75.0", "75.0", 276, 280, 276, 280], ["78.7", "78.7", 311, 315, 311, 315], ["70.8", "70.8", 340, 344, 340, 344], ["75.0", "75.0", 365, 369, 365, 369], ["79.4", "\\textbf{79.4}", 410, 414, 402, 415], ["78.0", "78.0", 452, 456, 452, 456], ["77.8", "77.8", 501, 505, 501, 505], ["66.6", "66.6", 540, 544, 540, 544], ["76.8", "76.8", 575, 579, 575, 579], ["66.6", "66.6", 602, 606, 602, 606], ["78.8", "78.8", 644, 648, 644, 648], ["78.6", "78.6", 690, 694, 690, 694], ["69.2", "69.2", 726, 730, 726, 730], ["77.2", "77.2", 758, 762, 758, 762], ["79.1", "79.1", 795, 799, 795, 799], ["78.2", "78.2", 836, 840, 836, 840], ["77.3", "77.3", 881, 885, 881, 885], ["77.8", "77.8", 930, 934, 930, 934], ["67.2", "67.2", 969, 973, 969, 973], ["77.4", "77.4", 1004, 1008, 1004, 1008], ["70.6", "70.6", 1031, 1035, 1031, 1035], ["78.8", "78.8", 1073, 1077, 1073, 1077], ["78.3", "78.3", 1119, 1123, 1119, 1123], ["69.5", "69.5", 1155, 1159, 1155, 1159], ["78.7", "78.7", 1187, 1191, 1187, 1191], ["74.8", "74.8", 1220, 1224, 1220, 1224], ["75.7", "75.7", 1244, 1248, 1244, 1248], ["74.9", "74.9", 1268, 1272, 1268, 1272], ["75.8", "75.8", 1295, 1299, 1295, 1299], ["78.1", "78.1", 1330, 1334, 1330, 1334], ["74.5", "74.5", 1361, 1365, 1361, 1365], ["74.4", "74.4", 1388, 1392, 1388, 1392], ["76.1", "76.1", 1412, 1416, 1412, 1416], ["73.9", "73.9", 1434, 1438, 1434, 1438], ["78.2", "78.2", 1461, 1465, 1461, 1465], ["33.2", "33.2", 1492, 1496, 1492, 1496], ["60.2", "60.2", 1553, 1557, 1553, 1557], ["63.2", "63.2", 1608, 1612, 1608, 1612], ["76.6", "76.6", 1632, 1636, 1632, 1636]], "text_chunk_selected": "In contrast, NLI-based prediction in which a target task is converted to an NLI-task and fed into an NLI model converts the target task to the fine-tuning task. \nHere, a model is given a premise and a hypothesis and tasked to predict if the premise entails the hypothesis, contradicts it, or is neutral towards it.  \n\\citet{yin_benchmarking_2019} proposed to use an NLI model for zero-shot topic classification, by inputting the text to classify as the premise and constructing for each topic a hypothesis of the form \\enquote{This text is about $<$topic$>$}. \nThey map the labels \\textit{neutral} and \\textit{contradiction} to \\textit{not-entailment}. We can then interpret a prediction of entailment as predicting that the input text belongs to the topic in the given hypothesis.\nConversely, \\textit{not-entailment} implies that the text is not about the topic.\n\\citet{wang_entailment_2021} show for a range of tasks, including offensive language identification, that this task re-formulation also benefits few-shot learning scenarios. \nRecently, \\citet{alkhamissi_token_2022} obtained large performance improvements in few-shot learning for hate speech detection by (1) decomposing the task into four subtasks and (2) additionally training the few-shot model on a knowledge base.\n\n\\paragraph{Experiment setup} To test if an input text contains hate speech, we need a hypothesis expressing that claim. \nHowever, there are many ways how the claim, that a given text contains hate speech, can be expressed. Choosing a sub-optimal way to express this claim will result in lower accuracy. \\citet{wang_entailment_2021} already tested four different hypotheses for hate speech or offensive language. We conduct an extensive evaluation by constructing and testing all grammatically correct sentences built with the following building blocks: \\textit{It/That/This + example/text + contains/is + hate speech/hateful/hateful content}. \nWe conduct all experiments with a BART-large model \\cite{lewis_bart_2020} that was fine-tuned on the \\textbf{M}ulti-Genre \\textbf{N}atural \\textbf{L}anguage \\textbf{I}nference dataset (MNLI) \\cite{williams_broad-coverage_2018} and has been made available via the Huggingface transformers library \\cite{wolf_transformers_2020} as \\texttt{bart-large-mnli}. \nThis model predicts either \\textit{contradiction}, \\textit{neutral}, or \\textit{entailment}. \nWe follow the recommendation of the model creators to ignore the logits for \\textit{neutral} and perform a softmax over the logits of \\textit{contradiction} and \\textit{entailment}. If the probability for entailment is equal or higher than 0.5 we consider this a prediction of \\textit{entailment} and thus \\textit{hate speech}.\\footnote{This procedure is equal to taking the argmax over \\textit{contradiction} and \\textit{entailment}.}\nWe evaluate on HateCheck since the functionalities in this dataset allow for an automatic in-depth error analysis and compare our results to the baselines provided by \\citet{rottger_hatecheck_2021}.\n\n\\paragraph{Results} Table \\ref{tab:compare-hypotheses} shows an abbreviated version of the results. The full results are given in Appendix \\ref{appsec:zero-shot-results-compare-hypos}.\nThe hypothesis \\enquote{That contains hate speech.} obtains the highest accuracy and beats the Google-Jigsaw API by 2.8pp. This is remarkable, since we can assume that the commercial systems were all trained to detect hateful content or hate speech, while this model has not been trained on a single example of hate speech detection or a similar task.\nThe two lowest scoring hypotheses lead to an accuracy of 66.6\\% meaning that an unlucky choice of hypothesis can cost more than 12pp accuracy. \n\nIn this section, we present four methods, which we call strategies, that aim to improve zero-shot hate speech detection. A strategy has the following components and structure: The aim is to assign a label $y=\\{0, 1\\}$ to input text $t$, where $1$ corresponds to the class \\textit{hate speech} and $0$ corresponds to the class \\textit{not-hate speech}.\nThe input text $t$ can be used in one or multiple a premises $p_0$ to $p_m$, that are used in conjunction with the main hypothesis $h_0$ and one or multiple supporting hypotheses $\\left[h_1, ..., h_n\\right]$ to obtain  NLI model predictions $m(p_i, h_j) \\in \\{0,1\\}$ where 0 corresponds to contradiction and $1$ corresponds to entailment.\nThe variables $i$ and $j$ are defined as: $i \\in \\left[0, ..., m\\right]$ and $j \\in \\left[0, ..., n\\right]$. \nThe rules for how to combine model predictions to obtain the final label $y$ are given by the individual strategies.\nAs the main hypothesis we use \\enquote{That contains hate speech.}, since it lead to the highest accuracy on HateCheck in Section \\ref{sec:evaluating-standard-0shot}.\nThe supporting hypotheses used to implement the strategies are listed in Table \\ref{tab:list-of-hypotheses}.\n\nOur zero-shot model wrongly classifies all examples of counterspeech that quote or reference hate speech as actual hate speech. \nReferences to hate speech without quotation marks are hard to identify.\nThus, for this work, we limit ourselves to counterspeech that quotes hate speech explicitly. \nWe propose a three-stage strategy to this phenomenon: (1) quotation identification, (2) hate speech classification of the quoted content, (3) detecting the stance of the post towards the quoted content. \nFormally, the input text $t$ is divided into premise $p_0$ which contains the quoted text and premise $p_1$ which contains the text around the quotes. The quoted text is represented as ``$\\left[X\\right]$'' in $p_1$. \nUsing the main hypothesis $h_0$ we predict if $p_0$ contains hate speech or not. We use the supporting hypothesis \\enquote{This text supports $\\left[X\\right].$} ($h_1$) to predict the stance of $p_1$ towards $p_0$. \nIf $p_0$ contains hate speech and $p_1$ has a supportive stance towards $p_0$, $t$ is classified as \\textit{hate speech}, otherwise it is classified as \\textit{not-hate speech}.\nThe strategy is depicted in Figure \\ref{fig:counterspeech-filter}.\n\nAs shown in Table \\ref{tab:full-error-analysis}, slurs that are reclaimed by members of a targeted group are often miss-classified as hate speech. Based on the observation that a reclaimed slur is often ascribed to oneself, we propose to use a supporting hypothesis that indicates if text is self-directed.\\footnote{Of course there are counterexamples to this rule, where reclaimed slurs are directed to others and not oneself. However, as long this approximation, as crude as it may be, helps to reduce false positives, it is a useful approximation.}\nIf the model predicts self-directedness a potential prediction of \\textit{hate speech} is overridden to \\textit{not-hate speech}. \n\nTable \\ref{tab:compare-hypotheses-full}, the extended version of Table \\ref{tab:compare-hypotheses}, contains all results for comparing hypotheses for zero-shot hate speech detection on HateCheck. \\enquote{average:$<$ \\texttt{expression}$>$} refers to the average accuracy of all hypotheses containing \\texttt{expression}. The highest accuracy is in bold. \n\nThe results show that in many cases the detection of a mentioned group is surprisingly accurate. The difference in accuracy between the best performing hypothesis and the worst performing hypothesis does not exceed 12\\%. This is a similar range to the differences found between hypotheses when testing if a text contains hate speech (see Table \\ref{tab:compare-hypotheses} and Table \\ref{tab:compare-hypotheses-full}). However, when looking at $F_1$ scores the differences are much larger, with more general terms, such as \\textit{faith} or \\textit{ethnicity} preforming worse than the specific terms \\textit{Muslims} and \\textit{black people}.", "table_source": "\\begin{table}[h!]\n\\centering\n\\small\n\\begin{tabular}{lr}\n\\hline\nhypothesis & accuracy (\\%) \\\\ \n\\hdashline\nContaining hate speech. & 74.7 \\\\\nContains hate speech. & 78.6 \\\\\nHate speech. & 72.9 \\\\\nHateful. & 71.8 \\\\\n\\hdashline\nIt contains hate speech. & 78.7 \\\\\nIt is hateful. & 75.0 \\\\\nIt contains hate speech. & 78.7 \\\\\nIt is hate speech. & 70.8 \\\\\nIt is hateful. & 75.0 \\\\\nThat contains hate speech. & \\textbf{79.4} \\\\\nThat contains hateful content. & 78.0 \\\\\nThat example contains hateful content. & 77.8 \\\\\nThat example is hate speech. & 66.6 \\\\\nThat example is hateful. & 76.8 \\\\\nThat is hateful. & 66.6 \\\\\nThat text contains hate speech. & 78.8 \\\\\nThat text contains hateful content. & 78.6 \\\\\nThat text is hate speech. & 69.2 \\\\\nThat text is hateful. & 77.2 \\\\\nThis contains hate speech. & 79.1 \\\\\nThis contains hateful content. & 78.2 \\\\\nThis example contains hate speech. & 77.3 \\\\\nThis example contains hateful content. & 77.8 \\\\\nThis example is hate speech. & 67.2 \\\\\nThis example is hateful. & 77.4 \\\\\nThis is hateful. & 70.6 \\\\\nThis text contains hate speech. & 78.8 \\\\\nThis text contains hateful content. & 78.3 \\\\\nThis text is hate speech. & 69.5 \\\\\nThis text is hateful. & 78.7 \\\\\n\\hdashline\naverage: It & 74.8 \\\\\naverage: This & 75.7 \\\\\naverage: That & 74.9 \\\\\naverage: hateful & 75.8 \\\\\naverage: hateful content & 78.1 \\\\\naverage: hate speech & 74.5 \\\\\naverage: example & 74.4 \\\\\naverage: text & 76.1 \\\\\naverage: is & 73.9 \\\\\naverage: contain & 78.2 \\\\\n\\hdashline\nSiftNinja & 33.2 \\\\\nBERT fine-tuned on \\citet{davidson_automated_2017} & 60.2 \\\\\nBERT fine-tuned on \\citet{founta_large_2018} & 63.2 \\\\\nGoogle-Jigsaw & 76.6 \\\\\n\\hline\n\\end{tabular}\n\\caption{Full evaluation of hypotheses, that claim hate speech exists in the input text, on HateCheck.}\n\\label{tab:compare-hypotheses-full}\n\\end{table}", "cell_list_gold": [{"value": "74.7", "char_index": [131, 135], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Containing hate speech.", "model settings": {"xx": "yy"}}, {"value": "78.6", "char_index": [163, 167], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Contains hate speech.", "model settings": {"xx": "yy"}}, {"value": "72.9", "char_index": [186, 190], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Hate speech.", "model settings": {"xx": "yy"}}, {"value": "71.8", "char_index": [205, 209], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Hateful.", "model settings": {"xx": "yy"}}, {"value": "78.7", "char_index": [251, 255], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "It contains hate speech.", "model settings": {"xx": "yy"}}, {"value": "75.0", "char_index": [276, 280], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "It is hateful.", "model settings": {"xx": "yy"}}, {"value": "78.7", "char_index": [311, 315], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "It contains hate speech.", "model settings": {"xx": "yy"}}, {"value": "70.8", "char_index": [340, 344], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "It is hate speech.", "model settings": {"xx": "yy"}}, {"value": "75.0", "char_index": [365, 369], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "It is hateful.", "model settings": {"xx": "yy"}}, {"value": "79.4", "char_index": [410, 414], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That contains hate speech.", "model settings": {"xx": "yy"}}, {"value": "78.0", "char_index": [452, 456], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That contains hateful content.", "model settings": {"xx": "yy"}}, {"value": "77.8", "char_index": [501, 505], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That example contains hateful content.", "model settings": {"xx": "yy"}}, {"value": "66.6", "char_index": [540, 544], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That example is hate speech.", "model settings": {"xx": "yy"}}, {"value": "76.8", "char_index": [575, 579], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That example is hateful.", "model settings": {"xx": "yy"}}, {"value": "66.6", "char_index": [602, 606], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That is hateful.", "model settings": {"xx": "yy"}}, {"value": "78.8", "char_index": [644, 648], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That text contains hate speech.", "model settings": {"xx": "yy"}}, {"value": "78.6", "char_index": [690, 694], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That text contains hateful content.", "model settings": {"xx": "yy"}}, {"value": "69.2", "char_index": [726, 730], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That text is hate speech.", "model settings": {"xx": "yy"}}, {"value": "77.2", "char_index": [758, 762], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That text is hateful.", "model settings": {"xx": "yy"}}, {"value": "79.1", "char_index": [795, 799], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This contains hate speech.", "model settings": {"xx": "yy"}}, {"value": "78.2", "char_index": [836, 840], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This contains hateful content.", "model settings": {"xx": "yy"}}, {"value": "77.3", "char_index": [881, 885], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This example contains hate speech.", "model settings": {"xx": "yy"}}, {"value": "77.8", "char_index": [930, 934], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This example contains hateful content.", "model settings": {"xx": "yy"}}, {"value": "67.2", "char_index": [969, 973], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This example is hate speech.", "model settings": {"xx": "yy"}}, {"value": "77.4", "char_index": [1004, 1008], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This example is hateful.", "model settings": {"xx": "yy"}}, {"value": "70.6", "char_index": [1031, 1035], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This is hateful.", "model settings": {"xx": "yy"}}, {"value": "78.8", "char_index": [1073, 1077], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This text contains hate speech.", "model settings": {"xx": "yy"}}, {"value": "78.3", "char_index": [1119, 1123], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This text contains hateful content.", "model settings": {"xx": "yy"}}, {"value": "69.5", "char_index": [1155, 1159], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This text is hate speech.", "model settings": {"xx": "yy"}}, {"value": "78.7", "char_index": [1187, 1191], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This text is hateful.", "model settings": {"xx": "yy"}}, {"value": "74.8", "char_index": [1220, 1224], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: It", "model settings": {"xx": "yy"}}, {"value": "75.7", "char_index": [1244, 1248], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: This", "model settings": {"xx": "yy"}}, {"value": "74.9", "char_index": [1268, 1272], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: That", "model settings": {"xx": "yy"}}, {"value": "75.8", "char_index": [1295, 1299], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: hateful", "model settings": {"xx": "yy"}}, {"value": "78.1", "char_index": [1330, 1334], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: hateful content", "model settings": {"xx": "yy"}}, {"value": "74.5", "char_index": [1361, 1365], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: hate speech", "model settings": {"xx": "yy"}}, {"value": "74.4", "char_index": [1388, 1392], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: example", "model settings": {"xx": "yy"}}, {"value": "76.1", "char_index": [1412, 1416], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: text", "model settings": {"xx": "yy"}}, {"value": "73.9", "char_index": [1434, 1438], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: is", "model settings": {"xx": "yy"}}, {"value": "78.2", "char_index": [1461, 1465], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "average: contain", "model settings": {"xx": "yy"}}, {"value": "33.2", "char_index": [1492, 1496], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "SiftNinja", "model settings": {"xx": "yy"}}, {"value": "60.2", "char_index": [1553, 1557], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT fine-tuned on \\citet{davidson_automated_2017}", "model settings": {"xx": "yy"}}, {"value": "63.2", "char_index": [1608, 1612], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT fine-tuned on \\citet{founta_large_2018}", "model settings": {"xx": "yy"}}, {"value": "76.6", "char_index": [1632, 1636], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Google-Jigsaw", "model settings": {"xx": "yy"}}]}, "2210.00910v1_table9": {"table_code": "\\begin{table*}[t]\n\\small\n\\centering\n\\begin{tabular}{l|rrrr}\nhypothesis & \\multicolumn{1}{l}{accuracy (\\%)} & \\multicolumn{1}{l}{$\\downarrow$ $F_1$ (\\%)} & \\multicolumn{1}{l}{recall (\\%)} & \\multicolumn{1}{l}{precision (\\%)} \\\\ \\hline\nThat example is about immigrants.      & 97.8 & 91.2 & 92.4 & 89.9 \\\\\nThis example is about immigrants.      & 97.7 & 90.8 & 92.4 & 89.2 \\\\\nThat is about immigrants.              & 97.2 & 88.8 & 89.2 & 88.4 \\\\\nThat text is about immigrants.         & 97.0 & 88.6 & 92.2 & 85.2 \\\\\nThis text is about immigrants.         & 96.3 & 86.4 & 93.7 & 80.1 \\\\\nThis is about immigrants.              & 96.4 & 86.2 & 91.6 & 81.4 \\\\\nThis text is about national origin.    & 77.7 & 42.2 & 65.4 & 31.1 \\\\\nThat text is about national origin.    & 78.0 & 41.3 & 62.4 & 30.9 \\\\\nThis is about national origin.         & 83.4 & 37.0 & 39.3 & 35.0 \\\\\nThat example is about national origin. & 81.4 & 30.0 & 32.2 & 28.1 \\\\\nThis example is about national origin. & 79.8 & 30.0 & 34.8 & 26.3 \\\\\nThat is about national origin.         & 86.5 & 24.6 & 17.7 & 40.2\n\\end{tabular}\n\\caption{Results for supporting hypotheses aimed at detecting mentions of immigrants. }\n\\label{tab:results-immigrants}\n\\end{table*}", "table_label": "{tab:results-immigrants}", "table_numeric_cells": [["97.8", "97.8", 275, 279, 275, 279], ["91.2", "91.2", 282, 286, 282, 286], ["92.4", "92.4", 289, 293, 289, 293], ["89.9", "89.9", 296, 300, 296, 300], ["97.7", "97.7", 345, 349, 345, 349], ["90.8", "90.8", 352, 356, 352, 356], ["92.4", "92.4", 359, 363, 359, 363], ["89.2", "89.2", 366, 370, 366, 370], ["97.2", "97.2", 415, 419, 415, 419], ["88.8", "88.8", 422, 426, 422, 426], ["89.2", "89.2", 429, 433, 429, 433], ["88.4", "88.4", 436, 440, 436, 440], ["97.0", "97.0", 485, 489, 485, 489], ["88.6", "88.6", 492, 496, 492, 496], ["92.2", "92.2", 499, 503, 499, 503], ["85.2", "85.2", 506, 510, 506, 510], ["96.3", "96.3", 555, 559, 555, 559], ["86.4", "86.4", 562, 566, 562, 566], ["93.7", "93.7", 569, 573, 569, 573], ["80.1", "80.1", 576, 580, 576, 580], ["96.4", "96.4", 625, 629, 625, 629], ["86.2", "86.2", 632, 636, 632, 636], ["91.6", "91.6", 639, 643, 639, 643], ["81.4", "81.4", 646, 650, 646, 650], ["77.7", "77.7", 695, 699, 695, 699], ["42.2", "42.2", 702, 706, 702, 706], ["65.4", "65.4", 709, 713, 709, 713], ["31.1", "31.1", 716, 720, 716, 720], ["78.0", "78.0", 765, 769, 765, 769], ["41.3", "41.3", 772, 776, 772, 776], ["62.4", "62.4", 779, 783, 779, 783], ["30.9", "30.9", 786, 790, 786, 790], ["83.4", "83.4", 835, 839, 835, 839], ["37.0", "37.0", 842, 846, 842, 846], ["39.3", "39.3", 849, 853, 849, 853], ["35.0", "35.0", 856, 860, 856, 860], ["81.4", "81.4", 905, 909, 905, 909], ["30.0", "30.0", 912, 916, 912, 916], ["32.2", "32.2", 919, 923, 919, 923], ["28.1", "28.1", 926, 930, 926, 930], ["79.8", "79.8", 975, 979, 975, 979], ["30.0", "30.0", 982, 986, 982, 986], ["34.8", "34.8", 989, 993, 989, 993], ["26.3", "26.3", 996, 1000, 996, 1000], ["86.5", "86.5", 1045, 1049, 1045, 1049], ["24.6", "24.6", 1052, 1056, 1052, 1056], ["17.7", "17.7", 1059, 1063, 1059, 1063], ["40.2", "40.2", 1066, 1070, 1066, 1070]], "text_chunk_selected": "In contrast, NLI-based prediction in which a target task is converted to an NLI-task and fed into an NLI model converts the target task to the fine-tuning task. \nHere, a model is given a premise and a hypothesis and tasked to predict if the premise entails the hypothesis, contradicts it, or is neutral towards it.  \n\\citet{yin_benchmarking_2019} proposed to use an NLI model for zero-shot topic classification, by inputting the text to classify as the premise and constructing for each topic a hypothesis of the form \\enquote{This text is about $<$topic$>$}. \nThey map the labels \\textit{neutral} and \\textit{contradiction} to \\textit{not-entailment}. We can then interpret a prediction of entailment as predicting that the input text belongs to the topic in the given hypothesis.\nConversely, \\textit{not-entailment} implies that the text is not about the topic.\n\\citet{wang_entailment_2021} show for a range of tasks, including offensive language identification, that this task re-formulation also benefits few-shot learning scenarios. \nRecently, \\citet{alkhamissi_token_2022} obtained large performance improvements in few-shot learning for hate speech detection by (1) decomposing the task into four subtasks and (2) additionally training the few-shot model on a knowledge base.\n\n\\paragraph{Experiment setup} To test if an input text contains hate speech, we need a hypothesis expressing that claim. \nHowever, there are many ways how the claim, that a given text contains hate speech, can be expressed. Choosing a sub-optimal way to express this claim will result in lower accuracy. \\citet{wang_entailment_2021} already tested four different hypotheses for hate speech or offensive language. We conduct an extensive evaluation by constructing and testing all grammatically correct sentences built with the following building blocks: \\textit{It/That/This + example/text + contains/is + hate speech/hateful/hateful content}. \nWe conduct all experiments with a BART-large model \\cite{lewis_bart_2020} that was fine-tuned on the \\textbf{M}ulti-Genre \\textbf{N}atural \\textbf{L}anguage \\textbf{I}nference dataset (MNLI) \\cite{williams_broad-coverage_2018} and has been made available via the Huggingface transformers library \\cite{wolf_transformers_2020} as \\texttt{bart-large-mnli}. \nThis model predicts either \\textit{contradiction}, \\textit{neutral}, or \\textit{entailment}. \nWe follow the recommendation of the model creators to ignore the logits for \\textit{neutral} and perform a softmax over the logits of \\textit{contradiction} and \\textit{entailment}. If the probability for entailment is equal or higher than 0.5 we consider this a prediction of \\textit{entailment} and thus \\textit{hate speech}.\\footnote{This procedure is equal to taking the argmax over \\textit{contradiction} and \\textit{entailment}.}\nWe evaluate on HateCheck since the functionalities in this dataset allow for an automatic in-depth error analysis and compare our results to the baselines provided by \\citet{rottger_hatecheck_2021}.\n\n\\paragraph{Results} Table \\ref{tab:compare-hypotheses} shows an abbreviated version of the results. The full results are given in Appendix \\ref{appsec:zero-shot-results-compare-hypos}.\nThe hypothesis \\enquote{That contains hate speech.} obtains the highest accuracy and beats the Google-Jigsaw API by 2.8pp. This is remarkable, since we can assume that the commercial systems were all trained to detect hateful content or hate speech, while this model has not been trained on a single example of hate speech detection or a similar task.\nThe two lowest scoring hypotheses lead to an accuracy of 66.6\\% meaning that an unlucky choice of hypothesis can cost more than 12pp accuracy. \n\nIn this section, we present four methods, which we call strategies, that aim to improve zero-shot hate speech detection. A strategy has the following components and structure: The aim is to assign a label $y=\\{0, 1\\}$ to input text $t$, where $1$ corresponds to the class \\textit{hate speech} and $0$ corresponds to the class \\textit{not-hate speech}.\nThe input text $t$ can be used in one or multiple a premises $p_0$ to $p_m$, that are used in conjunction with the main hypothesis $h_0$ and one or multiple supporting hypotheses $\\left[h_1, ..., h_n\\right]$ to obtain  NLI model predictions $m(p_i, h_j) \\in \\{0,1\\}$ where 0 corresponds to contradiction and $1$ corresponds to entailment.\nThe variables $i$ and $j$ are defined as: $i \\in \\left[0, ..., m\\right]$ and $j \\in \\left[0, ..., n\\right]$. \nThe rules for how to combine model predictions to obtain the final label $y$ are given by the individual strategies.\nAs the main hypothesis we use \\enquote{That contains hate speech.}, since it lead to the highest accuracy on HateCheck in Section \\ref{sec:evaluating-standard-0shot}.\nThe supporting hypotheses used to implement the strategies are listed in Table \\ref{tab:list-of-hypotheses}.\n\n\\subsection{Filtering By Target (FBT)}\n\\label{subsec:filtering-by-target}\nThe error analysis showed that we can improve zero-shot classification accuracy significantly by avoiding predictions of hate speech where no relevant target group occurs. We thus propose to avoid false positives by constructing a set of supporting hypotheses $\\left[h_1, ..., h_n\\right]$ to predict if text $t$ actually targets or mentions a protected group or characteristic. If no protected group or characteristic is predicted to occur in $t$, a potential prediction of \\textit{hate speech} is overridden to \\textit{not-hate speech}.\nFigure \\ref{fig:filtering-by-target} illustrates the method. \n\nTable \\ref{tab:compare-hypotheses-full}, the extended version of Table \\ref{tab:compare-hypotheses}, contains all results for comparing hypotheses for zero-shot hate speech detection on HateCheck. \\enquote{average:$<$ \\texttt{expression}$>$} refers to the average accuracy of all hypotheses containing \\texttt{expression}. The highest accuracy is in bold. \n\nWe use the same model and as in the previous zero-shot experiments for evaluation and test the performance for detecting mentions for all protected groups in HateCheck. We additionally test the detection of the supercategory \\textit{queer people} covering the two protected groups \\textit{gay people} and \\textit{transgender people} in HateCheck. When testing if a text revolves around gender, we treat both \\textit{women} and \\textit{transgender people} as positive classes and all other protected groups as negative classes. While this mapping obviously can result in incorrect labels (a text can be about gender even if another group is targeted), we assume that it holds true for examples in the HateCheck dataset.\n\nTable \\ref{tab:results-black-people} shows the results for detecting if \\textit{black people} are mentioned, Table \\ref{tab:results-muslim-people} for mentions of \\textit{Muslims}, Table \\ref{tab:results-immigrants} for mentions of immigrants, Table \\ref{tab:results-disabled-people} for mentions of \\textit{disabled people}, Table \\ref{tab:results-gay-people} for mentions of \\textit{gay people}, Table \\ref{tab:results-transgender-people} for mentions of \\textit{transgender people}, Table \\ref{tab:results-queer-people} for mentions of \\textit{queer people}, and Table \\ref{tab:results-gender} for detecting if a text is about \\textit{gender}.", "table_source": "\\begin{table*}[t]\n\\small\n\\centering\n\\begin{tabular}{l|rrrr}\nhypothesis & \\multicolumn{1}{l}{accuracy (\\%)} & \\multicolumn{1}{l}{$\\downarrow$ $F_1$ (\\%)} & \\multicolumn{1}{l}{recall (\\%)} & \\multicolumn{1}{l}{precision (\\%)} \\\\ \\hline\nThat example is about immigrants.      & 97.8 & 91.2 & 92.4 & 89.9 \\\\\nThis example is about immigrants.      & 97.7 & 90.8 & 92.4 & 89.2 \\\\\nThat is about immigrants.              & 97.2 & 88.8 & 89.2 & 88.4 \\\\\nThat text is about immigrants.         & 97.0 & 88.6 & 92.2 & 85.2 \\\\\nThis text is about immigrants.         & 96.3 & 86.4 & 93.7 & 80.1 \\\\\nThis is about immigrants.              & 96.4 & 86.2 & 91.6 & 81.4 \\\\\nThis text is about national origin.    & 77.7 & 42.2 & 65.4 & 31.1 \\\\\nThat text is about national origin.    & 78.0 & 41.3 & 62.4 & 30.9 \\\\\nThis is about national origin.         & 83.4 & 37.0 & 39.3 & 35.0 \\\\\nThat example is about national origin. & 81.4 & 30.0 & 32.2 & 28.1 \\\\\nThis example is about national origin. & 79.8 & 30.0 & 34.8 & 26.3 \\\\\nThat is about national origin.         & 86.5 & 24.6 & 17.7 & 40.2\n\\end{tabular}\n\\caption{Results for supporting hypotheses aimed at detecting mentions of immigrants. }\n\\label{tab:results-immigrants}\n\\end{table*}", "cell_list_gold": [{"value": "97.8", "char_index": [275, 279], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That example is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "91.2", "char_index": [282, 286], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That example is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "92.4", "char_index": [289, 293], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That example is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "89.9", "char_index": [296, 300], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That example is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "97.7", "char_index": [345, 349], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This example is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "90.8", "char_index": [352, 356], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This example is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "92.4", "char_index": [359, 363], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This example is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "89.2", "char_index": [366, 370], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This example is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "97.2", "char_index": [415, 419], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "88.8", "char_index": [422, 426], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "89.2", "char_index": [429, 433], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "88.4", "char_index": [436, 440], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "97.0", "char_index": [485, 489], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That text is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "88.6", "char_index": [492, 496], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That text is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "92.2", "char_index": [499, 503], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That text is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "85.2", "char_index": [506, 510], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That text is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "96.3", "char_index": [555, 559], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This text is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "86.4", "char_index": [562, 566], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This text is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "93.7", "char_index": [569, 573], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This text is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "80.1", "char_index": [576, 580], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This text is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "96.4", "char_index": [625, 629], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "86.2", "char_index": [632, 636], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "91.6", "char_index": [639, 643], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "81.4", "char_index": [646, 650], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This is about immigrants.", "model settings": {"xx": "yy"}}, {"value": "77.7", "char_index": [695, 699], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This text is about national origin.", "model settings": {"xx": "yy"}}, {"value": "42.2", "char_index": [702, 706], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This text is about national origin.", "model settings": {"xx": "yy"}}, {"value": "65.4", "char_index": [709, 713], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This text is about national origin.", "model settings": {"xx": "yy"}}, {"value": "31.1", "char_index": [716, 720], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This text is about national origin.", "model settings": {"xx": "yy"}}, {"value": "78.0", "char_index": [765, 769], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That text is about national origin.", "model settings": {"xx": "yy"}}, {"value": "41.3", "char_index": [772, 776], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That text is about national origin.", "model settings": {"xx": "yy"}}, {"value": "62.4", "char_index": [779, 783], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That text is about national origin.", "model settings": {"xx": "yy"}}, {"value": "30.9", "char_index": [786, 790], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That text is about national origin.", "model settings": {"xx": "yy"}}, {"value": "83.4", "char_index": [835, 839], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This is about national origin.", "model settings": {"xx": "yy"}}, {"value": "37.0", "char_index": [842, 846], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This is about national origin.", "model settings": {"xx": "yy"}}, {"value": "39.3", "char_index": [849, 853], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This is about national origin.", "model settings": {"xx": "yy"}}, {"value": "35.0", "char_index": [856, 860], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This is about national origin.", "model settings": {"xx": "yy"}}, {"value": "81.4", "char_index": [905, 909], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That example is about national origin.", "model settings": {"xx": "yy"}}, {"value": "30.0", "char_index": [912, 916], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That example is about national origin.", "model settings": {"xx": "yy"}}, {"value": "32.2", "char_index": [919, 923], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That example is about national origin.", "model settings": {"xx": "yy"}}, {"value": "28.1", "char_index": [926, 930], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That example is about national origin.", "model settings": {"xx": "yy"}}, {"value": "79.8", "char_index": [975, 979], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "This example is about national origin.", "model settings": {"xx": "yy"}}, {"value": "30.0", "char_index": [982, 986], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "This example is about national origin.", "model settings": {"xx": "yy"}}, {"value": "34.8", "char_index": [989, 993], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "This example is about national origin.", "model settings": {"xx": "yy"}}, {"value": "26.3", "char_index": [996, 1000], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "This example is about national origin.", "model settings": {"xx": "yy"}}, {"value": "86.5", "char_index": [1045, 1049], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "That is about national origin.", "model settings": {"xx": "yy"}}, {"value": "24.6", "char_index": [1052, 1056], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": ["F_1", "F1"], "experimental settings": {"xx": "yy"}, "model": "That is about national origin.", "model settings": {"xx": "yy"}}, {"value": "17.7", "char_index": [1059, 1063], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "recall", "experimental settings": {"xx": "yy"}, "model": "That is about national origin.", "model settings": {"xx": "yy"}}, {"value": "40.2", "char_index": [1066, 1070], "type": "Result", "training data/set": "HateCheck", "test data/set": "HateCheck immigrants", "task": "zero-shot hate speech detection", "metric": "precision", "experimental settings": {"xx": "yy"}, "model": "That is about national origin.", "model settings": {"xx": "yy"}}]}, "2210.00912v1_table0": {"table_code": "\\begin{table*}[t]\n\\centering\n\\caption{Accuracy comparison of image recognition on the PACS and Office-Home dataset, each single letter column represents an unseen target client. Our CCST with the overall domain style (K=3) outperforms other methods. We use FedAvg as our base FL framework. Jigen, RSC, and Mixstyle are applied within each client. The backbone networks utilized in PACS and Office-Home are ImageNet-pretrained ResNet50 and ResNet18 respectively. \\textcolor{Gray}{(\\dag: Since COPA did not release their code, we copy the results from their paper here. But it cannot directly compare with our results due to the setting difference.)}}\n\\vspace{-2mm}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{c@{\\hspace{3mm}}ccccc|ccccc|c}\n\\toprule\n\\multirow{4}{*}{Method} & \\multicolumn{5}{c}{PACS}                                & \\multicolumn{5}{c}{Office-Home}                         & \\multirow{4}{*}{Avg.} \\\\ \\cmidrule(lr){2-11} \n& \\multicolumn{1}{c}{P}         & A         & C         & S         & \\multicolumn{1}{c}{Avg.}     & A         & C         & P        & R        & \\multicolumn{1}{c}{Avg.}       &         \\\\ \\midrule\nFedAvg (AISTATS'17)~\\cite{mcmahan2017communication}& 95.51& 82.23& 78.20& 73.56& 82.37 \n& 60.08 & 45.59 & 69.48& 72.82& 61.99&72.18\\\\\nJigen (CVPR'19)~\\cite{carlucci2019domain}& 95.99& 84.72& 77.09& 72.16  & {82.49} \n&  60.29 & 46.16 & 69.26 & 72.59& 62.07&72.28\\\\\nRSC (ECCV'20)~\\cite{huang2020self}& 95.21& 83.15& 78.24& 74.62&{82.81}\n& 58.23 & 46.05  & 70.27 & \\textbf{73.39}&61.99&72.40\\\\\nMixStyle (ICLR'21)~\\cite{zhou2021domain}& 95.93& 85.99& \\textbf{80.03}& 75.46 & {84.35}\n& 58.44 & \\textbf{50.29} & {70.61} & 70.64 & 62.49&73.42\\\\\nFedDG (CVPR'21)~\\cite{liu2021feddg}& 96.23& 83.94& 79.27& 73.30& {83.19} \n & \\textbf{60.70} & 45.82 & {71.51} & {73.05}& 62.77&72.98\\\\\n \\textcolor{Gray}{COPA-Res18\\dag\\ (ICCV'21)~\\cite{Wu_2021_ICCV}}& \\textcolor{Gray}{94.60} & \\textcolor{Gray}{83.30}& \\textcolor{Gray}{79.80}& \\textcolor{Gray}{82.50}& \\textcolor{Gray}{85.10} & \\textcolor{Gray}{59.40}& \\textcolor{Gray}{55.10}& \\textcolor{Gray}{74.80}& \\textcolor{Gray}{75.00}& \\textcolor{Gray}{66.10} & \\textcolor{Gray}{75.60}\\\\\nCCST (Overall, K=3) & \\textbf{96.65}& \\textbf{88.33}& 78.20 & \\textbf{82.90} & \\textbf{86.52}\n & {59.05} & {50.06} & \\textbf{72.97}& {71.67}& \\textbf{63.56} & \\textbf{75.04} \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{table:all_results}\n\\end{table*}", "table_label": "{table:all_results}", "table_numeric_cells": [["95.51", "95.51", 1188, 1193, 1188, 1193], ["82.23", "82.23", 1195, 1200, 1195, 1200], ["78.20", "78.20", 1202, 1207, 1202, 1207], ["73.56", "73.56", 1209, 1214, 1209, 1214], ["82.37", "82.37", 1216, 1221, 1216, 1221], ["60.08", "60.08", 1225, 1230, 1225, 1230], ["45.59", "45.59", 1233, 1238, 1233, 1238], ["69.48", "69.48", 1241, 1246, 1241, 1246], ["72.82", "72.82", 1248, 1253, 1248, 1253], ["61.99", "61.99", 1255, 1260, 1255, 1260], ["72.18", "72.18", 1261, 1266, 1261, 1266], ["95.99", "95.99", 1312, 1317, 1312, 1317], ["84.72", "84.72", 1319, 1324, 1319, 1324], ["77.09", "77.09", 1326, 1331, 1326, 1331], ["72.16", "72.16", 1333, 1338, 1333, 1338], ["82.49", "{82.49}", 1343, 1348, 1342, 1349], ["60.29", "60.29", 1354, 1359, 1354, 1359], ["46.16", "46.16", 1362, 1367, 1362, 1367], ["69.26", "69.26", 1370, 1375, 1370, 1375], ["72.59", "72.59", 1378, 1383, 1378, 1383], ["62.07", "62.07", 1385, 1390, 1385, 1390], ["72.28", "72.28", 1391, 1396, 1391, 1396], ["95.21", "95.21", 1435, 1440, 1435, 1440], ["83.15", "83.15", 1442, 1447, 1442, 1447], ["78.24", "78.24", 1449, 1454, 1449, 1454], ["74.62", "74.62", 1456, 1461, 1456, 1461], ["82.81", "{82.81}", 1463, 1468, 1462, 1469], ["58.23", "58.23", 1472, 1477, 1472, 1477], ["46.05", "46.05", 1480, 1485, 1480, 1485], ["70.27", "70.27", 1489, 1494, 1489, 1494], ["73.39", "\\textbf{73.39}", 1505, 1510, 1497, 1511], ["61.99", "61.99", 1512, 1517, 1512, 1517], ["72.40", "72.40", 1518, 1523, 1518, 1523], ["95.93", "95.93", 1568, 1573, 1568, 1573], ["85.99", "85.99", 1575, 1580, 1575, 1580], ["80.03", "\\textbf{80.03}", 1590, 1595, 1582, 1596], ["75.46", "75.46", 1598, 1603, 1598, 1603], ["84.35", "{84.35}", 1607, 1612, 1606, 1613], ["58.44", "58.44", 1616, 1621, 1616, 1621], ["50.29", "\\textbf{50.29}", 1632, 1637, 1624, 1638], ["70.61", "{70.61}", 1642, 1647, 1641, 1648], ["70.64", "70.64", 1651, 1656, 1651, 1656], ["62.49", "62.49", 1659, 1664, 1659, 1664], ["73.42", "73.42", 1665, 1670, 1665, 1670], ["96.23", "96.23", 1710, 1715, 1710, 1715], ["83.94", "83.94", 1717, 1722, 1717, 1722], ["79.27", "79.27", 1724, 1729, 1724, 1729], ["73.30", "73.30", 1731, 1736, 1731, 1736], ["83.19", "{83.19}", 1739, 1744, 1738, 1745], ["60.70", "\\textbf{60.70}", 1758, 1763, 1750, 1764], ["45.82", "45.82", 1767, 1772, 1767, 1772], ["71.51", "{71.51}", 1776, 1781, 1775, 1782], ["73.05", "{73.05}", 1786, 1791, 1785, 1792], ["62.77", "62.77", 1794, 1799, 1794, 1799], ["72.98", "72.98", 1800, 1805, 1800, 1805], ["96.65", "\\textbf{96.65}", 2183, 2188, 2175, 2189], ["88.33", "\\textbf{88.33}", 2199, 2204, 2191, 2205], ["78.20", "78.20", 2207, 2212, 2207, 2212], ["82.90", "\\textbf{82.90}", 2223, 2228, 2215, 2229], ["86.52", "\\textbf{86.52}", 2240, 2245, 2232, 2246], ["59.05", "{59.05}", 2251, 2256, 2250, 2257], ["50.06", "{50.06}", 2261, 2266, 2260, 2267], ["72.97", "\\textbf{72.97}", 2278, 2283, 2270, 2284], ["71.67", "{71.67}", 2287, 2292, 2286, 2293], ["63.56", "\\textbf{63.56}", 2303, 2308, 2295, 2309], ["75.04", "\\textbf{75.04}", 2320, 2325, 2312, 2326]], "text_chunk_selected": "    \\textbf{(a)} We propose a simple yet effective framework named cross-client style transfer (CCST).\n    Our approach achieves new state-of-the-art generalization performance in FL setting on two standard DG benchmarks (PACS~\\cite{PACS}, OfficeHome~\\cite{officehome}) and a large-scale medical image dataset (Camelyon17~\\cite{camelyon17}).\n    \\textbf{(b)}  Two types of styles with corresponding sharing mechanisms are proposed, named \\textit{overall domain style} and \\textit{single image style}, which can be chosen according to different circumstances. The diversity level of our method is also flexible to be adjusted.\n    \\textbf{(c)} The proposed method is orthogonal to many other SOTA DG methods. Therefore, our method can be readily applied to those DG methods to have a further performance boost. We also study the effectiveness of several SOTA DG methods when they are applied in the FL setting for image recognition.\n    \\textbf{(d)} We give an intuitive (Section \\ref{sec:discussion}) and experimental analysis (Section~\\ref{sec:privacy}) on the privacy-preserving performance of our style vectors to demonstrate that one can hardly reconstruct the original images merely from the style vectors using the generator from a SOTA GAN \\cite{liu2021towards} in FL setting.\n\n\\section{Related Work}\n\\textbf{Domain generalization.}\n Domain generalization is a popular research field that aims to learn a model from multiple source domains such that the model can generalize on the unseen target domain. Many works are proposed towards solving the domain shifts from various directions under the centralized data setting. Those methods can be divided into three categories~\\cite{wang2021generalizing}, including manipulating data to enrich data diversity~\\cite{jackson2019style,xu2021fourier,shankar2018generalizing,zhou2021domain}, learning domain-invariant representations or disentangling domain-shared and specific features to enhance the generalization ability of model~\\cite{arjovsky2019invariant,piratla2020efficient,carlucci2019domain,zhao2020domain} and exploiting general learning strategies to promote generalizing capability~\\cite{li2019episodic,huang2020self,dou2019domain,du2020learning}.\n\nHowever, many of these methods require centralized data of different domains, violating the local data preservation in federated learning. Specifically, access for more than one domain is needed to augment data or generate new data in~\\cite{shankar2018generalizing,jackson2019style}, domain invariant representation learning or decomposing features is performed under the comparison across domains~\\cite{arjovsky2019invariant,piratla2020efficient,zhao2020domain} and some learning strategy based methods utilize extra one domain for meta-update~\\cite{li2019episodic,dou2019domain,du2020learning}. Nevertheless, some methods do not explicitly require centralized domains or can be adapted into federated learning with minor changes. For example, MixStyle~\\cite{zhou2021domain} can optionally conduct the style randomization in a single domain to augment data; \\cite{xu2021fourier} uses Fourier transformation to augmentation that is free of sharing data; JiGen~\\cite{carlucci2019domain} proposes a self-supervised task to enhance representation capability; RSC~\\cite{huang2020self} designs a learning strategy based on gradient operations without explicit multi-domain requirements.\n\n\\textbf{Neural style transfer.}\n\\label{style_transfer}\nNeural style transfer (NST) aims to transfer the style of an image to another content image with its semantic structure reserved. The development of NST has roughly gone through three stages: per-style-per-model (PSPM), multiple-style-per-model (MSPM) and arbitrary-style-per-model (ASPM) methods \\cite{StyleTransferReview}. PSPM methods \\cite{Gatys_2016_CVPR,Johnson2016PerceptualLF,Ulyanov2016TextureNF} can only transfer a single style for each trained model. MSPM methods \\cite{dumoulin2016learned,stylebank,Zhang2018MultistyleGN,Li2017DiversIfiedTS}  are able to transfer multiple styles with a single trained model. However, PSPM and MSPM are expensive to deploy when too many styles are required to be transferred in our setting. ASPM \\cite{chen2016fast,huang2017adain,Ghiasi2017,Li2017} can transfer arbitrary styles to any content images and is often faster than PSPM and MSPM, which is more suitable for our scenario. \n\n\\begin{equation}\n    \\begin{aligned}\n     &S _ {overall} ^ {C_n} = (\\mu(F_{all} ^ {C_n}), \\sigma(F_{all} ^ {C_n})), \\\\\n     &F_{all} ^ {C_n} = Stack(F_1 ^ {C_n}, F_2 ^ {C_n}, ..., F_M ^ {C_n}).\n     \\end{aligned}\n    \\end{equation}\n\n\\section{Experiments}\n\\subsection{Datasets}\nWe evaluate our method on two standard domain generalization datasets (PACS~\\cite{li2017deeper}, Office-Home~\\cite{venkateswara2017deep}) that consist of various image styles as domains and a real-world medical image dataset (Camelyon17~\\cite{camelyon17}). Specifically, PACS is a 7-class image recognition benchmark including 9,991 images with four different image style domains, including photo, art, cartoon, and sketch. Office-Home is another image recognition dataset that includes 15,588 images of 65 classes from four different domains (art, clipart, product, and real-world). Camelyon17 is a public tumor classification dataset, which has histology images from 5 hospitals. \n\n\\subsection{Experimental Settings}\n\\textbf{Experiment setup.} We take each domain as a single client and conduct the leave-one-domain-out experiments on PACS and Office-Home datasets. Specifically, we select one client as the target test domain and train our model on the other clients. For the medical dataset, following the setting of source/target domains in literature~\\cite{camelyon17,koh2021wilds}, we apply the leave-one-domain-out setting to hospital 4 and hospital 5. For the PACS dataset, we follow the JiGen~\\cite{carlucci2019domain} to split 90\\% data of each client as the training set and 10\\% of that as the validation set for source clients, while for unseen target clients, the entire data is used for testing. For OfficeHome and Camelyon17, which have more data samples, the ratio between train and validation set is 4:1 for each source client, and 20\\% data is utilized as the test set on the unseen target client. We compare our method with \\textbf{FedDG}~\\cite{liu2021feddg}, which aims to solve DG problems in federated learning for medical image segmentation. We also test the performance of three centralized DG methods under FL setting (FedAvg), including \\textbf{JiGen}~\\cite{carlucci2019domain}, \\textbf{RSC}~\\cite{huang2020self} and \\textbf{MixStyle}~\\cite{zhou2021domain}. For \\textbf{COPA}, due to its re-designed layers of ResNet18 and unknown train-validate-test split, we copy the results for reference only. We regard every single client as a centralized dataset and apply these methods locally in FL. We report the test accuracy on each unseen client by choosing the best validation model.\n\\\\\\textbf{Implementation details.} We utilize the pre-trained AdaIN~\\cite{huang2017adain} to perform style transfer. Following \\cite{huang2020self}, we choose ResNet~\\cite{he2016deep} pre-trained on ImageNet as our backbone for PACS and Office-Home datasets. For the Camelyon17 dataset, we follow \\cite{harmofl} to use the DenseNet121~\\cite{huang2017densely}. We use FedAvg~\\cite{mcmahan2017communication} as our FL framework and train the model using SGD optimizer with $1e^{-3}$ learning rate for 500 communication rounds with one local update epoch on the PACS and Office-Home dataset. For Camelyon17, we train 100 communication rounds considering its large data amount.\nThe JiGen and RSC can be directly integrated into the FedAvg without further modifications. We adapt the MixStyle into an intra-client version that shuffles styles inside each batch of data to fit the federated setting. \nAll hyper-parameters of compared methods are chosen based on corresponding papers. We follow the standard training procedure of FedAvg~\\cite{mcmahan2017communication} for federated training.\nThe value of M in  is $\\lceil dataset\\_size / 32\\rceil$ in our experiment. \nThe framework is implemented with PyTorch and is trained on a single NVIDIA RTX 2080 Ti GPU. \n\n\\subsection{Results}\n\\textbf{Comparison with state-of-the-arts.} We compare our approach with three centralized DG methods and a federated DG method for on standard DG benchmarks PACS and OfficeHome as well as a real-world medical image dataset Camelyon17. \nTable~\\ref{table:all_results} presents the quantitative results of the image recognition task for different target clients on both PACS and Office-Home datasets. Each single letter column shows the test accuracy of the global model with the best validation accuracy on an unseen client. \nAlthough all DG methods can have better performance based on FedAvg, our approach demonstrates a significant boost over others on both datasets. On the PACS benchmark, our method achieves the average accuracy of 86.52\\% , which is 3.47\\% better than the second best method FedDG. Especially for the unseen client S (Sketch), CCST outperforms other methods by more than 7\\%. When photo is the target domain, all the methods perform similarly because we start training based on the ImageNet-pretrained model, which already has very high performance on photo images.\nBesides the PACS benchmark, the performance of our approach on the Office-Home dataset also has consistent results. Specifically, CCST outperforms other methods on average with a testing accuracy of 63.56\\%. \nDue to the small discrepancy in domain styles, all those domain generalization methods bring smaller improvements (less than 1\\%) than on the PACS. Overall, CCST outperforms other DG methods by a large margin.\nFigure~\\ref{fig:camelyon17_results} shows the results on the Camelyon17 dataset, our method outperforms other DG methods both when hospital 4 and hospital 5 as the target client. Some DG method, such as JiGen, is even harmful when applied in FL setting on the Camelyon17 dataset.   ", "table_source": "\\begin{table*}[t]\n\\centering\n\\caption{Accuracy comparison of image recognition on the PACS and Office-Home dataset, each single letter column represents an unseen target client. Our CCST with the overall domain style (K=3) outperforms other methods. We use FedAvg as our base FL framework. Jigen, RSC, and Mixstyle are applied within each client. The backbone networks utilized in PACS and Office-Home are ImageNet-pretrained ResNet50 and ResNet18 respectively. \\textcolor{Gray}{(\\dag: Since COPA did not release their code, we copy the results from their paper here. But it cannot directly compare with our results due to the setting difference.)}}\n\\vspace{-2mm}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{c@{\\hspace{3mm}}ccccc|ccccc|c}\n\\toprule\n\\multirow{4}{*}{Method} & \\multicolumn{5}{c}{PACS}                                & \\multicolumn{5}{c}{Office-Home}                         & \\multirow{4}{*}{Avg.} \\\\ \\cmidrule(lr){2-11} \n& \\multicolumn{1}{c}{P}         & A         & C         & S         & \\multicolumn{1}{c}{Avg.}     & A         & C         & P        & R        & \\multicolumn{1}{c}{Avg.}       &         \\\\ \\midrule\nFedAvg (AISTATS'17)~\\cite{mcmahan2017communication}& 95.51& 82.23& 78.20& 73.56& 82.37 \n& 60.08 & 45.59 & 69.48& 72.82& 61.99&72.18\\\\\nJigen (CVPR'19)~\\cite{carlucci2019domain}& 95.99& 84.72& 77.09& 72.16  & {82.49} \n&  60.29 & 46.16 & 69.26 & 72.59& 62.07&72.28\\\\\nRSC (ECCV'20)~\\cite{huang2020self}& 95.21& 83.15& 78.24& 74.62&{82.81}\n& 58.23 & 46.05  & 70.27 & \\textbf{73.39}&61.99&72.40\\\\\nMixStyle (ICLR'21)~\\cite{zhou2021domain}& 95.93& 85.99& \\textbf{80.03}& 75.46 & {84.35}\n& 58.44 & \\textbf{50.29} & {70.61} & 70.64 & 62.49&73.42\\\\\nFedDG (CVPR'21)~\\cite{liu2021feddg}& 96.23& 83.94& 79.27& 73.30& {83.19} \n & \\textbf{60.70} & 45.82 & {71.51} & {73.05}& 62.77&72.98\\\\\n \\textcolor{Gray}{COPA-Res18\\dag\\ (ICCV'21)~\\cite{Wu_2021_ICCV}}& \\textcolor{Gray}{94.60} & \\textcolor{Gray}{83.30}& \\textcolor{Gray}{79.80}& \\textcolor{Gray}{82.50}& \\textcolor{Gray}{85.10} & \\textcolor{Gray}{59.40}& \\textcolor{Gray}{55.10}& \\textcolor{Gray}{74.80}& \\textcolor{Gray}{75.00}& \\textcolor{Gray}{66.10} & \\textcolor{Gray}{75.60}\\\\\nCCST (Overall, K=3) & \\textbf{96.65}& \\textbf{88.33}& 78.20 & \\textbf{82.90} & \\textbf{86.52}\n & {59.05} & {50.06} & \\textbf{72.97}& {71.67}& \\textbf{63.56} & \\textbf{75.04} \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{table:all_results}\n\\end{table*}", "cell_list_gold": [{"value": "95.51", "char_index": [1188, 1193], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "82.23", "char_index": [1195, 1200], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "78.20", "char_index": [1202, 1207], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "73.56", "char_index": [1209, 1214], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "82.37", "char_index": [1216, 1221], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "60.08", "char_index": [1225, 1230], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "45.59", "char_index": [1233, 1238], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "69.48", "char_index": [1241, 1246], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "72.82", "char_index": [1248, 1253], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "61.99", "char_index": [1255, 1260], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "72.18", "char_index": [1261, 1266], "type": "Result", "training data/set": "PACS Office-Home", "test data/set": "PACS Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "95.99", "char_index": [1312, 1317], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "84.72", "char_index": [1319, 1324], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "77.09", "char_index": [1326, 1331], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "72.16", "char_index": [1333, 1338], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "82.49", "char_index": [1343, 1348], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "60.29", "char_index": [1354, 1359], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "46.16", "char_index": [1362, 1367], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "69.26", "char_index": [1370, 1375], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "72.59", "char_index": [1378, 1383], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "62.07", "char_index": [1385, 1390], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "72.28", "char_index": [1391, 1396], "type": "Result", "training data/set": "PACS Office-Home", "test data/set": "PACS Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "95.21", "char_index": [1435, 1440], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "83.15", "char_index": [1442, 1447], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "78.24", "char_index": [1449, 1454], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "74.62", "char_index": [1456, 1461], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "82.81", "char_index": [1463, 1468], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "58.23", "char_index": [1472, 1477], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "46.05", "char_index": [1480, 1485], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "70.27", "char_index": [1489, 1494], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "73.39", "char_index": [1505, 1510], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "61.99", "char_index": [1512, 1517], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "72.40", "char_index": [1518, 1523], "type": "Result", "training data/set": "PACS Office-Home", "test data/set": "PACS Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "95.93", "char_index": [1568, 1573], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "85.99", "char_index": [1575, 1580], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "80.03", "char_index": [1590, 1595], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "75.46", "char_index": [1598, 1603], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "84.35", "char_index": [1607, 1612], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "58.44", "char_index": [1616, 1621], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "50.29", "char_index": [1632, 1637], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "70.61", "char_index": [1642, 1647], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "70.64", "char_index": [1651, 1656], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "62.49", "char_index": [1659, 1664], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "73.42", "char_index": [1665, 1670], "type": "Result", "training data/set": "PACS Office-Home", "test data/set": "PACS Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "96.23", "char_index": [1710, 1715], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "83.94", "char_index": [1717, 1722], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "79.27", "char_index": [1724, 1729], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "73.30", "char_index": [1731, 1736], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "83.19", "char_index": [1739, 1744], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "60.70", "char_index": [1758, 1763], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "45.82", "char_index": [1767, 1772], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "71.51", "char_index": [1776, 1781], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "73.05", "char_index": [1786, 1791], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "62.77", "char_index": [1794, 1799], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "72.98", "char_index": [1800, 1805], "type": "Result", "training data/set": "PACS Office-Home", "test data/set": "PACS Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "94.60", "char_index": [1891, 1896], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "83.30", "char_index": [1917, 1922], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "79.80", "char_index": [1942, 1947], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "82.50", "char_index": [1967, 1972], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "85.10", "char_index": [1992, 1997], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "59.40", "char_index": [2018, 2023], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "55.10", "char_index": [2043, 2048], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "74.80", "char_index": [2068, 2073], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "75.00", "char_index": [2093, 2098], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "66.10", "char_index": [2118, 2123], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "75.60", "char_index": [2144, 2149], "type": "Result", "training data/set": "PACS Office-Home", "test data/set": "PACS Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "COPA-Res18", "model settings": {"xx": "yy"}}, {"value": "96.65", "char_index": [2183, 2188], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "88.33", "char_index": [2199, 2204], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "78.20", "char_index": [2207, 2212], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "82.90", "char_index": [2223, 2228], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "86.52", "char_index": [2240, 2245], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "59.05", "char_index": [2251, 2256], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "50.06", "char_index": [2261, 2266], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "72.97", "char_index": [2278, 2283], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "71.67", "char_index": [2287, 2292], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "63.56", "char_index": [2303, 2308], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}, {"value": "75.04", "char_index": [2320, 2325], "type": "Result", "training data/set": "PACS Office-Home", "test data/set": "PACS Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST", "model settings": {"K": "3"}}]}, "2210.00912v1_table1": {"table_code": "\\begin{subtable}[h]{0.5\\textwidth}\n        \\centering\n        \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{ccccccc}\n\\toprule\n & \\multirow{2}{*}{Setting} & \\multicolumn{4}{c}{Unseen client}                                         & \\multirow{2}{*}{Average} \\\\ \\cline{3-6}\n                          &                          & P              & A             & C              & \\multicolumn{1}{c}{S} &                          \\\\ \\hline\n& FedAvg~\\cite{mcmahan2017communication} & 95.51          & 82.23          & 78.20          & 73.56                   & 82.37                    \\\\ \\cline{2-7}\n&  Single (K=1)           & 95.75&87.5&74.66&76.56&83.62\\\\\n& Single (K=2) &\\textbf{96.77}&86.23&75.73&80.12&84.71\\\\\n& Single (K=3)            & 96.65&86.63&74.53&81.85&84.84\\\\\n& Overall (K=1)           & 95.69&86.67&75.85&77.37&83.90\\\\\n&Overall (K=2) &96.41&\\textbf{88.72}&78.03&80.91&86.02\\\\\n& Overall (K=3)           & 96.65&88.33&\\textbf{78.20}&\\textbf{82.90}&\\textbf{86.52}\\\\\\hline            \n\\end{tabular}}\n       \\caption{Control experiment for CCST. \\vspace{-20pt}}\n       \n       \\label{tab:ablation}\n    \\end{subtable}", "table_label": "{tab:ablation}", "table_numeric_cells": [["95.51", "95.51", 484, 489, 484, 489], ["82.23", "82.23", 501, 506, 501, 506], ["78.20", "78.20", 518, 523, 518, 523], ["73.56", "73.56", 535, 540, 535, 540], ["82.37", "82.37", 561, 566, 561, 566], ["95.75", "95.75", 629, 634, 629, 634], ["87.5", "87.5", 635, 639, 635, 639], ["74.66", "74.66", 640, 645, 640, 645], ["76.56", "76.56", 646, 651, 646, 651], ["83.62", "83.62", 652, 657, 652, 657], ["96.77", "\\textbf{96.77}", 684, 689, 676, 690], ["86.23", "86.23", 691, 696, 691, 696], ["75.73", "75.73", 697, 702, 697, 702], ["80.12", "80.12", 703, 708, 703, 708], ["84.71", "84.71", 709, 714, 709, 714], ["96.65", "96.65", 745, 750, 745, 750], ["86.63", "86.63", 751, 756, 751, 756], ["74.53", "74.53", 757, 762, 757, 762], ["81.85", "81.85", 763, 768, 763, 768], ["84.84", "84.84", 769, 774, 769, 774], ["95.69", "95.69", 805, 810, 805, 810], ["86.67", "86.67", 811, 816, 811, 816], ["75.85", "75.85", 817, 822, 817, 822], ["77.37", "77.37", 823, 828, 823, 828], ["83.90", "83.90", 829, 834, 829, 834], ["96.41", "96.41", 853, 858, 853, 858], ["88.72", "\\textbf{88.72}", 867, 872, 859, 873], ["78.03", "78.03", 874, 879, 874, 879], ["80.91", "80.91", 880, 885, 880, 885], ["86.02", "86.02", 886, 891, 886, 891], ["96.65", "96.65", 922, 927, 922, 927], ["88.33", "88.33", 928, 933, 928, 933], ["78.20", "\\textbf{78.20}", 942, 947, 934, 948], ["82.90", "\\textbf{82.90}", 957, 962, 949, 963], ["86.52", "\\textbf{86.52}", 972, 977, 964, 978]], "text_chunk_selected": "    \\textbf{(a)} We propose a simple yet effective framework named cross-client style transfer (CCST).\n    Our approach achieves new state-of-the-art generalization performance in FL setting on two standard DG benchmarks (PACS~\\cite{PACS}, OfficeHome~\\cite{officehome}) and a large-scale medical image dataset (Camelyon17~\\cite{camelyon17}).\n    \\textbf{(b)}  Two types of styles with corresponding sharing mechanisms are proposed, named \\textit{overall domain style} and \\textit{single image style}, which can be chosen according to different circumstances. The diversity level of our method is also flexible to be adjusted.\n    \\textbf{(c)} The proposed method is orthogonal to many other SOTA DG methods. Therefore, our method can be readily applied to those DG methods to have a further performance boost. We also study the effectiveness of several SOTA DG methods when they are applied in the FL setting for image recognition.\n    \\textbf{(d)} We give an intuitive (Section \\ref{sec:discussion}) and experimental analysis (Section~\\ref{sec:privacy}) on the privacy-preserving performance of our style vectors to demonstrate that one can hardly reconstruct the original images merely from the style vectors using the generator from a SOTA GAN \\cite{liu2021towards} in FL setting.\n\n\\textbf{Neural style transfer.}\n\\label{style_transfer}\nNeural style transfer (NST) aims to transfer the style of an image to another content image with its semantic structure reserved. The development of NST has roughly gone through three stages: per-style-per-model (PSPM), multiple-style-per-model (MSPM) and arbitrary-style-per-model (ASPM) methods \\cite{StyleTransferReview}. PSPM methods \\cite{Gatys_2016_CVPR,Johnson2016PerceptualLF,Ulyanov2016TextureNF} can only transfer a single style for each trained model. MSPM methods \\cite{dumoulin2016learned,stylebank,Zhang2018MultistyleGN,Li2017DiversIfiedTS}  are able to transfer multiple styles with a single trained model. However, PSPM and MSPM are expensive to deploy when too many styles are required to be transferred in our setting. ASPM \\cite{chen2016fast,huang2017adain,Ghiasi2017,Li2017} can transfer arbitrary styles to any content images and is often faster than PSPM and MSPM, which is more suitable for our scenario. \n\n    where $\\{i_1,\\hdots,i_J\\}$ are randomly sampled image indices from client $C_n$. Sharing single image styles consumes relatively low computation but can lead to high communication costs for uploading multiple styles.\n    \\textbf{Overall domain style.} Domain style is the domain-level channel wise mean and standard variance, which considers all the images (pixels) in a client. Formally, assume client $C_n$ has $M$ training images with corresponding VGG features $\\{F_1 ^ {C_n}, F_2 ^ {C_n}, ..., F_M ^ {C_n}\\}$, the overall style  $S _ {overall} ^ {C_n}$ of this client is:\n\n\\begin{equation}\n    \\begin{aligned}\n     &S _ {overall} ^ {C_n} = (\\mu(F_{all} ^ {C_n}), \\sigma(F_{all} ^ {C_n})), \\\\\n     &F_{all} ^ {C_n} = Stack(F_1 ^ {C_n}, F_2 ^ {C_n}, ..., F_M ^ {C_n}).\n     \\end{aligned}\n    \\end{equation}\n\n\\subsubsection{Local style transfer.} \nWhen client $C_n$ receives the style bank $\\mathbf{B}$, the local data can be augmented by transferring styles in $\\mathbf{B}$ to existing images, which introduces styles of other domains into this client.\nA hyperparameter $K \\in \\{1,2,...,N\\}$ called \\textit{augmentation level}, is set to choose $K$ styles from style bank $\\mathbf{B}$ for the augmentation of each image, indicating the diversity of final augmented data set. Suppose the size of the original dataset is $d$, then after cross-client style transfer, the size of the augmented data set will become $d \\times K$. \n\n\\section{Experiments}\n\\subsection{Datasets}\nWe evaluate our method on two standard domain generalization datasets (PACS~\\cite{li2017deeper}, Office-Home~\\cite{venkateswara2017deep}) that consist of various image styles as domains and a real-world medical image dataset (Camelyon17~\\cite{camelyon17}). Specifically, PACS is a 7-class image recognition benchmark including 9,991 images with four different image style domains, including photo, art, cartoon, and sketch. Office-Home is another image recognition dataset that includes 15,588 images of 65 classes from four different domains (art, clipart, product, and real-world). Camelyon17 is a public tumor classification dataset, which has histology images from 5 hospitals. \n\n\\subsection{Experimental Settings}\n\\textbf{Experiment setup.} We take each domain as a single client and conduct the leave-one-domain-out experiments on PACS and Office-Home datasets. Specifically, we select one client as the target test domain and train our model on the other clients. For the medical dataset, following the setting of source/target domains in literature~\\cite{camelyon17,koh2021wilds}, we apply the leave-one-domain-out setting to hospital 4 and hospital 5. For the PACS dataset, we follow the JiGen~\\cite{carlucci2019domain} to split 90\\% data of each client as the training set and 10\\% of that as the validation set for source clients, while for unseen target clients, the entire data is used for testing. For OfficeHome and Camelyon17, which have more data samples, the ratio between train and validation set is 4:1 for each source client, and 20\\% data is utilized as the test set on the unseen target client. We compare our method with \\textbf{FedDG}~\\cite{liu2021feddg}, which aims to solve DG problems in federated learning for medical image segmentation. We also test the performance of three centralized DG methods under FL setting (FedAvg), including \\textbf{JiGen}~\\cite{carlucci2019domain}, \\textbf{RSC}~\\cite{huang2020self} and \\textbf{MixStyle}~\\cite{zhou2021domain}. For \\textbf{COPA}, due to its re-designed layers of ResNet18 and unknown train-validate-test split, we copy the results for reference only. We regard every single client as a centralized dataset and apply these methods locally in FL. We report the test accuracy on each unseen client by choosing the best validation model.\n\\\\\\textbf{Implementation details.} We utilize the pre-trained AdaIN~\\cite{huang2017adain} to perform style transfer. Following \\cite{huang2020self}, we choose ResNet~\\cite{he2016deep} pre-trained on ImageNet as our backbone for PACS and Office-Home datasets. For the Camelyon17 dataset, we follow \\cite{harmofl} to use the DenseNet121~\\cite{huang2017densely}. We use FedAvg~\\cite{mcmahan2017communication} as our FL framework and train the model using SGD optimizer with $1e^{-3}$ learning rate for 500 communication rounds with one local update epoch on the PACS and Office-Home dataset. For Camelyon17, we train 100 communication rounds considering its large data amount.\nThe JiGen and RSC can be directly integrated into the FedAvg without further modifications. We adapt the MixStyle into an intra-client version that shuffles styles inside each batch of data to fit the federated setting. \nAll hyper-parameters of compared methods are chosen based on corresponding papers. We follow the standard training procedure of FedAvg~\\cite{mcmahan2017communication} for federated training.\nThe value of M in  is $\\lceil dataset\\_size / 32\\rceil$ in our experiment. \nThe framework is implemented with PyTorch and is trained on a single NVIDIA RTX 2080 Ti GPU. \n\n\\textbf{Control experiments on CCST.}\nWe conduct control experiments to investigate two types of image style with different augmentation levels. In Table \\ref{tab:ablation}, \\textit{Single} and \\textit{Overall} represents single image style and overall domain style mentioned in Section \\ref{sec:method} respectively. Different augmentation level $K$ indicates the intensity of augmentation.  We evaluate the four settings on the PACS benchmark with ResNet50. \nFor each kind of style, larger augmentation level K leads to a better performance. It is worth mentioning that the performance achieved by K=2 is similar with K=3, which indicates that our method can already achieve good performance with a relatively large K. \nFor different types of style, overall domain style shows more improvement than single image style because the overall style is able to represent a more general and accurate domain statistics, while single image styles may differ a lot due to randomness.", "table_source": "\\begin{table*}[t]\n    \\caption{\\textbf{(a)} Performance of our approach using ResNet50 as the backbone with four different image style transfer settings compared with the baseline of FedAvg on the PACS benchmark. Each column represents a single unseen target client. \\textbf{(b)} Performance of our approach with test time adaptation (Tent)       \\cite{wang2021tent} on the PACS benchmark using ResNet50.}\n    \\vspace{-5pt}\n    \\begin{subtable}[h]{0.5\\textwidth}\n        \\centering\n        \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{ccccccc}\n\\toprule\n & \\multirow{2}{*}{Setting} & \\multicolumn{4}{c}{Unseen client}                                         & \\multirow{2}{*}{Average} \\\\ \\cline{3-6}\n                          &                          & P              & A             & C              & \\multicolumn{1}{c}{S} &                          \\\\ \\hline\n& FedAvg~\\cite{mcmahan2017communication} & 95.51          & 82.23          & 78.20          & 73.56                   & 82.37                    \\\\ \\cline{2-7}\n&  Single (K=1)           & 95.75&87.5&74.66&76.56&83.62\\\\\n& Single (K=2) &\\textbf{96.77}&86.23&75.73&80.12&84.71\\\\\n& Single (K=3)            & 96.65&86.63&74.53&81.85&84.84\\\\\n& Overall (K=1)           & 95.69&86.67&75.85&77.37&83.90\\\\\n&Overall (K=2) &96.41&\\textbf{88.72}&78.03&80.91&86.02\\\\\n& Overall (K=3)           & 96.65&88.33&\\textbf{78.20}&\\textbf{82.90}&\\textbf{86.52}\\\\\\hline            \n\\end{tabular}}\n       \\caption{Control experiment for CCST. \\vspace{-20pt}}\n       \n       \\label{tab:ablation}\n    \\end{subtable}\n    \\hfill\n    \\begin{subtable}[h]{0.49\\textwidth}\n        \\centering\n        \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{cccccc}\n\\toprule\n\\multirow{2}{*}{Setting} & \\multicolumn{4}{c}{Unseen client}                                         & \\multirow{2}{*}{Average} \\\\ \\cline{2-5}\n                                              & P              & A             & C              & \\multicolumn{1}{c}{S} &                          \\\\ \\hline\nEoA~\\cite{arpit2021ensembledg} & {98.00} & {90.50} & {83.40} & {82.50} & {88.60} \\\\\\hline\nSingle (K=1) & 97.78& 89.55& 84.51& 82.79& 88.66 \\\\\nSingle (K=2) &97.54& 89.40& 84.43& 85.42& 89.20\\\\\nSingle (K=3) & 98.08& 89.75& 86.05& 86.03& 89.98 \\\\\nOverall (K=1) & 97.78& \\textbf{90.92}& 86.01& 83.66& 89.59 \\\\\nOverall (K=2) & \\textbf{98.38}& 90.72& 86.47& 85.62& 90.30 \\\\\nOverall (K=3)  & 98.14& 90.87& \\textbf{86.77}& \\textbf{86.10}& \\textbf{90.47}  \\\\ \\hline\n\\end{tabular}}\n        \\caption{CCST+Tent~\\cite{wang2021tent}. \\vspace{-20pt}}\n\n        \\label{tab:add_tent}\n     \\end{subtable}\n     \n     \\label{tab:new_table}\n\\end{table*}", "cell_list_gold": [{"value": "95.51", "char_index": [484, 489], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "82.23", "char_index": [501, 506], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "78.20", "char_index": [518, 523], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "73.56", "char_index": [535, 540], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "82.37", "char_index": [561, 566], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "95.75", "char_index": [629, 634], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "87.5", "char_index": [635, 639], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "74.66", "char_index": [640, 645], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "76.56", "char_index": [646, 651], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "83.62", "char_index": [652, 657], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "96.77", "char_index": [684, 689], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "86.23", "char_index": [691, 696], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "75.73", "char_index": [697, 702], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "80.12", "char_index": [703, 708], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "84.71", "char_index": [709, 714], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "96.65", "char_index": [745, 750], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "86.63", "char_index": [751, 756], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "74.53", "char_index": [757, 762], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "81.85", "char_index": [763, 768], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "84.84", "char_index": [769, 774], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "95.69", "char_index": [805, 810], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "86.67", "char_index": [811, 816], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "75.85", "char_index": [817, 822], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "77.37", "char_index": [823, 828], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "83.90", "char_index": [829, 834], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "96.41", "char_index": [853, 858], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "88.72", "char_index": [867, 872], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "78.03", "char_index": [874, 879], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "80.91", "char_index": [880, 885], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "86.02", "char_index": [886, 891], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "96.65", "char_index": [922, 927], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "88.33", "char_index": [928, 933], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "78.20", "char_index": [942, 947], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "82.90", "char_index": [957, 962], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "86.52", "char_index": [972, 977], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}]}, "2210.00912v1_table2": {"table_code": "\\begin{subtable}[h]{0.49\\textwidth}\n        \\centering\n        \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{cccccc}\n\\toprule\n\\multirow{2}{*}{Setting} & \\multicolumn{4}{c}{Unseen client}                                         & \\multirow{2}{*}{Average} \\\\ \\cline{2-5}\n                                              & P              & A             & C              & \\multicolumn{1}{c}{S} &                          \\\\ \\hline\nEoA~\\cite{arpit2021ensembledg} & {98.00} & {90.50} & {83.40} & {82.50} & {88.60} \\\\\\hline\nSingle (K=1) & 97.78& 89.55& 84.51& 82.79& 88.66 \\\\\nSingle (K=2) &97.54& 89.40& 84.43& 85.42& 89.20\\\\\nSingle (K=3) & 98.08& 89.75& 86.05& 86.03& 89.98 \\\\\nOverall (K=1) & 97.78& \\textbf{90.92}& 86.01& 83.66& 89.59 \\\\\nOverall (K=2) & \\textbf{98.38}& 90.72& 86.47& 85.62& 90.30 \\\\\nOverall (K=3)  & 98.14& 90.87& \\textbf{86.77}& \\textbf{86.10}& \\textbf{90.47}  \\\\ \\hline\n\\end{tabular}}\n        \\caption{CCST+Tent~\\cite{wang2021tent}. \\vspace{-20pt}}\n\n        \\label{tab:add_tent}\n     \\end{subtable}", "table_label": "{tab:add_tent}", "table_numeric_cells": [["98.00", "{98.00}", 465, 470, 464, 471], ["90.50", "{90.50}", 475, 480, 474, 481], ["83.40", "{83.40}", 485, 490, 484, 491], ["82.50", "{82.50}", 495, 500, 494, 501], ["88.60", "{88.60}", 505, 510, 504, 511], ["97.78", "97.78", 536, 541, 536, 541], ["89.55", "89.55", 543, 548, 543, 548], ["84.51", "84.51", 550, 555, 550, 555], ["82.79", "82.79", 557, 562, 557, 562], ["88.66", "88.66", 564, 569, 564, 569], ["97.54", "97.54", 587, 592, 587, 592], ["89.40", "89.40", 594, 599, 594, 599], ["84.43", "84.43", 601, 606, 601, 606], ["85.42", "85.42", 608, 613, 608, 613], ["89.20", "89.20", 615, 620, 615, 620], ["98.08", "98.08", 638, 643, 638, 643], ["89.75", "89.75", 645, 650, 645, 650], ["86.05", "86.05", 652, 657, 652, 657], ["86.03", "86.03", 659, 664, 659, 664], ["89.98", "89.98", 666, 671, 666, 671], ["97.78", "97.78", 691, 696, 691, 696], ["90.92", "\\textbf{90.92}", 706, 711, 698, 712], ["86.01", "86.01", 714, 719, 714, 719], ["83.66", "83.66", 721, 726, 721, 726], ["89.59", "89.59", 728, 733, 728, 733], ["98.38", "\\textbf{98.38}", 761, 766, 753, 767], ["90.72", "90.72", 769, 774, 769, 774], ["86.47", "86.47", 776, 781, 776, 781], ["85.62", "85.62", 783, 788, 783, 788], ["90.30", "90.30", 790, 795, 790, 795], ["98.14", "98.14", 816, 821, 816, 821], ["90.87", "90.87", 823, 828, 823, 828], ["86.77", "\\textbf{86.77}", 838, 843, 830, 844], ["86.10", "\\textbf{86.10}", 854, 859, 846, 860], ["90.47", "\\textbf{90.47}", 870, 875, 862, 876]], "text_chunk_selected": "    \\textbf{(a)} We propose a simple yet effective framework named cross-client style transfer (CCST).\n    Our approach achieves new state-of-the-art generalization performance in FL setting on two standard DG benchmarks (PACS~\\cite{PACS}, OfficeHome~\\cite{officehome}) and a large-scale medical image dataset (Camelyon17~\\cite{camelyon17}).\n    \\textbf{(b)}  Two types of styles with corresponding sharing mechanisms are proposed, named \\textit{overall domain style} and \\textit{single image style}, which can be chosen according to different circumstances. The diversity level of our method is also flexible to be adjusted.\n    \\textbf{(c)} The proposed method is orthogonal to many other SOTA DG methods. Therefore, our method can be readily applied to those DG methods to have a further performance boost. We also study the effectiveness of several SOTA DG methods when they are applied in the FL setting for image recognition.\n    \\textbf{(d)} We give an intuitive (Section \\ref{sec:discussion}) and experimental analysis (Section~\\ref{sec:privacy}) on the privacy-preserving performance of our style vectors to demonstrate that one can hardly reconstruct the original images merely from the style vectors using the generator from a SOTA GAN \\cite{liu2021towards} in FL setting.\n\n\\section{Related Work}\n\\textbf{Domain generalization.}\n Domain generalization is a popular research field that aims to learn a model from multiple source domains such that the model can generalize on the unseen target domain. Many works are proposed towards solving the domain shifts from various directions under the centralized data setting. Those methods can be divided into three categories~\\cite{wang2021generalizing}, including manipulating data to enrich data diversity~\\cite{jackson2019style,xu2021fourier,shankar2018generalizing,zhou2021domain}, learning domain-invariant representations or disentangling domain-shared and specific features to enhance the generalization ability of model~\\cite{arjovsky2019invariant,piratla2020efficient,carlucci2019domain,zhao2020domain} and exploiting general learning strategies to promote generalizing capability~\\cite{li2019episodic,huang2020self,dou2019domain,du2020learning}.\n\n\\textbf{Neural style transfer.}\n\\label{style_transfer}\nNeural style transfer (NST) aims to transfer the style of an image to another content image with its semantic structure reserved. The development of NST has roughly gone through three stages: per-style-per-model (PSPM), multiple-style-per-model (MSPM) and arbitrary-style-per-model (ASPM) methods \\cite{StyleTransferReview}. PSPM methods \\cite{Gatys_2016_CVPR,Johnson2016PerceptualLF,Ulyanov2016TextureNF} can only transfer a single style for each trained model. MSPM methods \\cite{dumoulin2016learned,stylebank,Zhang2018MultistyleGN,Li2017DiversIfiedTS}  are able to transfer multiple styles with a single trained model. However, PSPM and MSPM are expensive to deploy when too many styles are required to be transferred in our setting. ASPM \\cite{chen2016fast,huang2017adain,Ghiasi2017,Li2017} can transfer arbitrary styles to any content images and is often faster than PSPM and MSPM, which is more suitable for our scenario. \n\n\\begin{equation}\n    \\begin{aligned}\n     &S _ {overall} ^ {C_n} = (\\mu(F_{all} ^ {C_n}), \\sigma(F_{all} ^ {C_n})), \\\\\n     &F_{all} ^ {C_n} = Stack(F_1 ^ {C_n}, F_2 ^ {C_n}, ..., F_M ^ {C_n}).\n     \\end{aligned}\n    \\end{equation}\n\n\\subsection{Experimental Settings}\n\\textbf{Experiment setup.} We take each domain as a single client and conduct the leave-one-domain-out experiments on PACS and Office-Home datasets. Specifically, we select one client as the target test domain and train our model on the other clients. For the medical dataset, following the setting of source/target domains in literature~\\cite{camelyon17,koh2021wilds}, we apply the leave-one-domain-out setting to hospital 4 and hospital 5. For the PACS dataset, we follow the JiGen~\\cite{carlucci2019domain} to split 90\\% data of each client as the training set and 10\\% of that as the validation set for source clients, while for unseen target clients, the entire data is used for testing. For OfficeHome and Camelyon17, which have more data samples, the ratio between train and validation set is 4:1 for each source client, and 20\\% data is utilized as the test set on the unseen target client. We compare our method with \\textbf{FedDG}~\\cite{liu2021feddg}, which aims to solve DG problems in federated learning for medical image segmentation. We also test the performance of three centralized DG methods under FL setting (FedAvg), including \\textbf{JiGen}~\\cite{carlucci2019domain}, \\textbf{RSC}~\\cite{huang2020self} and \\textbf{MixStyle}~\\cite{zhou2021domain}. For \\textbf{COPA}, due to its re-designed layers of ResNet18 and unknown train-validate-test split, we copy the results for reference only. We regard every single client as a centralized dataset and apply these methods locally in FL. We report the test accuracy on each unseen client by choosing the best validation model.\n\\\\\\textbf{Implementation details.} We utilize the pre-trained AdaIN~\\cite{huang2017adain} to perform style transfer. Following \\cite{huang2020self}, we choose ResNet~\\cite{he2016deep} pre-trained on ImageNet as our backbone for PACS and Office-Home datasets. For the Camelyon17 dataset, we follow \\cite{harmofl} to use the DenseNet121~\\cite{huang2017densely}. We use FedAvg~\\cite{mcmahan2017communication} as our FL framework and train the model using SGD optimizer with $1e^{-3}$ learning rate for 500 communication rounds with one local update epoch on the PACS and Office-Home dataset. For Camelyon17, we train 100 communication rounds considering its large data amount.\nThe JiGen and RSC can be directly integrated into the FedAvg without further modifications. We adapt the MixStyle into an intra-client version that shuffles styles inside each batch of data to fit the federated setting. \nAll hyper-parameters of compared methods are chosen based on corresponding papers. We follow the standard training procedure of FedAvg~\\cite{mcmahan2017communication} for federated training.\nThe value of M in  is $\\lceil dataset\\_size / 32\\rceil$ in our experiment. \nThe framework is implemented with PyTorch and is trained on a single NVIDIA RTX 2080 Ti GPU. \n\n\\subsection{Results}\n\\textbf{Comparison with state-of-the-arts.} We compare our approach with three centralized DG methods and a federated DG method for on standard DG benchmarks PACS and OfficeHome as well as a real-world medical image dataset Camelyon17. \nTable~\\ref{table:all_results} presents the quantitative results of the image recognition task for different target clients on both PACS and Office-Home datasets. Each single letter column shows the test accuracy of the global model with the best validation accuracy on an unseen client. \nAlthough all DG methods can have better performance based on FedAvg, our approach demonstrates a significant boost over others on both datasets. On the PACS benchmark, our method achieves the average accuracy of 86.52\\% , which is 3.47\\% better than the second best method FedDG. Especially for the unseen client S (Sketch), CCST outperforms other methods by more than 7\\%. When photo is the target domain, all the methods perform similarly because we start training based on the ImageNet-pretrained model, which already has very high performance on photo images.\nBesides the PACS benchmark, the performance of our approach on the Office-Home dataset also has consistent results. Specifically, CCST outperforms other methods on average with a testing accuracy of 63.56\\%. \nDue to the small discrepancy in domain styles, all those domain generalization methods bring smaller improvements (less than 1\\%) than on the PACS. Overall, CCST outperforms other DG methods by a large margin.\nFigure~\\ref{fig:camelyon17_results} shows the results on the Camelyon17 dataset, our method outperforms other DG methods both when hospital 4 and hospital 5 as the target client. Some DG method, such as JiGen, is even harmful when applied in FL setting on the Camelyon17 dataset.   \n\n\\textbf{Control experiments on CCST.}\nWe conduct control experiments to investigate two types of image style with different augmentation levels. In Table \\ref{tab:ablation}, \\textit{Single} and \\textit{Overall} represents single image style and overall domain style mentioned in Section \\ref{sec:method} respectively. Different augmentation level $K$ indicates the intensity of augmentation.  We evaluate the four settings on the PACS benchmark with ResNet50. \nFor each kind of style, larger augmentation level K leads to a better performance. It is worth mentioning that the performance achieved by K=2 is similar with K=3, which indicates that our method can already achieve good performance with a relatively large K. \nFor different types of style, overall domain style shows more improvement than single image style because the overall style is able to represent a more general and accurate domain statistics, while single image styles may differ a lot due to randomness.\n\n\\textbf{Orthogonality.}\nOur method is orthogonal to many other DG methods and can lead to additive performance via combined utilization. As many traditional domain generalization methods require centralized data and need to make use of various styles to achieve a domain robust model, CCST can serve as an initial step to benefit traditional DG methods with diversified styles. As shown in Figure~\\ref{fig:extra_boost}, we plot the average test accuracy of FedDG and three centralized DG methods on the PACS benchmark before and after applying our CCST. From the average accuracy, we can see all DG methods benefit a further boost with the help of cross-client style transfer (CCST). Interestingly, we find the performance when tested on sketch client (S) gains the largest improvement with CCST.\nBesides, we also extend our method with Tent~\\cite{wang2021tent} which uses entropy to update parameters in batch normalization layers at test time. As shown in Table~\\ref{tab:add_tent}, with this method combined in federated setting, our method surpasses the state-of-the-art DG method EoA~\\cite{arpit2021ensembledg} on the PACS benchmark in centralized setting with an average test accuracy of 90.47\\%. ", "table_source": "\\begin{table*}[t]\n    \\caption{\\textbf{(a)} Performance of our approach using ResNet50 as the backbone with four different image style transfer settings compared with the baseline of FedAvg on the PACS benchmark. Each column represents a single unseen target client. \\textbf{(b)} Performance of our approach with test time adaptation (Tent)       \\cite{wang2021tent} on the PACS benchmark using ResNet50.}\n    \\vspace{-5pt}\n    \\begin{subtable}[h]{0.5\\textwidth}\n        \\centering\n        \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{ccccccc}\n\\toprule\n & \\multirow{2}{*}{Setting} & \\multicolumn{4}{c}{Unseen client}                                         & \\multirow{2}{*}{Average} \\\\ \\cline{3-6}\n                          &                          & P              & A             & C              & \\multicolumn{1}{c}{S} &                          \\\\ \\hline\n& FedAvg~\\cite{mcmahan2017communication} & 95.51          & 82.23          & 78.20          & 73.56                   & 82.37                    \\\\ \\cline{2-7}\n&  Single (K=1)           & 95.75&87.5&74.66&76.56&83.62\\\\\n& Single (K=2) &\\textbf{96.77}&86.23&75.73&80.12&84.71\\\\\n& Single (K=3)            & 96.65&86.63&74.53&81.85&84.84\\\\\n& Overall (K=1)           & 95.69&86.67&75.85&77.37&83.90\\\\\n&Overall (K=2) &96.41&\\textbf{88.72}&78.03&80.91&86.02\\\\\n& Overall (K=3)           & 96.65&88.33&\\textbf{78.20}&\\textbf{82.90}&\\textbf{86.52}\\\\\\hline            \n\\end{tabular}}\n       \\caption{Control experiment for CCST. \\vspace{-20pt}}\n       \n       \\label{tab:ablation}\n    \\end{subtable}\n    \\hfill\n    \\begin{subtable}[h]{0.49\\textwidth}\n        \\centering\n        \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{cccccc}\n\\toprule\n\\multirow{2}{*}{Setting} & \\multicolumn{4}{c}{Unseen client}                                         & \\multirow{2}{*}{Average} \\\\ \\cline{2-5}\n                                              & P              & A             & C              & \\multicolumn{1}{c}{S} &                          \\\\ \\hline\nEoA~\\cite{arpit2021ensembledg} & {98.00} & {90.50} & {83.40} & {82.50} & {88.60} \\\\\\hline\nSingle (K=1) & 97.78& 89.55& 84.51& 82.79& 88.66 \\\\\nSingle (K=2) &97.54& 89.40& 84.43& 85.42& 89.20\\\\\nSingle (K=3) & 98.08& 89.75& 86.05& 86.03& 89.98 \\\\\nOverall (K=1) & 97.78& \\textbf{90.92}& 86.01& 83.66& 89.59 \\\\\nOverall (K=2) & \\textbf{98.38}& 90.72& 86.47& 85.62& 90.30 \\\\\nOverall (K=3)  & 98.14& 90.87& \\textbf{86.77}& \\textbf{86.10}& \\textbf{90.47}  \\\\ \\hline\n\\end{tabular}}\n        \\caption{CCST+Tent~\\cite{wang2021tent}. \\vspace{-20pt}}\n\n        \\label{tab:add_tent}\n     \\end{subtable}\n     \n     \\label{tab:new_table}\n\\end{table*}", "cell_list_gold": [{"value": "98.00", "char_index": [465, 470], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "EoA", "model settings": {"xx": "yy"}}, {"value": "90.50", "char_index": [475, 480], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "EoA", "model settings": {"xx": "yy"}}, {"value": "83.40", "char_index": [485, 490], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "EoA", "model settings": {"xx": "yy"}}, {"value": "82.50", "char_index": [495, 500], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "EoA", "model settings": {"xx": "yy"}}, {"value": "88.60", "char_index": [505, 510], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "EoA", "model settings": {"xx": "yy"}}, {"value": "97.78", "char_index": [536, 541], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "1"}}, {"value": "89.55", "char_index": [543, 548], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "1"}}, {"value": "84.51", "char_index": [550, 555], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "1"}}, {"value": "82.79", "char_index": [557, 562], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "1"}}, {"value": "88.66", "char_index": [564, 569], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "1"}}, {"value": "97.54", "char_index": [587, 592], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "2"}}, {"value": "89.40", "char_index": [594, 599], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "2"}}, {"value": "84.43", "char_index": [601, 606], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "2"}}, {"value": "85.42", "char_index": [608, 613], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "2"}}, {"value": "89.20", "char_index": [615, 620], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "2"}}, {"value": "98.08", "char_index": [638, 643], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "3"}}, {"value": "89.75", "char_index": [645, 650], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "3"}}, {"value": "86.05", "char_index": [652, 657], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "3"}}, {"value": "86.03", "char_index": [659, 664], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "3"}}, {"value": "89.98", "char_index": [666, 671], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Single", "model settings": {"K": "3"}}, {"value": "97.78", "char_index": [691, 696], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "1"}}, {"value": "90.92", "char_index": [706, 711], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "1"}}, {"value": "86.01", "char_index": [714, 719], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "1"}}, {"value": "83.66", "char_index": [721, 726], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "1"}}, {"value": "89.59", "char_index": [728, 733], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "1"}}, {"value": "98.38", "char_index": [761, 766], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "2"}}, {"value": "90.72", "char_index": [769, 774], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "2"}}, {"value": "86.47", "char_index": [776, 781], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "2"}}, {"value": "85.62", "char_index": [783, 788], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "2"}}, {"value": "90.30", "char_index": [790, 795], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "2"}}, {"value": "98.14", "char_index": [816, 821], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "3"}}, {"value": "90.87", "char_index": [823, 828], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "3"}}, {"value": "86.77", "char_index": [838, 843], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "3"}}, {"value": "86.10", "char_index": [854, 859], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "3"}}, {"value": "90.47", "char_index": [870, 875], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST+Tent Overall", "model settings": {"K": "3"}}]}, "2210.00912v1_table3": {"table_code": "\\begin{table}[t]\n        \\centering\n        \\resizebox{0.5\\textwidth}{!}{\n        \\begin{tabular}{ccccccc}\n\\toprule\n & \\multirow{2}{*}{Setting} & \\multicolumn{4}{c}{Unseen client}                                         & \\multirow{2}{*}{Average} \\\\ \\cline{3-6}\n                          &                          & P              & A             & C              & \\multicolumn{1}{c}{S} &                          \\\\ \\hline\n& FedAvg (AISTATS'17)~\\cite{mcmahan2017communication}    &95.21&82.91&78.80&73.99&82.73 \\\\\n& Jigen (CVPR'19)~\\cite{carlucci2019domain}         &95.63&83.25&81.10&71.95&82.98 \\\\\n& RSC (ECCV'20)~\\cite{huang2020self}           &94.55&83.20&79.99&72.79&85.31 \\\\\n& MixStyle (ICLR'21)~\\cite{zhou2021domain}           &96.47&86.89&81.06&76.81&82.63 \\\\\n& FedDG (CVPR'21)~\\cite{liu2021feddg}  &95.93&84.28&79.44&73.89&83.89 \\\\\n& CCST (Overall,K=3)           & \\textbf{96.65}&\\textbf{88.33}&\\textbf{78.20}&\\textbf{82.90}&\\textbf{86.52}\\\\\\hline            \n\\end{tabular}}\n       \\caption{Compare the results of our CCST (Overall, K=3) with baselines that are trained with local iterations=3.}\n       \\vspace{-3mm}\n       \\label{tab:budget}\n    \\end{table}", "table_label": "{tab:budget}", "table_numeric_cells": [["95.21", "95.21", 484, 489, 484, 489], ["82.91", "82.91", 490, 495, 490, 495], ["78.80", "78.80", 496, 501, 496, 501], ["73.99", "73.99", 502, 507, 502, 507], ["82.73", "82.73", 508, 513, 508, 513], ["95.63", "95.63", 570, 575, 570, 575], ["83.25", "83.25", 576, 581, 576, 581], ["81.10", "81.10", 582, 587, 582, 587], ["71.95", "71.95", 588, 593, 588, 593], ["82.98", "82.98", 594, 599, 594, 599], ["94.55", "94.55", 651, 656, 651, 656], ["83.20", "83.20", 657, 662, 657, 662], ["79.99", "79.99", 663, 668, 663, 668], ["72.79", "72.79", 669, 674, 669, 674], ["85.31", "85.31", 675, 680, 675, 680], ["96.47", "96.47", 738, 743, 738, 743], ["86.89", "86.89", 744, 749, 744, 749], ["81.06", "81.06", 750, 755, 750, 755], ["76.81", "76.81", 756, 761, 756, 761], ["82.63", "82.63", 762, 767, 762, 767], ["95.93", "95.93", 811, 816, 811, 816], ["84.28", "84.28", 817, 822, 817, 822], ["79.44", "79.44", 823, 828, 823, 828], ["73.89", "73.89", 829, 834, 829, 834], ["83.89", "83.89", 835, 840, 835, 840], ["96.65", "\\textbf{96.65}", 885, 890, 877, 891], ["88.33", "\\textbf{88.33}", 900, 905, 892, 906], ["78.20", "\\textbf{78.20}", 915, 920, 907, 921], ["82.90", "\\textbf{82.90}", 930, 935, 922, 936], ["86.52", "\\textbf{86.52}", 945, 950, 937, 951]], "text_chunk_selected": "    \\textbf{(a)} We propose a simple yet effective framework named cross-client style transfer (CCST).\n    Our approach achieves new state-of-the-art generalization performance in FL setting on two standard DG benchmarks (PACS~\\cite{PACS}, OfficeHome~\\cite{officehome}) and a large-scale medical image dataset (Camelyon17~\\cite{camelyon17}).\n    \\textbf{(b)}  Two types of styles with corresponding sharing mechanisms are proposed, named \\textit{overall domain style} and \\textit{single image style}, which can be chosen according to different circumstances. The diversity level of our method is also flexible to be adjusted.\n    \\textbf{(c)} The proposed method is orthogonal to many other SOTA DG methods. Therefore, our method can be readily applied to those DG methods to have a further performance boost. We also study the effectiveness of several SOTA DG methods when they are applied in the FL setting for image recognition.\n    \\textbf{(d)} We give an intuitive (Section \\ref{sec:discussion}) and experimental analysis (Section~\\ref{sec:privacy}) on the privacy-preserving performance of our style vectors to demonstrate that one can hardly reconstruct the original images merely from the style vectors using the generator from a SOTA GAN \\cite{liu2021towards} in FL setting.\n\nHowever, many of these methods require centralized data of different domains, violating the local data preservation in federated learning. Specifically, access for more than one domain is needed to augment data or generate new data in~\\cite{shankar2018generalizing,jackson2019style}, domain invariant representation learning or decomposing features is performed under the comparison across domains~\\cite{arjovsky2019invariant,piratla2020efficient,zhao2020domain} and some learning strategy based methods utilize extra one domain for meta-update~\\cite{li2019episodic,dou2019domain,du2020learning}. Nevertheless, some methods do not explicitly require centralized domains or can be adapted into federated learning with minor changes. For example, MixStyle~\\cite{zhou2021domain} can optionally conduct the style randomization in a single domain to augment data; \\cite{xu2021fourier} uses Fourier transformation to augmentation that is free of sharing data; JiGen~\\cite{carlucci2019domain} proposes a self-supervised task to enhance representation capability; RSC~\\cite{huang2020self} designs a learning strategy based on gradient operations without explicit multi-domain requirements.\n\n    where $\\{i_1,\\hdots,i_J\\}$ are randomly sampled image indices from client $C_n$. Sharing single image styles consumes relatively low computation but can lead to high communication costs for uploading multiple styles.\n    \\textbf{Overall domain style.} Domain style is the domain-level channel wise mean and standard variance, which considers all the images (pixels) in a client. Formally, assume client $C_n$ has $M$ training images with corresponding VGG features $\\{F_1 ^ {C_n}, F_2 ^ {C_n}, ..., F_M ^ {C_n}\\}$, the overall style  $S _ {overall} ^ {C_n}$ of this client is:\n\n\\begin{equation}\n    \\begin{aligned}\n     &S _ {overall} ^ {C_n} = (\\mu(F_{all} ^ {C_n}), \\sigma(F_{all} ^ {C_n})), \\\\\n     &F_{all} ^ {C_n} = Stack(F_1 ^ {C_n}, F_2 ^ {C_n}, ..., F_M ^ {C_n}).\n     \\end{aligned}\n    \\end{equation}\n\n\\subsection{Experimental Settings}\n\\textbf{Experiment setup.} We take each domain as a single client and conduct the leave-one-domain-out experiments on PACS and Office-Home datasets. Specifically, we select one client as the target test domain and train our model on the other clients. For the medical dataset, following the setting of source/target domains in literature~\\cite{camelyon17,koh2021wilds}, we apply the leave-one-domain-out setting to hospital 4 and hospital 5. For the PACS dataset, we follow the JiGen~\\cite{carlucci2019domain} to split 90\\% data of each client as the training set and 10\\% of that as the validation set for source clients, while for unseen target clients, the entire data is used for testing. For OfficeHome and Camelyon17, which have more data samples, the ratio between train and validation set is 4:1 for each source client, and 20\\% data is utilized as the test set on the unseen target client. We compare our method with \\textbf{FedDG}~\\cite{liu2021feddg}, which aims to solve DG problems in federated learning for medical image segmentation. We also test the performance of three centralized DG methods under FL setting (FedAvg), including \\textbf{JiGen}~\\cite{carlucci2019domain}, \\textbf{RSC}~\\cite{huang2020self} and \\textbf{MixStyle}~\\cite{zhou2021domain}. For \\textbf{COPA}, due to its re-designed layers of ResNet18 and unknown train-validate-test split, we copy the results for reference only. We regard every single client as a centralized dataset and apply these methods locally in FL. We report the test accuracy on each unseen client by choosing the best validation model.\n\\\\\\textbf{Implementation details.} We utilize the pre-trained AdaIN~\\cite{huang2017adain} to perform style transfer. Following \\cite{huang2020self}, we choose ResNet~\\cite{he2016deep} pre-trained on ImageNet as our backbone for PACS and Office-Home datasets. For the Camelyon17 dataset, we follow \\cite{harmofl} to use the DenseNet121~\\cite{huang2017densely}. We use FedAvg~\\cite{mcmahan2017communication} as our FL framework and train the model using SGD optimizer with $1e^{-3}$ learning rate for 500 communication rounds with one local update epoch on the PACS and Office-Home dataset. For Camelyon17, we train 100 communication rounds considering its large data amount.\nThe JiGen and RSC can be directly integrated into the FedAvg without further modifications. We adapt the MixStyle into an intra-client version that shuffles styles inside each batch of data to fit the federated setting. \nAll hyper-parameters of compared methods are chosen based on corresponding papers. We follow the standard training procedure of FedAvg~\\cite{mcmahan2017communication} for federated training.\nThe value of M in  is $\\lceil dataset\\_size / 32\\rceil$ in our experiment. \nThe framework is implemented with PyTorch and is trained on a single NVIDIA RTX 2080 Ti GPU. \n\n\\subsection{Results}\n\\textbf{Comparison with state-of-the-arts.} We compare our approach with three centralized DG methods and a federated DG method for on standard DG benchmarks PACS and OfficeHome as well as a real-world medical image dataset Camelyon17. \nTable~\\ref{table:all_results} presents the quantitative results of the image recognition task for different target clients on both PACS and Office-Home datasets. Each single letter column shows the test accuracy of the global model with the best validation accuracy on an unseen client. \nAlthough all DG methods can have better performance based on FedAvg, our approach demonstrates a significant boost over others on both datasets. On the PACS benchmark, our method achieves the average accuracy of 86.52\\% , which is 3.47\\% better than the second best method FedDG. Especially for the unseen client S (Sketch), CCST outperforms other methods by more than 7\\%. When photo is the target domain, all the methods perform similarly because we start training based on the ImageNet-pretrained model, which already has very high performance on photo images.\nBesides the PACS benchmark, the performance of our approach on the Office-Home dataset also has consistent results. Specifically, CCST outperforms other methods on average with a testing accuracy of 63.56\\%. \nDue to the small discrepancy in domain styles, all those domain generalization methods bring smaller improvements (less than 1\\%) than on the PACS. Overall, CCST outperforms other DG methods by a large margin.\nFigure~\\ref{fig:camelyon17_results} shows the results on the Camelyon17 dataset, our method outperforms other DG methods both when hospital 4 and hospital 5 as the target client. Some DG method, such as JiGen, is even harmful when applied in FL setting on the Camelyon17 dataset.   \n\n\\textbf{Orthogonality.}\nOur method is orthogonal to many other DG methods and can lead to additive performance via combined utilization. As many traditional domain generalization methods require centralized data and need to make use of various styles to achieve a domain robust model, CCST can serve as an initial step to benefit traditional DG methods with diversified styles. As shown in Figure~\\ref{fig:extra_boost}, we plot the average test accuracy of FedDG and three centralized DG methods on the PACS benchmark before and after applying our CCST. From the average accuracy, we can see all DG methods benefit a further boost with the help of cross-client style transfer (CCST). Interestingly, we find the performance when tested on sketch client (S) gains the largest improvement with CCST.\nBesides, we also extend our method with Tent~\\cite{wang2021tent} which uses entropy to update parameters in batch normalization layers at test time. As shown in Table~\\ref{tab:add_tent}, with this method combined in federated setting, our method surpasses the state-of-the-art DG method EoA~\\cite{arpit2021ensembledg} on the PACS benchmark in centralized setting with an average test accuracy of 90.47\\%. \n\n\\section{Training budget}\nTo be fairer in the training budget, we increase the local training iterations of baselines methods from 1 to 3 to compare with our overall (K=3) method. The results are shown in Table \\ref{tab:budget}. According to the results, more local iterations do not lead to obvious accuracy improvement for baseline methods, and our CCST (Overall, K=3) still outperforms all the baseline methods.", "table_source": "\\begin{table}[t]\n        \\centering\n        \\resizebox{0.5\\textwidth}{!}{\n        \\begin{tabular}{ccccccc}\n\\toprule\n & \\multirow{2}{*}{Setting} & \\multicolumn{4}{c}{Unseen client}                                         & \\multirow{2}{*}{Average} \\\\ \\cline{3-6}\n                          &                          & P              & A             & C              & \\multicolumn{1}{c}{S} &                          \\\\ \\hline\n& FedAvg (AISTATS'17)~\\cite{mcmahan2017communication}    &95.21&82.91&78.80&73.99&82.73 \\\\\n& Jigen (CVPR'19)~\\cite{carlucci2019domain}         &95.63&83.25&81.10&71.95&82.98 \\\\\n& RSC (ECCV'20)~\\cite{huang2020self}           &94.55&83.20&79.99&72.79&85.31 \\\\\n& MixStyle (ICLR'21)~\\cite{zhou2021domain}           &96.47&86.89&81.06&76.81&82.63 \\\\\n& FedDG (CVPR'21)~\\cite{liu2021feddg}  &95.93&84.28&79.44&73.89&83.89 \\\\\n& CCST (Overall,K=3)           & \\textbf{96.65}&\\textbf{88.33}&\\textbf{78.20}&\\textbf{82.90}&\\textbf{86.52}\\\\\\hline            \n\\end{tabular}}\n       \\caption{Compare the results of our CCST (Overall, K=3) with baselines that are trained with local iterations=3.}\n       \\vspace{-3mm}\n       \\label{tab:budget}\n    \\end{table}", "cell_list_gold": [{"value": "95.21", "char_index": [484, 489], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "82.91", "char_index": [490, 495], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "78.80", "char_index": [496, 501], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "73.99", "char_index": [502, 507], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "82.73", "char_index": [508, 513], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "95.63", "char_index": [570, 575], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "83.25", "char_index": [576, 581], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "81.10", "char_index": [582, 587], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "71.95", "char_index": [588, 593], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "82.98", "char_index": [594, 599], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"local iterations": "3"}, "model": "Jigen", "model settings": {"xx": "yy"}}, {"value": "94.55", "char_index": [651, 656], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "83.20", "char_index": [657, 662], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "79.99", "char_index": [663, 668], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "72.79", "char_index": [669, 674], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "85.31", "char_index": [675, 680], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"local iterations": "3"}, "model": "RSC", "model settings": {"xx": "yy"}}, {"value": "96.47", "char_index": [738, 743], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "86.89", "char_index": [744, 749], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "81.06", "char_index": [750, 755], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "76.81", "char_index": [756, 761], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "82.63", "char_index": [762, 767], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"local iterations": "3"}, "model": "MixStyle", "model settings": {"xx": "yy"}}, {"value": "95.93", "char_index": [811, 816], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "84.28", "char_index": [817, 822], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "79.44", "char_index": [823, 828], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "73.89", "char_index": [829, 834], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "83.89", "char_index": [835, 840], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"local iterations": "3"}, "model": "FedDG", "model settings": {"xx": "yy"}}, {"value": "96.65", "char_index": [885, 890], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "CCST", "model settings": {"Overall": "true", "K": "3"}}, {"value": "88.33", "char_index": [900, 905], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "CCST", "model settings": {"Overall": "true", "K": "3"}}, {"value": "78.20", "char_index": [915, 920], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "CCST", "model settings": {"Overall": "true", "K": "3"}}, {"value": "82.90", "char_index": [930, 935], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"local iterations": "3"}, "model": "CCST", "model settings": {"Overall": "true", "K": "3"}}, {"value": "86.52", "char_index": [945, 950], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Average accuracy", "experimental settings": {"local iterations": "3"}, "model": "CCST", "model settings": {"Overall": "true", "K": "3"}}]}, "2210.00912v1_table5": {"table_code": "\\begin{table*}[t]\n\\centering\n\\caption{Results of our CCST with different image style types and K values under PACS and Office-Home dataset. The backbone network is ResNet18. Each column represents a single unseen target client. }\n\\begin{tabular}{cccccc|ccccc}\n\\toprule\n\\multirow{4}{*}{Method} & \\multicolumn{5}{c}{PACS}                                & \\multicolumn{5}{c}{Office-Home}                         \\\\ \\cmidrule(lr){2-11} \n& \\multicolumn{1}{c}{P}         & A         & C         & S         & \\multicolumn{1}{c}{Avg.}     & A         & C         & P        & R       & \\multicolumn{1}{c}{Avg.}    \\\\ \\midrule\n\nFedAvg~\\cite{mcmahan2017communication}& 91.44&75.98&73.21&61.08&75.43\n& 60.08 & 45.59 & 69.48& \\textbf{72.82}& 61.99\\\\\nSingle(K=1)  &94.07&77.73&70.99&72.82&78.90\n&55.14&43.64&68.58&68.92&59.07 \\\\\nSingle(K=2)  &\\textbf{95.27}&79.05&72.82&77.88&81.26\n& 57.61&48.68&71.17&71.44&62.23\\\\\nSingle(K=3)  &94.79&80.27&71.72&\\textbf{80.86}&81.91\n&58.44&45.70&72.30&71.56&62.00\\\\\nOverall(K=1)  &94.19&79.88&72.14&75.41&80.41\n& 59.47&47.88&67.91&70.87&61.53\\\\\nOverall(K=2)  &93.95&79.79&72.18&77.96&80.97\n&57.82&\\textbf{50.52}&71.28&70.99&62.65\\\\ \nOverall(K=3)  &95.21&\\textbf{81.25}&\\textbf{73.34}&80.27&\\textbf{82.52}\n&{59.05}&50.06&\\textbf{72.97}&{71.67}&\\textbf{63.44}\\\\ \\bottomrule\n\\end{tabular}\n\\label{table:res18}\n\\end{table*}", "table_label": "{table:res18}", "table_numeric_cells": [["91.44", "91.44", 660, 665, 660, 665], ["75.98", "75.98", 666, 671, 666, 671], ["73.21", "73.21", 672, 677, 672, 677], ["61.08", "61.08", 678, 683, 678, 683], ["75.43", "75.43", 684, 689, 684, 689], ["60.08", "60.08", 692, 697, 692, 697], ["45.59", "45.59", 700, 705, 700, 705], ["69.48", "69.48", 708, 713, 708, 713], ["72.82", "\\textbf{72.82}", 723, 728, 715, 729], ["61.99", "61.99", 731, 736, 731, 736], ["94.07", "94.07", 753, 758, 753, 758], ["77.73", "77.73", 759, 764, 759, 764], ["70.99", "70.99", 765, 770, 765, 770], ["72.82", "72.82", 771, 776, 771, 776], ["78.90", "78.90", 777, 782, 777, 782], ["55.14", "55.14", 784, 789, 784, 789], ["43.64", "43.64", 790, 795, 790, 795], ["68.58", "68.58", 796, 801, 796, 801], ["68.92", "68.92", 802, 807, 802, 807], ["59.07", "59.07", 808, 813, 808, 813], ["95.27", "\\textbf{95.27}", 839, 844, 831, 845], ["79.05", "79.05", 846, 851, 846, 851], ["72.82", "72.82", 852, 857, 852, 857], ["77.88", "77.88", 858, 863, 858, 863], ["81.26", "81.26", 864, 869, 864, 869], ["57.61", "57.61", 872, 877, 872, 877], ["48.68", "48.68", 878, 883, 878, 883], ["71.17", "71.17", 884, 889, 884, 889], ["71.44", "71.44", 890, 895, 890, 895], ["62.23", "62.23", 896, 901, 896, 901], ["94.79", "94.79", 918, 923, 918, 923], ["80.27", "80.27", 924, 929, 924, 929], ["71.72", "71.72", 930, 935, 930, 935], ["80.86", "\\textbf{80.86}", 944, 949, 936, 950], ["81.91", "81.91", 951, 956, 951, 956], ["58.44", "58.44", 958, 963, 958, 963], ["45.70", "45.70", 964, 969, 964, 969], ["72.30", "72.30", 970, 975, 970, 975], ["71.56", "71.56", 976, 981, 976, 981], ["62.00", "62.00", 982, 987, 982, 987], ["94.19", "94.19", 1005, 1010, 1005, 1010], ["79.88", "79.88", 1011, 1016, 1011, 1016], ["72.14", "72.14", 1017, 1022, 1017, 1022], ["75.41", "75.41", 1023, 1028, 1023, 1028], ["80.41", "80.41", 1029, 1034, 1029, 1034], ["59.47", "59.47", 1037, 1042, 1037, 1042], ["47.88", "47.88", 1043, 1048, 1043, 1048], ["67.91", "67.91", 1049, 1054, 1049, 1054], ["70.87", "70.87", 1055, 1060, 1055, 1060], ["61.53", "61.53", 1061, 1066, 1061, 1066], ["93.95", "93.95", 1084, 1089, 1084, 1089], ["79.79", "79.79", 1090, 1095, 1090, 1095], ["72.18", "72.18", 1096, 1101, 1096, 1101], ["77.96", "77.96", 1102, 1107, 1102, 1107], ["80.97", "80.97", 1108, 1113, 1108, 1113], ["57.82", "57.82", 1115, 1120, 1115, 1120], ["50.52", "\\textbf{50.52}", 1129, 1134, 1121, 1135], ["71.28", "71.28", 1136, 1141, 1136, 1141], ["70.99", "70.99", 1142, 1147, 1142, 1147], ["62.65", "62.65", 1148, 1153, 1148, 1153], ["95.21", "95.21", 1172, 1177, 1172, 1177], ["81.25", "\\textbf{81.25}", 1186, 1191, 1178, 1192], ["73.34", "\\textbf{73.34}", 1201, 1206, 1193, 1207], ["80.27", "80.27", 1208, 1213, 1208, 1213], ["82.52", "\\textbf{82.52}", 1222, 1227, 1214, 1228], ["59.05", "{59.05}", 1231, 1236, 1230, 1237], ["50.06", "50.06", 1238, 1243, 1238, 1243], ["72.97", "\\textbf{72.97}", 1252, 1257, 1244, 1258], ["71.67", "{71.67}", 1260, 1265, 1259, 1266], ["63.44", "\\textbf{63.44}", 1275, 1280, 1267, 1281]], "text_chunk_selected": "    \\textbf{(a)} We propose a simple yet effective framework named cross-client style transfer (CCST).\n    Our approach achieves new state-of-the-art generalization performance in FL setting on two standard DG benchmarks (PACS~\\cite{PACS}, OfficeHome~\\cite{officehome}) and a large-scale medical image dataset (Camelyon17~\\cite{camelyon17}).\n    \\textbf{(b)}  Two types of styles with corresponding sharing mechanisms are proposed, named \\textit{overall domain style} and \\textit{single image style}, which can be chosen according to different circumstances. The diversity level of our method is also flexible to be adjusted.\n    \\textbf{(c)} The proposed method is orthogonal to many other SOTA DG methods. Therefore, our method can be readily applied to those DG methods to have a further performance boost. We also study the effectiveness of several SOTA DG methods when they are applied in the FL setting for image recognition.\n    \\textbf{(d)} We give an intuitive (Section \\ref{sec:discussion}) and experimental analysis (Section~\\ref{sec:privacy}) on the privacy-preserving performance of our style vectors to demonstrate that one can hardly reconstruct the original images merely from the style vectors using the generator from a SOTA GAN \\cite{liu2021towards} in FL setting.\n\n\\begin{equation}\n    \\begin{aligned}\n     &S _ {overall} ^ {C_n} = (\\mu(F_{all} ^ {C_n}), \\sigma(F_{all} ^ {C_n})), \\\\\n     &F_{all} ^ {C_n} = Stack(F_1 ^ {C_n}, F_2 ^ {C_n}, ..., F_M ^ {C_n}).\n     \\end{aligned}\n    \\end{equation}\n\n\\begin{itemize}\n\\renewcommand{\\labelitemi}{\\textbullet}\n    \\item Style bank $\\mathbf{B} _ {single}$ for single image styles: \n    \\begin{equation}\n     \\mathbf{B} _ {single} = \\{ S ^ {C_n} _ {bank} | n = 1,2,...N \\}.\n    \\end{equation}\n    \\item Style bank $\\mathbf{B} _ {overall}$ for overall domain style:\n    \\begin{equation}\n     \\mathbf{B} _ {overall} = \\{ S _ {overall} ^ {C_n} | n = 1,2,...,N \\}.\n    \\end{equation}\n\\end{itemize}\n\n\\subsection{Experimental Settings}\n\\textbf{Experiment setup.} We take each domain as a single client and conduct the leave-one-domain-out experiments on PACS and Office-Home datasets. Specifically, we select one client as the target test domain and train our model on the other clients. For the medical dataset, following the setting of source/target domains in literature~\\cite{camelyon17,koh2021wilds}, we apply the leave-one-domain-out setting to hospital 4 and hospital 5. For the PACS dataset, we follow the JiGen~\\cite{carlucci2019domain} to split 90\\% data of each client as the training set and 10\\% of that as the validation set for source clients, while for unseen target clients, the entire data is used for testing. For OfficeHome and Camelyon17, which have more data samples, the ratio between train and validation set is 4:1 for each source client, and 20\\% data is utilized as the test set on the unseen target client. We compare our method with \\textbf{FedDG}~\\cite{liu2021feddg}, which aims to solve DG problems in federated learning for medical image segmentation. We also test the performance of three centralized DG methods under FL setting (FedAvg), including \\textbf{JiGen}~\\cite{carlucci2019domain}, \\textbf{RSC}~\\cite{huang2020self} and \\textbf{MixStyle}~\\cite{zhou2021domain}. For \\textbf{COPA}, due to its re-designed layers of ResNet18 and unknown train-validate-test split, we copy the results for reference only. We regard every single client as a centralized dataset and apply these methods locally in FL. We report the test accuracy on each unseen client by choosing the best validation model.\n\\\\\\textbf{Implementation details.} We utilize the pre-trained AdaIN~\\cite{huang2017adain} to perform style transfer. Following \\cite{huang2020self}, we choose ResNet~\\cite{he2016deep} pre-trained on ImageNet as our backbone for PACS and Office-Home datasets. For the Camelyon17 dataset, we follow \\cite{harmofl} to use the DenseNet121~\\cite{huang2017densely}. We use FedAvg~\\cite{mcmahan2017communication} as our FL framework and train the model using SGD optimizer with $1e^{-3}$ learning rate for 500 communication rounds with one local update epoch on the PACS and Office-Home dataset. For Camelyon17, we train 100 communication rounds considering its large data amount.\nThe JiGen and RSC can be directly integrated into the FedAvg without further modifications. We adapt the MixStyle into an intra-client version that shuffles styles inside each batch of data to fit the federated setting. \nAll hyper-parameters of compared methods are chosen based on corresponding papers. We follow the standard training procedure of FedAvg~\\cite{mcmahan2017communication} for federated training.\nThe value of M in  is $\\lceil dataset\\_size / 32\\rceil$ in our experiment. \nThe framework is implemented with PyTorch and is trained on a single NVIDIA RTX 2080 Ti GPU. \n\n\\subsection{Results}\n\\textbf{Comparison with state-of-the-arts.} We compare our approach with three centralized DG methods and a federated DG method for on standard DG benchmarks PACS and OfficeHome as well as a real-world medical image dataset Camelyon17. \nTable~\\ref{table:all_results} presents the quantitative results of the image recognition task for different target clients on both PACS and Office-Home datasets. Each single letter column shows the test accuracy of the global model with the best validation accuracy on an unseen client. \nAlthough all DG methods can have better performance based on FedAvg, our approach demonstrates a significant boost over others on both datasets. On the PACS benchmark, our method achieves the average accuracy of 86.52\\% , which is 3.47\\% better than the second best method FedDG. Especially for the unseen client S (Sketch), CCST outperforms other methods by more than 7\\%. When photo is the target domain, all the methods perform similarly because we start training based on the ImageNet-pretrained model, which already has very high performance on photo images.\nBesides the PACS benchmark, the performance of our approach on the Office-Home dataset also has consistent results. Specifically, CCST outperforms other methods on average with a testing accuracy of 63.56\\%. \nDue to the small discrepancy in domain styles, all those domain generalization methods bring smaller improvements (less than 1\\%) than on the PACS. Overall, CCST outperforms other DG methods by a large margin.\nFigure~\\ref{fig:camelyon17_results} shows the results on the Camelyon17 dataset, our method outperforms other DG methods both when hospital 4 and hospital 5 as the target client. Some DG method, such as JiGen, is even harmful when applied in FL setting on the Camelyon17 dataset.   \n\n\\textbf{Control experiments on CCST.}\nWe conduct control experiments to investigate two types of image style with different augmentation levels. In Table \\ref{tab:ablation}, \\textit{Single} and \\textit{Overall} represents single image style and overall domain style mentioned in Section \\ref{sec:method} respectively. Different augmentation level $K$ indicates the intensity of augmentation.  We evaluate the four settings on the PACS benchmark with ResNet50. \nFor each kind of style, larger augmentation level K leads to a better performance. It is worth mentioning that the performance achieved by K=2 is similar with K=3, which indicates that our method can already achieve good performance with a relatively large K. \nFor different types of style, overall domain style shows more improvement than single image style because the overall style is able to represent a more general and accurate domain statistics, while single image styles may differ a lot due to randomness.\n\n\\section{Visualization of style transfer results on the Office-Home}\nIn this section, we show the qualitative results by visualizing images before and after the AdaIN~\\cite{huang2017adain}-based style transfer. In Figure~\\ref{fig:officehome_style_transfer}, we show images of four different target domains in the Office-Home dataset~\\cite{officehome}. Except for the art domain, samples from the other three domains show less domain gap. For each domain, we visualize the generated images using both random single image style and overall domain style. According to our experiment results, the overall style is usually more effective than using the single image style. Random single image style sometimes may choose an image that is not representative for the whole domain. For example, in Figure~\\ref{fig:officehome_style_transfer}, when transferring the clock image with the Clipart style into real-world style, the stylized image with overall style has a more colorful and representative style than that using random single image style.\n\n\\section{Additional experimental results}\nWe show the results of our CCST with ResNet~\\cite{he2016deep} as the backbone network on the PACS and Office-Home dataset in Table~\\ref{table:res18}. For PACS dataset, using ResNet18 (Table~\\ref{table:res18}) and ResNet50 (Table 2a) as backbone have consistent results: the overall style with K=3 leads to the best performance. When using ResNet18 as the backbone, the improvement upon baseline is more significant than that of using ResNet50. ", "table_source": "\\begin{table*}[t]\n\\centering\n\\caption{Results of our CCST with different image style types and K values under PACS and Office-Home dataset. The backbone network is ResNet18. Each column represents a single unseen target client. }\n\\begin{tabular}{cccccc|ccccc}\n\\toprule\n\\multirow{4}{*}{Method} & \\multicolumn{5}{c}{PACS}                                & \\multicolumn{5}{c}{Office-Home}                         \\\\ \\cmidrule(lr){2-11} \n& \\multicolumn{1}{c}{P}         & A         & C         & S         & \\multicolumn{1}{c}{Avg.}     & A         & C         & P        & R       & \\multicolumn{1}{c}{Avg.}    \\\\ \\midrule\n\nFedAvg~\\cite{mcmahan2017communication}& 91.44&75.98&73.21&61.08&75.43\n& 60.08 & 45.59 & 69.48& \\textbf{72.82}& 61.99\\\\\nSingle(K=1)  &94.07&77.73&70.99&72.82&78.90\n&55.14&43.64&68.58&68.92&59.07 \\\\\nSingle(K=2)  &\\textbf{95.27}&79.05&72.82&77.88&81.26\n& 57.61&48.68&71.17&71.44&62.23\\\\\nSingle(K=3)  &94.79&80.27&71.72&\\textbf{80.86}&81.91\n&58.44&45.70&72.30&71.56&62.00\\\\\nOverall(K=1)  &94.19&79.88&72.14&75.41&80.41\n& 59.47&47.88&67.91&70.87&61.53\\\\\nOverall(K=2)  &93.95&79.79&72.18&77.96&80.97\n&57.82&\\textbf{50.52}&71.28&70.99&62.65\\\\ \nOverall(K=3)  &95.21&\\textbf{81.25}&\\textbf{73.34}&80.27&\\textbf{82.52}\n&{59.05}&50.06&\\textbf{72.97}&{71.67}&\\textbf{63.44}\\\\ \\bottomrule\n\\end{tabular}\n\\label{table:res18}\n\\end{table*}", "cell_list_gold": [{"value": "91.44", "char_index": [660, 665], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "75.98", "char_index": [666, 671], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "73.21", "char_index": [672, 677], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "61.08", "char_index": [678, 683], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "75.43", "char_index": [684, 689], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "60.08", "char_index": [692, 697], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "45.59", "char_index": [700, 705], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "69.48", "char_index": [708, 713], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "72.82", "char_index": [723, 728], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "61.99", "char_index": [731, 736], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "FedAvg", "model settings": {"xx": "yy"}}, {"value": "94.07", "char_index": [753, 758], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "77.73", "char_index": [759, 764], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "70.99", "char_index": [765, 770], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "72.82", "char_index": [771, 776], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "78.90", "char_index": [777, 782], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "55.14", "char_index": [784, 789], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "43.64", "char_index": [790, 795], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "68.58", "char_index": [796, 801], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "68.92", "char_index": [802, 807], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "59.07", "char_index": [808, 813], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "1"}}, {"value": "95.27", "char_index": [839, 844], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "79.05", "char_index": [846, 851], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "72.82", "char_index": [852, 857], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "77.88", "char_index": [858, 863], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "81.26", "char_index": [864, 869], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "57.61", "char_index": [872, 877], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "48.68", "char_index": [878, 883], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "71.17", "char_index": [884, 889], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "71.44", "char_index": [890, 895], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "62.23", "char_index": [896, 901], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "2"}}, {"value": "94.79", "char_index": [918, 923], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "80.27", "char_index": [924, 929], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "71.72", "char_index": [930, 935], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "80.86", "char_index": [944, 949], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "81.91", "char_index": [951, 956], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "58.44", "char_index": [958, 963], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "45.70", "char_index": [964, 969], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "72.30", "char_index": [970, 975], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "71.56", "char_index": [976, 981], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "62.00", "char_index": [982, 987], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Single", "model settings": {"K": "3"}}, {"value": "94.19", "char_index": [1005, 1010], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "79.88", "char_index": [1011, 1016], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "72.14", "char_index": [1017, 1022], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "75.41", "char_index": [1023, 1028], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "80.41", "char_index": [1029, 1034], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "59.47", "char_index": [1037, 1042], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "47.88", "char_index": [1043, 1048], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "67.91", "char_index": [1049, 1054], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "70.87", "char_index": [1055, 1060], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "61.53", "char_index": [1061, 1066], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "1"}}, {"value": "93.95", "char_index": [1084, 1089], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "79.79", "char_index": [1090, 1095], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "72.18", "char_index": [1096, 1101], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "77.96", "char_index": [1102, 1107], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "80.97", "char_index": [1108, 1113], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "57.82", "char_index": [1115, 1120], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "50.52", "char_index": [1129, 1134], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "71.28", "char_index": [1136, 1141], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "70.99", "char_index": [1142, 1147], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "62.65", "char_index": [1148, 1153], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "2"}}, {"value": "95.21", "char_index": [1172, 1177], "type": "Result", "training data/set": "PACS", "test data/set": "PACS P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "81.25", "char_index": [1186, 1191], "type": "Result", "training data/set": "PACS", "test data/set": "PACS A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "73.34", "char_index": [1201, 1206], "type": "Result", "training data/set": "PACS", "test data/set": "PACS C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "80.27", "char_index": [1208, 1213], "type": "Result", "training data/set": "PACS", "test data/set": "PACS S", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "82.52", "char_index": [1222, 1227], "type": "Result", "training data/set": "PACS", "test data/set": "PACS", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "59.05", "char_index": [1231, 1236], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home A", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "50.06", "char_index": [1238, 1243], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home C", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "72.97", "char_index": [1252, 1257], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home P", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "71.67", "char_index": [1260, 1265], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home R", "task": "image recognition", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}, {"value": "63.44", "char_index": [1275, 1280], "type": "Result", "training data/set": "Office-Home", "test data/set": "Office-Home", "task": "image recognition", "metric": "Avg. accuracy", "experimental settings": {"xx": "yy"}, "model": "CCST Overall", "model settings": {"K": "3"}}]}, "2210.01091v1_table0": {"table_code": "\\begin{table}[]\n\\centering\n\\caption{\\label{Analysis - 1} Statistics and examples of the datasets of the analysis - 1}\n\\begin{tabular}{ |p{3cm}||p{4cm}|p{3.2cm}|p{4.3cm}|}\n \\hline\n \\multicolumn{4}{|c|}{Statistics of the datasets in analysis - 1} \\\\\n \\hline\n \\centering \\textbf{Data Type}& \\textbf{IMDB Movie Reviews} &\\textbf{SMS Spam Collection}&\\textbf{French and Spanish Reviews}\\\\\n \\hline\n \\centering \\textbf{Train}   & 35,000    & 3,900&140 \\\\\n \\centering \\textbf{Test}& 15,000  & 1,370   &60\\\\\n \\hline\n \\hline\n \\multicolumn{4}{|c|}{Snippets of the datasets} \\\\\n \\hline\n \\centering \\textbf{Data Requirement}&\\centering \\textbf{Dataset}&\\centering\\textbf{Text}& \\textbf{Classification} \\\\\n \\hline\n {Fine-Tuning}&{IMDB Movie Reviews}&{Probably my all-time favorite dedication ... \"up\" for this movie.}   & positive \\\\\n \\hline\n {Fine-Tuning}&{IMDB Movie Reviews}&{An awful film! It must have been up against  ...with same brevity.}   & negative \\\\\n \\hline\n {Target Dataset} &{SMS Spam Collection}&{Ok lar... Joking wif u oni...}   & not spam \\\\\n \\hline\n {Target Dataset} &{SMS Spam Collection}&{Free entry in 2 a wkly comp to win FA Cup final tkts ... 08452810075over18's}   & spam \\\\\n \\hline\n  {Cross Lingual} &{French and Spanish Reviews}&{Je pensais que c'\u00e9tait une ... int\u00e9ressante que \"Superman\" une super com\u00e9die \u00e0 aller voir entre amis.}   & positive \\\\\n \\hline\n {Cross Lingual} &{French and Spanish Reviews}&{En gros, il y a une famille o\u00f9 un petit ... Jake : ignorez-les.\t}   & negative \\\\\n \\hline\n\\end{tabular}\n\\end{table}", "table_label": "{Analysis - 1}", "table_numeric_cells": [["35,000", "35,000", 423, 429, 423, 429], ["3,900", "3,900", 435, 440, 435, 440], ["140", "140", 441, 444, 441, 444], ["15,000", "15,000", 475, 481, 475, 481], ["1,370", "1,370", 485, 490, 485, 490], ["60", "60", 494, 496, 494, 496]], "text_chunk_selected": "\\section{Introduction}\nTL is when a model is pre-trained on a rich dataset before fine-tuning or using feature-based transfer for a domain-specific task \\cite{you2020co,raffel2020exploring,houlsby2019parameter}. The role of TL in carrying out NLP tasks has increased significantly due to the need for understanding task interests in target domains where there is a lack of large enough datasets. In such cases, the transfer of knowledge from other domains is known to mitigate the problems of overfitting and yield strong model performance while carrying out the predictions\\cite{alyafeai2020survey,durrani2021transfer}. \n\n\\section{Related Work}\nSeveral works have shown the effectiveness of TL across various domains starting from image segmentation to several tasks in NLP \\cite{kim2020effectiveness, salza2022effectiveness,bengio2012deep}. Felbo et al. represented the understanding of emotions, sentiment and sarcasm across the text from various domains using the knowledge from a huge data source and transferring it to the model when the prediction was performed on the target domains \\cite{felbo2017using}. There is an improvement in the prediction against the model performance when trained only using the target dataset.       \n\n\\subsubsection{Analysis - 1: text classification}\nThe datasets to be used: \n\n\\begin{itemize}\n\\item IMDB Movie Reviews Dataset: A large dataset classifying the movie reviews into two classes: positive and negative \\cite{maas-EtAl:2011:ACL-HLT2011}. \n\\item Small SMS Spam Collection: A section of a large dataset classifying the messages to be spam or not. \n\\item French and Spanish Reviews Dataset: A small dataset of reviews in French and Spanish classified into positive and negative.\n\\end{itemize}\n\n\\subsubsection{Analysis - 2: sentimental analysis}\nThe datasets to be used: \n\n\\begin{itemize}\n\\item IMDB Genre Classification Dataset: A large dataset classifying the movie into 27 different genres using the descriptions.\n\\item Small Section of GoEmotions Dataset: A fine grained annotated dataset with sentences being classified into 28 emotions including neutral. \n\\item French - German Emotions Dataset: A small dataset with machine translation sentences classified into 13 different emotions. \n\\end{itemize}\n\n\\begin{itemize}\n\\item Paraphrase Adversaries from Word Scrambling (PAWS): A large dataset containing human-labeled sentence similarity \\cite{paws2019naacl, pawsx2019emnlp}. \n\\item Financial Domain Dataset: A small dataset consisting of related ticker names used in the stock exchange.\n\\item Section of PAWS-X Dataset: A small dataset containing cross-lingual sentence similarity data.  \n\\end{itemize}\n\n\\subsubsection{Knowledge transferred models' performance in text classification}\n\\label{NT}", "table_source": "\\begin{table}[]\n\\centering\n\\caption{\\label{Analysis - 1} Statistics and examples of the datasets of the analysis - 1}\n\\begin{tabular}{ |p{3cm}||p{4cm}|p{3.2cm}|p{4.3cm}|}\n \\hline\n \\multicolumn{4}{|c|}{Statistics of the datasets in analysis - 1} \\\\\n \\hline\n \\centering \\textbf{Data Type}& \\textbf{IMDB Movie Reviews} &\\textbf{SMS Spam Collection}&\\textbf{French and Spanish Reviews}\\\\\n \\hline\n \\centering \\textbf{Train}   & 35,000    & 3,900&140 \\\\\n \\centering \\textbf{Test}& 15,000  & 1,370   &60\\\\\n \\hline\n \\hline\n \\multicolumn{4}{|c|}{Snippets of the datasets} \\\\\n \\hline\n \\centering \\textbf{Data Requirement}&\\centering \\textbf{Dataset}&\\centering\\textbf{Text}& \\textbf{Classification} \\\\\n \\hline\n {Fine-Tuning}&{IMDB Movie Reviews}&{Probably my all-time favorite dedication ... \"up\" for this movie.}   & positive \\\\\n \\hline\n {Fine-Tuning}&{IMDB Movie Reviews}&{An awful film! It must have been up against  ...with same brevity.}   & negative \\\\\n \\hline\n {Target Dataset} &{SMS Spam Collection}&{Ok lar... Joking wif u oni...}   & not spam \\\\\n \\hline\n {Target Dataset} &{SMS Spam Collection}&{Free entry in 2 a wkly comp to win FA Cup final tkts ... 08452810075over18's}   & spam \\\\\n \\hline\n  {Cross Lingual} &{French and Spanish Reviews}&{Je pensais que c'\u00e9tait une ... int\u00e9ressante que \"Superman\" une super com\u00e9die \u00e0 aller voir entre amis.}   & positive \\\\\n \\hline\n {Cross Lingual} &{French and Spanish Reviews}&{En gros, il y a une famille o\u00f9 un petit ... Jake : ignorez-les.\t}   & negative \\\\\n \\hline\n\\end{tabular}\n\\end{table}", "cell_list_gold": [{"value": "35,000", "char_index": [423, 429], "type": "Data Stat.", "dataset": "IMDB Movie Reviews", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "3,900", "char_index": [435, 440], "type": "Data Stat.", "dataset": "SMS Spam Collection", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "140", "char_index": [441, 444], "type": "Data Stat.", "dataset": "French and Spanish Reviews", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "15,000", "char_index": [475, 481], "type": "Data Stat.", "dataset": "IMDB Movie Reviews", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "1,370", "char_index": [485, 490], "type": "Data Stat.", "dataset": "SMS Spam Collection", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "60", "char_index": [494, 496], "type": "Data Stat.", "dataset": "French and Spanish Reviews", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}]}, "2210.01091v1_table1": {"table_code": "\\begin{table}[]\n\\centering\n\\caption{\\label{Analysis - 2} Statistics and examples of the datasets of the analysis - 2}\n\\begin{tabular}{ |p{3cm}||p{4cm}|p{3.2cm}|p{4.3cm}|}\n \\hline\n \\multicolumn{4}{|c|}{Statistics of the datasets in analysis - 2} \\\\\n \\hline\n \\centering \\textbf{Data Type}& \\textbf{IMDB Genre Classification} &\\textbf{Section of GoEmotion}&\\textbf{French - German Emotions}\\\\\n \\hline\n \\centering \\textbf{Train}   & 33,657    & 3,800&1232 \\\\\n \\centering \\textbf{Test}& 14,425  & 1,400   &538\\\\\n \\hline\n \\hline\n \\multicolumn{4}{|c|}{Snippets of the datasets} \\\\\n \\hline\n \\centering \\textbf{Data Requirement}&\\centering \\textbf{Dataset}&\\centering\\textbf{Text}& \\textbf{Classification} \\\\\n \\hline\n {Fine-Tuning}&{IMDB Genre Classification}&{Listening in to a conversation between his doctor and parents ... sweetheart Peggy Blue.}   & drama \\\\\n \\hline\n {Fine-Tuning}&{IMDB Genre Classification}&{A brother and sister with a past incestuous  ...who get too close to him.}   & thriller \\\\\n \\hline\n {Target Dataset} &{Section of GoEmotion}&{I\u2019m really sorry about your situation :( ...}   & sadness \\\\\n \\hline\n {Target Dataset} &{Section of GoEmotion}&{It's wonderful because it's awful. At not with}   & admiration \\\\\n \\hline\n  {Cross Lingual} &{French - German Emotions}&{Moi moi gros triste..}   & sadness \\\\\n \\hline\n {Cross Lingual} &{French - German Emotions}&{Gleiche Kleidung und die T\u00fcr ist auf der anderen Seite schwarz - sieht so echt aus !!}   & approval \\\\\n \\hline\n\\end{tabular}\n\\end{table}", "table_label": "{Analysis - 2}", "table_numeric_cells": [["33,657", "33,657", 429, 435, 429, 435], ["3,800", "3,800", 441, 446, 441, 446], ["1232", "1232", 447, 451, 447, 451], ["14,425", "14,425", 482, 488, 482, 488], ["1,400", "1,400", 492, 497, 492, 497], ["538", "538", 501, 504, 501, 504]], "text_chunk_selected": "\\section{Introduction}\nTL is when a model is pre-trained on a rich dataset before fine-tuning or using feature-based transfer for a domain-specific task \\cite{you2020co,raffel2020exploring,houlsby2019parameter}. The role of TL in carrying out NLP tasks has increased significantly due to the need for understanding task interests in target domains where there is a lack of large enough datasets. In such cases, the transfer of knowledge from other domains is known to mitigate the problems of overfitting and yield strong model performance while carrying out the predictions\\cite{alyafeai2020survey,durrani2021transfer}. \n\n\\section{Related Work}\nSeveral works have shown the effectiveness of TL across various domains starting from image segmentation to several tasks in NLP \\cite{kim2020effectiveness, salza2022effectiveness,bengio2012deep}. Felbo et al. represented the understanding of emotions, sentiment and sarcasm across the text from various domains using the knowledge from a huge data source and transferring it to the model when the prediction was performed on the target domains \\cite{felbo2017using}. There is an improvement in the prediction against the model performance when trained only using the target dataset.       \n\n\\subsubsection{Analysis - 1: text classification}\nThe datasets to be used: \n\n\\begin{itemize}\n\\item IMDB Movie Reviews Dataset: A large dataset classifying the movie reviews into two classes: positive and negative \\cite{maas-EtAl:2011:ACL-HLT2011}. \n\\item Small SMS Spam Collection: A section of a large dataset classifying the messages to be spam or not. \n\\item French and Spanish Reviews Dataset: A small dataset of reviews in French and Spanish classified into positive and negative.\n\\end{itemize}\n\n\\subsubsection{Analysis - 2: sentimental analysis}\nThe datasets to be used: \n\n\\begin{itemize}\n\\item IMDB Genre Classification Dataset: A large dataset classifying the movie into 27 different genres using the descriptions.\n\\item Small Section of GoEmotions Dataset: A fine grained annotated dataset with sentences being classified into 28 emotions including neutral. \n\\item French - German Emotions Dataset: A small dataset with machine translation sentences classified into 13 different emotions. \n\\end{itemize}\n\n\\begin{itemize}\n\\item Paraphrase Adversaries from Word Scrambling (PAWS): A large dataset containing human-labeled sentence similarity \\cite{paws2019naacl, pawsx2019emnlp}. \n\\item Financial Domain Dataset: A small dataset consisting of related ticker names used in the stock exchange.\n\\item Section of PAWS-X Dataset: A small dataset containing cross-lingual sentence similarity data.  \n\\end{itemize}\n\n\\subsubsection{Knowledge transferred models' performance in text classification}\n\\label{NT}", "table_source": "\\begin{table}[]\n\\centering\n\\caption{\\label{Analysis - 2} Statistics and examples of the datasets of the analysis - 2}\n\\begin{tabular}{ |p{3cm}||p{4cm}|p{3.2cm}|p{4.3cm}|}\n \\hline\n \\multicolumn{4}{|c|}{Statistics of the datasets in analysis - 2} \\\\\n \\hline\n \\centering \\textbf{Data Type}& \\textbf{IMDB Genre Classification} &\\textbf{Section of GoEmotion}&\\textbf{French - German Emotions}\\\\\n \\hline\n \\centering \\textbf{Train}   & 33,657    & 3,800&1232 \\\\\n \\centering \\textbf{Test}& 14,425  & 1,400   &538\\\\\n \\hline\n \\hline\n \\multicolumn{4}{|c|}{Snippets of the datasets} \\\\\n \\hline\n \\centering \\textbf{Data Requirement}&\\centering \\textbf{Dataset}&\\centering\\textbf{Text}& \\textbf{Classification} \\\\\n \\hline\n {Fine-Tuning}&{IMDB Genre Classification}&{Listening in to a conversation between his doctor and parents ... sweetheart Peggy Blue.}   & drama \\\\\n \\hline\n {Fine-Tuning}&{IMDB Genre Classification}&{A brother and sister with a past incestuous  ...who get too close to him.}   & thriller \\\\\n \\hline\n {Target Dataset} &{Section of GoEmotion}&{I\u2019m really sorry about your situation :( ...}   & sadness \\\\\n \\hline\n {Target Dataset} &{Section of GoEmotion}&{It's wonderful because it's awful. At not with}   & admiration \\\\\n \\hline\n  {Cross Lingual} &{French - German Emotions}&{Moi moi gros triste..}   & sadness \\\\\n \\hline\n {Cross Lingual} &{French - German Emotions}&{Gleiche Kleidung und die T\u00fcr ist auf der anderen Seite schwarz - sieht so echt aus !!}   & approval \\\\\n \\hline\n\\end{tabular}\n\\end{table}", "cell_list_gold": [{"value": "33,657", "char_index": [429, 435], "type": "Data Stat.", "dataset": "IMDB Genre Classification", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "3,800", "char_index": [441, 446], "type": "Data Stat.", "dataset": "Section of GoEmotion", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "1232", "char_index": [447, 451], "type": "Data Stat.", "dataset": "French - German Emotions", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "14,425", "char_index": [482, 488], "type": "Data Stat.", "dataset": "IMDB Genre Classification", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "1,400", "char_index": [492, 497], "type": "Data Stat.", "dataset": "Section of GoEmotion", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "538", "char_index": [501, 504], "type": "Data Stat.", "dataset": "French - German Emotions", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}]}, "2210.01091v1_table2": {"table_code": "\\begin{table}[]\n\\centering\n\\caption{\\label{Analysis - 3} Statistics and examples of the datasets of the analysis - 3}\n\\begin{tabular}{ |p{3cm}||p{4cm}|p{3.2cm}|p{4.3cm}|}\n \\hline\n \\multicolumn{4}{|c|}{Statistics of the datasets in analysis - 3} \\\\\n \\hline\n \\centering \\textbf{Data Type}& \\textbf{PAWS} &\\textbf{Financial Domain}&\\textbf{Section of PAWS-X}\\\\\n \\hline\n \\centering \\textbf{Train}   & 34,580    & 1,500&2800 \\\\\n \\centering \\textbf{Test}& 14,820  & 612   &1200\\\\\n \\hline\n \\hline\n \\multicolumn{4}{|c|}{Snippets of the datasets} \\\\\n \\hline\n \\centering \\textbf{Data Requirement}&\\centering \\textbf{Dataset}&\\centering\\textbf{Text-1}& \\textbf{Text-2} \\\\\n \\hline\n {Fine-Tuning}&{PAWS}&{In Paris , in October 1560 , ... England through Scotland.}   & {In October 1560 , he secretly met with ... England .} \\\\\n \\hline\n {Fine-Tuning}&{PAWS}&{The NBA season of 1975 -- 76 was the ... Basketball Association.}   & {The 1975 -- 76 season of the National ... of the NBA} \\\\\n \\hline\n {Target Dataset} &{Financial Domain}&{vanguard small cap index adm}   & {vanguard small-cap index fund inst} \\\\\n \\hline\n {Target Dataset} &{Financial Domain}&{schwab intl large company index etf}   & {schwab strategic tr fundamental intl large co index etf} \\\\\n \\hline\n  {Cross Lingual} &{Section of PAWS-X}&{El Prudential Building (HMB) anteriormente Houston ...}   & {fue un rascacielos en el Centro...} \\\\\n \\hline\n {Cross Lingual} &{Section of PAWS-X}&{L'exception tait entre fin 2005 et 2009 lorsqu'il ...}   & {La rivi re Tabaci est un affluent de la rivi} \\\\\n \\hline\n\\end{tabular}\n\\end{table}", "table_label": "{Analysis - 3}", "table_numeric_cells": [["34,580", "34,580", 397, 403, 397, 403], ["1,500", "1,500", 409, 414, 409, 414], ["2800", "2800", 415, 419, 415, 419], ["14,820", "14,820", 450, 456, 450, 456], ["612", "612", 460, 463, 460, 463], ["1200", "1200", 467, 471, 467, 471]], "text_chunk_selected": "\\section{Introduction}\nTL is when a model is pre-trained on a rich dataset before fine-tuning or using feature-based transfer for a domain-specific task \\cite{you2020co,raffel2020exploring,houlsby2019parameter}. The role of TL in carrying out NLP tasks has increased significantly due to the need for understanding task interests in target domains where there is a lack of large enough datasets. In such cases, the transfer of knowledge from other domains is known to mitigate the problems of overfitting and yield strong model performance while carrying out the predictions\\cite{alyafeai2020survey,durrani2021transfer}. \n\n\\section{Related Work}\nSeveral works have shown the effectiveness of TL across various domains starting from image segmentation to several tasks in NLP \\cite{kim2020effectiveness, salza2022effectiveness,bengio2012deep}. Felbo et al. represented the understanding of emotions, sentiment and sarcasm across the text from various domains using the knowledge from a huge data source and transferring it to the model when the prediction was performed on the target domains \\cite{felbo2017using}. There is an improvement in the prediction against the model performance when trained only using the target dataset.       \n\n\\begin{itemize}\n\\item IMDB Movie Reviews Dataset: A large dataset classifying the movie reviews into two classes: positive and negative \\cite{maas-EtAl:2011:ACL-HLT2011}. \n\\item Small SMS Spam Collection: A section of a large dataset classifying the messages to be spam or not. \n\\item French and Spanish Reviews Dataset: A small dataset of reviews in French and Spanish classified into positive and negative.\n\\end{itemize}\n\n\\subsubsection{Analysis - 2: sentimental analysis}\nThe datasets to be used: \n\n\\begin{itemize}\n\\item IMDB Genre Classification Dataset: A large dataset classifying the movie into 27 different genres using the descriptions.\n\\item Small Section of GoEmotions Dataset: A fine grained annotated dataset with sentences being classified into 28 emotions including neutral. \n\\item French - German Emotions Dataset: A small dataset with machine translation sentences classified into 13 different emotions. \n\\end{itemize}\n\n\\begin{itemize}\n\\item Paraphrase Adversaries from Word Scrambling (PAWS): A large dataset containing human-labeled sentence similarity \\cite{paws2019naacl, pawsx2019emnlp}. \n\\item Financial Domain Dataset: A small dataset consisting of related ticker names used in the stock exchange.\n\\item Section of PAWS-X Dataset: A small dataset containing cross-lingual sentence similarity data.  \n\\end{itemize}\n\n\\subsubsection{Knowledge transferred models' performance in text classification}\n\\label{NT}\n\nIn our study, we nevertheless saw some interesting knowledge transfer results, like in Section \\ref{PT}, where the positive transfer of genre knowledge helped predict better sentiments over the small section of GoEmotions. In Section \\ref{NT}, where the models with binary sentiment knowledge didn't help in predicting the spam. ", "table_source": "\\begin{table}[]\n\\centering\n\\caption{\\label{Analysis - 3} Statistics and examples of the datasets of the analysis - 3}\n\\begin{tabular}{ |p{3cm}||p{4cm}|p{3.2cm}|p{4.3cm}|}\n \\hline\n \\multicolumn{4}{|c|}{Statistics of the datasets in analysis - 3} \\\\\n \\hline\n \\centering \\textbf{Data Type}& \\textbf{PAWS} &\\textbf{Financial Domain}&\\textbf{Section of PAWS-X}\\\\\n \\hline\n \\centering \\textbf{Train}   & 34,580    & 1,500&2800 \\\\\n \\centering \\textbf{Test}& 14,820  & 612   &1200\\\\\n \\hline\n \\hline\n \\multicolumn{4}{|c|}{Snippets of the datasets} \\\\\n \\hline\n \\centering \\textbf{Data Requirement}&\\centering \\textbf{Dataset}&\\centering\\textbf{Text-1}& \\textbf{Text-2} \\\\\n \\hline\n {Fine-Tuning}&{PAWS}&{In Paris , in October 1560 , ... England through Scotland.}   & {In October 1560 , he secretly met with ... England .} \\\\\n \\hline\n {Fine-Tuning}&{PAWS}&{The NBA season of 1975 -- 76 was the ... Basketball Association.}   & {The 1975 -- 76 season of the National ... of the NBA} \\\\\n \\hline\n {Target Dataset} &{Financial Domain}&{vanguard small cap index adm}   & {vanguard small-cap index fund inst} \\\\\n \\hline\n {Target Dataset} &{Financial Domain}&{schwab intl large company index etf}   & {schwab strategic tr fundamental intl large co index etf} \\\\\n \\hline\n  {Cross Lingual} &{Section of PAWS-X}&{El Prudential Building (HMB) anteriormente Houston ...}   & {fue un rascacielos en el Centro...} \\\\\n \\hline\n {Cross Lingual} &{Section of PAWS-X}&{L'exception tait entre fin 2005 et 2009 lorsqu'il ...}   & {La rivi re Tabaci est un affluent de la rivi} \\\\\n \\hline\n\\end{tabular}\n\\end{table}", "cell_list_gold": [{"value": "34,580", "char_index": [397, 403], "type": "Data Stat.", "dataset": "PAWS", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "1,500", "char_index": [409, 414], "type": "Data Stat.", "dataset": "Financial Domain", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "2800", "char_index": [415, 419], "type": "Data Stat.", "dataset": "Section of PAWS-X", "attribute name": "number of examples", "sub-set/group name": "Train", "dataset features": {"xx": "yy"}}, {"value": "14,820", "char_index": [450, 456], "type": "Data Stat.", "dataset": "PAWS", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "612", "char_index": [460, 463], "type": "Data Stat.", "dataset": "Financial Domain", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}, {"value": "1200", "char_index": [467, 471], "type": "Data Stat.", "dataset": "Section of PAWS-X", "attribute name": "number of examples", "sub-set/group name": "Test", "dataset features": {"xx": "yy"}}]}, "2210.01091v1_table3": {"table_code": "\\begin{table}[]\n\\centering\n\\caption{\\label{Results} Comparing the Average Accuracy of Models' with and without knowledge transfer from the source dataset}\n\\begin{tabular}{|p{2cm}||p{2.75cm}|p{1cm}|p{1.2cm}|p{1.5cm}|p{1.75cm}|p{1cm}|p{1.2cm}|}\n \\hline\n \\centering \\textbf{Task Kind}&\\centering \\textbf{Task Name}& \\textbf{BERT} &\\textbf{BERT (fine-tuned)}&\\textbf{ROBERTa} &\\textbf{ROBERTa (fine-tuned)}&\\textbf{XLNet} &\\textbf{XLNet (fine-tuned)}\\\\\n \\hline\n \\centering \\textbf{Text Classification}&\\centering \\textbf{Small SMS Spam Collection}   & 0.71    & 0.621&0.732&0.548 &0.576 & 0.48 \\\\\n \\hline\n \\centering \\textbf{Text Classification}&\\centering \\textbf{French and Spanish Reviews Dataset}& 0.63  & 0.672   &0.58 &0.51 &0.44 & 0.46\\\\\n  \\hline\n  \\centering \\textbf{Sentimental Analysis}&\\centering \\textbf{Small Section of GoEmotions Dataset}   & 0.832    & 0.89&0.71 &0.76 &0.59 & 0.62\\\\\n \\hline\n \\centering \\textbf{Sentimental Analysis}&\\centering \\textbf{French - German Emotions Dataset}& 0.68  & 0.61   &0.716 &0.66 &0.57 & 0.53\\\\\n  \\hline\n  \\centering \\textbf{Sentence Similarity}&\\centering \\textbf{Financial Domain Dataset}   & 0.67    & 0.52 &0.72 & 0.67 &0.55 & 0.47 \\\\\n \\hline\n \\centering \\textbf{Sentence Similarity}&\\centering \\textbf{Section of PAWS-X Dataset}& 0.67  & 0.61   &0.736 & 0.711 &0.58 & 0.62\\\\\n \\hline\n \\end{tabular}\n\\end{table}", "table_label": "{Results}", "table_numeric_cells": [["0.71", "0.71", 548, 552, 548, 552], ["0.621", "0.621", 558, 563, 558, 563], ["0.732", "0.732", 564, 569, 564, 569], ["0.548", "0.548", 570, 575, 570, 575], ["0.576", "0.576", 577, 582, 577, 582], ["0.48", "0.48", 585, 589, 585, 589], ["0.63", "0.63", 698, 702, 698, 702], ["0.672", "0.672", 706, 711, 706, 711], ["0.58", "0.58", 715, 719, 715, 719], ["0.51", "0.51", 721, 725, 721, 725], ["0.44", "0.44", 727, 731, 727, 731], ["0.46", "0.46", 734, 738, 734, 738], ["0.832", "0.832", 853, 858, 853, 858], ["0.89", "0.89", 864, 868, 864, 868], ["0.71", "0.71", 869, 873, 869, 873], ["0.76", "0.76", 875, 879, 875, 879], ["0.59", "0.59", 881, 885, 881, 885], ["0.62", "0.62", 888, 892, 888, 892], ["0.68", "0.68", 999, 1003, 999, 1003], ["0.61", "0.61", 1007, 1011, 1007, 1011], ["0.716", "0.716", 1015, 1020, 1015, 1020], ["0.66", "0.66", 1022, 1026, 1022, 1026], ["0.57", "0.57", 1028, 1032, 1028, 1032], ["0.53", "0.53", 1035, 1039, 1035, 1039], ["0.67", "0.67", 1142, 1146, 1142, 1146], ["0.52", "0.52", 1152, 1156, 1152, 1156], ["0.72", "0.72", 1158, 1162, 1158, 1162], ["0.67", "0.67", 1165, 1169, 1165, 1169], ["0.55", "0.55", 1171, 1175, 1171, 1175], ["0.47", "0.47", 1178, 1182, 1178, 1182], ["0.67", "0.67", 1282, 1286, 1282, 1286], ["0.61", "0.61", 1290, 1294, 1290, 1294], ["0.736", "0.736", 1298, 1303, 1298, 1303], ["0.711", "0.711", 1306, 1311, 1306, 1311], ["0.58", "0.58", 1313, 1317, 1313, 1317], ["0.62", "0.62", 1320, 1324, 1320, 1324]], "text_chunk_selected": "\\section{Introduction}\nTL is when a model is pre-trained on a rich dataset before fine-tuning or using feature-based transfer for a domain-specific task \\cite{you2020co,raffel2020exploring,houlsby2019parameter}. The role of TL in carrying out NLP tasks has increased significantly due to the need for understanding task interests in target domains where there is a lack of large enough datasets. In such cases, the transfer of knowledge from other domains is known to mitigate the problems of overfitting and yield strong model performance while carrying out the predictions\\cite{alyafeai2020survey,durrani2021transfer}. \n\n\\section{Related Work}\nSeveral works have shown the effectiveness of TL across various domains starting from image segmentation to several tasks in NLP \\cite{kim2020effectiveness, salza2022effectiveness,bengio2012deep}. Felbo et al. represented the understanding of emotions, sentiment and sarcasm across the text from various domains using the knowledge from a huge data source and transferring it to the model when the prediction was performed on the target domains \\cite{felbo2017using}. There is an improvement in the prediction against the model performance when trained only using the target dataset.       \n\n\\subsubsection{Analysis - 1: text classification}\nThe datasets to be used: \n\n\\begin{itemize}\n\\item IMDB Movie Reviews Dataset: A large dataset classifying the movie reviews into two classes: positive and negative \\cite{maas-EtAl:2011:ACL-HLT2011}. \n\\item Small SMS Spam Collection: A section of a large dataset classifying the messages to be spam or not. \n\\item French and Spanish Reviews Dataset: A small dataset of reviews in French and Spanish classified into positive and negative.\n\\end{itemize}\n\n\\subsubsection{Analysis - 2: sentimental analysis}\nThe datasets to be used: \n\n\\begin{itemize}\n\\item IMDB Genre Classification Dataset: A large dataset classifying the movie into 27 different genres using the descriptions.\n\\item Small Section of GoEmotions Dataset: A fine grained annotated dataset with sentences being classified into 28 emotions including neutral. \n\\item French - German Emotions Dataset: A small dataset with machine translation sentences classified into 13 different emotions. \n\\end{itemize}\n\n\\begin{itemize}\n\\item Paraphrase Adversaries from Word Scrambling (PAWS): A large dataset containing human-labeled sentence similarity \\cite{paws2019naacl, pawsx2019emnlp}. \n\\item Financial Domain Dataset: A small dataset consisting of related ticker names used in the stock exchange.\n\\item Section of PAWS-X Dataset: A small dataset containing cross-lingual sentence similarity data.  \n\\end{itemize}\n\n\\section{Results}\nWe have divided our analyses into two experiments based on the parameters to check model performance. In the first experiment, the models have trained on the target dataset for the tasks: text classification, sentimental analysis, and sentence similarity. Similarly, in the second experiment, the models were fine-tuned with the first dataset of each analysis. Then, the models were made to make predictions on the target dataset using the acquired knowledge from the source.", "table_source": "\\begin{table}[]\n\\centering\n\\caption{\\label{Results} Comparing the Average Accuracy of Models' with and without knowledge transfer from the source dataset}\n\\begin{tabular}{|p{2cm}||p{2.75cm}|p{1cm}|p{1.2cm}|p{1.5cm}|p{1.75cm}|p{1cm}|p{1.2cm}|}\n \\hline\n \\centering \\textbf{Task Kind}&\\centering \\textbf{Task Name}& \\textbf{BERT} &\\textbf{BERT (fine-tuned)}&\\textbf{ROBERTa} &\\textbf{ROBERTa (fine-tuned)}&\\textbf{XLNet} &\\textbf{XLNet (fine-tuned)}\\\\\n \\hline\n \\centering \\textbf{Text Classification}&\\centering \\textbf{Small SMS Spam Collection}   & 0.71    & 0.621&0.732&0.548 &0.576 & 0.48 \\\\\n \\hline\n \\centering \\textbf{Text Classification}&\\centering \\textbf{French and Spanish Reviews Dataset}& 0.63  & 0.672   &0.58 &0.51 &0.44 & 0.46\\\\\n  \\hline\n  \\centering \\textbf{Sentimental Analysis}&\\centering \\textbf{Small Section of GoEmotions Dataset}   & 0.832    & 0.89&0.71 &0.76 &0.59 & 0.62\\\\\n \\hline\n \\centering \\textbf{Sentimental Analysis}&\\centering \\textbf{French - German Emotions Dataset}& 0.68  & 0.61   &0.716 &0.66 &0.57 & 0.53\\\\\n  \\hline\n  \\centering \\textbf{Sentence Similarity}&\\centering \\textbf{Financial Domain Dataset}   & 0.67    & 0.52 &0.72 & 0.67 &0.55 & 0.47 \\\\\n \\hline\n \\centering \\textbf{Sentence Similarity}&\\centering \\textbf{Section of PAWS-X Dataset}& 0.67  & 0.61   &0.736 & 0.711 &0.58 & 0.62\\\\\n \\hline\n \\end{tabular}\n\\end{table}", "cell_list_gold": [{"value": "0.71", "char_index": [548, 552], "type": "Result", "training data/set": "Small SMS Spam Collection", "test data/set": "Small SMS Spam Collection", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"xx": "yy"}}, {"value": "0.621", "char_index": [558, 563], "type": "Result", "training data/set": "Small SMS Spam Collection", "test data/set": "Small SMS Spam Collection", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"fine-tuned": "true"}}, {"value": "0.732", "char_index": [564, 569], "type": "Result", "training data/set": "Small SMS Spam Collection", "test data/set": "Small SMS Spam Collection", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"xx": "yy"}}, {"value": "0.548", "char_index": [570, 575], "type": "Result", "training data/set": "Small SMS Spam Collection", "test data/set": "Small SMS Spam Collection", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"fine-tuned": "true"}}, {"value": "0.576", "char_index": [577, 582], "type": "Result", "training data/set": "Small SMS Spam Collection", "test data/set": "Small SMS Spam Collection", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"xx": "yy"}}, {"value": "0.48", "char_index": [585, 589], "type": "Result", "training data/set": "Small SMS Spam Collection", "test data/set": "Small SMS Spam Collection", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"fine-tuned": "true"}}, {"value": "0.63", "char_index": [698, 702], "type": "Result", "training data/set": "French and Spanish Reviews Dataset", "test data/set": "French and Spanish Reviews Dataset", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"xx": "yy"}}, {"value": "0.672", "char_index": [706, 711], "type": "Result", "training data/set": "French and Spanish Reviews Dataset", "test data/set": "French and Spanish Reviews Dataset", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"fine-tuned": "true"}}, {"value": "0.58", "char_index": [715, 719], "type": "Result", "training data/set": "French and Spanish Reviews Dataset", "test data/set": "French and Spanish Reviews Dataset", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"xx": "yy"}}, {"value": "0.51", "char_index": [721, 725], "type": "Result", "training data/set": "French and Spanish Reviews Dataset", "test data/set": "French and Spanish Reviews Dataset", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"fine-tuned": "true"}}, {"value": "0.44", "char_index": [727, 731], "type": "Result", "training data/set": "French and Spanish Reviews Dataset", "test data/set": "French and Spanish Reviews Dataset", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"xx": "yy"}}, {"value": "0.46", "char_index": [734, 738], "type": "Result", "training data/set": "French and Spanish Reviews Dataset", "test data/set": "French and Spanish Reviews Dataset", "task": "Text Classification", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"fine-tuned": "true"}}, {"value": "0.832", "char_index": [853, 858], "type": "Result", "training data/set": "Small Section of GoEmotions Dataset", "test data/set": "Small Section of GoEmotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"xx": "yy"}}, {"value": "0.89", "char_index": [864, 868], "type": "Result", "training data/set": "Small Section of GoEmotions Dataset", "test data/set": "Small Section of GoEmotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"fine-tuned": "true"}}, {"value": "0.71", "char_index": [869, 873], "type": "Result", "training data/set": "Small Section of GoEmotions Dataset", "test data/set": "Small Section of GoEmotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"xx": "yy"}}, {"value": "0.76", "char_index": [875, 879], "type": "Result", "training data/set": "Small Section of GoEmotions Dataset", "test data/set": "Small Section of GoEmotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"fine-tuned": "true"}}, {"value": "0.59", "char_index": [881, 885], "type": "Result", "training data/set": "Small Section of GoEmotions Dataset", "test data/set": "Small Section of GoEmotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"xx": "yy"}}, {"value": "0.62", "char_index": [888, 892], "type": "Result", "training data/set": "Small Section of GoEmotions Dataset", "test data/set": "Small Section of GoEmotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"fine-tuned": "true"}}, {"value": "0.68", "char_index": [999, 1003], "type": "Result", "training data/set": "French - German Emotions Dataset", "test data/set": "French - German Emotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"xx": "yy"}}, {"value": "0.61", "char_index": [1007, 1011], "type": "Result", "training data/set": "French - German Emotions Dataset", "test data/set": "French - German Emotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"fine-tuned": "true"}}, {"value": "0.716", "char_index": [1015, 1020], "type": "Result", "training data/set": "French - German Emotions Dataset", "test data/set": "French - German Emotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"xx": "yy"}}, {"value": "0.66", "char_index": [1022, 1026], "type": "Result", "training data/set": "French - German Emotions Dataset", "test data/set": "French - German Emotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"fine-tuned": "true"}}, {"value": "0.57", "char_index": [1028, 1032], "type": "Result", "training data/set": "French - German Emotions Dataset", "test data/set": "French - German Emotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"xx": "yy"}}, {"value": "0.53", "char_index": [1035, 1039], "type": "Result", "training data/set": "French - German Emotions Dataset", "test data/set": "French - German Emotions Dataset", "task": "Sentimental Analysis", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"fine-tuned": "true"}}, {"value": "0.67", "char_index": [1142, 1146], "type": "Result", "training data/set": "Financial Domain Dataset", "test data/set": "Financial Domain Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"xx": "yy"}}, {"value": "0.52", "char_index": [1152, 1156], "type": "Result", "training data/set": "Financial Domain Dataset", "test data/set": "Financial Domain Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"fine-tuned": "true"}}, {"value": "0.72", "char_index": [1158, 1162], "type": "Result", "training data/set": "Financial Domain Dataset", "test data/set": "Financial Domain Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"xx": "yy"}}, {"value": "0.67", "char_index": [1165, 1169], "type": "Result", "training data/set": "Financial Domain Dataset", "test data/set": "Financial Domain Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"fine-tuned": "true"}}, {"value": "0.55", "char_index": [1171, 1175], "type": "Result", "training data/set": "Financial Domain Dataset", "test data/set": "Financial Domain Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"xx": "yy"}}, {"value": "0.47", "char_index": [1178, 1182], "type": "Result", "training data/set": "Financial Domain Dataset", "test data/set": "Financial Domain Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"fine-tuned": "true"}}, {"value": "0.67", "char_index": [1282, 1286], "type": "Result", "training data/set": "Section of PAWS-X Dataset", "test data/set": "Section of PAWS-X Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"xx": "yy"}}, {"value": "0.61", "char_index": [1290, 1294], "type": "Result", "training data/set": "Section of PAWS-X Dataset", "test data/set": "Section of PAWS-X Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT", "model settings": {"fine-tuned": "true"}}, {"value": "0.736", "char_index": [1298, 1303], "type": "Result", "training data/set": "Section of PAWS-X Dataset", "test data/set": "Section of PAWS-X Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"xx": "yy"}}, {"value": "0.711", "char_index": [1306, 1311], "type": "Result", "training data/set": "Section of PAWS-X Dataset", "test data/set": "Section of PAWS-X Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "ROBERTa", "model settings": {"fine-tuned": "true"}}, {"value": "0.58", "char_index": [1313, 1317], "type": "Result", "training data/set": "Section of PAWS-X Dataset", "test data/set": "Section of PAWS-X Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"xx": "yy"}}, {"value": "0.62", "char_index": [1320, 1324], "type": "Result", "training data/set": "Section of PAWS-X Dataset", "test data/set": "Section of PAWS-X Dataset", "task": "Sentence Similarity", "metric": "accuracy", "experimental settings": {"xx": "yy"}, "model": "XLNet", "model settings": {"fine-tuned": "true"}}]}, "2211.05783v1_table11": {"table_code": "\\subfloat[\n\\textbf{Flow to depth transfer: performance comparison}.\n\\label{tab:transfer_flow_to_depth}\n]{\n\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\\begin{tabular}{lccccccccccccc}\n    \\toprule\n    \n    \n    Model & Abs Rel & Sq Rel & RMSE & RMSE log \\\\\n    \n    \\midrule\n    \n    rand init, w/o ft & 0.536 & 1.309 & 1.300 & 0.584 \\\\\n    flow init, w/o ft & 0.198 & 0.364 & 0.599 & 0.235 \\\\\n    \n    \\midrule\n    \n    rand init, ft (50K) & 0.074 & 0.028 & 0.225 & 0.103 \\\\\n    flow init, ft (50K) & 0.066 & 0.023 & 0.203 & 0.092 \\\\\n    \n    rand init, ft (100K) & 0.069 & 0.025 & 0.211 & 0.097 \\\\\n    flow init, ft (100K) & \\textbf{0.063} & \\textbf{0.021} & \\textbf{0.193} & \\textbf{0.088} \\\\\n\n    \\bottomrule\n\\end{tabular}\n\\end{center}}\\end{minipage}\n}", "table_label": "{tab:transfer_flow_to_depth}", "table_numeric_cells": [["0.536", "0.536", 306, 311, 306, 311], ["1.309", "1.309", 314, 319, 314, 319], ["1.300", "1.300", 322, 327, 322, 327], ["0.584", "0.584", 330, 335, 330, 335], ["0.198", "0.198", 363, 368, 363, 368], ["0.364", "0.364", 371, 376, 371, 376], ["0.599", "0.599", 379, 384, 379, 384], ["0.235", "0.235", 387, 392, 387, 392], ["0.074", "0.074", 445, 450, 445, 450], ["0.028", "0.028", 453, 458, 453, 458], ["0.225", "0.225", 461, 466, 461, 466], ["0.103", "0.103", 469, 474, 469, 474], ["0.066", "0.066", 504, 509, 504, 509], ["0.023", "0.023", 512, 517, 512, 517], ["0.203", "0.203", 520, 525, 520, 525], ["0.092", "0.092", 528, 533, 528, 533], ["0.069", "0.069", 569, 574, 569, 574], ["0.025", "0.025", 577, 582, 577, 582], ["0.211", "0.211", 585, 590, 585, 590], ["0.097", "0.097", 593, 598, 593, 598], ["0.063", "\\textbf{0.063}", 637, 642, 629, 643], ["0.021", "\\textbf{0.021}", 654, 659, 646, 660], ["0.193", "\\textbf{0.193}", 671, 676, 663, 677], ["0.088", "\\textbf{0.088}", 688, 693, 680, 694]], "text_chunk_selected": "Our unified model naturally enables cross-task transfer since each task uses exactly the same learnable parameters for feature extraction. For example, without any finetuning, a pretrained optical flow model can be directly used for the task of rectified stereo matching and unrectified stereo depth estimation. Moreover, when finetuning with the pretrained flow model as initialization, we not only enjoy faster training speed for stereo and depth, but also achieve better performance, as evidenced by our experiments (Table~\\ref{tab:cross_task_transfer}).\n\n{Our unified model with only one task-agnostic hierarchical matching refinement outperforms RAFT~\\cite{teed2020raft} with 31 refinement steps on the challenging Sintel~\\cite{butler2012naturalistic} dataset while running faster (Fig.~\\ref{fig:flow_iter_vs_epe} and Table~\\ref{tab:flow_raft_vs_gmflow}), demonstrating the effectiveness and efficiency of our method. Our final model that uses a few additional task-specific refinement steps outperforms or compares favorably to recent state-of-the-art methods on 10 popular flow/stereo/depth datasets (KITTI Flow~\\cite{menze2015object}, Sintel~\\cite{butler2012naturalistic}, Middlebury~\\cite{scharstein2014high}, KITTI Stereo~\\cite{menze2015object}, ETH3D Stereo~\\cite{schops2017multi}, Argoverse Stereo~\\cite{chang2019argoverse}, ScanNet~\\cite{dai2017scannet}, SUN3D~\\cite{xiao2013sun3d}, RGBD-SLAM~\\cite{sturm2012benchmark} and Scenes11~\\cite{ummenhofer2017demon}), while being simpler and more efficient in terms of model design and inference speed.}\n\nFor unrectified stereo depth estimation, we assume the camera intrinsic and extrinsic parameters (${\\bm K}_1, {\\bm E}_1, {\\bm K}_2, {\\bm E}_2$) for image ${\\bm I}_1$ and ${\\bm I}_2$ are known (\\emph{i.e}\\onedot, posed images). They can be obtained via additional sensors like IMU and GPS, or reliably estimated using Structure-from-Motion software like COLMAP~\\cite{schonberger2016structure}. To estimate depth, we take an approach similar to the classic plane-sweep stereo method~\\cite{collins1996space}. More specifically, we first discretize a predefined depth range $[d_{\\mathrm{min}}, d_{\\mathrm{max}}]$ as $[d_1, d_2, \\cdots, d_N]$ (in our implementation, we discretize the inverse depth domain, while we use depth here for ease of notation). Then for each depth candidate $d_i (i=1, 2, \\cdots, N)$, we compute the 2D correspondences $\\hat{{\\bm G}}_{\\mathrm{2D}} \\in \\mathbb{R}^{H \\times W \\times 2}$ in ${\\bm F}_2$ given the current depth value:\n\nOur unified model thus far with only one task-agnostic hierarchical matching refinement is able to outperform RAFT~\\cite{teed2020raft} with 31 refinements while running faster (Fig.~\\ref{fig:flow_iter_vs_epe} and Table~\\ref{tab:flow_raft_vs_gmflow}), which demonstrates the effectiveness and efficiency of our global matching-based formulation. However, our matching-based method is also complementary to previous cost volume and convolution-based local regression approach. We observe the strength of our unified matching method mainly in the presence of large motion (Table~\\ref{tab:flow_conv_vs_softmax} and \\ref{tab:flow_raft_vs_gmflow}). For small motions, it might not be necessary to perform global matching (Table~\\ref{tab:global_local_match}) and in this case, local regression is advantageous. To achieve the best system-level performance, one straightforward way is to combine the strengths of these two kinds of flow estimation approaches. That is, the local regression method is used as a post-processing step to our unified model. This further improves fine-grained details and regions that are hard to match. \n\n\\noindent {\\bf Sintel.} Table~\\ref{tab:flow_raft_vs_gmflow} shows the results on Things validation set and Sintel (clean and final) training sets after training on Chairs and Things training sets. Without using any refinement, our method achieves better performance on Things and Sintel (clean) than RAFT with 11 refinements. By using an additional task-agnostic hierarchical matching refinement at $1/4$ feature resolution (Sec.~\\ref{sec:hierarchical_match}), our method outperforms RAFT with 31 refinements, especially on large motion ($s_{40+}$). Fig.~\\ref{fig:flow_iter_vs_epe} visualizes the results. Furthermore, our model enjoys faster inference speed compared to RAFT and also does not require a large number of sequential processing. On the high-end A100 GPU, our model gains more speedup compared to RAFT's sequential architecture ($2.29\\times$ \\emph{vs}\\onedot. $1.87 \\times$, \\emph{i.e}\\onedot, ours: $151 \\to 66$, RAFT: $170 \\to 91$), reflecting that our method can benefit more from advanced hardware acceleration and demonstrating its potential for further speed optimization.\n\n\\noindent {\\bf Stereo cross-attention: 1D \\emph{vs}\\onedot \\ 2D.} Unlike 2D optical flow, rectified stereo matching is a 1D correspondence task that corresponding pixels lie on the same horizontal scanline. Thus, it's not necessary to perform 2D cross-attention in the Transformer to model cross-view interactions and 1D horizontal cross-attention is sufficient. As shown in Table~\\ref{tab:stereo_1d_2d_attn}, using 1D cross-attention is not only more efficient in terms of inference time (measured for KITTI resolution ($384\\times 1248$) on a single V100 GPU), but also leads to better performance since unnecessary matching information is avoided. We note that the parameter-free cross-attention operation (2D, 1D or any other forms) doesn't affect the learnable parameters (\\emph{i.e}\\onedot, the linear projection layers) of the Transformer, and thus the pretrained model for optical flow and stereo matching tasks can still be shared.\n\n\\noindent \\textbf{Training schedule.} For ablation experiments, we train our model on the ScanNet for 50K iterations, with a batch size of 80 and a learning rate of 4e-4. The depth range for training and testing is set to $[0.5, 10]$ meters, and the number of depth candidates in the matching layer (Eq.~\\eqref{eq:depth_match}) is set to $64$. The training and testing process on SUN3D~\\cite{xiao2013sun3d}, RGBD-SLAM~\\cite{sturm2012benchmark} and Scenes11~\\cite{ummenhofer2017demon} datasets will be elaborated in Sec.~\\ref{sec:depth_benchmark}.\n\n\\subsection{Cross-Task Transfer}\nOne unique benefit of our unified model is that it naturally enables cross-task transfer since all the learnable parameters are exactly the same. More specifically, we can directly use a pretrained optical flow model and apply it to both rectified stereo matching and unrectified stereo depth estimation tasks. As shown in Table~\\ref{tab:transfer_flow_to_stereo} and Table~\\ref{tab:transfer_flow_to_depth}, our pretrained optical flow model performs significantly better than a random initialized model. Notably, no previous optical flow model can be directly used for unrectified stereo depth estimation, while our model achieves promising results as visualized in Fig.~\\ref{fig:vis_flow2depth}. The pretrained flow model can be further finetuned for stereo and depth tasks, which not only leads to faster training speed, but also achieves better performance than random initialization (Table~\\ref{tab:cross_task_transfer}). ", "table_source": "\\begin{table*}[t]\n\\centering\n\\subfloat[\n\\textbf{Flow to stereo transfer: error curves of disparity prediction error \\emph{vs}\\onedot \\ numbers of training steps}.\n\\label{fig:stereo_randinit_flowinit}\n]{\n\\centering\n\\begin{minipage}{0.42\\linewidth}\n\\includegraphics[width=\\linewidth]{figure/stereo_randinit_flowinit.pdf}\n\\end{minipage}\n}\n\\hspace{2em}\n\\subfloat[\n\\textbf{Flow to depth transfer: error curves of depth prediction error \\emph{vs}\\onedot \\ numbers of training steps}.\n\\label{fig:depth_randinit_flowinit}\n]{\n\\begin{minipage}{0.42\\linewidth}\n\\includegraphics[width=\\linewidth]{figure/depth_randinit_flowinit.pdf}\n\\end{minipage}\n}\n\\\\\n\\subfloat[\n\\textbf{Flow to stereo transfer: performance comparison}.\n\\label{tab:transfer_flow_to_stereo}\n]{\n\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\\begin{tabular}{lccccccccccccc}\n\\\\\n    \\toprule\n    \n    \\multirow{2}{*}[-2pt]{Model } & \\multicolumn{2}{c}{Things} &\n    \\multicolumn{2}{c}{KITTI} \\\\\n    \\addlinespace[-10pt]  \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \n    \\addlinespace[-10pt] \\\\\n    & EPE & D1 & EPE & D1 \\\\\n\n    \\midrule\n    \n    rand init, w/o ft & 75.24 & 96.06 & 103.47 & 96.95 \\\\\n    flow init, w/o ft & 2.58 & 18.19 & 1.98 & 17.60 \\\\\n    \n    \\midrule\n    \n    rand init, ft (50K) & 1.22 & 3.70 & 1.61 & 10.53 \\\\\n    flow init, ft (50K) & 1.10 & 3.04 & 1.39 & 7.56 \\\\\n    \n    rand init, ft (100K) & 1.11 & 3.05 & 1.58 & 9.93 \\\\\n    flow init, ft (100K) & \\textbf{1.00} & \\textbf{2.77} & \\textbf{1.37} & \\textbf{7.38} \\\\\n\n    \\bottomrule\n\\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\\hspace{2em}\n\\subfloat[\n\\textbf{Flow to depth transfer: performance comparison}.\n\\label{tab:transfer_flow_to_depth}\n]{\n\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\\begin{tabular}{lccccccccccccc}\n    \\toprule\n    \n    \n    Model & Abs Rel & Sq Rel & RMSE & RMSE log \\\\\n    \n    \\midrule\n    \n    rand init, w/o ft & 0.536 & 1.309 & 1.300 & 0.584 \\\\\n    flow init, w/o ft & 0.198 & 0.364 & 0.599 & 0.235 \\\\\n    \n    \\midrule\n    \n    rand init, ft (50K) & 0.074 & 0.028 & 0.225 & 0.103 \\\\\n    flow init, ft (50K) & 0.066 & 0.023 & 0.203 & 0.092 \\\\\n    \n    rand init, ft (100K) & 0.069 & 0.025 & 0.211 & 0.097 \\\\\n    flow init, ft (100K) & \\textbf{0.063} & \\textbf{0.021} & \\textbf{0.193} & \\textbf{0.088} \\\\\n\n    \\bottomrule\n\\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\\\\\n\n\\caption{\\textbf{Cross-task transfer}. We show the comparisons of error curves between random initialization and using a pretrained optical flow model as initialization in Fig.~\\ref{fig:stereo_randinit_flowinit} and Fig.~\\ref{fig:depth_randinit_flowinit}. The performance comparisons of different models: without any finetuning or finetuned with different initialization (rand init \\emph{vs}\\onedot \\ flow init) and different numbers of total training steps (50K \\emph{vs}\\onedot \\ 100K) are shown in Table~\\ref{tab:transfer_flow_to_stereo} and Table~\\ref{tab:transfer_flow_to_depth}.}\n\\label{tab:cross_task_transfer}\n\\end{table*}", "cell_list_gold": [{"value": "0.536", "char_index": [306, 311], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, w/o ft"}}, {"value": "1.309", "char_index": [314, 319], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, w/o ft"}}, {"value": "1.300", "char_index": [322, 327], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, w/o ft"}}, {"value": "0.584", "char_index": [330, 335], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, w/o ft"}}, {"value": "0.198", "char_index": [363, 368], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, w/o ft"}}, {"value": "0.364", "char_index": [371, 376], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, w/o ft"}}, {"value": "0.599", "char_index": [379, 384], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, w/o ft"}}, {"value": "0.235", "char_index": [387, 392], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, w/o ft"}}, {"value": "0.074", "char_index": [445, 450], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, ft (50K)"}}, {"value": "0.028", "char_index": [453, 458], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, ft (50K)"}}, {"value": "0.225", "char_index": [461, 466], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, ft (50K)"}}, {"value": "0.103", "char_index": [469, 474], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, ft (50K)"}}, {"value": "0.066", "char_index": [504, 509], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, ft (50K)"}}, {"value": "0.023", "char_index": [512, 517], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, ft (50K)"}}, {"value": "0.203", "char_index": [520, 525], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, ft (50K)"}}, {"value": "0.092", "char_index": [528, 533], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, ft (50K)"}}, {"value": "0.069", "char_index": [569, 574], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, ft (100K)"}}, {"value": "0.025", "char_index": [577, 582], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, ft (100K)"}}, {"value": "0.211", "char_index": [585, 590], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, ft (100K)"}}, {"value": "0.097", "char_index": [593, 598], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "rand init, ft (100K)"}}, {"value": "0.063", "char_index": [637, 642], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, ft (100K)"}}, {"value": "0.021", "char_index": [654, 659], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, ft (100K)"}}, {"value": "0.193", "char_index": [671, 676], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, ft (100K)"}}, {"value": "0.088", "char_index": [688, 693], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"depth range": "[0.5, 10]", "depth candidates": "64"}, "model": "GMFlow", "model settings": {"configuration": "flow init, ft (100K)"}}]}, "2211.05783v1_table18": {"table_code": "\\begin{table}[t]\n    \\centering\n    \n    \\begin{tabular}{lccccccccccccc}\n    \\toprule\n    \n    Model & bad 1.0 & bad 2.0 & bad 4.0 \\\\\n    \n    \\midrule\n    GANet~\\cite{zhang2019ga} & 6.56 & 1.10 & 0.54 \\\\\n    AANet~\\cite{xu2020aanet} & 5.01 & 1.66 & 0.75 \\\\\n    CFNet~\\cite{shen2021cfnet} & 3.31 & 0.77 & 0.31 \\\\\n    RAFT-Stereo~\\cite{lipson2021raft} & 2.44 & 0.44 & 0.15 \\\\\n    CREStereo~\\cite{li2022practical} & \\textbf{0.98} & \\textbf{0.22} & 0.10 \\\\\n    GMStereo & 1.83 & 0.25 & \\textbf{0.08} \\\\\n\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Stereo performance on ETH3D two-view stereo test set}. }\n    \\label{tab:stereo_eth3d_test}\n\\end{table}", "table_label": "{tab:stereo_eth3d_test}", "table_numeric_cells": [["6.56", "6.56", 183, 187, 183, 187], ["1.10", "1.10", 190, 194, 190, 194], ["0.54", "0.54", 197, 201, 197, 201], ["5.01", "5.01", 236, 240, 236, 240], ["1.66", "1.66", 243, 247, 243, 247], ["0.75", "0.75", 250, 254, 250, 254], ["3.31", "3.31", 291, 295, 291, 295], ["0.77", "0.77", 298, 302, 298, 302], ["0.31", "0.31", 305, 309, 305, 309], ["2.44", "2.44", 353, 357, 353, 357], ["0.44", "0.44", 360, 364, 360, 364], ["0.15", "0.15", 367, 371, 367, 371], ["0.98", "\\textbf{0.98}", 422, 426, 414, 427], ["0.22", "\\textbf{0.22}", 438, 442, 430, 443], ["0.10", "0.10", 446, 450, 446, 450], ["1.83", "1.83", 469, 473, 469, 473], ["0.25", "0.25", 476, 480, 476, 480], ["0.08", "\\textbf{0.08}", 491, 495, 483, 496]], "text_chunk_selected": "\\noindent \\textbf{Training schedule.} For methodology comparison (Sec.~\\ref{sec:method_comp}) and ablation experiments (Sec.~\\ref{sec:ablation}), we first train our default $1/8$ feature resolution model on Chairs dataset for 100K iterations, with a batch size of 16 and a learning rate of 4e-4. We then finetune the model on Things dataset for 200K iterations, with a batch size of 8 and a learning rate of 2e-4. The models thus far all use bilinear upsamling to upsample to the full resolution flow prediction for simplicity. For later experiments, we use RAFT's convex upsampling~\\cite{teed2020raft} and our models are trained on Things dataset for 800K iterations, which leads to improved performance~\\cite{xu2022gmflow}. For the final fine-tuning process on Sintel and KITTI datasets for benchmark comparisons, we report details in Sec.~\\ref{sec:flow_benchmark}.\n\n\\subsection{Stereo Matching}\n\\noindent \\textbf{Datasets and evaluation setup.} We first train on the synthetic Scene Flow~\\cite{mayer2016large} training set, and then evaluate on the Scene Flow test set and the KITTI 2015~\\cite{menze2015object} training set for cross-dataset generalization. Unlike previous representative stereo networks~\\cite{chang2018pyramid,zhang2019ga,xu2020aanet} that usually rely on a predefined disparity range (typically 192 pixels) to construct the local cost volume, our method is more flexible and can support unconstrained disparity prediction. To avoid extremely large disparity values in the data, we mask the pixels whose disparities exceed 400 pixels during both training and evaluation. Finally, we perform finetuning on KITTI 2015 Stereo, Middlebury Stereo, Argoverse Stereo and ETH3D Stereo datasets and report the performance on the online benchmarks.\n\n\\noindent \\textbf{Training schedule.} For ablation experiments, we train our model on the ScanNet for 50K iterations, with a batch size of 80 and a learning rate of 4e-4. The depth range for training and testing is set to $[0.5, 10]$ meters, and the number of depth candidates in the matching layer (Eq.~\\eqref{eq:depth_match}) is set to $64$. The training and testing process on SUN3D~\\cite{xiao2013sun3d}, RGBD-SLAM~\\cite{sturm2012benchmark} and Scenes11~\\cite{ummenhofer2017demon} datasets will be elaborated in Sec.~\\ref{sec:depth_benchmark}.\n\n\\noindent \\textbf{Sintel.} Following RAFT~\\cite{teed2020raft}, we further finetune our Things trained model on several mixed datasets that consist of KITTI 2015~\\cite{menze2015object}, HD1K~\\cite{kondermann2016hci}, FlyingThings3D~\\cite{mayer2016large} and Sintel~\\cite{butler2012naturalistic} training sets. We first finetune on the mixed dataset for 200K iterations with a batch size of 8 and a learning rate of 2e-4. Then we finetune on the Sintel training sets only with a larger crop size $416 \\times 1024$ for 5K iterations. The batch size is 8 and the learning rate is 1e-4. To generate the flow prediction results on the Sintel test sets, we first resize the original images to $416 \\times 1024$ and then resize the prediction back to the original image resolution for submission. The results on Sintel test set are shown in Table~\\ref{tab:flow_sintel_test}. We achieve state-of-the-art results on the highly competitive Sintel (clean) dataset. On Sintel (final) dataset, our performance is only second to the recent FlowFormer~\\cite{huang2022flowformer} model, which uses a Transformer model that is pretrained on the large scale ImageNet dataset and is more computationally expensive due to the large number of sequential refinements like RAFT. The visual comparisons with RAFT are shown in Fig.~\\ref{fig:vis_sintel_compare}, our method can better capture the motion of fast-moving objects like the moving hand.\n\n\\noindent \\textbf{KITTI.} We further finetune our VKITTI2 trained GMFlow+ model (Table~\\ref{tab:flow_gen_kitti}) on the mixed KITTI 2012 and KITTI 2015 training sets for 30K iterations. The batch size is 8 and the learning rate is 2e-4. The comparison results with previous methods are shown in Table~\\ref{tab:flow_kitti_test}. Again, we achieve better performance than RAFT.\n\n\\noindent \\textbf{KITTI.} We first finetune our Scene Flow pretrained model on the Virtual KITTI 2~\\cite{cabon2020virtual} dataset for 30K iterations, with a batch size of 16 and a learning rate of 4e-4. We then further finetune on the mixed KITTI 2012 and KITTI 2015 training sets for 10K iterations, with a batch size of 16 and a learning rate of 4e-4. The final model is used to generate the disparity prediction results on KITTI 2015 test set for submission to the online benchmark. The results are shown in Table~\\ref{tab:stereo_kitti15_test}. We achieve competitive performance compared with the state-of-the-art methods LEAStereo~\\cite{cheng2020hierarchical} and CREStereo~\\cite{li2022practical}. Besides, our model runs about $2\\times$ faster since we don't rely on any 3D convolutions (unlike LEAStereo) or a large number ($20+$) of sequential refinements (unlike CREStereo). Compared with previous lightweight stereo model AANet~\\cite{xu2020aanet}, our method performs much better. Besides, our model can be implemented with pure PyTorch, without requiring to build additional CUDA ops like AANet, which demonstrates that our method achieves a better speed-accuracy trade-off and has more practical advantages. \n\n\\noindent \\textbf{Middlebury.} Following CREStereo~\\cite{li2022practical}, we collect several public stereo datasets for training. More specifically, we first finetune the Scene Flow pretrained model on the mixed Scene Flow~\\cite{mayer2016large}, Tartan Air~\\cite{tartanair2020iros}, Falling Things~\\cite{tremblay2018falling}, CARLA HR-VS~\\cite{yang2019hierarchical}, CREStereo Dataset~\\cite{li2022practical}, InStereo2K~\\cite{bao2020instereo2k} and Middlebury~\\cite{hirschmuller2007evaluation,scharstein2014high} datasets for 100K iterations. The batch size is 16 and the learning rate is 4e-4. We use a random crop size of $480\\times 640$ for training at this stage. To adapt the model to high resolution (\\emph{e.g}\\onedot, $1536\\times 2048$ for Middlebury), we perform another round of finetuning with a larger random crop size of $768\\times 1024$. The training datasets include CARLA HR-VS~\\cite{yang2019hierarchical}, CREStereo Dataset~\\cite{li2022practical}, InStereo2K~\\cite{bao2020instereo2k}, Falling Things~\\cite{tremblay2018falling} and Middlebury~\\cite{hirschmuller2007evaluation, scharstein2014high}. At inference, we resize all the Middlebury full resolution test images to $1536 \\times 2048$ for prediction and finally resize the predicted disparities back to the original image resolution for evaluation. The results are shown in Table~\\ref{tab:stereo_middlebury_test}. Our GMStereo achieves the first place in terms of the RMS (Root Mean Square) disparity error metric. Besides, our method shows much higher efficiency than CREStereo~\\cite{li2022practical} ($5\\times$ faster) and RAFT-Stereo~\\cite{lipson2021raft} ($15 \\times$ faster) on such a high-resolution dataset. We also show some visual comparisons in Fig.~\\ref{fig:vis_middlebury_compare}, our method produces sharper object structures than CREStereo~\\cite{li2022practical} and RAFT-Stereo~\\cite{lipson2021raft}.\n\n\\noindent \\textbf{ETH3D.} Similar to the training process on the Middlebury dataset, we use several public stereo datasets for training. More specifically, we first finetune the Scene Flow pretrained model on the mixed Scene Flow~\\cite{mayer2016large}, Tartan Air~\\cite{tartanair2020iros}, Sintel Stereo~\\cite{butler2012naturalistic}, CREStereo Dataset~\\cite{li2022practical}, InStereo2K~\\cite{bao2020instereo2k} and ETH3D~\\cite{schops2017multi} datasets for 100K iterations. The batch size is 24 and the learning rate is 4e-4. Then we perform another round of finetuning on the mixed CREStereo Dataset~\\cite{li2022practical}, InStereo2K~\\cite{bao2020instereo2k} and ETH3D~\\cite{schops2017multi} datasets for 30K iterations. Again, the batch size is 24 and the learning rate is 4e-4. The final model is used to produce the prediction results for submission and the results are shown in Table~\\ref{tab:stereo_eth3d_test}. We achieve the second place in terms of the `bad 1.0' and `bad 2.0' metrics and the first place in terms of the `bad 4.0' metric.", "table_source": "\\begin{table}[t]\n    \\centering\n    \n    \\begin{tabular}{lccccccccccccc}\n    \\toprule\n    \n    Model & bad 1.0 & bad 2.0 & bad 4.0 \\\\\n    \n    \\midrule\n    GANet~\\cite{zhang2019ga} & 6.56 & 1.10 & 0.54 \\\\\n    AANet~\\cite{xu2020aanet} & 5.01 & 1.66 & 0.75 \\\\\n    CFNet~\\cite{shen2021cfnet} & 3.31 & 0.77 & 0.31 \\\\\n    RAFT-Stereo~\\cite{lipson2021raft} & 2.44 & 0.44 & 0.15 \\\\\n    CREStereo~\\cite{li2022practical} & \\textbf{0.98} & \\textbf{0.22} & 0.10 \\\\\n    GMStereo & 1.83 & 0.25 & \\textbf{0.08} \\\\\n\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Stereo performance on ETH3D two-view stereo test set}. }\n    \\label{tab:stereo_eth3d_test}\n\\end{table}", "cell_list_gold": [{"value": "6.56", "char_index": [183, 187], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 1.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "GANet", "model settings": {"xx": "yy"}}, {"value": "1.10", "char_index": [190, 194], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 2.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "GANet", "model settings": {"xx": "yy"}}, {"value": "0.54", "char_index": [197, 201], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 3.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "GANet", "model settings": {"xx": "yy"}}, {"value": "5.01", "char_index": [236, 240], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 1.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "AANet", "model settings": {"xx": "yy"}}, {"value": "1.66", "char_index": [243, 247], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 2.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "AANet", "model settings": {"xx": "yy"}}, {"value": "0.75", "char_index": [250, 254], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 3.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "AANet", "model settings": {"xx": "yy"}}, {"value": "3.31", "char_index": [291, 295], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 1.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "CFNet", "model settings": {"xx": "yy"}}, {"value": "0.77", "char_index": [298, 302], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 2.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "CFNet", "model settings": {"xx": "yy"}}, {"value": "0.31", "char_index": [305, 309], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 3.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "CFNet", "model settings": {"xx": "yy"}}, {"value": "2.44", "char_index": [353, 357], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 1.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "RAFT-Stereo", "model settings": {"xx": "yy"}}, {"value": "0.44", "char_index": [360, 364], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 2.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "RAFT-Stereo", "model settings": {"xx": "yy"}}, {"value": "0.15", "char_index": [367, 371], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 3.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "RAFT-Stereo", "model settings": {"xx": "yy"}}, {"value": "0.98", "char_index": [422, 426], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 1.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "CREStereo", "model settings": {"xx": "yy"}}, {"value": "0.22", "char_index": [438, 442], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 2.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "CREStereo", "model settings": {"xx": "yy"}}, {"value": "0.10", "char_index": [446, 450], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 3.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "CREStereo", "model settings": {"xx": "yy"}}, {"value": "1.83", "char_index": [469, 473], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 1.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "GMStereo", "model settings": {"xx": "yy"}}, {"value": "0.25", "char_index": [476, 480], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 2.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "GMStereo", "model settings": {"xx": "yy"}}, {"value": "0.08", "char_index": [491, 495], "type": "Result", "training data/set": "Scene Flow Tartan Air Sintel Stereo CREStereo InStereo2K ETH3D", "test data/set": "ETH3D", "task": "stereo matching", "metric": "bad 3.0", "experimental settings": {"learning rate": "4e-4 4e-4", "batch size": "24 24", "iterations": "100K 30K"}, "model": "GMStereo", "model settings": {"xx": "yy"}}]}, "2211.05783v1_table1": {"table_code": "\\subfloat[\n\\textbf{Transformer components}. Cross-attention contributes most.\n\\label{tab:transformer}\n]{\n\\centering\n\\begin{minipage}{0.45\\linewidth}\n{\\begin{center}\n\\begin{tabular}{lccccccccccccccc}\n    \\toprule\n    \n    \\multirow{2}{*}[-2pt]{setup} & \\multicolumn{1}{c}{Things (val)} & \\multicolumn{2}{c}{Sintel (train)} &  \\multirow{2}{*}[-2pt]{\\begin{tabular}[x]{@{}c@{}}Param\\\\(M) \\end{tabular}} \\\\\n    \\addlinespace[-10pt] \\\\\n    \\cmidrule(lr){2-2} \\cmidrule(lr){3-4}\n    \\addlinespace[-10pt] \\\\\n    & clean & clean & final & \\\\\n    \\midrule\n\n    full & \\textbf{6.67} & \\textbf{2.28} & \\textbf{3.44} & 4.2 \\\\\n    \n    w/o cross attn. & 10.84 & 4.48 & 6.32 & 3.8  \\\\\n    w/o position & 8.38 & 2.85 & 4.28 & 4.2 \\\\\n    w/o FFN & 8.71 & 3.10 & 4.43 & 1.8 & \\\\\n    w/o self attn. & 7.04 & 2.49 & 3.69 & 3.8 \\\\\n    \n    \\bottomrule\n    \\end{tabular}\n\\end{center}}\n\\end{minipage}\n}", "table_label": "{tab:transformer}", "table_numeric_cells": [["6.67", "\\textbf{6.67}", 567, 571, 559, 572], ["2.28", "\\textbf{2.28}", 583, 587, 575, 588], ["3.44", "\\textbf{3.44}", 599, 603, 591, 604], ["4.2", "4.2", 607, 610, 607, 610], ["10.84", "10.84", 641, 646, 641, 646], ["4.48", "4.48", 649, 653, 649, 653], ["6.32", "6.32", 656, 660, 656, 660], ["3.8", "3.8", 663, 666, 663, 666], ["8.38", "8.38", 690, 694, 690, 694], ["2.85", "2.85", 697, 701, 697, 701], ["4.28", "4.28", 704, 708, 704, 708], ["4.2", "4.2", 711, 714, 711, 714], ["8.71", "8.71", 732, 736, 732, 736], ["3.10", "3.10", 739, 743, 739, 743], ["4.43", "4.43", 746, 750, 746, 750], ["1.8", "1.8", 753, 756, 753, 756], ["7.04", "7.04", 783, 787, 783, 787], ["2.49", "2.49", 790, 794, 790, 794], ["3.69", "3.69", 797, 801, 797, 801], ["3.8", "3.8", 804, 807, 804, 807]], "text_chunk_selected": "Recent iterative 2D methods like RAFT-Stereo~\\cite{lipson2021raft} and CREStereo~\\cite{li2022practical} mostly follow the high-level design of the RAFT~\\cite{teed2020raft} architecture for optical flow, while introducing several task-specific components (\\emph{e.g}\\onedot, 1D correlation) to make such a method suitable for the stereo matching task. In contrast, we show that our matching-based perspective enables to use the \\textit{same} model for both optical flow and stereo matching, with exactly the same learnable parameters. Besides, our model is also more efficient since we don't rely on any 3D convolutions or a large number of sequential refinements. On the other hand, although MC-CNN~\\cite{Zbontar2016Stereo} also tries to learn strong features for matching, the features in MC-CNN are extracted \\textit{independently} with a convolutional network, without considering cross-view interactions. However, as evidenced by our results, cross-view interactions are crucial for strong and discriminative features (see Table~\\ref{tab:flow_conv_vs_softmax} and Table~\\ref{tab:transformer}). Perhaps the most related stereo work to ours is STTR~\\cite{li2021revisiting}, which also uses a Transformer and matching-based disparity computation. However, STTR relies on a complex optimal transport matching layer and doesn\u2019t produce predictions for occluded pixels, while we use a much simpler softmax operation and a simple flow propagation layer to handle occlusions. Moreover, STTR is designed to solve the stereo matching task, while we are seeking a unified model applicable to three different dense correspondence estimation tasks.\n\n\\subsection{Depth Estimation}\nLearning-based depth estimation methods can be broadly categorized into monocular and multi-view approaches. Monocular methods~\\cite{garg2016unsupervised,godard2017unsupervised,zhou2017unsupervised,xu2019rdn4depth,godard2019digging,ranftl2020towards} take a single image as input and use generic network architectures like ResNet~\\cite{he2016deep} to predict the dense depth map, while multi-view methods~\\cite{ummenhofer2017demon,im2019dpsnet,Tang2019BANetDB,Teed2020DeepV2D,yao2018mvsnet} usually focus on how to encode the geometric inductive bias (cost volume, warping, \\emph{etc}\\onedot) into the network architecture. Compared with monocular methods, multi-view depth estimation can better leverage the information from additional viewpoints and usually lead to improved performance~\\cite{watson2021temporal}. Since multi-view information (\\emph{e.g}\\onedot, video sequences) are usually readily available for many applications, we consider multi-view depth estimation in this paper. A popular multi-view depth pipeline is using the plane-sweep stereo~\\cite{im2019dpsnet,gu2020cascade} approach, where different depth planes are tested for correctness. However, like rectified stereo matching, the state-of-the-art methods are usually dominated by 3D convolution-based approaches~\\cite{im2019dpsnet,Teed2020DeepV2D}, which accordingly introduces cubic computational complexity.  In this paper, we approach this task from an explicit matching-based perspective and use a Transformer to obtain strong features for matching, achieving highly competitive performance without relying on any 3D convolutions. This is different from the recent work TransMVSNet~\\cite{ding2022transmvsnet}, which still relies on 3D convolutions for cost volume post-processing and where the Transformer is used before the cost volume construction stage. Thus, our method is simpler and more lightweight. We also note that we only consider the two-view depth estimation setting in this paper, but our method can be easily extended to the multi-view case. \n\nKey to our formulation lies in obtaining high-quality discriminative features for matching. To achieve this, we combine a common convolutional network (CNN) with a Transformer~\\cite{vaswani2017attention} as the feature extractor. More specifically, we first use a weight-sharing ResNet~\\cite{he2016deep} to extract $8\\times$ downsampled features to keep computation tractable, similar to previous flow methods~\\cite{sun2018pwc,teed2020raft}. However, the two features from the CNN are extracted independently, without considering their mutual relations yet. Integrating knowledge from the potential matching candidates in another image can intuitively enhance the feature's distinctiveness and surpass ambiguities, as demonstrated by sparse matching methods~\\cite{sarlin2020superglue}. This can be naturally implemented with the cross-attention mechanism, which is able to selectively aggregate information from another image by measuring cross-view feature similarities. We also use a self-attention layer to further improve the feature's quality by considering larger context than the convolutional layer, and a two-layer feed-forward network (FFN, \\emph{i.e}\\onedot, MLP) to further increase the capacity of the network following the original Transformer~\\cite{vaswani2017attention}'s design. The self-attention, cross-attention and FFN constitute a Transformer block, and our final Transformer architecture is a stack of six Transformer blocks which gradually improve the performance (Table.~\\ref{tab:flow_conv_vs_softmax}).\n\nSpecifically, for the extracted convolutional features $\\Tilde{\\bm F}_1$ and $\\Tilde{\\bm F}_2$, we first add fixed 2D sine and cosine positional encodings (following DETR~\\cite{carion2020end}) to the features since they lack spatial information. Adding the position information also makes the matching process consider not only the feature similarity but also their spatial distance, which can help resolve ambiguities and improve performance (Table~\\ref{tab:transformer}). Then the features are fed into the Transformer for feature enhancement. More specifically, for self-attention, the query, key and value in the attention mechanism~\\cite{vaswani2017attention} are the same feature. For cross-attention, the key and value are same but different from the query to model cross-view interactions. This process is performed for both $\\Tilde{\\bm F}_1$ and $\\Tilde{\\bm F}_2$ symmetrically:\n\nOne issue with the standard Transformer architecture~\\cite{vaswani2017attention} is the quadratic computational complexity due to the pair-wise attention operation. To improve efficiency, we adopt the shifted local window attention strategy from Swin Transformer~\\cite{liu2021Swin}. However, unlike Swin that uses a \\emph{fixed window size}, we split the feature to \\emph{fixed number of local windows} to make the window size adaptive with the feature's spatial size. In this way, the attention mechanism can model long-range dependencies on high-resolution feature maps and accordingly better performance for large displacements can be achieved.  Specifically, we split the input feature of size $H \\times W$ to $K \\times K$ windows (each with size $\\frac{H}{K} \\times \\frac{W}{K}$, better for large displacements flow if $K$ is smaller, see Table~\\ref{tab:split_attn}), and perform self- and cross-attentions within each local window independently. For every two consecutive local windows, we shift the window partition by $(\\frac{H}{2K}, \\frac{W}{2K})$ to introduce cross-window connections. In our method, we split into $2 \\times 2$ windows (each with size $\\frac{H}{2} \\times \\frac{W}{2}$), which leads to a good speed-accuracy trade-off (Table~\\ref{tab:split_attn}).\n\n\\noindent {\\bf Flow estimation approach.} We compare our Transformer and softmax-based flow estimation method with cost volume and convolution-based approaches. Specifically, we adopt the state-of-the-art cost volume construction method in RAFT~\\cite{teed2020raft} that concatenates 4 local cost volumes at 4 scales, where each cost volume has a dimension of $H \\times W \\times (2R+1)^2$. Here $H$ and $W$ denote the feature's spatial size, and the search range $R$ is set to 4 following RAFT. To regress flow, we stack different numbers of convolutional residual blocks~\\cite{he2016deep} to see how the performance varies. The final optical flow is obtained with a $3 \\times 3$ convolution with 2 output channels. For our method, we stack different numbers of Transformer blocks for feature enhancement and the final optical flow is obtained with a global correlation and softmax layer. Table~\\ref{tab:flow_conv_vs_softmax} shows that the performance improvement of our method is more significant compared to the cost volume and convolution-based approach. For instance, our method with 2 Transformer blocks is already able to outperform 8 convolution blocks, especially in the presence of large motions ($s_{40+}$). The performance can be further improved by stacking more layers, surpassing the cost volume and convolution-based approach by a large margin. We also replace the Transformer in our model with a convolutional network for feature enhancement, which leads to a large drop in performance. This is largely due to the unique advantage of the cross-attention mechanism for modeling cross-view interactions (see Table~\\ref{tab:transformer} for detailed evaluations of the Transformer components), which enables aggregation the information from the other frame by considering cross-view similarities and thus greatly improves the quality of the extracted features. This is not achievable with convolutions~\\cite{Zbontar2016Stereo}.\n\n\\noindent {\\bf Transformer components.} We ablate different Transformer components in Table~\\ref{tab:transformer}. The cross-attention contributes most, since it models the cross-view interactions between two features, which integrates the knowledge from another image and greatly improves the quality of the extracted features. Also, the position information makes the matching process position-dependent, which can help alleviate the ambiguities in pure feature similarity-based matching. Removing the feed-forward network (FFN) reduces a large number of parameters, while also leading to a moderate performance drop. The self-attention aggregates contextual cues within the same feature, leading to additional performance gains.\n\n\\noindent {\\bf Model components.} We ablate different components of our full model in Table~\\ref{tab:stereo_ablation}. The results are consistent with those for the optical flow task in Table~\\ref{tab:transformer} and Table~\\ref{tab:prop}. That is, the cross-attention contributes most, but the other components also contribute to the performance gains.", "table_source": "\\begin{table*}[t]\n\\centering\n\\subfloat[\n\\textbf{Transformer components}. Cross-attention contributes most.\n\\label{tab:transformer}\n]{\n\\centering\n\\begin{minipage}{0.45\\linewidth}\n{\\begin{center}\n\\begin{tabular}{lccccccccccccccc}\n    \\toprule\n    \n    \\multirow{2}{*}[-2pt]{setup} & \\multicolumn{1}{c}{Things (val)} & \\multicolumn{2}{c}{Sintel (train)} &  \\multirow{2}{*}[-2pt]{\\begin{tabular}[x]{@{}c@{}}Param\\\\(M) \\end{tabular}} \\\\\n    \\addlinespace[-10pt] \\\\\n    \\cmidrule(lr){2-2} \\cmidrule(lr){3-4}\n    \\addlinespace[-10pt] \\\\\n    & clean & clean & final & \\\\\n    \\midrule\n\n    full & \\textbf{6.67} & \\textbf{2.28} & \\textbf{3.44} & 4.2 \\\\\n    \n    w/o cross attn. & 10.84 & 4.48 & 6.32 & 3.8  \\\\\n    w/o position & 8.38 & 2.85 & 4.28 & 4.2 \\\\\n    w/o FFN & 8.71 & 3.10 & 4.43 & 1.8 & \\\\\n    w/o self attn. & 7.04 & 2.49 & 3.69 & 3.8 \\\\\n    \n    \\bottomrule\n    \\end{tabular}\n\\end{center}}\n\\end{minipage}\n}\n\\hspace{2em}\n\\subfloat[\n\\textbf{Numbers of window splits in shifted local attention}. $2 \\times 2$ represents a good speed-accuracy trade-off.\n\\label{tab:split_attn}\n]{\n\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\\begin{tabular}{cccccccccccccccc}\n    \\toprule\n    \n    \\multirow{2}{*}[-2pt]{\\#splits} & \\multicolumn{4}{c}{Things (val, clean)}  &  \\multirow{2}{*}[-2pt]{\\begin{tabular}[x]{@{}c@{}}Time\\\\(ms) \\end{tabular}} \\\\\n    \\addlinespace[-10pt] \\\\\n    \\cmidrule(lr){2-5} \n    \\addlinespace[-10pt] \\\\\n    & EPE & $s_{0-10}$ & $s_{10-40}$ & $s_{40+}$  \\\\\n    \\midrule\n    \n    $1 \\times 1$ & 6.34 & 1.26 & 2.37 & 16.36 & 105  \\\\\n    \\underline{$2 \\times 2$} & 6.67 & 1.26 & 2.40 & 17.37 & 53 \\\\\n    $4 \\times 4$ & 7.32 & 1.29 & 2.58 & 19.26 & 35 \\\\\n    \\bottomrule\n    \\\\\n\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\\\\\n\\subfloat[\n\\textbf{Global \\emph{vs}\\onedot \\ local matching}. Global matching is significantly better for large motions while being fast to compute.\n\\label{tab:global_local_match}\n]{\n\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\\setlength{\\tabcolsep}{3pt} \n\\begin{tabular}{ccccccccccccccc}\n    \\toprule\n    \n    \\multirow{2}{*}[-2pt]{\\begin{tabular}[x]{@{}c@{}}matching \\\\space \\end{tabular}} & \\multicolumn{4}{c}{Things (val, clean)}  & \\multirow{2}{*}[-2pt]{\\begin{tabular}[x]{@{}c@{}}Time\\\\(ms) \\end{tabular}} \\\\\n    \\addlinespace[-10pt] \\\\\n    \\cmidrule(lr){2-5} \n    \\addlinespace[-10pt] \\\\\n    & EPE & $s_{0-10}$ & $s_{10-40}$ & $s_{40+}$  \\\\\n    \\midrule\n    \n    global & \\textbf{6.67} & 1.26 & \\textbf{2.40} & \\textbf{17.37} & 52.6 \\\\\n    local $3 \\times 3$ & 31.78 & 1.19 & 12.40 & 85.39 & 51.2 \\\\\n    local $5 \\times 5$ & 26.51 & \\textbf{0.89} & 6.67 & 76.76 & 51.5 \\\\\n    local $ 9 \\times 9$ & 19.88 & 1.01 & 2.44 & 61.06 & 52.9 \\\\\n\n    \\bottomrule\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\\hspace{2em}\n\\subfloat[\n\\textbf{Flow propagation} greatly improves unmatched pixels. \n\\label{tab:prop}\n]{\n\\begin{minipage}{0.45\\linewidth}{\\begin{center}\n\\setlength{\\tabcolsep}{1pt} \n\\begin{tabular}{lccccccccccccccc}\n    \\toprule\n    \n    \\multirow{2}{*}[-2pt]{prop.} & \\multicolumn{3}{c}{Sintel (clean)}  &  \\multicolumn{3}{c}{Sintel (final)} \\\\\n    \\addlinespace[-10pt] \\\\\n    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \n    \\addlinespace[-10pt] \\\\\n    & all & matched & unmatched & all & matched & unmatched  \\\\\n    \\midrule\n    \n    w/o  & 2.28 & \\textbf{1.06} & 15.54 & 3.44 & \\textbf{1.95} & 19.50  \\\\\n    w/  & \\textbf{1.89} & 1.10 & \\textbf{10.39} & \\textbf{3.13} & 1.98 & \\textbf{15.52} \\\\\n    \n\n    \\bottomrule\n    \\\\\n\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\\vspace{-10pt}\n\\caption{\\textbf{GMFlow ablations for optical flow task}. All models are trained on Chairs and Things training sets.}\n\\label{tab:flow_ablations}\n\\end{table*}", "cell_list_gold": [{"value": "6.67", "char_index": [567, 571], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Things (val) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "full", "Param (M)": "4.2"}}, {"value": "2.28", "char_index": [583, 587], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "full", "Param (M)": "4.2"}}, {"value": "3.44", "char_index": [599, 603], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) final", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "full", "Param (M)": "4.2"}}, {"value": "4.2", "char_index": [607, 610], "type": "Hyper-parameter/Architecture", "model": "GMFlow full", "parameter/architecture name": ["Param (M)", "number of parameters (M)"], "dataset": ["FlyingChairs FlyingThings3D", "Chairs Things"]}, {"value": "10.84", "char_index": [641, 646], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Things (val) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o cross attn.", "Param (M)": "3.8"}}, {"value": "4.48", "char_index": [649, 653], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o cross attn.", "Param (M)": "3.8"}}, {"value": "6.32", "char_index": [656, 660], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) final", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o cross attn.", "Param (M)": "3.8"}}, {"value": "3.8", "char_index": [663, 666], "type": "Hyper-parameter/Architecture", "model": "GMFlow w/o cross attn.", "parameter/architecture name": ["Param (M)", "number of parameters (M)"], "dataset": ["FlyingChairs FlyingThings3D", "Chairs Things"]}, {"value": "8.38", "char_index": [690, 694], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Things (val) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o position", "Param (M)": "4.2"}}, {"value": "2.85", "char_index": [697, 701], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o position", "Param (M)": "4.2"}}, {"value": "4.28", "char_index": [704, 708], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) final", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o position", "Param (M)": "4.2"}}, {"value": "4.2", "char_index": [711, 714], "type": "Hyper-parameter/Architecture", "model": "GMFlow w/o position", "parameter/architecture name": ["Param (M)", "number of parameters (M)"], "dataset": ["FlyingChairs FlyingThings3D", "Chairs Things"]}, {"value": "8.71", "char_index": [732, 736], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Things (val) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o FFN", "Param (M)": "1.8"}}, {"value": "3.10", "char_index": [739, 743], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o FFN", "Param (M)": "1.8"}}, {"value": "4.43", "char_index": [746, 750], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) final", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o FFN", "Param (M)": "1.8"}}, {"value": "1.8", "char_index": [753, 756], "type": "Hyper-parameter/Architecture", "model": "GMFlow w/o FFN", "parameter/architecture name": ["Param (M)", "number of parameters (M)"], "dataset": ["FlyingChairs FlyingThings3D", "Chairs Things"]}, {"value": "7.04", "char_index": [783, 787], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Things (val) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o self attn.", "Param (M)": "3.8"}}, {"value": "2.49", "char_index": [790, 794], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) clean", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o self attn.", "Param (M)": "3.8"}}, {"value": "3.69", "char_index": [797, 801], "type": "Result", "training data/set": ["FlyingChairs FlyingThings3D", "Chairs Things"], "test data/set": "Sintel (train) final", "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o self attn.", "Param (M)": "3.8"}}, {"value": "3.8", "char_index": [804, 807], "type": "Hyper-parameter/Architecture", "model": "GMFlow w/o self attn.", "parameter/architecture name": ["Param (M)", "number of parameters (M)"], "dataset": ["FlyingChairs FlyingThings3D", "Chairs Things"]}]}, "2211.05783v1_table6": {"table_code": "\\begin{table}[t]\n    \\centering\n    \\setlength{\\tabcolsep}{2pt} \n    \\begin{tabular}{llcccccccccccccc}\n    \\toprule\n    \n    Training data & Method & EPE & F1-all & $s_{0-10}$ & $s_{10-40}$ & $s_{40+}$ &   \\\\\n\n    \n    \\midrule\n    \n    \\multirow{2}{*}[-2pt]{C + T} & RAFT & 5.32 & 17.46 & 0.67 & 1.58 & 13.68 \\\\\n    & GMFlow & 7.77 & 23.40 & 0.74 & 2.19 & 20.34 \\\\\n    & GMFlow+ & 5.74 & 17.63 & 0.64 & 1.69 & 14.86 \\\\\n    \n    \\midrule\n    \n    \\multirow{2}{*}[-2pt]{C + T + VK} & RAFT & 2.45 & 7.90 & \\textbf{0.43} & 1.18 & 5.70 \\\\\n    & GMFlow & 2.85 & 10.77 & 0.49 & 1.16 & 6.87 \\\\\n    & GMFlow+ & \\textbf{2.25} & \\textbf{7.20} & 0.48 & \\textbf{1.10} & \\textbf{5.12} \\\\\n\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Generalization on KITTI 2015 optical flow dataset} after training on synthetic Chairs (C), Things (T) and Virtual KITTI 2 (VK) datasets. \n    }\n    \\label{tab:flow_gen_kitti}\n    \n    \n\\end{table}", "table_label": "{tab:flow_gen_kitti}", "table_numeric_cells": [["5.32", "5.32", 275, 279, 275, 279], ["17.46", "17.46", 282, 287, 282, 287], ["0.67", "0.67", 290, 294, 290, 294], ["1.58", "1.58", 297, 301, 297, 301], ["13.68", "13.68", 304, 309, 304, 309], ["7.77", "7.77", 328, 332, 328, 332], ["23.40", "23.40", 335, 340, 335, 340], ["0.74", "0.74", 343, 347, 343, 347], ["2.19", "2.19", 350, 354, 350, 354], ["20.34", "20.34", 357, 362, 357, 362], ["5.74", "5.74", 382, 386, 382, 386], ["17.63", "17.63", 389, 394, 389, 394], ["0.64", "0.64", 397, 401, 397, 401], ["1.69", "1.69", 404, 408, 404, 408], ["14.86", "14.86", 411, 416, 411, 416], ["2.45", "2.45", 490, 494, 490, 494], ["7.90", "7.90", 497, 501, 497, 501], ["0.43", "\\textbf{0.43}", 512, 516, 504, 517], ["1.18", "1.18", 520, 524, 520, 524], ["5.70", "5.70", 527, 531, 527, 531], ["2.85", "2.85", 550, 554, 550, 554], ["10.77", "10.77", 557, 562, 557, 562], ["0.49", "0.49", 565, 569, 565, 569], ["1.16", "1.16", 572, 576, 572, 576], ["6.87", "6.87", 579, 583, 579, 583], ["2.25", "\\textbf{2.25}", 611, 615, 603, 616], ["7.20", "\\textbf{7.20}", 627, 631, 619, 632], ["0.48", "0.48", 635, 639, 635, 639], ["1.10", "\\textbf{1.10}", 650, 654, 642, 655], ["5.12", "\\textbf{5.12}", 666, 670, 658, 671]], "text_chunk_selected": "{Our unified model with only one task-agnostic hierarchical matching refinement outperforms RAFT~\\cite{teed2020raft} with 31 refinement steps on the challenging Sintel~\\cite{butler2012naturalistic} dataset while running faster (Fig.~\\ref{fig:flow_iter_vs_epe} and Table~\\ref{tab:flow_raft_vs_gmflow}), demonstrating the effectiveness and efficiency of our method. Our final model that uses a few additional task-specific refinement steps outperforms or compares favorably to recent state-of-the-art methods on 10 popular flow/stereo/depth datasets (KITTI Flow~\\cite{menze2015object}, Sintel~\\cite{butler2012naturalistic}, Middlebury~\\cite{scharstein2014high}, KITTI Stereo~\\cite{menze2015object}, ETH3D Stereo~\\cite{schops2017multi}, Argoverse Stereo~\\cite{chang2019argoverse}, ScanNet~\\cite{dai2017scannet}, SUN3D~\\cite{xiao2013sun3d}, RGBD-SLAM~\\cite{sturm2012benchmark} and Scenes11~\\cite{ummenhofer2017demon}), while being simpler and more efficient in terms of model design and inference speed.}\n\n\\subsection{Optical Flow}\n\\noindent \\textbf{Datasets and evaluation setup.} Following previous optical flow methods~\\cite{ilg2017flownet,sun2018pwc,teed2020raft}, we first train on the FlyingChairs (Chairs)~\\cite{dosovitskiy2015flownet} and FlyingThings3D (Things)~\\cite{mayer2016large} datasets, and then evaluate on Sintel~\\cite{butler2012naturalistic} and KITTI~\\cite{menze2015object} training sets for cross-dataset generalization. We also evaluate on the Things validation set to see how the model performs on the same-domain data. Finally, we perform additional fine-tuning on Sintel and KITTI training sets and report the performance on the online benchmarks.\n\n\\noindent \\textbf{Training schedule.} For methodology comparison (Sec.~\\ref{sec:method_comp}) and ablation experiments (Sec.~\\ref{sec:ablation}), we first train our default $1/8$ feature resolution model on Chairs dataset for 100K iterations, with a batch size of 16 and a learning rate of 4e-4. We then finetune the model on Things dataset for 200K iterations, with a batch size of 8 and a learning rate of 2e-4. The models thus far all use bilinear upsamling to upsample to the full resolution flow prediction for simplicity. For later experiments, we use RAFT's convex upsampling~\\cite{teed2020raft} and our models are trained on Things dataset for 800K iterations, which leads to improved performance~\\cite{xu2022gmflow}. For the final fine-tuning process on Sintel and KITTI datasets for benchmark comparisons, we report details in Sec.~\\ref{sec:flow_benchmark}.\n\n\\noindent {\\bf Sintel.} Table~\\ref{tab:flow_raft_vs_gmflow} shows the results on Things validation set and Sintel (clean and final) training sets after training on Chairs and Things training sets. Without using any refinement, our method achieves better performance on Things and Sintel (clean) than RAFT with 11 refinements. By using an additional task-agnostic hierarchical matching refinement at $1/4$ feature resolution (Sec.~\\ref{sec:hierarchical_match}), our method outperforms RAFT with 31 refinements, especially on large motion ($s_{40+}$). Fig.~\\ref{fig:flow_iter_vs_epe} visualizes the results. Furthermore, our model enjoys faster inference speed compared to RAFT and also does not require a large number of sequential processing. On the high-end A100 GPU, our model gains more speedup compared to RAFT's sequential architecture ($2.29\\times$ \\emph{vs}\\onedot. $1.87 \\times$, \\emph{i.e}\\onedot, ours: $151 \\to 66$, RAFT: $170 \\to 91$), reflecting that our method can benefit more from advanced hardware acceleration and demonstrating its potential for further speed optimization.\n\n\\noindent {\\bf KITTI.} Table~\\ref{tab:flow_gen_kitti} shows the generalization results on KITTI training set after training on Chairs and Things training sets. In this evaluation setting, our method doesn't outperform RAFT, which is mainly caused by the gap between the synthetic training sets and the real-world testing dataset. One key reason behind our inferior performance is that RAFT, relying on fully convolutional neural networks, benefits from the inductive biases in convolution layers, which requires a relatively smaller size training data to generalize to a new dataset in comparison with Transformers~\\cite{dosovitskiy2020image, d2021convit,xu2021vitae,zhang2022vitaev2}. To substantiate this claim, we finetune both RAFT and our GMFlow on the additional Virtual KITTI 2~\\cite{cabon2020virtual} dataset. The results in Table~\\ref{tab:flow_gen_kitti} verify that the performance gap becomes smaller when more data is available. We also train another version GMFlow+ that uses 6 additional local regression refinements (Sec.~\\ref{sec:local_reg_refine}), we can observe from Table~\\ref{tab:flow_gen_kitti} that GMFlow+ outperforms RAFT on KITTI dataset.\n\n\\noindent \\textbf{Training schedule.} For ablation experiments, we train our model on the ScanNet for 50K iterations, with a batch size of 80 and a learning rate of 4e-4. The depth range for training and testing is set to $[0.5, 10]$ meters, and the number of depth candidates in the matching layer (Eq.~\\eqref{eq:depth_match}) is set to $64$. The training and testing process on SUN3D~\\cite{xiao2013sun3d}, RGBD-SLAM~\\cite{sturm2012benchmark} and Scenes11~\\cite{ummenhofer2017demon} datasets will be elaborated in Sec.~\\ref{sec:depth_benchmark}.\n\n\\noindent \\textbf{Sintel.} Following RAFT~\\cite{teed2020raft}, we further finetune our Things trained model on several mixed datasets that consist of KITTI 2015~\\cite{menze2015object}, HD1K~\\cite{kondermann2016hci}, FlyingThings3D~\\cite{mayer2016large} and Sintel~\\cite{butler2012naturalistic} training sets. We first finetune on the mixed dataset for 200K iterations with a batch size of 8 and a learning rate of 2e-4. Then we finetune on the Sintel training sets only with a larger crop size $416 \\times 1024$ for 5K iterations. The batch size is 8 and the learning rate is 1e-4. To generate the flow prediction results on the Sintel test sets, we first resize the original images to $416 \\times 1024$ and then resize the prediction back to the original image resolution for submission. The results on Sintel test set are shown in Table~\\ref{tab:flow_sintel_test}. We achieve state-of-the-art results on the highly competitive Sintel (clean) dataset. On Sintel (final) dataset, our performance is only second to the recent FlowFormer~\\cite{huang2022flowformer} model, which uses a Transformer model that is pretrained on the large scale ImageNet dataset and is more computationally expensive due to the large number of sequential refinements like RAFT. The visual comparisons with RAFT are shown in Fig.~\\ref{fig:vis_sintel_compare}, our method can better capture the motion of fast-moving objects like the moving hand.\n\n\\noindent \\textbf{KITTI.} We further finetune our VKITTI2 trained GMFlow+ model (Table~\\ref{tab:flow_gen_kitti}) on the mixed KITTI 2012 and KITTI 2015 training sets for 30K iterations. The batch size is 8 and the learning rate is 2e-4. The comparison results with previous methods are shown in Table~\\ref{tab:flow_kitti_test}. Again, we achieve better performance than RAFT.", "table_source": "\\begin{table}[t]\n    \\centering\n    \\setlength{\\tabcolsep}{2pt} \n    \\begin{tabular}{llcccccccccccccc}\n    \\toprule\n    \n    Training data & Method & EPE & F1-all & $s_{0-10}$ & $s_{10-40}$ & $s_{40+}$ &   \\\\\n\n    \n    \\midrule\n    \n    \\multirow{2}{*}[-2pt]{C + T} & RAFT & 5.32 & 17.46 & 0.67 & 1.58 & 13.68 \\\\\n    & GMFlow & 7.77 & 23.40 & 0.74 & 2.19 & 20.34 \\\\\n    & GMFlow+ & 5.74 & 17.63 & 0.64 & 1.69 & 14.86 \\\\\n    \n    \\midrule\n    \n    \\multirow{2}{*}[-2pt]{C + T + VK} & RAFT & 2.45 & 7.90 & \\textbf{0.43} & 1.18 & 5.70 \\\\\n    & GMFlow & 2.85 & 10.77 & 0.49 & 1.16 & 6.87 \\\\\n    & GMFlow+ & \\textbf{2.25} & \\textbf{7.20} & 0.48 & \\textbf{1.10} & \\textbf{5.12} \\\\\n\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Generalization on KITTI 2015 optical flow dataset} after training on synthetic Chairs (C), Things (T) and Virtual KITTI 2 (VK) datasets. \n    }\n    \\label{tab:flow_gen_kitti}\n    \n    \n\\end{table}", "cell_list_gold": [{"value": "5.32", "char_index": [275, 279], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "17.46", "char_index": [282, 287], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": "F1-all", "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "0.67", "char_index": [290, 294], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{0-10}", "EPE motion magnitude 0-10"], "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "1.58", "char_index": [297, 301], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{10-40}", "EPE motion magnitude 10-40"], "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "13.68", "char_index": [304, 309], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{40+}", "EPE motion magnitude 40+"], "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "7.77", "char_index": [328, 332], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "23.40", "char_index": [335, 340], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": "F1-all", "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "0.74", "char_index": [343, 347], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{0-10}", "EPE motion magnitude 0-10"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "2.19", "char_index": [350, 354], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{10-40}", "EPE motion magnitude 10-40"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "20.34", "char_index": [357, 362], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{40+}", "EPE motion magnitude 40+"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "5.74", "char_index": [382, 386], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "17.63", "char_index": [389, 394], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": "F1-all", "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "0.64", "char_index": [397, 401], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{0-10}", "EPE motion magnitude 0-10"], "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "1.69", "char_index": [404, 408], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{10-40}", "EPE motion magnitude 10-40"], "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "14.86", "char_index": [411, 416], "type": "Result", "training data/set": ["C + T", "Chair Things"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{40+}", "EPE motion magnitude 40+"], "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "2.45", "char_index": [490, 494], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "7.90", "char_index": [497, 501], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": "F1-all", "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "0.43", "char_index": [512, 516], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{0-10}", "EPE motion magnitude 0-10"], "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "1.18", "char_index": [520, 524], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{10-40}", "EPE motion magnitude 10-40"], "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "5.70", "char_index": [527, 531], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{40+}", "EPE motion magnitude 40+"], "experimental settings": {"xx": "yy"}, "model": "RAFT", "model settings": {"xx": "yy"}}, {"value": "2.85", "char_index": [550, 554], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "10.77", "char_index": [557, 562], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": "F1-all", "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "0.49", "char_index": [565, 569], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{0-10}", "EPE motion magnitude 0-10"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "1.16", "char_index": [572, 576], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{10-40}", "EPE motion magnitude 10-40"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "6.87", "char_index": [579, 583], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{40+}", "EPE motion magnitude 40+"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"xx": "yy"}}, {"value": "2.25", "char_index": [611, 615], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["end-point-error", "EPE"], "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "7.20", "char_index": [627, 631], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": "F1-all", "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "0.48", "char_index": [635, 639], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{0-10}", "EPE motion magnitude 0-10"], "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "1.10", "char_index": [650, 654], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{10-40}", "EPE motion magnitude 10-40"], "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}, {"value": "5.12", "char_index": [666, 670], "type": "Result", "training data/set": ["C + T + VK", "Chair Things Virtual KITTI 2"], "test data/set": ["KITTI", "KITTI train"], "task": "Optical Flow", "metric": ["EPE s_{40+}", "EPE motion magnitude 40+"], "experimental settings": {"xx": "yy"}, "model": "GMFlow+", "model settings": {"xx": "yy"}}]}, "2211.05783v1_table8": {"table_code": "\\subfloat[\n\\textbf{Unrectified stereo depth estimation task}.\n\\label{tab:depth_ablation}\n]{\n\\begin{minipage}{0.5\\linewidth}{\n\\begin{center}\n\\setlength{\\tabcolsep}{2pt} \n\\begin{tabular}{lccccccccccccc}\n    \\toprule\n    \n    \n    setup & Abs Rel & Sq Rel & RMSE & RMSE log & Param (M) \\\\\n    \n    \\midrule\n    \n    full & \\textbf{0.074} & \\textbf{0.028} & \\textbf{0.225} & \\textbf{0.103} & 4.7 \\\\\n    w/o cross attn. & 0.095 & 0.043 & 0.284 & 0.132 & 4.3 \\\\\n    w/o position & 0.078 & 0.031 & 0.237 & 0.109 & 4.7 \\\\\n    w/o FFN & 0.089 & 0.041 & 0.276 & 0.127 & 2.3 \\\\\n    w/o self attn. & 0.081 & 0.034 & 0.248 & 0.114 & 4.3 \\\\\n    w/o propagation & 0.091 & 0.045 & 0.293 & 0.144 & 4.6 \\\\\n\n    \\bottomrule\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}", "table_label": "{tab:depth_ablation}", "table_numeric_cells": [["0.074", "\\textbf{0.074}", 328, 333, 320, 334], ["0.028", "\\textbf{0.028}", 345, 350, 337, 351], ["0.225", "\\textbf{0.225}", 362, 367, 354, 368], ["0.103", "\\textbf{0.103}", 379, 384, 371, 385], ["4.7", "4.7", 388, 391, 388, 391], ["0.095", "0.095", 417, 422, 417, 422], ["0.043", "0.043", 425, 430, 425, 430], ["0.284", "0.284", 433, 438, 433, 438], ["0.132", "0.132", 441, 446, 441, 446], ["4.3", "4.3", 449, 452, 449, 452], ["0.078", "0.078", 475, 480, 475, 480], ["0.031", "0.031", 483, 488, 483, 488], ["0.237", "0.237", 491, 496, 491, 496], ["0.109", "0.109", 499, 504, 499, 504], ["4.7", "4.7", 507, 510, 507, 510], ["0.089", "0.089", 528, 533, 528, 533], ["0.041", "0.041", 536, 541, 536, 541], ["0.276", "0.276", 544, 549, 544, 549], ["0.127", "0.127", 552, 557, 552, 557], ["2.3", "2.3", 560, 563, 560, 563], ["0.081", "0.081", 588, 593, 588, 593], ["0.034", "0.034", 596, 601, 596, 601], ["0.248", "0.248", 604, 609, 604, 609], ["0.114", "0.114", 612, 617, 612, 617], ["4.3", "4.3", 620, 623, 620, 623], ["0.091", "0.091", 649, 654, 649, 654], ["0.045", "0.045", 657, 662, 657, 662], ["0.293", "0.293", 665, 670, 665, 670], ["0.144", "0.144", 673, 678, 673, 678], ["4.6", "4.6", 681, 684, 681, 684]], "text_chunk_selected": "{Our unified model with only one task-agnostic hierarchical matching refinement outperforms RAFT~\\cite{teed2020raft} with 31 refinement steps on the challenging Sintel~\\cite{butler2012naturalistic} dataset while running faster (Fig.~\\ref{fig:flow_iter_vs_epe} and Table~\\ref{tab:flow_raft_vs_gmflow}), demonstrating the effectiveness and efficiency of our method. Our final model that uses a few additional task-specific refinement steps outperforms or compares favorably to recent state-of-the-art methods on 10 popular flow/stereo/depth datasets (KITTI Flow~\\cite{menze2015object}, Sintel~\\cite{butler2012naturalistic}, Middlebury~\\cite{scharstein2014high}, KITTI Stereo~\\cite{menze2015object}, ETH3D Stereo~\\cite{schops2017multi}, Argoverse Stereo~\\cite{chang2019argoverse}, ScanNet~\\cite{dai2017scannet}, SUN3D~\\cite{xiao2013sun3d}, RGBD-SLAM~\\cite{sturm2012benchmark} and Scenes11~\\cite{ummenhofer2017demon}), while being simpler and more efficient in terms of model design and inference speed.}\n\nFor unrectified stereo depth estimation, we assume the camera intrinsic and extrinsic parameters (${\\bm K}_1, {\\bm E}_1, {\\bm K}_2, {\\bm E}_2$) for image ${\\bm I}_1$ and ${\\bm I}_2$ are known (\\emph{i.e}\\onedot, posed images). They can be obtained via additional sensors like IMU and GPS, or reliably estimated using Structure-from-Motion software like COLMAP~\\cite{schonberger2016structure}. To estimate depth, we take an approach similar to the classic plane-sweep stereo method~\\cite{collins1996space}. More specifically, we first discretize a predefined depth range $[d_{\\mathrm{min}}, d_{\\mathrm{max}}]$ as $[d_1, d_2, \\cdots, d_N]$ (in our implementation, we discretize the inverse depth domain, while we use depth here for ease of notation). Then for each depth candidate $d_i (i=1, 2, \\cdots, N)$, we compute the 2D correspondences $\\hat{{\\bm G}}_{\\mathrm{2D}} \\in \\mathbb{R}^{H \\times W \\times 2}$ in ${\\bm F}_2$ given the current depth value:\n\n\\begin{equation}\n\\label{eq:depth_match}\n    {\\bm M}_{\\mathrm{depth}} = \\mathrm{softmax} ({\\bm C}_{\\mathrm{depth}}) \\in \\mathbb{R}^{H \\times W \\times N}.\n\\end{equation}\n\n\\noindent \\textbf{Training schedule.} For methodology comparison (Sec.~\\ref{sec:method_comp}) and ablation experiments (Sec.~\\ref{sec:ablation}), we first train our default $1/8$ feature resolution model on Chairs dataset for 100K iterations, with a batch size of 16 and a learning rate of 4e-4. We then finetune the model on Things dataset for 200K iterations, with a batch size of 8 and a learning rate of 2e-4. The models thus far all use bilinear upsamling to upsample to the full resolution flow prediction for simplicity. For later experiments, we use RAFT's convex upsampling~\\cite{teed2020raft} and our models are trained on Things dataset for 800K iterations, which leads to improved performance~\\cite{xu2022gmflow}. For the final fine-tuning process on Sintel and KITTI datasets for benchmark comparisons, we report details in Sec.~\\ref{sec:flow_benchmark}.\n\n\\noindent {\\bf Model components.} We ablate different components of our full model in Table~\\ref{tab:stereo_ablation}. The results are consistent with those for the optical flow task in Table~\\ref{tab:transformer} and Table~\\ref{tab:prop}. That is, the cross-attention contributes most, but the other components also contribute to the performance gains.\n\n\\noindent \\textbf{Metrics.} Following previous methods~\\cite{Tang2019BANetDB, im2019dpsnet}, we use 4 error metrics for evaluation of the depth quality, including Absolute Relative difference (Abs Rel), Squared Relative difference (Sq Rel), Root Mean Squared Error (RMSE) and RMSE in log scale (RMSE log).\n\n\\noindent \\textbf{Training schedule.} For ablation experiments, we train our model on the ScanNet for 50K iterations, with a batch size of 80 and a learning rate of 4e-4. The depth range for training and testing is set to $[0.5, 10]$ meters, and the number of depth candidates in the matching layer (Eq.~\\eqref{eq:depth_match}) is set to $64$. The training and testing process on SUN3D~\\cite{xiao2013sun3d}, RGBD-SLAM~\\cite{sturm2012benchmark} and Scenes11~\\cite{ummenhofer2017demon} datasets will be elaborated in Sec.~\\ref{sec:depth_benchmark}.\n\n\\noindent {\\bf Model components.} We ablate different components of our full model in Table~\\ref{tab:depth_ablation}. The results are consistent with those for optical flow and stereo matching tasks in Table~\\ref{tab:transformer}, Table~\\ref{tab:prop} and Table~\\ref{tab:stereo_ablation}. That is, the cross-attention contributes most, but the other components also contribute to the performance gains.", "table_source": "\\begin{table*}[t]\n\\centering\n\\subfloat[\n\\textbf{Rectified stereo matching task}.\n\\label{tab:stereo_ablation}\n]{\n\\centering\n\\begin{minipage}{0.43\\linewidth}\n{\\begin{center}\n\\begin{tabular}{lccccccccccccc}\n    \\toprule\n\n    \\multirow{2}{*}[-2pt]{setup} &  \\multicolumn{2}{c}{Things} &\n    \\multicolumn{2}{c}{KITTI} & \\multirow{2}{*}[-2pt]{\\begin{tabular}[x]{@{}c@{}}Param\\\\(M) \\end{tabular}} \\\\\n    \\addlinespace[-10pt]  \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \n    \\addlinespace[-10pt] \\\\\n    & EPE & D1 & EPE & D1 & \\\\\n    \n    \\midrule\n    \n    full & \\textbf{1.22} & \\textbf{3.70} & \\textbf{1.61} & 10.53 & 4.7 \\\\\n    w/o cross attn. & 1.96 & 7.84 & 5.40 & 31.97 & 4.3 \\\\\n    w/o position & 1.24 & 4.46 & 1.72 & 12.07 & 4.7 \\\\\n    w/o FFN & 1.39 & 4.93 & 1.95 & 14.86 & 2.3 \\\\\n    w/o self attn. & 1.35 & 4.47 & 1.87 & 13.04 & 4.3 \\\\\n    w/o propagation & 2.33 & 6.08 & 1.76 & \\textbf{10.09} & 4.6 \\\\\n\n    \\bottomrule\n    \\end{tabular}\n\\end{center}}\n\\end{minipage}\n}\n\\hspace{2em}\n\\subfloat[\n\\textbf{Unrectified stereo depth estimation task}.\n\\label{tab:depth_ablation}\n]{\n\\begin{minipage}{0.5\\linewidth}{\n\\begin{center}\n\\setlength{\\tabcolsep}{2pt} \n\\begin{tabular}{lccccccccccccc}\n    \\toprule\n    \n    \n    setup & Abs Rel & Sq Rel & RMSE & RMSE log & Param (M) \\\\\n    \n    \\midrule\n    \n    full & \\textbf{0.074} & \\textbf{0.028} & \\textbf{0.225} & \\textbf{0.103} & 4.7 \\\\\n    w/o cross attn. & 0.095 & 0.043 & 0.284 & 0.132 & 4.3 \\\\\n    w/o position & 0.078 & 0.031 & 0.237 & 0.109 & 4.7 \\\\\n    w/o FFN & 0.089 & 0.041 & 0.276 & 0.127 & 2.3 \\\\\n    w/o self attn. & 0.081 & 0.034 & 0.248 & 0.114 & 4.3 \\\\\n    w/o propagation & 0.091 & 0.045 & 0.293 & 0.144 & 4.6 \\\\\n\n    \\bottomrule\n    \\end{tabular}\n\\end{center}}\\end{minipage}\n}\n\n\\caption{\\textbf{Ablations of Transformer components and the propagation strategy}. Cross-attention contributes most, consistent with the analysis in optical flow task (Table~\\ref{tab:transformer}).}\n\\label{tab:stereo_depth_ablations}\n\\end{table*}", "cell_list_gold": [{"value": "0.074", "char_index": [328, 333], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "full", "Param (M)": "4.7"}}, {"value": "0.028", "char_index": [345, 350], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "full", "Param (M)": "4.7"}}, {"value": "0.225", "char_index": [362, 367], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "full", "Param (M)": "4.7"}}, {"value": "0.103", "char_index": [379, 384], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "full", "Param (M)": "4.7"}}, {"value": "4.7", "char_index": [388, 391], "type": "Hyper-parameter/Architecture", "model": "GMFlow", "parameter/architecture name": ["number of parameters", "Param (M)"], "dataset": "ScanNet"}, {"value": "0.095", "char_index": [417, 422], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o cross attn.", "Param (M)": "4.3"}}, {"value": "0.043", "char_index": [425, 430], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o cross attn.", "Param (M)": "4.3"}}, {"value": "0.284", "char_index": [433, 438], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o cross attn.", "Param (M)": "4.3"}}, {"value": "0.132", "char_index": [441, 446], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o cross attn.", "Param (M)": "4.3"}}, {"value": "4.3", "char_index": [449, 452], "type": "Hyper-parameter/Architecture", "model": "GMFlow", "parameter/architecture name": ["number of parameters", "Param (M)"], "dataset": "ScanNet"}, {"value": "0.078", "char_index": [475, 480], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o position", "Param (M)": "4.7"}}, {"value": "0.031", "char_index": [483, 488], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o position", "Param (M)": "4.7"}}, {"value": "0.237", "char_index": [491, 496], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o position", "Param (M)": "4.7"}}, {"value": "0.109", "char_index": [499, 504], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o position", "Param (M)": "4.7"}}, {"value": "4.7", "char_index": [507, 510], "type": "Hyper-parameter/Architecture", "model": "GMFlow", "parameter/architecture name": ["number of parameters", "Param (M)"], "dataset": "ScanNet"}, {"value": "0.089", "char_index": [528, 533], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o FFN", "Param (M)": "2.3"}}, {"value": "0.041", "char_index": [536, 541], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o FFN", "Param (M)": "2.3"}}, {"value": "0.276", "char_index": [544, 549], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o FFN", "Param (M)": "2.3"}}, {"value": "0.127", "char_index": [552, 557], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o FFN", "Param (M)": "2.3"}}, {"value": "2.3", "char_index": [560, 563], "type": "Hyper-parameter/Architecture", "model": "GMFlow", "parameter/architecture name": ["number of parameters", "Param (M)"], "dataset": "ScanNet"}, {"value": "0.081", "char_index": [588, 593], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o self attn.", "Param (M)": "4.3"}}, {"value": "0.034", "char_index": [596, 601], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o self attn.", "Param (M)": "4.3"}}, {"value": "0.248", "char_index": [604, 609], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o self attn.", "Param (M)": "4.3"}}, {"value": "0.114", "char_index": [612, 617], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o self attn.", "Param (M)": "4.3"}}, {"value": "4.3", "char_index": [620, 623], "type": "Hyper-parameter/Architecture", "model": "GMFlow", "parameter/architecture name": ["number of parameters", "Param (M)"], "dataset": "ScanNet"}, {"value": "0.091", "char_index": [649, 654], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Abs Rel", "Absolute Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o propagation", "Param (M)": "4.6"}}, {"value": "0.045", "char_index": [657, 662], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["Sq Rel", "Squared Relative difference"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o propagation", "Param (M)": "4.6"}}, {"value": "0.293", "char_index": [665, 670], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE", "Root Mean Squared Error"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o propagation", "Param (M)": "4.6"}}, {"value": "0.144", "char_index": [673, 678], "type": "Result", "training data/set": "ScanNet", "test data/set": "ScanNet", "task": ["Depth Prediction", "Depth Estimation"], "metric": ["RMSE log", "RMSE in log scale"], "experimental settings": {"xx": "yy"}, "model": "GMFlow", "model settings": {"configuration": "w/o propagation", "Param (M)": "4.6"}}, {"value": "4.6", "char_index": [681, 684], "type": "Hyper-parameter/Architecture", "model": "GMFlow", "parameter/architecture name": ["number of parameters", "Param (M)"], "dataset": "ScanNet"}]}}