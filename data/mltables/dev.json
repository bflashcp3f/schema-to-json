{"2211.05596v1_table2": {"table_code": "\\begin{table}[!htbp]\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}lllllll@{}}\n\\toprule\nMode            & \\multicolumn{3}{l}{Bus Booking} & \\multicolumn{3}{l}{Hotel Reservation} \\\\ \\midrule\n                & 345M      & 1.3B     & 5B       & 345M        & 1.3B       & 5B         \\\\ \\midrule\nZero Shot       & 0.755     & 0.762    & 0.787    & 0.379       & 0.448      & 0.467      \\\\\nFS - 10 samples & 0.907     & 0.789    & 0.942    & 0.793       & 0.720       & 0.939      \\\\\nFS - 50 samples & 0.953     & 0.965    & 0.975    & 0.957       & 0.968      & 0.970    \\\\ \\bottomrule     \n\\end{tabular}\n}\n\\caption{Zero-shot and Few Shot (FS) performance on the held out domains of the SGD dataset. The columns indicate the size of the Megatron-GPT model.}\n\\label{tab:ood-sgd}\n\\end{table}", "table_label": "{tab:ood-sgd}", "table_numeric_cells": [["345M", "345M", 210, 214, 210, 214], ["1.3B", "1.3B", 222, 226, 222, 226], ["5B", "5B", 233, 235, 233, 235], ["345M", "345M", 244, 248, 244, 248], ["1.3B", "1.3B", 258, 262, 258, 262], ["5B", "5B", 271, 273, 271, 273], ["0.755", "0.755", 312, 317, 312, 317], ["0.762", "0.762", 324, 329, 324, 329], ["0.787", "0.787", 335, 340, 335, 340], ["0.379", "0.379", 346, 351, 346, 351], ["0.448", "0.448", 360, 365, 360, 365], ["0.467", "0.467", 373, 378, 373, 378], ["0.907", "0.907", 405, 410, 405, 410], ["0.789", "0.789", 417, 422, 417, 422], ["0.942", "0.942", 428, 433, 428, 433], ["0.793", "0.793", 439, 444, 439, 444], ["0.720", "0.720", 453, 458, 453, 458], ["0.939", "0.939", 467, 472, 467, 472], ["0.953", "0.953", 499, 504, 499, 504], ["0.965", "0.965", 511, 516, 511, 516], ["0.975", "0.975", 522, 527, 522, 527], ["0.957", "0.957", 533, 538, 533, 538], ["0.968", "0.968", 547, 552, 547, 552], ["0.970", "0.970", 560, 565, 560, 565]], "text_chunk_selected": "We observe that using such \\textit{canonical forms} as labels for the intent classification task allows the model to generalize better to domains that are adjacent, but not seen at train time (\\textit{e.g.}, \\textit{Flight Reservations} $\\rightarrow$ \\textit{Bus Bookings}). We also find that it is beneficial to do a two-stage P-tuning for domain adaption, \\textit{i.e.}, once we have a p-tuned large language model on a wide set of domains, we can continue p-tuning this model on a small set of labelled samples from the target domain to allow the model to generalize better. We find that this few-shot approach works very well and this has promising implications for developers for dialogue systems; with minimal effort it would be feasible to adapt an existing model pre-trained on multiple domains to a new domain. In summary, our contributions are:\n\n\\begin{itemize}\n\\item We cast the problem of intent classification into a generative approach and rewrite intent labels in a more descriptive format (\\textit{canonical forms}).\n    \\item When using such canonical forms, generative approaches with Large Language Models(LLMs) show promising results when compared with traditional methods for intent classification.\n\\item Generative models generalize very well to unseen domains in zero-shot and few-shot settings when compared with BERT-style approaches. \n\\item We demonstrate the sample efficiency of p-tuning LLMs where we can achieve close to full dataset performance with a fraction of the data.\n\\end{itemize}\n\n\\subsection{Datasets}\n\\label{sec:datasets}\nWe consider two widely known datasets in the dialogue community, the Schema Guided Dialogue (SGD) dataset \\citep{sgd-dataset} and the Virtual Assistant dataset \\citep{assistant-dataset}.\\\\\\\\\n\\textbf{Schema Guided Dialogue} - This dataset covers 16 domains and has over 16k annotated conversations. The domains span a variety of user actions, including setting calendars and alarms, travel booking (car rentals, flights, buses and trains), music, weather, movies, and more. The dataset also contains \\textit{multi-domain} dialogues where the utterances switch between domains. For the purpose of our experiments, we consider only the \\textit{single-domain} dialogues with 37 intents across all utterances. \\\\\\\\\n\\textbf{Virtual Assistant Dataset} - This dataset covers 21 domains with 64 intents across all utterances. As the name suggests, the domains relate to user queries over a wide range of topics, including operating smart-home devices, media consumption, weather and travel. It has over 25k annotated user utterances that identify intents and slot values.\n\n\\begin{itemize}\n    \\item Using Fasttext Embeddings \\citep{bojanowski2016enriching}: We take the mean of all the embedding vectors of the generated canonical form and consider the vector obtained to be the representation of the whole sequence. We compute similar vectors for all the canonical form labels and consider the canonical form label that has the maximum cosine similarity with the generated one as the model's prediction. \n    \\item Using Sentence Transformers \\citep{reimers-2019-sentence-bert}: We use the \\textit{miniLM-QA} \\citep{minilm} transformer model that has been pretrained on multiple datasets on the text entailment/semantic search task, \\textit{i.e.}, given a query and a set of keys (documents/labels), it ranks the keys in order of relevance. We give as input to the model the generated canonical form (query) and the list of canonical form labels (keys). The model then returns the closest canonical form label to the generated canonical form which we consider as the prediction.  \n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{BERT-based finetuned model} (Intent Classification): We finetune BERT models on the datasets described in section \\ref{sec:datasets}. While some of the Megatron-GPT models we use are larger than the BERT model in terms of number of parameters, it should be noted that the LM parameters are frozen during the training stage of p-tuning and only the weights of the LSTM (~14M parameters) are updated.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Schema Guided Dialogue} (SGD): We hold out utterances corresponding to \\textit{bus bookings} and \\textit{hotel reservations} to form our test set. The train set includes utterances from adjacent domains: flight booking and restaurant reservations. This should be a relatively easy setting for the language model to generalize to.\n    \\item \\textbf{Virtual Assistant}: To make things more challenging, we hold out utterances corresponding to \\textit{operating IOT devices} and \\textit{media consumption commands} (\\textit{e.g.}, commands that are variants of \"play\" - play movie, play audiobook). The train set does not have utterances from similar domains and this setting is more challenging for the model.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Zero-shot}: P-tune the model on the train set and evaluate zero-shot on the unseen domain test set.\n    \\item \\textbf{Few-shot}: After p-tuning on the train set, we do a second stage p-tuning on a set of \\textit{k} samples from the target domain. Unless otherwise noted, \\textit{k} here is 5, 10, 50 or 100 samples. \n\\end{itemize}\n\n\\subsubsection{Out-of-Domain}\nThe out-of-domain setting is where the advantage of using a LLM becomes apparent. It is not feasible to expect a finetuned BERT model to generalize to an unseen domain not present in the train set. Such models continue to predict that the intent belongs to one of the intent labels they see during training. The p-tuned Megatron-GPT models, on the other hand, show impressive zero-shot and few-shot generalization capabilities on the SGD dataset (Table \\ref{tab:ood-sgd}). For instance, having seen intents such as \\textit{\"buy flight roundtrip tickets\"} when presented with utterances for \\textit{Flight Reservations} in training, we can expect the model to reasonably generalize to utterances from \\textit{Bus Reservations} with utterances like \"Get me a return trip on the bus\" with the model's prediction for the intent being \\textit{\"buy bus roundtrip tickets\"}.", "table_source": "\\begin{table}[!htbp]\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}lllllll@{}}\n\\toprule\nMode            & \\multicolumn{3}{l}{Bus Booking} & \\multicolumn{3}{l}{Hotel Reservation} \\\\ \\midrule\n                & 345M      & 1.3B     & 5B       & 345M        & 1.3B       & 5B         \\\\ \\midrule\nZero Shot       & 0.755     & 0.762    & 0.787    & 0.379       & 0.448      & 0.467      \\\\\nFS - 10 samples & 0.907     & 0.789    & 0.942    & 0.793       & 0.720       & 0.939      \\\\\nFS - 50 samples & 0.953     & 0.965    & 0.975    & 0.957       & 0.968      & 0.970    \\\\ \\bottomrule     \n\\end{tabular}\n}\n\\caption{Zero-shot and Few Shot (FS) performance on the held out domains of the SGD dataset. The columns indicate the size of the Megatron-GPT model.}\n\\label{tab:ood-sgd}\n\\end{table}", "cell_list_gold": [{"value": "345M", "char_index": [210, 214], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["model size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "1.3B", "char_index": [222, 226], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["model size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "5B", "char_index": [233, 235], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["model size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "345M", "char_index": [244, 248], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["model size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "1.3B", "char_index": [258, 262], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["model size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "5B", "char_index": [271, 273], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["model size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "0.755", "char_index": [312, 317], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "Zero Shot", "out-of-domain": "true"}, "model settings": {"model size": "345M"}}, {"value": "0.762", "char_index": [324, 329], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "Zero Shot", "out-of-domain": "true"}, "model settings": {"model size": "1.3B"}}, {"value": "0.787", "char_index": [335, 340], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "Zero Shot", "out-of-domain": "true"}, "model settings": {"model size": "5B"}}, {"value": "0.379", "char_index": [346, 351], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "Zero Shot", "out-of-domain": "true"}, "model settings": {"model size": "345M"}}, {"value": "0.448", "char_index": [360, 365], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "Zero Shot", "out-of-domain": "true"}, "model settings": {"model size": "1.3B"}}, {"value": "0.467", "char_index": [373, 378], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "Zero Shot", "out-of-domain": "true"}, "model settings": {"model size": "5B"}}, {"value": "0.907", "char_index": [405, 410], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 10 samples", "out-of-domain": "true"}, "model settings": {"model size": "345M"}}, {"value": "0.789", "char_index": [417, 422], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 10 samples", "out-of-domain": "true"}, "model settings": {"model size": "1.3B"}}, {"value": "0.942", "char_index": [428, 433], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 10 samples", "out-of-domain": "true"}, "model settings": {"model size": "5B"}}, {"value": "0.793", "char_index": [439, 444], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 10 samples", "out-of-domain": "true"}, "model settings": {"model size": "345M"}}, {"value": "0.720", "char_index": [453, 458], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 10 samples", "out-of-domain": "true"}, "model settings": {"model size": "1.3B"}}, {"value": "0.939", "char_index": [467, 472], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 10 samples", "out-of-domain": "true"}, "model settings": {"model size": "5B"}}, {"value": "0.953", "char_index": [499, 504], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 50 samples", "out-of-domain": "true"}, "model settings": {"model size": "345M"}}, {"value": "0.965", "char_index": [511, 516], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 50 samples", "out-of-domain": "true"}, "model settings": {"model size": "1.3B"}}, {"value": "0.975", "char_index": [522, 527], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Booking", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 50 samples", "out-of-domain": "true"}, "model settings": {"model size": "5B"}}, {"value": "0.957", "char_index": [533, 538], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 50 samples", "out-of-domain": "true"}, "model settings": {"model size": "345M"}}, {"value": "0.968", "char_index": [547, 552], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 50 samples", "out-of-domain": "true"}, "model settings": {"model size": "1.3B"}}, {"value": "0.970", "char_index": [560, 565], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Hotel Reservation", "task": "Intent Classification", "metric": "Accuracy", "model": "Megatron-GPT", "experimental settings": {"mode": "FS - 50 samples", "out-of-domain": "true"}, "model settings": {"model size": "5B"}}]}, "2211.05596v1_table7": {"table_code": "\\begin{table}[h]\n\\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}l|c|ccc@{}}\n\\toprule\n\\textbf{\\#Samples/Intent} & \\textbf{Train Size} & \\multicolumn{3}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n                          && 345M        & 1.3B        & 5B        \\\\\\midrule\n10                        &640& 0.69        & 0.81        & 0.84      \\\\\n20                        &1280& 0.74        & 0.84        & 0.91     \\\\\n30                        &1920&  0.79        &0.87         & 0.91     \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{Accuracy on the Assistant test set when using only k samples per intent. The columns indicate the size of the Megatron-GPT model used.}\n\\label{tab:asst-fs-indomain}\n\\end{table}", "table_label": "{tab:asst-fs-indomain}", "table_numeric_cells": [["345M", "345M", 226, 230, 226, 230], ["1.3B", "1.3B", 240, 244, 240, 244], ["5B", "5B", 254, 256, 254, 256], ["10", "10", 275, 277, 275, 277], ["640", "640", 302, 305, 302, 305], ["0.69", "0.69", 307, 311, 307, 311], ["0.81", "0.81", 321, 325, 321, 325], ["0.84", "0.84", 335, 339, 335, 339], ["20", "20", 348, 350, 348, 350], ["1280", "1280", 375, 379, 375, 379], ["0.74", "0.74", 381, 385, 381, 385], ["0.84", "0.84", 395, 399, 395, 399], ["0.91", "0.91", 409, 413, 409, 413], ["30", "30", 421, 423, 421, 423], ["1920", "1920", 448, 452, 448, 452], ["0.79", "0.79", 455, 459, 455, 459], ["0.87", "0.87", 468, 472, 468, 472], ["0.91", "0.91", 483, 487, 483, 487]], "text_chunk_selected": "\\begin{itemize}\n    \\item \\textbf{Schema Guided Dialogue} (SGD): We hold out utterances corresponding to \\textit{bus bookings} and \\textit{hotel reservations} to form our test set. The train set includes utterances from adjacent domains: flight booking and restaurant reservations. This should be a relatively easy setting for the language model to generalize to.\n    \\item \\textbf{Virtual Assistant}: To make things more challenging, we hold out utterances corresponding to \\textit{operating IOT devices} and \\textit{media consumption commands} (\\textit{e.g.}, commands that are variants of \"play\" - play movie, play audiobook). The train set does not have utterances from similar domains and this setting is more challenging for the model.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Zero-shot}: P-tune the model on the train set and evaluate zero-shot on the unseen domain test set.\n    \\item \\textbf{Few-shot}: After p-tuning on the train set, we do a second stage p-tuning on a set of \\textit{k} samples from the target domain. Unless otherwise noted, \\textit{k} here is 5, 10, 50 or 100 samples. \n\\end{itemize}\n\n\\subsubsection{In-domain}\nWe find that both the p-tuned GPT model as well as the BERT baseline perform very well on the standard in-domain split where both the train and test set come from the same distribution (Table \\ref{tab:indomain}). The classification accuracy of Megatron-GPT increases as we increase the model size. The trend of results remains consistent for both the SGD and Assistant datasets. \n\n\\subsubsection{Out-of-Domain}\nThe out-of-domain setting is where the advantage of using a LLM becomes apparent. It is not feasible to expect a finetuned BERT model to generalize to an unseen domain not present in the train set. Such models continue to predict that the intent belongs to one of the intent labels they see during training. The p-tuned Megatron-GPT models, on the other hand, show impressive zero-shot and few-shot generalization capabilities on the SGD dataset (Table \\ref{tab:ood-sgd}). For instance, having seen intents such as \\textit{\"buy flight roundtrip tickets\"} when presented with utterances for \\textit{Flight Reservations} in training, we can expect the model to reasonably generalize to utterances from \\textit{Bus Reservations} with utterances like \"Get me a return trip on the bus\" with the model's prediction for the intent being \\textit{\"buy bus roundtrip tickets\"}.\n\nRephrasing the canonical form for the intent \\textit{SearchFlightOneWay} to \\textit{search for flights one way} helps the model to avoid making the spurious correlation and the performance in the zero-shot setting (Table \\ref{tab:ood-sgd-original}) is significantly improved.\n\nHowever, the few-shot setting (Table \\ref{tab:ood-sgd-original-fs}) alleviates this problem of sensitivity of the model to the canonical form structure. When we provide the model with a few samples from the the target domain, it learns to associate that the important words to distinguish between the domains are \\textit{flight} and \\textit{bus} and not \\textit{ticket}.\n\n\\begin{itemize}\n    \\item \\textbf{Similarity in structure}: Use similar verbs for similar actions/domains, \\textit{e.g.}, \\textbf{book} a flight, \\textbf{book} bus tickets, \\textbf{search} for hotels, \\textbf{search} for restaurant reservations.\n    \\item \\textbf{ Compositional}: Using similar structures for canonical forms in similar domains naturally lends to compositionality. This makes it easier for the model to generalize in the zero-shot/few-shot setting while still allowing the developers to easily map the generations to a supported service on the backend.\n    \\item \\textbf{Looks like natural language}: Since LLMs are pretrained on very large corpora of natural language, the benefit of pre-training is realized when the canonical forms resemble natural language rather than complex semantic forms. Making discrete intents look more like typical verb phrases brings out the expressive nature of language models. \n\\end{itemize}\n\n\\subsection{Do we need the entire training set for p-tuning?}\nWe look for the fewest labelled samples for p-tuning needed to get an accuracy close to accessing the entire train set. We randomly sample \\textit{k} samples per intent ($k \\in {5, 10, 20, 30}$) to form the train set the model is p-tuned on, and evaluate on the same test set as above. The train and test sets are from the in-domain setting for both SGD (Table \\ref{tab:sgd-fs-indomain}) and Assistant (Table \\ref{tab:asst-fs-indomain}) datasets.", "table_source": "\\begin{table}[h]\n\\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}l|c|ccc@{}}\n\\toprule\n\\textbf{\\#Samples/Intent} & \\textbf{Train Size} & \\multicolumn{3}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n                          && 345M        & 1.3B        & 5B        \\\\\\midrule\n10                        &640& 0.69        & 0.81        & 0.84      \\\\\n20                        &1280& 0.74        & 0.84        & 0.91     \\\\\n30                        &1920&  0.79        &0.87         & 0.91     \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{Accuracy on the Assistant test set when using only k samples per intent. The columns indicate the size of the Megatron-GPT model used.}\n\\label{tab:asst-fs-indomain}\n\\end{table}", "cell_list_gold": [{"value": "345M", "char_index": [226, 230], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "1.3B", "char_index": [240, 244], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "5B", "char_index": [254, 256], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "10", "char_index": [275, 277], "type": "Data Stat.", "dataset": ["Assistant", "Virtual Assistant"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "640", "char_index": [302, 305], "type": "Data Stat.", "dataset": ["Assistant", "Virtual Assistant"], "sub-set/group name": "Train", "attribute name": ["number of samples", "size"], "dataset features": {"xx": "yy"}}, {"value": "0.69", "char_index": [307, 311], "type": "Result", "training data/set": "Assistant 640 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "10", "train size": "640"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.81", "char_index": [321, 325], "type": "Result", "training data/set": "Assistant 640 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "10", "train size": "640"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.84", "char_index": [335, 339], "type": "Result", "training data/set": "Assistant 640 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "10", "train size": "640"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "20", "char_index": [348, 350], "type": "Data Stat.", "dataset": ["Assistant", "Virtual Assistant"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "1280", "char_index": [375, 379], "type": "Data Stat.", "dataset": ["Assistant", "Virtual Assistant"], "sub-set/group name": "Train", "attribute name": ["number of samples", "size"], "dataset features": {"xx": "yy"}}, {"value": "0.74", "char_index": [381, 385], "type": "Result", "training data/set": "Assistant 1280 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "20", "train size": "1280"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.84", "char_index": [395, 399], "type": "Result", "training data/set": "Assistant 1280 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "20", "train size": "1280"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.91", "char_index": [409, 413], "type": "Result", "training data/set": "Assistant 1280 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "20", "train size": "1280"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "30", "char_index": [421, 423], "type": "Data Stat.", "dataset": ["Assistant", "Virtual Assistant"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "1920", "char_index": [448, 452], "type": "Data Stat.", "dataset": ["Assistant", "Virtual Assistant"], "sub-set/group name": "Train", "attribute name": ["number of samples", "size"], "dataset features": {"xx": "yy"}}, {"value": "0.79", "char_index": [455, 459], "type": "Result", "training data/set": "Assistant 1920 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "30", "train size": "1920"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.87", "char_index": [468, 472], "type": "Result", "training data/set": "Assistant 1920 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "30", "train size": "1920"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.91", "char_index": [483, 487], "type": "Result", "training data/set": "Assistant 1920 samples", "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "30", "train size": "1920"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}]}, "2211.05596v1_table8": {"table_code": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}l|c|ccc@{}}\n\\toprule\n\\textbf{\\#Samples/Intent} &  \\multicolumn{2}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n                          & 345M        & BERT                \\\\\\midrule\n10                        & 0.77        & 0.75             \\\\\n20                        & 0.82        & 0.767             \\\\\n30                        &  0.84        &0.773           \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Accuracy on the SGD test set when using only k samples per intent. MegatronGPT-345M is more sample efficient than BERT-Large.}\n\\label{tab:gptvsbert}\n\\end{table}", "table_label": "{tab:gptvsbert}", "table_numeric_cells": [["345M", "345M", 175, 179, 175, 179], ["10", "10", 220, 222, 220, 222], ["0.77", "0.77", 248, 252, 248, 252], ["0.75", "0.75", 262, 266, 262, 266], ["20", "20", 282, 284, 282, 284], ["0.82", "0.82", 310, 314, 310, 314], ["0.767", "0.767", 324, 329, 324, 329], ["30", "30", 345, 347, 345, 347], ["0.84", "0.84", 374, 378, 374, 378], ["0.773", "0.773", 387, 392, 387, 392]], "text_chunk_selected": "\\begin{itemize}\n    \\item \\textbf{BERT-based finetuned model} (Intent Classification): We finetune BERT models on the datasets described in section \\ref{sec:datasets}. While some of the Megatron-GPT models we use are larger than the BERT model in terms of number of parameters, it should be noted that the LM parameters are frozen during the training stage of p-tuning and only the weights of the LSTM (~14M parameters) are updated.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Schema Guided Dialogue} (SGD): We hold out utterances corresponding to \\textit{bus bookings} and \\textit{hotel reservations} to form our test set. The train set includes utterances from adjacent domains: flight booking and restaurant reservations. This should be a relatively easy setting for the language model to generalize to.\n    \\item \\textbf{Virtual Assistant}: To make things more challenging, we hold out utterances corresponding to \\textit{operating IOT devices} and \\textit{media consumption commands} (\\textit{e.g.}, commands that are variants of \"play\" - play movie, play audiobook). The train set does not have utterances from similar domains and this setting is more challenging for the model.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Zero-shot}: P-tune the model on the train set and evaluate zero-shot on the unseen domain test set.\n    \\item \\textbf{Few-shot}: After p-tuning on the train set, we do a second stage p-tuning on a set of \\textit{k} samples from the target domain. Unless otherwise noted, \\textit{k} here is 5, 10, 50 or 100 samples. \n\\end{itemize}\n\n\\subsubsection{In-domain}\nWe find that both the p-tuned GPT model as well as the BERT baseline perform very well on the standard in-domain split where both the train and test set come from the same distribution (Table \\ref{tab:indomain}). The classification accuracy of Megatron-GPT increases as we increase the model size. The trend of results remains consistent for both the SGD and Assistant datasets. \n\n\\subsubsection{Out-of-Domain}\nThe out-of-domain setting is where the advantage of using a LLM becomes apparent. It is not feasible to expect a finetuned BERT model to generalize to an unseen domain not present in the train set. Such models continue to predict that the intent belongs to one of the intent labels they see during training. The p-tuned Megatron-GPT models, on the other hand, show impressive zero-shot and few-shot generalization capabilities on the SGD dataset (Table \\ref{tab:ood-sgd}). For instance, having seen intents such as \\textit{\"buy flight roundtrip tickets\"} when presented with utterances for \\textit{Flight Reservations} in training, we can expect the model to reasonably generalize to utterances from \\textit{Bus Reservations} with utterances like \"Get me a return trip on the bus\" with the model's prediction for the intent being \\textit{\"buy bus roundtrip tickets\"}.\n\n\\begin{itemize}\n    \\item \\textbf{Similarity in structure}: Use similar verbs for similar actions/domains, \\textit{e.g.}, \\textbf{book} a flight, \\textbf{book} bus tickets, \\textbf{search} for hotels, \\textbf{search} for restaurant reservations.\n    \\item \\textbf{ Compositional}: Using similar structures for canonical forms in similar domains naturally lends to compositionality. This makes it easier for the model to generalize in the zero-shot/few-shot setting while still allowing the developers to easily map the generations to a supported service on the backend.\n    \\item \\textbf{Looks like natural language}: Since LLMs are pretrained on very large corpora of natural language, the benefit of pre-training is realized when the canonical forms resemble natural language rather than complex semantic forms. Making discrete intents look more like typical verb phrases brings out the expressive nature of language models. \n\\end{itemize}\n\n\\subsection{Do we need the entire training set for p-tuning?}\nWe look for the fewest labelled samples for p-tuning needed to get an accuracy close to accessing the entire train set. We randomly sample \\textit{k} samples per intent ($k \\in {5, 10, 20, 30}$) to form the train set the model is p-tuned on, and evaluate on the same test set as above. The train and test sets are from the in-domain setting for both SGD (Table \\ref{tab:sgd-fs-indomain}) and Assistant (Table \\ref{tab:asst-fs-indomain}) datasets.\n\n\\subsubsection{Comparison with BERT}\nWe observe that Megatron-GPT is more sample efficient than BERT-type models, even when adjusting for the number of parameters. We use the 345M parameter version of the Megatron-GPT for a fair comparison. We finetune BERT-Large and p-tune the GPT model on the same training subset of the SGD dataset. Results are shown in Table \\ref{tab:gptvsbert}.", "table_source": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}l|c|ccc@{}}\n\\toprule\n\\textbf{\\#Samples/Intent} &  \\multicolumn{2}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n                          & 345M        & BERT                \\\\\\midrule\n10                        & 0.77        & 0.75             \\\\\n20                        & 0.82        & 0.767             \\\\\n30                        &  0.84        &0.773           \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Accuracy on the SGD test set when using only k samples per intent. MegatronGPT-345M is more sample efficient than BERT-Large.}\n\\label{tab:gptvsbert}\n\\end{table}", "cell_list_gold": [{"value": "345M", "char_index": [175, 179], "type": "Hyper-parameter/Architecture", "model": "MegatronGPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "10", "char_index": [220, 222], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "0.77", "char_index": [248, 252], "type": "Result", "training data/set": "SGD", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "10"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.75", "char_index": [262, 266], "type": "Result", "training data/set": "SGD", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "10"}, "model": "BERT", "model settings": {"Model Size": "Large"}}, {"value": "20", "char_index": [282, 284], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "0.82", "char_index": [310, 314], "type": "Result", "training data/set": "SGD", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "20"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.767", "char_index": [324, 329], "type": "Result", "training data/set": "SGD", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "20"}, "model": "BERT", "model settings": {"Model Size": "Large"}}, {"value": "30", "char_index": [345, 347], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "0.84", "char_index": [374, 378], "type": "Result", "training data/set": "SGD", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "30"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.773", "char_index": [387, 392], "type": "Result", "training data/set": "SGD", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "30"}, "model": "BERT", "model settings": {"Model Size": "Large"}}]}, "2211.05599v1_table1": {"table_code": "\\begin{table}[]\n\\begin{tabular}{lll}\n\\hline\nModel & R-1 & BERTScore \\\\ \\hline\nmulti-lexsum-long & 38 & 85 \\\\\nmulti-lexsum-tiny & 3 & 81 \\\\\nms2 & 3 & 81 \\\\\nmultixscience & 15 & 80 \\\\\ntopic lists & 1 & 76 \\\\ \\hline\n\\end{tabular}\n\\caption{\n\\label{study_evaluation}\nRouge-1 (R-1) and BERTScore (F1) results for each models topic representations measured against.\n}\n\\end{table}", "table_label": "{study_evaluation}", "table_numeric_cells": [["38", "38", 98, 100, 98, 100], ["85", "85", 103, 105, 103, 105], ["3", "3", 129, 130, 129, 130], ["81", "81", 133, 135, 133, 135], ["3", "3", 145, 146, 145, 146], ["81", "81", 149, 151, 149, 151], ["15", "15", 171, 173, 171, 173], ["80", "80", 176, 178, 176, 178], ["1", "1", 196, 197, 196, 197], ["76", "76", 200, 202, 200, 202]], "text_chunk_selected": "\\begin{abstract}\nTopic models represent groups of documents as a list of words (the topic labels). This work asks whether an alternative approach to topic labeling can be developed that is closer to a natural language description of a topic than a word list. To this end, we present an approach to generating human-like topic labels using abstractive multi-document summarization (MDS). We investigate our approach with an exploratory case study. We model topics in citation sentences in order to understand what further research needs to be done to fully operationalize MDS for topic labeling. Our case study shows that in addition to more human-like topics there are additional advantages to evaluation by using clustering and summarization measures instead of topic model measures. However, we find that there are several developments needed before we can design a well-powered study to evaluate MDS for topic modeling fully. Namely, improving cluster cohesion, improving the factuality and faithfulness of MDS, and increasing the number of documents that might be supported by MDS. We present a number of ideas on how these can be tackled and conclude with some thoughts on how topic modeling can also be used to improve MDS in general.\n\\end{abstract}\n\nTopic models have tended to represent a topic as a list of words. Traditional topic labels are supposed to be \u201ca set of terms, when viewed together, enable human recognition of an identifiable category\u201d \\citep{hoyle_is_2021}. However, a set of terms do not align with our intuitive understandings of what a topic is: a common theme or concept explicated as a word, phrase, or natural language description \\citep{shadrova_topic_2021}. In this paper, we present an exploratory case study using multi-document summaries (MDS) as labels for clusters of citations in order to understand current limitations and future work needed for using abstractive topic labels for human-like topics of scientific documents. To our knowledge, it is the first work that proposes to use MDS for topic labeling on top of topic clusters constructed with contextualized embeddings.\n\nSome work has anticipated this challenge by developing topic representations with phrases \\citep{popa_bart-tl_2021} and summaries \\citep{basave_automatic_2014, gourru_united_2018, wan_automatic_2016}. But those works tend to be extractive, drawing the phrase or summary from a single document in the cluster\\footnote{see \\citet{alokaili_automatic_2020,popa_bart-tl_2021} for recent abstractive works.}. In the extractive setting, \\textit{there may be no existing and fluent phrase or sentence that is capable of describing all documents in the cluster} or there may be multiple and even conflicting subtopics in the cluster that require a longer abstractive representation for producing a factual summary.\n\n\\subsection{Topic modeling as clustering and MDS}\nIn order to address the issues presented above, we propose using abstractive MDS as an approach to topic labeling. Topic modeling can be reframed as a set of two tasks: (1) finding meaningful clusters for documents \\citep{sia_tired_2020, zhang_is_2022} and (2) performing MDS on those individual clusters to find meaningful topic labels. In this framework, LDA \\cite{blei_latent_2003} uses document-word distributions to construct clusters and word lists drawn from those clusters as a form of MDS. Since we are looking at abstractive MDS that moves beyond word lists, we propose that the \\textit{topic representation be a sentence or paragraph} but there is no reason why an abstractive MDS can't be trained to generate phrases or even word lists (see \\citet{alokaili_automatic_2020}) since word lists may still be appropriate in some situations.\n\n\\subsection{Evaluation}\nTopic model evaluation is challenging (see \\citet{chang_reading_2009, hoyle_is_2021, doogan_topic_2021}). Traditional metrics like coherence (NPMI), perplexity, and diversity scores are studied in the context of topic word lists and validated with correlation to human ratings of the utility or coherence of those topic word lists. Since we suggest developing abstractive topic representations, we want a way to compare various forms of both abstractive and extractive topic representations presented by the model. Since we are treating representation as a summarization task and this task includes measures that work across extractive and abstractive settings, we suggest that we start with standard summarization metrics such as overlap metrics like Rogue (as used in \\citet{cui_topic-guided_2021} or semantic metrics such as BERTScore \\citep{zhang_is_2022} (as used in \\citet{alokaili_automatic_2020}).\n\n\\section{Case study: how has a scientific document been cited?}\nTo evaluate our proposed method, we chose topic modeling over scientific documents as a setting. While several methods exist for determining citation intent function \\citep{basuki_sdcf_2022, nicholson_scite_2021} and the relationship between two papers \\citep{luu_explaining_2021}, there is very little work on topic models over citations (for some representative work on \"citation summary\" see \\citet{elkiss_blind_2008, wang_generating_2021, zou_citation_2021}). Topic representations of citations are interesting for characterizing trends in how a paper has been cited or helping researchers identify relevant citations to read among potentially thousands of other citations. In this work, we treat topic labels as a \"citation intent\" label and use the proposed approach to understand the utility of MDS for topic modeling in this setting.\n\nIn order to identify meaningful groups of clusters we use contextualized topic models (CTM) \\citep{bianchi_pre-training_2021} since this method uses contextualized word embeddings (we used SPECTER for constructing embeddings \\citep{cohan-etal-2020-specter}). We selected CTM since we still get word lists as topic labels which we used for evaluation. In order to select the number of topics hyperparameter, we trained CTM several times steadily increasing the number of topics from 3 to 50 and selected the best model according to coherence (NPMI) resulting in a 10 topic model (see Appendix \\ref{ctm} for more details) over 183 citation statements.\n\nTable \\ref{study_evaluation} shows the Rogue-1 (R-1) and BERTScore (average F1 across topics) for each of the models selected for generating topic representations using MDS as well as the topic lists generated by CTM. It is important to underscore that R-1 and BERTScore are not validated against human studies for topic representations and this is simply a small case study on what an approach might look like. In spite of this, our results paint an initial picture of how these methods perform, especially when compared to model outputs (see Appendix \\ref{outputs} for samples). Topic word lists have the worst R-1 and BERTScore. The MDS models do a little bit better with multi-lexsum-long having the best overall score. multixscience also does well with regards to R-1. Since multixscience and multi-lexsum-long are long form summaries, it appears that R-1 is potentially biased towards longer summaries and may not be a good measure across representations, in particular it may be uninformative for evaluating the performance of topic lists. ms2 and multi-lexsum-tiny are smaller and have better BERTScore then multixscience indicating they might provide more semantically similar representations. We are also not sure whether BERTScore suffers from the same bias towards longer or more sentence-like inputs.", "table_source": "\\begin{table}[]\n\\begin{tabular}{lll}\n\\hline\nModel & R-1 & BERTScore \\\\ \\hline\nmulti-lexsum-long & 38 & 85 \\\\\nmulti-lexsum-tiny & 3 & 81 \\\\\nms2 & 3 & 81 \\\\\nmultixscience & 15 & 80 \\\\\ntopic lists & 1 & 76 \\\\ \\hline\n\\end{tabular}\n\\caption{\n\\label{study_evaluation}\nRouge-1 (R-1) and BERTScore (F1) results for each models topic representations measured against.\n}\n\\end{table}", "cell_list_gold": [{"value": "38", "char_index": [98, 100], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["Rouge-1", "R-1"], "experimental settings": {"xx": "yy"}, "model": "multi-lexsum-long", "model settings": {"xx": "yy"}}, {"value": "85", "char_index": [103, 105], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["BERTScore", "F1", "BERTScore (F1)"], "experimental settings": {"xx": "yy"}, "model": "multi-lexsum-long", "model settings": {"xx": "yy"}}, {"value": "3", "char_index": [129, 130], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["Rouge-1", "R-1"], "experimental settings": {"xx": "yy"}, "model": "multi-lexsum-tiny", "model settings": {"xx": "yy"}}, {"value": "81", "char_index": [133, 135], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["BERTScore", "F1", "BERTScore (F1)"], "experimental settings": {"xx": "yy"}, "model": "multi-lexsum-tiny", "model settings": {"xx": "yy"}}, {"value": "3", "char_index": [145, 146], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["Rouge-1", "R-1"], "experimental settings": {"xx": "yy"}, "model": "ms2", "model settings": {"xx": "yy"}}, {"value": "81", "char_index": [149, 151], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["BERTScore", "F1", "BERTScore (F1)"], "experimental settings": {"xx": "yy"}, "model": "ms2", "model settings": {"xx": "yy"}}, {"value": "15", "char_index": [171, 173], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["Rouge-1", "R-1"], "experimental settings": {"xx": "yy"}, "model": "multixscience", "model settings": {"xx": "yy"}}, {"value": "80", "char_index": [176, 178], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["BERTScore", "F1", "BERTScore (F1)"], "experimental settings": {"xx": "yy"}, "model": "multixscience", "model settings": {"xx": "yy"}}, {"value": "1", "char_index": [196, 197], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["Rouge-1", "R-1"], "experimental settings": {"xx": "yy"}, "model": "topic lists", "model settings": {"xx": "yy"}}, {"value": "76", "char_index": [200, 202], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": ["topic modeling", "topic representation"], "metric": ["BERTScore", "F1", "BERTScore (F1)"], "experimental settings": {"xx": "yy"}, "model": "topic lists", "model settings": {"xx": "yy"}}]}, "2211.05673v1_table2": {"table_code": "\\begin{table*}[h]\n\\centering\n{\n\\small\n\\tabcolsep=0.11cm\n\\begin{tabular}{lrrrrrrrrrrrrrrrrr}\n                                                               & G            & O            & P            & CD           & FJ           & PJ           & A            & CP           & AA           & S            & L            & CA           & Ap           & P            & SE           & DC           & other        \\\\ \\hline\nGalenus                                                        & \\textbf{416} & 7            & 5            & 1            & 1            & 4            & 5            & 5            & 6            & 3            & 3            & 5            & 1            & 0            & 6            & 10           & 22           \\\\ \\hline\nOrigenes                                                       & 2            & \\textbf{396} & 0            & 1            & 6            & 4            & 5            & 12           & 1            & 3            & 0            & 24           & 0            & 0            & 6            & 5            & 35           \\\\ \\hline\nPlutarchus                                                     & 6            & 3            & \\textbf{390} & 3            & 9            & 8            & 17           & 2            & 2            & 5            & 2            & 5            & 12           & 1            & 6            & 13           & 16           \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Cassius\\\\ Dio\\end{tabular}          & 1            & 0            & 8            & \\textbf{428} & 5            & 2            & 2            & 0            & 7            & 8            & 2            & 1            & 17           & 6            & 0            & 7            & 6            \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Flavius\\\\ Josephus\\end{tabular}     & 3            & 3            & 10           & 5            & \\textbf{418} & 2            & 4            & 6            & 8            & 9            & 6            & 1            & 8            & 0            & 4            & 9            & 4            \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Philo\\\\ Judaeus\\end{tabular}        & 5            & 10           & 13           & 3            & 16           & \\textbf{403} & 3            & 3            & 2            & 3            & 3            & 12           & 0            & 0            & 11           & 8            & 5            \\\\ \\hline\nAthenaeus                                                      & 11           & 6            & 17           & 4            & 4            & 2            & \\textbf{368} & 4            & 7            & 11           & 9            & 7            & 2            & 6            & 6            & 14           & 22           \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Claudius \\\\ Ptolemaeus\\end{tabular} & 3            & 0            & 0            & 0            & 0            & 1            & 0            & \\textbf{480} & 0            & 8            & 0            & 0            & 0            & 0            & 5            & 0            & 3            \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Aelius\\\\ Aristides\\end{tabular}     & 7            & 6            & 6            & 6            & 7            & 2            & 5            & 0            & \\textbf{368} & 8            & 10           & 6            & 1            & 3            & 3            & 40           & 22           \\\\ \\hline\nStrabo                                                         & 4            & 5            & 9            & 0            & 3            & 2            & 7            & 1            & 9            & \\textbf{432} & 4            & 1            & 3            & 6            & 4            & 4            & 6            \\\\ \\hline\nLucianus                                                       & 2            & 3            & 6            & 1            & 5            & 4            & 9            & 0            & 13           & 9            & \\textbf{360} & 12           & 5            & 6            & 6            & 30           & 29           \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Clemens\\\\ Alexandrinus\\end{tabular} & 8            & 28           & 3            & 4            & 10           & 14           & 4            & 1            & 6            & 5            & 8            & \\textbf{349} & 0            & 5            & 17           & 11           & 27           \\\\ \\hline\nAppianus                                                       & 1            & 0            & 10           & 18           & 8            & 2            & 2            & 1            & 3            & 2            & 5            & 3            & \\textbf{437} & 0            & 0            & 3            & 5            \\\\ \\hline\nPausanias                                                      & 0            & 1            & 1            & 2            & 0            & 0            & 2            & 0            & 4            & 3            & 2            & 3            & 2            & \\textbf{472} & 0            & 3            & 5            \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Sextus\\\\ Empiricus\\end{tabular}     & 2            & 4            & 6            & 0            & 1            & 1            & 4            & 1            & 2            & 2            & 1            & 11           & 0            & 0            & \\textbf{446} & 7            & 12           \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Dio\\\\ Chrysostomus\\end{tabular}     & 2            & 4            & 12           & 9            & 5            & 3            & 7            & 0            & 9            & 10           & 10           & 9            & 6            & 4            & 2            & \\textbf{398} & 10           \\\\ \\hline\nother                                                          & 17           & 23           & 22           & 7            & 6            & 15           & 32           & 14           & 10           & 6            & 12           & 18           & 6            & 9            & 40           & 21           & \\textbf{242} \\\\ \\hline\n\\end{tabular}}\n\\caption{The confusion matrix of the obtained authorship classifier. Every horizontal line sums up to 500 sentences by the corresponding author that were set aside for validation. Every column shows the number of sentences labelled by classifier as sentences authored by the corresponding author.}\n\\label{tab:ac}\n\\end{table*}", "table_label": "{tab:ac}", "table_numeric_cells": [["416", "\\textbf{416}", 493, 496, 485, 497], ["7", "7", 500, 501, 500, 501], ["5", "5", 515, 516, 515, 516], ["1", "1", 530, 531, 530, 531], ["1", "1", 545, 546, 545, 546], ["4", "4", 560, 561, 560, 561], ["5", "5", 575, 576, 575, 576], ["5", "5", 590, 591, 590, 591], ["6", "6", 605, 606, 605, 606], ["3", "3", 620, 621, 620, 621], ["3", "3", 635, 636, 635, 636], ["5", "5", 650, 651, 650, 651], ["1", "1", 665, 666, 665, 666], ["0", "0", 680, 681, 680, 681], ["6", "6", 695, 696, 695, 696], ["10", "10", 710, 712, 710, 712], ["22", "22", 725, 727, 725, 727], ["2", "2", 813, 814, 813, 814], ["396", "\\textbf{396}", 836, 839, 828, 840], ["0", "0", 843, 844, 843, 844], ["1", "1", 858, 859, 858, 859], ["6", "6", 873, 874, 873, 874], ["4", "4", 888, 889, 888, 889], ["5", "5", 903, 904, 903, 904], ["12", "12", 918, 920, 918, 920], ["1", "1", 933, 934, 933, 934], ["3", "3", 948, 949, 948, 949], ["0", "0", 963, 964, 963, 964], ["24", "24", 978, 980, 978, 980], ["0", "0", 993, 994, 993, 994], ["0", "0", 1008, 1009, 1008, 1009], ["6", "6", 1023, 1024, 1023, 1024], ["5", "5", 1038, 1039, 1038, 1039], ["35", "35", 1053, 1055, 1053, 1055], ["6", "6", 1141, 1142, 1141, 1142], ["3", "3", 1156, 1157, 1156, 1157], ["390", "\\textbf{390}", 1179, 1182, 1171, 1183], ["3", "3", 1186, 1187, 1186, 1187], ["9", "9", 1201, 1202, 1201, 1202], ["8", "8", 1216, 1217, 1216, 1217], ["17", "17", 1231, 1233, 1231, 1233], ["2", "2", 1246, 1247, 1246, 1247], ["2", "2", 1261, 1262, 1261, 1262], ["5", "5", 1276, 1277, 1276, 1277], ["2", "2", 1291, 1292, 1291, 1292], ["5", "5", 1306, 1307, 1306, 1307], ["12", "12", 1321, 1323, 1321, 1323], ["1", "1", 1336, 1337, 1336, 1337], ["6", "6", 1351, 1352, 1351, 1352], ["13", "13", 1366, 1368, 1366, 1368], ["16", "16", 1381, 1383, 1381, 1383], ["1", "1", 1469, 1470, 1469, 1470], ["0", "0", 1484, 1485, 1484, 1485], ["8", "8", 1499, 1500, 1499, 1500], ["428", "\\textbf{428}", 1522, 1525, 1514, 1526], ["5", "5", 1529, 1530, 1529, 1530], ["2", "2", 1544, 1545, 1544, 1545], ["2", "2", 1559, 1560, 1559, 1560], ["0", "0", 1574, 1575, 1574, 1575], ["7", "7", 1589, 1590, 1589, 1590], ["8", "8", 1604, 1605, 1604, 1605], ["2", "2", 1619, 1620, 1619, 1620], ["1", "1", 1634, 1635, 1634, 1635], ["17", "17", 1649, 1651, 1649, 1651], ["6", "6", 1664, 1665, 1664, 1665], ["0", "0", 1679, 1680, 1679, 1680], ["7", "7", 1694, 1695, 1694, 1695], ["6", "6", 1709, 1710, 1709, 1710], ["3", "3", 1797, 1798, 1797, 1798], ["3", "3", 1812, 1813, 1812, 1813], ["10", "10", 1827, 1829, 1827, 1829], ["5", "5", 1842, 1843, 1842, 1843], ["418", "\\textbf{418}", 1865, 1868, 1857, 1869], ["2", "2", 1872, 1873, 1872, 1873], ["4", "4", 1887, 1888, 1887, 1888], ["6", "6", 1902, 1903, 1902, 1903], ["8", "8", 1917, 1918, 1917, 1918], ["9", "9", 1932, 1933, 1932, 1933], ["6", "6", 1947, 1948, 1947, 1948], ["1", "1", 1962, 1963, 1962, 1963], ["8", "8", 1977, 1978, 1977, 1978], ["0", "0", 1992, 1993, 1992, 1993], ["4", "4", 2007, 2008, 2007, 2008], ["9", "9", 2022, 2023, 2022, 2023], ["4", "4", 2037, 2038, 2037, 2038], ["5", "5", 2125, 2126, 2125, 2126], ["10", "10", 2140, 2142, 2140, 2142], ["13", "13", 2155, 2157, 2155, 2157], ["3", "3", 2170, 2171, 2170, 2171], ["16", "16", 2185, 2187, 2185, 2187], ["403", "\\textbf{403}", 2208, 2211, 2200, 2212], ["3", "3", 2215, 2216, 2215, 2216], ["3", "3", 2230, 2231, 2230, 2231], ["2", "2", 2245, 2246, 2245, 2246], ["3", "3", 2260, 2261, 2260, 2261], ["3", "3", 2275, 2276, 2275, 2276], ["12", "12", 2290, 2292, 2290, 2292], ["0", "0", 2305, 2306, 2305, 2306], ["0", "0", 2320, 2321, 2320, 2321], ["11", "11", 2335, 2337, 2335, 2337], ["8", "8", 2350, 2351, 2350, 2351], ["5", "5", 2365, 2366, 2365, 2366], ["11", "11", 2453, 2455, 2453, 2455], ["6", "6", 2468, 2469, 2468, 2469], ["17", "17", 2483, 2485, 2483, 2485], ["4", "4", 2498, 2499, 2498, 2499], ["4", "4", 2513, 2514, 2513, 2514], ["2", "2", 2528, 2529, 2528, 2529], ["368", "\\textbf{368}", 2551, 2554, 2543, 2555], ["4", "4", 2558, 2559, 2558, 2559], ["7", "7", 2573, 2574, 2573, 2574], ["11", "11", 2588, 2590, 2588, 2590], ["9", "9", 2603, 2604, 2603, 2604], ["7", "7", 2618, 2619, 2618, 2619], ["2", "2", 2633, 2634, 2633, 2634], ["6", "6", 2648, 2649, 2648, 2649], ["6", "6", 2663, 2664, 2663, 2664], ["14", "14", 2678, 2680, 2678, 2680], ["22", "22", 2693, 2695, 2693, 2695], ["3", "3", 2781, 2782, 2781, 2782], ["0", "0", 2796, 2797, 2796, 2797], ["0", "0", 2811, 2812, 2811, 2812], ["0", "0", 2826, 2827, 2826, 2827], ["0", "0", 2841, 2842, 2841, 2842], ["1", "1", 2856, 2857, 2856, 2857], ["0", "0", 2871, 2872, 2871, 2872], ["480", "\\textbf{480}", 2894, 2897, 2886, 2898], ["0", "0", 2901, 2902, 2901, 2902], ["8", "8", 2916, 2917, 2916, 2917], ["0", "0", 2931, 2932, 2931, 2932], ["0", "0", 2946, 2947, 2946, 2947], ["0", "0", 2961, 2962, 2961, 2962], ["0", "0", 2976, 2977, 2976, 2977], ["5", "5", 2991, 2992, 2991, 2992], ["0", "0", 3006, 3007, 3006, 3007], ["3", "3", 3021, 3022, 3021, 3022], ["7", "7", 3109, 3110, 3109, 3110], ["6", "6", 3124, 3125, 3124, 3125], ["6", "6", 3139, 3140, 3139, 3140], ["6", "6", 3154, 3155, 3154, 3155], ["7", "7", 3169, 3170, 3169, 3170], ["2", "2", 3184, 3185, 3184, 3185], ["5", "5", 3199, 3200, 3199, 3200], ["0", "0", 3214, 3215, 3214, 3215], ["368", "\\textbf{368}", 3237, 3240, 3229, 3241], ["8", "8", 3244, 3245, 3244, 3245], ["10", "10", 3259, 3261, 3259, 3261], ["6", "6", 3274, 3275, 3274, 3275], ["1", "1", 3289, 3290, 3289, 3290], ["3", "3", 3304, 3305, 3304, 3305], ["3", "3", 3319, 3320, 3319, 3320], ["40", "40", 3334, 3336, 3334, 3336], ["22", "22", 3349, 3351, 3349, 3351], ["4", "4", 3437, 3438, 3437, 3438], ["5", "5", 3452, 3453, 3452, 3453], ["9", "9", 3467, 3468, 3467, 3468], ["0", "0", 3482, 3483, 3482, 3483], ["3", "3", 3497, 3498, 3497, 3498], ["2", "2", 3512, 3513, 3512, 3513], ["7", "7", 3527, 3528, 3527, 3528], ["1", "1", 3542, 3543, 3542, 3543], ["9", "9", 3557, 3558, 3557, 3558], ["432", "\\textbf{432}", 3580, 3583, 3572, 3584], ["4", "4", 3587, 3588, 3587, 3588], ["1", "1", 3602, 3603, 3602, 3603], ["3", "3", 3617, 3618, 3617, 3618], ["6", "6", 3632, 3633, 3632, 3633], ["4", "4", 3647, 3648, 3647, 3648], ["4", "4", 3662, 3663, 3662, 3663], ["6", "6", 3677, 3678, 3677, 3678], ["2", "2", 3765, 3766, 3765, 3766], ["3", "3", 3780, 3781, 3780, 3781], ["6", "6", 3795, 3796, 3795, 3796], ["1", "1", 3810, 3811, 3810, 3811], ["5", "5", 3825, 3826, 3825, 3826], ["4", "4", 3840, 3841, 3840, 3841], ["9", "9", 3855, 3856, 3855, 3856], ["0", "0", 3870, 3871, 3870, 3871], ["13", "13", 3885, 3887, 3885, 3887], ["9", "9", 3900, 3901, 3900, 3901], ["360", "\\textbf{360}", 3923, 3926, 3915, 3927], ["12", "12", 3930, 3932, 3930, 3932], ["5", "5", 3945, 3946, 3945, 3946], ["6", "6", 3960, 3961, 3960, 3961], ["6", "6", 3975, 3976, 3975, 3976], ["30", "30", 3990, 3992, 3990, 3992], ["29", "29", 4005, 4007, 4005, 4007], ["8", "8", 4093, 4094, 4093, 4094], ["28", "28", 4108, 4110, 4108, 4110], ["3", "3", 4123, 4124, 4123, 4124], ["4", "4", 4138, 4139, 4138, 4139], ["10", "10", 4153, 4155, 4153, 4155], ["14", "14", 4168, 4170, 4168, 4170], ["4", "4", 4183, 4184, 4183, 4184], ["1", "1", 4198, 4199, 4198, 4199], ["6", "6", 4213, 4214, 4213, 4214], ["5", "5", 4228, 4229, 4228, 4229], ["8", "8", 4243, 4244, 4243, 4244], ["349", "\\textbf{349}", 4266, 4269, 4258, 4270], ["0", "0", 4273, 4274, 4273, 4274], ["5", "5", 4288, 4289, 4288, 4289], ["17", "17", 4303, 4305, 4303, 4305], ["11", "11", 4318, 4320, 4318, 4320], ["27", "27", 4333, 4335, 4333, 4335], ["1", "1", 4421, 4422, 4421, 4422], ["0", "0", 4436, 4437, 4436, 4437], ["10", "10", 4451, 4453, 4451, 4453], ["18", "18", 4466, 4468, 4466, 4468], ["8", "8", 4481, 4482, 4481, 4482], ["2", "2", 4496, 4497, 4496, 4497], ["2", "2", 4511, 4512, 4511, 4512], ["1", "1", 4526, 4527, 4526, 4527], ["3", "3", 4541, 4542, 4541, 4542], ["2", "2", 4556, 4557, 4556, 4557], ["5", "5", 4571, 4572, 4571, 4572], ["3", "3", 4586, 4587, 4586, 4587], ["437", "\\textbf{437}", 4609, 4612, 4601, 4613], ["0", "0", 4616, 4617, 4616, 4617], ["0", "0", 4631, 4632, 4631, 4632], ["3", "3", 4646, 4647, 4646, 4647], ["5", "5", 4661, 4662, 4661, 4662], ["0", "0", 4749, 4750, 4749, 4750], ["1", "1", 4764, 4765, 4764, 4765], ["1", "1", 4779, 4780, 4779, 4780], ["2", "2", 4794, 4795, 4794, 4795], ["0", "0", 4809, 4810, 4809, 4810], ["0", "0", 4824, 4825, 4824, 4825], ["2", "2", 4839, 4840, 4839, 4840], ["0", "0", 4854, 4855, 4854, 4855], ["4", "4", 4869, 4870, 4869, 4870], ["3", "3", 4884, 4885, 4884, 4885], ["2", "2", 4899, 4900, 4899, 4900], ["3", "3", 4914, 4915, 4914, 4915], ["2", "2", 4929, 4930, 4929, 4930], ["472", "\\textbf{472}", 4952, 4955, 4944, 4956], ["0", "0", 4959, 4960, 4959, 4960], ["3", "3", 4974, 4975, 4974, 4975], ["5", "5", 4989, 4990, 4989, 4990], ["2", "2", 5077, 5078, 5077, 5078], ["4", "4", 5092, 5093, 5092, 5093], ["6", "6", 5107, 5108, 5107, 5108], ["0", "0", 5122, 5123, 5122, 5123], ["1", "1", 5137, 5138, 5137, 5138], ["1", "1", 5152, 5153, 5152, 5153], ["4", "4", 5167, 5168, 5167, 5168], ["1", "1", 5182, 5183, 5182, 5183], ["2", "2", 5197, 5198, 5197, 5198], ["2", "2", 5212, 5213, 5212, 5213], ["1", "1", 5227, 5228, 5227, 5228], ["11", "11", 5242, 5244, 5242, 5244], ["0", "0", 5257, 5258, 5257, 5258], ["0", "0", 5272, 5273, 5272, 5273], ["446", "\\textbf{446}", 5295, 5298, 5287, 5299], ["7", "7", 5302, 5303, 5302, 5303], ["12", "12", 5317, 5319, 5317, 5319], ["2", "2", 5405, 5406, 5405, 5406], ["4", "4", 5420, 5421, 5420, 5421], ["12", "12", 5435, 5437, 5435, 5437], ["9", "9", 5450, 5451, 5450, 5451], ["5", "5", 5465, 5466, 5465, 5466], ["3", "3", 5480, 5481, 5480, 5481], ["7", "7", 5495, 5496, 5495, 5496], ["0", "0", 5510, 5511, 5510, 5511], ["9", "9", 5525, 5526, 5525, 5526], ["10", "10", 5540, 5542, 5540, 5542], ["10", "10", 5555, 5557, 5555, 5557], ["9", "9", 5570, 5571, 5570, 5571], ["6", "6", 5585, 5586, 5585, 5586], ["4", "4", 5600, 5601, 5600, 5601], ["2", "2", 5615, 5616, 5615, 5616], ["398", "\\textbf{398}", 5638, 5641, 5630, 5642], ["10", "10", 5645, 5647, 5645, 5647], ["17", "17", 5733, 5735, 5733, 5735], ["23", "23", 5748, 5750, 5748, 5750], ["22", "22", 5763, 5765, 5763, 5765], ["7", "7", 5778, 5779, 5778, 5779], ["6", "6", 5793, 5794, 5793, 5794], ["15", "15", 5808, 5810, 5808, 5810], ["32", "32", 5823, 5825, 5823, 5825], ["14", "14", 5838, 5840, 5838, 5840], ["10", "10", 5853, 5855, 5853, 5855], ["6", "6", 5868, 5869, 5868, 5869], ["12", "12", 5883, 5885, 5883, 5885], ["18", "18", 5898, 5900, 5898, 5900], ["6", "6", 5913, 5914, 5913, 5914], ["9", "9", 5928, 5929, 5928, 5929], ["40", "40", 5943, 5945, 5943, 5945], ["21", "21", 5958, 5960, 5958, 5960], ["242", "\\textbf{242}", 5981, 5984, 5973, 5985]], "text_chunk_selected": "\\begin{itemize}\n    \\item we use a transfer learning approach to train BERT for Ancient Greek;\n    \\item we demonstrate that this language model is useful for authorship attribution of Ancient Greek texts;\n    \\item we obtain results that may be used as evidence in the process of authorship attribution of the Pseudo-Plutarchean texts;\n    \\item we obtain new insights into the paths along which the reception of ancient philosophy was developed.\n\\end{itemize}\n\n\\section{Data}\nThe data were taken from the digital history projects at the Chair of\nAncient History of the University of Leipzig. The Plutarch texts were transformed into a digital representation under professional supervision. Data can be obtained from the following free repositories Perseus Digital Library\\footnote{https://github.com/PerseusDL/canonical-greekLit} and First Thousand Years of Greek\\footnote{https://github.com/ThomasK81/TEItoCEX} as part of Open Greek and Latin\\footnote{https://opengreekandlatin.org}. The resulting representation was stored in XML format (TEI guidelines) and enriched with metadata. The XML structure and metadata were removed. The strings were transferred into lowercase letters. The diacritics were removed. We did not touch hyphenation, punctuation, or any multilingual remains, nor did we apply any special language-related transformations.\nThe resulting data set consists of 1 244 documents with 199 809 paragraphs or 14 373 311 words.\n\n\\subsection{Tokenizers}\nThe tokenization of words into sub-word tokens is a crucial preprocessing step that can affect the performance of the model. Up to this point, we were using \"words\" as a linguistic term, whereas we understand tokens as the output of a tokenization algorithm. Thus, there would be one or more tokens that represent every word. Counting the number of tokens used on average to represent a word or, conversely, an average number of words per token gives an estimate of how fit  the tokenization is for the data set. In particular, the average number of words per token varies from 0 to 1, and the closer it is to 1, the more words are represented with one token. Various researchers have shown that corpus-specific tokenization could be beneficial for an NLP task. For example, \\citet{sennrich-etal-2016-neural} show that optimal vocabulary is dependent on the frequencies of the words in the target corpus. \\citet{lakew2019controlling} and \\citet{aji2020neural} partially discuss the tokenization in the setting of cross-language transfer. Though \\citet{aji2020neural} demonstrate that there is no clear evidence that one parent is better than another for cross-lingual transfer learning, they also show that token matching and joint vocabulary \\cite{nguyen2017transfer} are the best ways to handle the embedding layer during transfer learning. \n\n\\subsection{Training Ancient Greek BERT via Transfer Learning}\nIf related tasks are available, we can fine-tune the model first on a related task with more data before fine-tuning it on the target task, see \\cite{ruder2019transfer}. This helps particularly for tasks with limited data \\cite{phang2018sentence} and improves sample efficiency on the target task \\cite{yogatama2019learning}. Since we have a limited amount of Ancient Greek texts, we want to do language transfer from Modern Greek to Ancient Greek training a masked language\nmodel (MLM)\\footnote{https://github.com/huggingface/transformers/blob/master/ examples/language-modeling/run\\_language\\_modeling.py} of the Ancient Greek text. \n\nFrom Dio Chrysostomus we had only 5580 sentences available for training, but this amount of data could be sufficient \\cite{zhang2020revisiting} to train a well-performing BERT-based classifier. For the author classifier, to avoid bias from different sentences,  we sample 5 580 sentences from every author in this list. We also include the  label \"Others\" for 5 580\nrandom sentences by less prolific authors who were not included in this shortlist. The choice of the number of authors balances the requirement to have enough data to train the classifier and the wish to include as many authors in the authorship attribution classifier as possible. \n\nWe use this data set to train BERT Classifiers similarly to \\cite{fabien2020bertaa}. Table \\ref{tab:au} shows the validation accuracy for Modern Greek and Multilingual BERTs after MLM on Ancient Greek and 10 epochs of classifier training\\footnote{AdamW, LR = 2e-5, eps = 1e-8,  linear\\_schedule}. Table \\ref{tab:au}  also shows a standard NLTK Naive Bayes Classifier trained on the 2 000 most frequent unigrams as a reference point for authorship attribution accuracy.\n\nSince Modern Greek BERT fine-tuned for authorship attribution after MLM shows the best validation accuracy, we use it for the subsequent analysis. Table \\ref{tab:ac} shows the confusion matrix of the resulting classifier on 500 validation sentences by every author.\n\nTable \\ref{tab:ac} shows that the authorship attribution model works rather well. The errors mostly happen in sentences that have a topical affinity to another author or on authors with similar regional backgrounds. That observation suggests developing a separate regional classifier that might help authorship attribution. This classifier is described in detail in the next Subsection.", "table_source": "\\begin{table*}[h]\n\\centering\n{\n\\small\n\\tabcolsep=0.11cm\n\\begin{tabular}{lrrrrrrrrrrrrrrrrr}\n                                                               & G            & O            & P            & CD           & FJ           & PJ           & A            & CP           & AA           & S            & L            & CA           & Ap           & P            & SE           & DC           & other        \\\\ \\hline\nGalenus                                                        & \\textbf{416} & 7            & 5            & 1            & 1            & 4            & 5            & 5            & 6            & 3            & 3            & 5            & 1            & 0            & 6            & 10           & 22           \\\\ \\hline\nOrigenes                                                       & 2            & \\textbf{396} & 0            & 1            & 6            & 4            & 5            & 12           & 1            & 3            & 0            & 24           & 0            & 0            & 6            & 5            & 35           \\\\ \\hline\nPlutarchus                                                     & 6            & 3            & \\textbf{390} & 3            & 9            & 8            & 17           & 2            & 2            & 5            & 2            & 5            & 12           & 1            & 6            & 13           & 16           \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Cassius\\\\ Dio\\end{tabular}          & 1            & 0            & 8            & \\textbf{428} & 5            & 2            & 2            & 0            & 7            & 8            & 2            & 1            & 17           & 6            & 0            & 7            & 6            \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Flavius\\\\ Josephus\\end{tabular}     & 3            & 3            & 10           & 5            & \\textbf{418} & 2            & 4            & 6            & 8            & 9            & 6            & 1            & 8            & 0            & 4            & 9            & 4            \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Philo\\\\ Judaeus\\end{tabular}        & 5            & 10           & 13           & 3            & 16           & \\textbf{403} & 3            & 3            & 2            & 3            & 3            & 12           & 0            & 0            & 11           & 8            & 5            \\\\ \\hline\nAthenaeus                                                      & 11           & 6            & 17           & 4            & 4            & 2            & \\textbf{368} & 4            & 7            & 11           & 9            & 7            & 2            & 6            & 6            & 14           & 22           \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Claudius \\\\ Ptolemaeus\\end{tabular} & 3            & 0            & 0            & 0            & 0            & 1            & 0            & \\textbf{480} & 0            & 8            & 0            & 0            & 0            & 0            & 5            & 0            & 3            \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Aelius\\\\ Aristides\\end{tabular}     & 7            & 6            & 6            & 6            & 7            & 2            & 5            & 0            & \\textbf{368} & 8            & 10           & 6            & 1            & 3            & 3            & 40           & 22           \\\\ \\hline\nStrabo                                                         & 4            & 5            & 9            & 0            & 3            & 2            & 7            & 1            & 9            & \\textbf{432} & 4            & 1            & 3            & 6            & 4            & 4            & 6            \\\\ \\hline\nLucianus                                                       & 2            & 3            & 6            & 1            & 5            & 4            & 9            & 0            & 13           & 9            & \\textbf{360} & 12           & 5            & 6            & 6            & 30           & 29           \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Clemens\\\\ Alexandrinus\\end{tabular} & 8            & 28           & 3            & 4            & 10           & 14           & 4            & 1            & 6            & 5            & 8            & \\textbf{349} & 0            & 5            & 17           & 11           & 27           \\\\ \\hline\nAppianus                                                       & 1            & 0            & 10           & 18           & 8            & 2            & 2            & 1            & 3            & 2            & 5            & 3            & \\textbf{437} & 0            & 0            & 3            & 5            \\\\ \\hline\nPausanias                                                      & 0            & 1            & 1            & 2            & 0            & 0            & 2            & 0            & 4            & 3            & 2            & 3            & 2            & \\textbf{472} & 0            & 3            & 5            \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Sextus\\\\ Empiricus\\end{tabular}     & 2            & 4            & 6            & 0            & 1            & 1            & 4            & 1            & 2            & 2            & 1            & 11           & 0            & 0            & \\textbf{446} & 7            & 12           \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Dio\\\\ Chrysostomus\\end{tabular}     & 2            & 4            & 12           & 9            & 5            & 3            & 7            & 0            & 9            & 10           & 10           & 9            & 6            & 4            & 2            & \\textbf{398} & 10           \\\\ \\hline\nother                                                          & 17           & 23           & 22           & 7            & 6            & 15           & 32           & 14           & 10           & 6            & 12           & 18           & 6            & 9            & 40           & 21           & \\textbf{242} \\\\ \\hline\n\\end{tabular}}\n\\caption{The confusion matrix of the obtained authorship classifier. Every horizontal line sums up to 500 sentences by the corresponding author that were set aside for validation. Every column shows the number of sentences labelled by classifier as sentences authored by the corresponding author.}\n\\label{tab:ac}\n\\end{table*}", "cell_list_gold": [{"value": "416", "char_index": [493, 496], "type": "Other"}, {"value": "7", "char_index": [500, 501], "type": "Other"}, {"value": "5", "char_index": [515, 516], "type": "Other"}, {"value": "1", "char_index": [530, 531], "type": "Other"}, {"value": "1", "char_index": [545, 546], "type": "Other"}, {"value": "4", "char_index": [560, 561], "type": "Other"}, {"value": "5", "char_index": [575, 576], "type": "Other"}, {"value": "5", "char_index": [590, 591], "type": "Other"}, {"value": "6", "char_index": [605, 606], "type": "Other"}, {"value": "3", "char_index": [620, 621], "type": "Other"}, {"value": "3", "char_index": [635, 636], "type": "Other"}, {"value": "5", "char_index": [650, 651], "type": "Other"}, {"value": "1", "char_index": [665, 666], "type": "Other"}, {"value": "0", "char_index": [680, 681], "type": "Other"}, {"value": "6", "char_index": [695, 696], "type": "Other"}, {"value": "10", "char_index": [710, 712], "type": "Other"}, {"value": "22", "char_index": [725, 727], "type": "Other"}, {"value": "2", "char_index": [813, 814], "type": "Other"}, {"value": "396", "char_index": [836, 839], "type": "Other"}, {"value": "0", "char_index": [843, 844], "type": "Other"}, {"value": "1", "char_index": [858, 859], "type": "Other"}, {"value": "6", "char_index": [873, 874], "type": "Other"}, {"value": "4", "char_index": [888, 889], "type": "Other"}, {"value": "5", "char_index": [903, 904], "type": "Other"}, {"value": "12", "char_index": [918, 920], "type": "Other"}, {"value": "1", "char_index": [933, 934], "type": "Other"}, {"value": "3", "char_index": [948, 949], "type": "Other"}, {"value": "0", "char_index": [963, 964], "type": "Other"}, {"value": "24", "char_index": [978, 980], "type": "Other"}, {"value": "0", "char_index": [993, 994], "type": "Other"}, {"value": "0", "char_index": [1008, 1009], "type": "Other"}, {"value": "6", "char_index": [1023, 1024], "type": "Other"}, {"value": "5", "char_index": [1038, 1039], "type": "Other"}, {"value": "35", "char_index": [1053, 1055], "type": "Other"}, {"value": "6", "char_index": [1141, 1142], "type": "Other"}, {"value": "3", "char_index": [1156, 1157], "type": "Other"}, {"value": "390", "char_index": [1179, 1182], "type": "Other"}, {"value": "3", "char_index": [1186, 1187], "type": "Other"}, {"value": "9", "char_index": [1201, 1202], "type": "Other"}, {"value": "8", "char_index": [1216, 1217], "type": "Other"}, {"value": "17", "char_index": [1231, 1233], "type": "Other"}, {"value": "2", "char_index": [1246, 1247], "type": "Other"}, {"value": "2", "char_index": [1261, 1262], "type": "Other"}, {"value": "5", "char_index": [1276, 1277], "type": "Other"}, {"value": "2", "char_index": [1291, 1292], "type": "Other"}, {"value": "5", "char_index": [1306, 1307], "type": "Other"}, {"value": "12", "char_index": [1321, 1323], "type": "Other"}, {"value": "1", "char_index": [1336, 1337], "type": "Other"}, {"value": "6", "char_index": [1351, 1352], "type": "Other"}, {"value": "13", "char_index": [1366, 1368], "type": "Other"}, {"value": "16", "char_index": [1381, 1383], "type": "Other"}, {"value": "1", "char_index": [1469, 1470], "type": "Other"}, {"value": "0", "char_index": [1484, 1485], "type": "Other"}, {"value": "8", "char_index": [1499, 1500], "type": "Other"}, {"value": "428", "char_index": [1522, 1525], "type": "Other"}, {"value": "5", "char_index": [1529, 1530], "type": "Other"}, {"value": "2", "char_index": [1544, 1545], "type": "Other"}, {"value": "2", "char_index": [1559, 1560], "type": "Other"}, {"value": "0", "char_index": [1574, 1575], "type": "Other"}, {"value": "7", "char_index": [1589, 1590], "type": "Other"}, {"value": "8", "char_index": [1604, 1605], "type": "Other"}, {"value": "2", "char_index": [1619, 1620], "type": "Other"}, {"value": "1", "char_index": [1634, 1635], "type": "Other"}, {"value": "17", "char_index": [1649, 1651], "type": "Other"}, {"value": "6", "char_index": [1664, 1665], "type": "Other"}, {"value": "0", "char_index": [1679, 1680], "type": "Other"}, {"value": "7", "char_index": [1694, 1695], "type": "Other"}, {"value": "6", "char_index": [1709, 1710], "type": "Other"}, {"value": "3", "char_index": [1797, 1798], "type": "Other"}, {"value": "3", "char_index": [1812, 1813], "type": "Other"}, {"value": "10", "char_index": [1827, 1829], "type": "Other"}, {"value": "5", "char_index": [1842, 1843], "type": "Other"}, {"value": "418", "char_index": [1865, 1868], "type": "Other"}, {"value": "2", "char_index": [1872, 1873], "type": "Other"}, {"value": "4", "char_index": [1887, 1888], "type": "Other"}, {"value": "6", "char_index": [1902, 1903], "type": "Other"}, {"value": "8", "char_index": [1917, 1918], "type": "Other"}, {"value": "9", "char_index": [1932, 1933], "type": "Other"}, {"value": "6", "char_index": [1947, 1948], "type": "Other"}, {"value": "1", "char_index": [1962, 1963], "type": "Other"}, {"value": "8", "char_index": [1977, 1978], "type": "Other"}, {"value": "0", "char_index": [1992, 1993], "type": "Other"}, {"value": "4", "char_index": [2007, 2008], "type": "Other"}, {"value": "9", "char_index": [2022, 2023], "type": "Other"}, {"value": "4", "char_index": [2037, 2038], "type": "Other"}, {"value": "5", "char_index": [2125, 2126], "type": "Other"}, {"value": "10", "char_index": [2140, 2142], "type": "Other"}, {"value": "13", "char_index": [2155, 2157], "type": "Other"}, {"value": "3", "char_index": [2170, 2171], "type": "Other"}, {"value": "16", "char_index": [2185, 2187], "type": "Other"}, {"value": "403", "char_index": [2208, 2211], "type": "Other"}, {"value": "3", "char_index": [2215, 2216], "type": "Other"}, {"value": "3", "char_index": [2230, 2231], "type": "Other"}, {"value": "2", "char_index": [2245, 2246], "type": "Other"}, {"value": "3", "char_index": [2260, 2261], "type": "Other"}, {"value": "3", "char_index": [2275, 2276], "type": "Other"}, {"value": "12", "char_index": [2290, 2292], "type": "Other"}, {"value": "0", "char_index": [2305, 2306], "type": "Other"}, {"value": "0", "char_index": [2320, 2321], "type": "Other"}, {"value": "11", "char_index": [2335, 2337], "type": "Other"}, {"value": "8", "char_index": [2350, 2351], "type": "Other"}, {"value": "5", "char_index": [2365, 2366], "type": "Other"}, {"value": "11", "char_index": [2453, 2455], "type": "Other"}, {"value": "6", "char_index": [2468, 2469], "type": "Other"}, {"value": "17", "char_index": [2483, 2485], "type": "Other"}, {"value": "4", "char_index": [2498, 2499], "type": "Other"}, {"value": "4", "char_index": [2513, 2514], "type": "Other"}, {"value": "2", "char_index": [2528, 2529], "type": "Other"}, {"value": "368", "char_index": [2551, 2554], "type": "Other"}, {"value": "4", "char_index": [2558, 2559], "type": "Other"}, {"value": "7", "char_index": [2573, 2574], "type": "Other"}, {"value": "11", "char_index": [2588, 2590], "type": "Other"}, {"value": "9", "char_index": [2603, 2604], "type": "Other"}, {"value": "7", "char_index": [2618, 2619], "type": "Other"}, {"value": "2", "char_index": [2633, 2634], "type": "Other"}, {"value": "6", "char_index": [2648, 2649], "type": "Other"}, {"value": "6", "char_index": [2663, 2664], "type": "Other"}, {"value": "14", "char_index": [2678, 2680], "type": "Other"}, {"value": "22", "char_index": [2693, 2695], "type": "Other"}, {"value": "3", "char_index": [2781, 2782], "type": "Other"}, {"value": "0", "char_index": [2796, 2797], "type": "Other"}, {"value": "0", "char_index": [2811, 2812], "type": "Other"}, {"value": "0", "char_index": [2826, 2827], "type": "Other"}, {"value": "0", "char_index": [2841, 2842], "type": "Other"}, {"value": "1", "char_index": [2856, 2857], "type": "Other"}, {"value": "0", "char_index": [2871, 2872], "type": "Other"}, {"value": "480", "char_index": [2894, 2897], "type": "Other"}, {"value": "0", "char_index": [2901, 2902], "type": "Other"}, {"value": "8", "char_index": [2916, 2917], "type": "Other"}, {"value": "0", "char_index": [2931, 2932], "type": "Other"}, {"value": "0", "char_index": [2946, 2947], "type": "Other"}, {"value": "0", "char_index": [2961, 2962], "type": "Other"}, {"value": "0", "char_index": [2976, 2977], "type": "Other"}, {"value": "5", "char_index": [2991, 2992], "type": "Other"}, {"value": "0", "char_index": [3006, 3007], "type": "Other"}, {"value": "3", "char_index": [3021, 3022], "type": "Other"}, {"value": "7", "char_index": [3109, 3110], "type": "Other"}, {"value": "6", "char_index": [3124, 3125], "type": "Other"}, {"value": "6", "char_index": [3139, 3140], "type": "Other"}, {"value": "6", "char_index": [3154, 3155], "type": "Other"}, {"value": "7", "char_index": [3169, 3170], "type": "Other"}, {"value": "2", "char_index": [3184, 3185], "type": "Other"}, {"value": "5", "char_index": [3199, 3200], "type": "Other"}, {"value": "0", "char_index": [3214, 3215], "type": "Other"}, {"value": "368", "char_index": [3237, 3240], "type": "Other"}, {"value": "8", "char_index": [3244, 3245], "type": "Other"}, {"value": "10", "char_index": [3259, 3261], "type": "Other"}, {"value": "6", "char_index": [3274, 3275], "type": "Other"}, {"value": "1", "char_index": [3289, 3290], "type": "Other"}, {"value": "3", "char_index": [3304, 3305], "type": "Other"}, {"value": "3", "char_index": [3319, 3320], "type": "Other"}, {"value": "40", "char_index": [3334, 3336], "type": "Other"}, {"value": "22", "char_index": [3349, 3351], "type": "Other"}, {"value": "4", "char_index": [3437, 3438], "type": "Other"}, {"value": "5", "char_index": [3452, 3453], "type": "Other"}, {"value": "9", "char_index": [3467, 3468], "type": "Other"}, {"value": "0", "char_index": [3482, 3483], "type": "Other"}, {"value": "3", "char_index": [3497, 3498], "type": "Other"}, {"value": "2", "char_index": [3512, 3513], "type": "Other"}, {"value": "7", "char_index": [3527, 3528], "type": "Other"}, {"value": "1", "char_index": [3542, 3543], "type": "Other"}, {"value": "9", "char_index": [3557, 3558], "type": "Other"}, {"value": "432", "char_index": [3580, 3583], "type": "Other"}, {"value": "4", "char_index": [3587, 3588], "type": "Other"}, {"value": "1", "char_index": [3602, 3603], "type": "Other"}, {"value": "3", "char_index": [3617, 3618], "type": "Other"}, {"value": "6", "char_index": [3632, 3633], "type": "Other"}, {"value": "4", "char_index": [3647, 3648], "type": "Other"}, {"value": "4", "char_index": [3662, 3663], "type": "Other"}, {"value": "6", "char_index": [3677, 3678], "type": "Other"}, {"value": "2", "char_index": [3765, 3766], "type": "Other"}, {"value": "3", "char_index": [3780, 3781], "type": "Other"}, {"value": "6", "char_index": [3795, 3796], "type": "Other"}, {"value": "1", "char_index": [3810, 3811], "type": "Other"}, {"value": "5", "char_index": [3825, 3826], "type": "Other"}, {"value": "4", "char_index": [3840, 3841], "type": "Other"}, {"value": "9", "char_index": [3855, 3856], "type": "Other"}, {"value": "0", "char_index": [3870, 3871], "type": "Other"}, {"value": "13", "char_index": [3885, 3887], "type": "Other"}, {"value": "9", "char_index": [3900, 3901], "type": "Other"}, {"value": "360", "char_index": [3923, 3926], "type": "Other"}, {"value": "12", "char_index": [3930, 3932], "type": "Other"}, {"value": "5", "char_index": [3945, 3946], "type": "Other"}, {"value": "6", "char_index": [3960, 3961], "type": "Other"}, {"value": "6", "char_index": [3975, 3976], "type": "Other"}, {"value": "30", "char_index": [3990, 3992], "type": "Other"}, {"value": "29", "char_index": [4005, 4007], "type": "Other"}, {"value": "8", "char_index": [4093, 4094], "type": "Other"}, {"value": "28", "char_index": [4108, 4110], "type": "Other"}, {"value": "3", "char_index": [4123, 4124], "type": "Other"}, {"value": "4", "char_index": [4138, 4139], "type": "Other"}, {"value": "10", "char_index": [4153, 4155], "type": "Other"}, {"value": "14", "char_index": [4168, 4170], "type": "Other"}, {"value": "4", "char_index": [4183, 4184], "type": "Other"}, {"value": "1", "char_index": [4198, 4199], "type": "Other"}, {"value": "6", "char_index": [4213, 4214], "type": "Other"}, {"value": "5", "char_index": [4228, 4229], "type": "Other"}, {"value": "8", "char_index": [4243, 4244], "type": "Other"}, {"value": "349", "char_index": [4266, 4269], "type": "Other"}, {"value": "0", "char_index": [4273, 4274], "type": "Other"}, {"value": "5", "char_index": [4288, 4289], "type": "Other"}, {"value": "17", "char_index": [4303, 4305], "type": "Other"}, {"value": "11", "char_index": [4318, 4320], "type": "Other"}, {"value": "27", "char_index": [4333, 4335], "type": "Other"}, {"value": "1", "char_index": [4421, 4422], "type": "Other"}, {"value": "0", "char_index": [4436, 4437], "type": "Other"}, {"value": "10", "char_index": [4451, 4453], "type": "Other"}, {"value": "18", "char_index": [4466, 4468], "type": "Other"}, {"value": "8", "char_index": [4481, 4482], "type": "Other"}, {"value": "2", "char_index": [4496, 4497], "type": "Other"}, {"value": "2", "char_index": [4511, 4512], "type": "Other"}, {"value": "1", "char_index": [4526, 4527], "type": "Other"}, {"value": "3", "char_index": [4541, 4542], "type": "Other"}, {"value": "2", "char_index": [4556, 4557], "type": "Other"}, {"value": "5", "char_index": [4571, 4572], "type": "Other"}, {"value": "3", "char_index": [4586, 4587], "type": "Other"}, {"value": "437", "char_index": [4609, 4612], "type": "Other"}, {"value": "0", "char_index": [4616, 4617], "type": "Other"}, {"value": "0", "char_index": [4631, 4632], "type": "Other"}, {"value": "3", "char_index": [4646, 4647], "type": "Other"}, {"value": "5", "char_index": [4661, 4662], "type": "Other"}, {"value": "0", "char_index": [4749, 4750], "type": "Other"}, {"value": "1", "char_index": [4764, 4765], "type": "Other"}, {"value": "1", "char_index": [4779, 4780], "type": "Other"}, {"value": "2", "char_index": [4794, 4795], "type": "Other"}, {"value": "0", "char_index": [4809, 4810], "type": "Other"}, {"value": "0", "char_index": [4824, 4825], "type": "Other"}, {"value": "2", "char_index": [4839, 4840], "type": "Other"}, {"value": "0", "char_index": [4854, 4855], "type": "Other"}, {"value": "4", "char_index": [4869, 4870], "type": "Other"}, {"value": "3", "char_index": [4884, 4885], "type": "Other"}, {"value": "2", "char_index": [4899, 4900], "type": "Other"}, {"value": "3", "char_index": [4914, 4915], "type": "Other"}, {"value": "2", "char_index": [4929, 4930], "type": "Other"}, {"value": "472", "char_index": [4952, 4955], "type": "Other"}, {"value": "0", "char_index": [4959, 4960], "type": "Other"}, {"value": "3", "char_index": [4974, 4975], "type": "Other"}, {"value": "5", "char_index": [4989, 4990], "type": "Other"}, {"value": "2", "char_index": [5077, 5078], "type": "Other"}, {"value": "4", "char_index": [5092, 5093], "type": "Other"}, {"value": "6", "char_index": [5107, 5108], "type": "Other"}, {"value": "0", "char_index": [5122, 5123], "type": "Other"}, {"value": "1", "char_index": [5137, 5138], "type": "Other"}, {"value": "1", "char_index": [5152, 5153], "type": "Other"}, {"value": "4", "char_index": [5167, 5168], "type": "Other"}, {"value": "1", "char_index": [5182, 5183], "type": "Other"}, {"value": "2", "char_index": [5197, 5198], "type": "Other"}, {"value": "2", "char_index": [5212, 5213], "type": "Other"}, {"value": "1", "char_index": [5227, 5228], "type": "Other"}, {"value": "11", "char_index": [5242, 5244], "type": "Other"}, {"value": "0", "char_index": [5257, 5258], "type": "Other"}, {"value": "0", "char_index": [5272, 5273], "type": "Other"}, {"value": "446", "char_index": [5295, 5298], "type": "Other"}, {"value": "7", "char_index": [5302, 5303], "type": "Other"}, {"value": "12", "char_index": [5317, 5319], "type": "Other"}, {"value": "2", "char_index": [5405, 5406], "type": "Other"}, {"value": "4", "char_index": [5420, 5421], "type": "Other"}, {"value": "12", "char_index": [5435, 5437], "type": "Other"}, {"value": "9", "char_index": [5450, 5451], "type": "Other"}, {"value": "5", "char_index": [5465, 5466], "type": "Other"}, {"value": "3", "char_index": [5480, 5481], "type": "Other"}, {"value": "7", "char_index": [5495, 5496], "type": "Other"}, {"value": "0", "char_index": [5510, 5511], "type": "Other"}, {"value": "9", "char_index": [5525, 5526], "type": "Other"}, {"value": "10", "char_index": [5540, 5542], "type": "Other"}, {"value": "10", "char_index": [5555, 5557], "type": "Other"}, {"value": "9", "char_index": [5570, 5571], "type": "Other"}, {"value": "6", "char_index": [5585, 5586], "type": "Other"}, {"value": "4", "char_index": [5600, 5601], "type": "Other"}, {"value": "2", "char_index": [5615, 5616], "type": "Other"}, {"value": "398", "char_index": [5638, 5641], "type": "Other"}, {"value": "10", "char_index": [5645, 5647], "type": "Other"}, {"value": "17", "char_index": [5733, 5735], "type": "Other"}, {"value": "23", "char_index": [5748, 5750], "type": "Other"}, {"value": "22", "char_index": [5763, 5765], "type": "Other"}, {"value": "7", "char_index": [5778, 5779], "type": "Other"}, {"value": "6", "char_index": [5793, 5794], "type": "Other"}, {"value": "15", "char_index": [5808, 5810], "type": "Other"}, {"value": "32", "char_index": [5823, 5825], "type": "Other"}, {"value": "14", "char_index": [5838, 5840], "type": "Other"}, {"value": "10", "char_index": [5853, 5855], "type": "Other"}, {"value": "6", "char_index": [5868, 5869], "type": "Other"}, {"value": "12", "char_index": [5883, 5885], "type": "Other"}, {"value": "18", "char_index": [5898, 5900], "type": "Other"}, {"value": "6", "char_index": [5913, 5914], "type": "Other"}, {"value": "9", "char_index": [5928, 5929], "type": "Other"}, {"value": "40", "char_index": [5943, 5945], "type": "Other"}, {"value": "21", "char_index": [5958, 5960], "type": "Other"}, {"value": "242", "char_index": [5981, 5984], "type": "Other"}]}, "2211.05673v1_table3": {"table_code": "\\begin{table*}[h]\n\\centering\n\\begin{tabular}{lrrrr}\n\\hline\n               \\multicolumn{1}{c}{Predicted Region}      & \\multicolumn{1}{c}{Pergamon Region} & \\multicolumn{1}{c}{Alexandria Region} & \\multicolumn{1}{c}{Delphi Region} & \\multicolumn{1}{c}{Other Regions} \\\\ \\hline\n Pergamon   & \\textbf{83\\%}                               & 3\\%                                  & 3\\%                             & 7\\%                              \\\\\n Alexandria & 5\\%                                & \\textbf{77\\%}                                 & 7 \\%                             & 10\\%                               \\\\\n Delphi      & 4\\%                                & 5\\%                                   & \\textbf{81\\%}                             & 8\\%                              \\\\\n Other      & 8\\%                                & 15\\%                         & 9\\%                             & \\textbf{75\\%}                             \\\\ \\hline\n\\end{tabular}\n\\caption{Results of the BERT-based regional classifier on 4000 sentences set aside for validation.}\n\\label{tab:region}\n\\end{table*}", "table_label": "{tab:region}", "table_numeric_cells": [["83", "\\textbf{83\\%}", 298, 300, 290, 303], ["3", "3\\%", 336, 337, 336, 339], ["3", "3\\%", 375, 376, 375, 378], ["7", "7\\%", 409, 410, 409, 412], ["5", "5\\%", 459, 460, 459, 462], ["77", "\\textbf{77\\%}", 504, 506, 496, 509], ["7", "7 \\%", 544, 545, 544, 548], ["10", "10\\%", 579, 581, 579, 583], ["4", "4\\%", 632, 633, 632, 635], ["5", "5\\%", 669, 670, 669, 672], ["81", "\\textbf{81\\%}", 717, 719, 709, 722], ["8", "8\\%", 753, 754, 753, 756], ["8", "8\\%", 803, 804, 803, 806], ["15", "15\\%", 840, 842, 840, 844], ["9", "9\\%", 871, 872, 871, 874], ["75", "\\textbf{75\\%}", 913, 915, 905, 918]], "text_chunk_selected": "Transformer artificial neural networks have shown spectacular success in machine translation and answering search queries by internally extracting, after extensive pretraining, abstract patterns, and long-range dependencies in human language samples through encoder hierarchies \\cite{vaswani2017attention}. Therefore, it seems natural to apply such schemes to other tasks that traditionally depended on human language understanding. Thus, this paper uses versions of BERT to investigate questions of authorship in Ancient Greek literature. Such models have already been used for authorship attribution, but the\nactual number of cases is still limited. \\citet{fabien2020bertaa} demonstrate that fine-tuning of a pretrained BERT  language model with an additional dense layer and a softmax activation allows performing authorship classification. \\citet{polignano2020contextualized} use BERT for author profiling in social media and conclude that despite encouraging results in terms of reliability, the computational power required for running such a model is too demanding for the task. These results show that transformers could be successfully used for authorship attribution, yet the application of these models to historical texts is still limited. Specifically, \\citet{bamman2020latin} develop BERT for Latin and \\cite{assael2019restoring} train an LSTM language model of Ancient Greek. There is also a char-BERT implementation\\footnote{https://github.com/brennannicholson/ancient-greek-char-bert}, but we do not know of any public full BERT model trained to work with Ancient Greek. We are also unaware of any examples when BERT was used successfully for authorship attribution of historical documents. \n\n\\begin{itemize}\n    \\item we use a transfer learning approach to train BERT for Ancient Greek;\n    \\item we demonstrate that this language model is useful for authorship attribution of Ancient Greek texts;\n    \\item we obtain results that may be used as evidence in the process of authorship attribution of the Pseudo-Plutarchean texts;\n    \\item we obtain new insights into the paths along which the reception of ancient philosophy was developed.\n\\end{itemize}\n\n\\section{Data}\nThe data were taken from the digital history projects at the Chair of\nAncient History of the University of Leipzig. The Plutarch texts were transformed into a digital representation under professional supervision. Data can be obtained from the following free repositories Perseus Digital Library\\footnote{https://github.com/PerseusDL/canonical-greekLit} and First Thousand Years of Greek\\footnote{https://github.com/ThomasK81/TEItoCEX} as part of Open Greek and Latin\\footnote{https://opengreekandlatin.org}. The resulting representation was stored in XML format (TEI guidelines) and enriched with metadata. The XML structure and metadata were removed. The strings were transferred into lowercase letters. The diacritics were removed. We did not touch hyphenation, punctuation, or any multilingual remains, nor did we apply any special language-related transformations.\nThe resulting data set consists of 1 244 documents with 199 809 paragraphs or 14 373 311 words.\n\n\\subsection{Tokenizers}\nThe tokenization of words into sub-word tokens is a crucial preprocessing step that can affect the performance of the model. Up to this point, we were using \"words\" as a linguistic term, whereas we understand tokens as the output of a tokenization algorithm. Thus, there would be one or more tokens that represent every word. Counting the number of tokens used on average to represent a word or, conversely, an average number of words per token gives an estimate of how fit  the tokenization is for the data set. In particular, the average number of words per token varies from 0 to 1, and the closer it is to 1, the more words are represented with one token. Various researchers have shown that corpus-specific tokenization could be beneficial for an NLP task. For example, \\citet{sennrich-etal-2016-neural} show that optimal vocabulary is dependent on the frequencies of the words in the target corpus. \\citet{lakew2019controlling} and \\citet{aji2020neural} partially discuss the tokenization in the setting of cross-language transfer. Though \\citet{aji2020neural} demonstrate that there is no clear evidence that one parent is better than another for cross-lingual transfer learning, they also show that token matching and joint vocabulary \\cite{nguyen2017transfer} are the best ways to handle the embedding layer during transfer learning. \n\nSince each model has its own specific tokenizer before pretraining, one wants to measure how well each of them works with Ancient Greek. To do that, one could take two sample data sets: a sample of Modern Greek\nWikipedia (referred to in Table \\ref{tab:tok} as \u201cmodern\u201d) and a comparable sample of Ancient Greek (\u201cancient\u201d  in Table \\ref{tab:tok}). Each data sample is tokenized with both the Modern Greek BERT tokenizer\\footnote{https://huggingface.co/nlpaueb/bert-base-greek-uncased-v} and the Multilingual BERT tokenizer\\footnote{https://huggingface.co/bert-base-multilingual-cased}. One can speculate that the model uses shorter tokens to adopt grammatical information and deal with longer, rarely observed words. In contrast, the representations with longer tokens could be useful for semantically intensive problems. These longer, semantically charged tokens may vary significantly on various downstream tasks. Thus, the average length of a token and the average number of words per token, shown in Table \\ref{tab:tok}, could be coarsely used as an estimate of the resulting tokenization. One could claim that the higher these values, the more apt the tokenizer is for the task. Indeed, higher average length of a token and number of words per token mean that longer, more semantically charged tokens could be matched for transfer; see \\cite{singh2019bert,aji2020neural,samenko2021fine} for a detailed discussion of various tokenization properties.\n\n\\subsection{Training Ancient Greek BERT via Transfer Learning}\nIf related tasks are available, we can fine-tune the model first on a related task with more data before fine-tuning it on the target task, see \\cite{ruder2019transfer}. This helps particularly for tasks with limited data \\cite{phang2018sentence} and improves sample efficiency on the target task \\cite{yogatama2019learning}. Since we have a limited amount of Ancient Greek texts, we want to do language transfer from Modern Greek to Ancient Greek training a masked language\nmodel (MLM)\\footnote{https://github.com/huggingface/transformers/blob/master/ examples/language-modeling/run\\_language\\_modeling.py} of the Ancient Greek text. \n\nWe use this data set to train BERT Classifiers similarly to \\cite{fabien2020bertaa}. Table \\ref{tab:au} shows the validation accuracy for Modern Greek and Multilingual BERTs after MLM on Ancient Greek and 10 epochs of classifier training\\footnote{AdamW, LR = 2e-5, eps = 1e-8,  linear\\_schedule}. Table \\ref{tab:au}  also shows a standard NLTK Naive Bayes Classifier trained on the 2 000 most frequent unigrams as a reference point for authorship attribution accuracy.\n\nSince Ancient Greek was used in various regions and territories, one might expect that the texts also show regional peculiarities. Such peculiarities\nthen should also be detectable by a dedicated regional BERT Classifier. We constructed three coarse regions for the origins of the authors in the data set: a region surrounding Delphi, where Plutarch was working, a region in the proximity of Alexandria, and the region of ancient Ionia, namely the ancient region on the central part of the western coast of modern Anatolia, see Figure \\ref{fig:map}. After balancing texts written by authors in these three regions with the fourth label that includes random sentences from authors outside of these regions, we train another BERT-based classifier to achieve a $0.79$ validation accuracy for the region of the author. Table \\ref{tab:region} shows the results of the obtained classifier on the validation set.", "table_source": "\\begin{table*}[h]\n\\centering\n\\begin{tabular}{lrrrr}\n\\hline\n               \\multicolumn{1}{c}{Predicted Region}      & \\multicolumn{1}{c}{Pergamon Region} & \\multicolumn{1}{c}{Alexandria Region} & \\multicolumn{1}{c}{Delphi Region} & \\multicolumn{1}{c}{Other Regions} \\\\ \\hline\n Pergamon   & \\textbf{83\\%}                               & 3\\%                                  & 3\\%                             & 7\\%                              \\\\\n Alexandria & 5\\%                                & \\textbf{77\\%}                                 & 7 \\%                             & 10\\%                               \\\\\n Delphi      & 4\\%                                & 5\\%                                   & \\textbf{81\\%}                             & 8\\%                              \\\\\n Other      & 8\\%                                & 15\\%                         & 9\\%                             & \\textbf{75\\%}                             \\\\ \\hline\n\\end{tabular}\n\\caption{Results of the BERT-based regional classifier on 4000 sentences set aside for validation.}\n\\label{tab:region}\n\\end{table*}", "cell_list_gold": [{"value": "83", "char_index": [298, 300], "type": "Other"}, {"value": "3", "char_index": [336, 337], "type": "Other"}, {"value": "3", "char_index": [375, 376], "type": "Other"}, {"value": "7", "char_index": [409, 410], "type": "Other"}, {"value": "5", "char_index": [459, 460], "type": "Other"}, {"value": "77", "char_index": [504, 506], "type": "Other"}, {"value": "7", "char_index": [544, 545], "type": "Other"}, {"value": "10", "char_index": [579, 581], "type": "Other"}, {"value": "4", "char_index": [632, 633], "type": "Other"}, {"value": "5", "char_index": [669, 670], "type": "Other"}, {"value": "81", "char_index": [717, 719], "type": "Other"}, {"value": "8", "char_index": [753, 754], "type": "Other"}, {"value": "8", "char_index": [803, 804], "type": "Other"}, {"value": "15", "char_index": [840, 842], "type": "Other"}, {"value": "9", "char_index": [871, 872], "type": "Other"}, {"value": "75", "char_index": [913, 915], "type": "Other"}]}, "2211.05673v1_table5": {"table_code": "\\begin{table}[h!]\n\\centering\n\\begin{tabular}{lll}\n\\hline\n                             & Validation Acc. \\\\ \\hline\n\\textbf{Greek BERT with MLM}    &\\textbf{83.8\\%}  \\\\\nGreek BERT without MLM & 83.4\\%  \\\\\n\\textbf{M-BERT with MLM}       & \\textbf{83.8\\%}  \\\\\nM-BERT without MLM     & 81.6\\%  \\\\\nNaive Bayes            & 75.8\\%   \\\\\nRandom Author Assignment  & 11.2\\%   \\\\\\hline\n\\end{tabular}\n\\caption{Accuracy of the authorship classifier on the test set for the document-balanced splitting.}\n\\label{appd:allmodels}\n\\end{table}", "table_label": "{appd:allmodels}", "table_numeric_cells": [["83.8", "\\textbf{83.8\\%}", 155, 159, 147, 162], ["83.4", "83.4\\%", 192, 196, 192, 198], ["83.8", "\\textbf{83.8\\%}", 244, 248, 236, 251], ["81.6", "81.6\\%", 281, 285, 281, 287], ["75.8", "75.8\\%", 317, 321, 317, 323], ["11.2", "11.2\\%", 357, 361, 357, 363]], "text_chunk_selected": "\\begin{itemize}\n    \\item we use a transfer learning approach to train BERT for Ancient Greek;\n    \\item we demonstrate that this language model is useful for authorship attribution of Ancient Greek texts;\n    \\item we obtain results that may be used as evidence in the process of authorship attribution of the Pseudo-Plutarchean texts;\n    \\item we obtain new insights into the paths along which the reception of ancient philosophy was developed.\n\\end{itemize}\n\n\\subsection{Tokenizers}\nThe tokenization of words into sub-word tokens is a crucial preprocessing step that can affect the performance of the model. Up to this point, we were using \"words\" as a linguistic term, whereas we understand tokens as the output of a tokenization algorithm. Thus, there would be one or more tokens that represent every word. Counting the number of tokens used on average to represent a word or, conversely, an average number of words per token gives an estimate of how fit  the tokenization is for the data set. In particular, the average number of words per token varies from 0 to 1, and the closer it is to 1, the more words are represented with one token. Various researchers have shown that corpus-specific tokenization could be beneficial for an NLP task. For example, \\citet{sennrich-etal-2016-neural} show that optimal vocabulary is dependent on the frequencies of the words in the target corpus. \\citet{lakew2019controlling} and \\citet{aji2020neural} partially discuss the tokenization in the setting of cross-language transfer. Though \\citet{aji2020neural} demonstrate that there is no clear evidence that one parent is better than another for cross-lingual transfer learning, they also show that token matching and joint vocabulary \\cite{nguyen2017transfer} are the best ways to handle the embedding layer during transfer learning. \n\nSince each model has its own specific tokenizer before pretraining, one wants to measure how well each of them works with Ancient Greek. To do that, one could take two sample data sets: a sample of Modern Greek\nWikipedia (referred to in Table \\ref{tab:tok} as \u201cmodern\u201d) and a comparable sample of Ancient Greek (\u201cancient\u201d  in Table \\ref{tab:tok}). Each data sample is tokenized with both the Modern Greek BERT tokenizer\\footnote{https://huggingface.co/nlpaueb/bert-base-greek-uncased-v} and the Multilingual BERT tokenizer\\footnote{https://huggingface.co/bert-base-multilingual-cased}. One can speculate that the model uses shorter tokens to adopt grammatical information and deal with longer, rarely observed words. In contrast, the representations with longer tokens could be useful for semantically intensive problems. These longer, semantically charged tokens may vary significantly on various downstream tasks. Thus, the average length of a token and the average number of words per token, shown in Table \\ref{tab:tok}, could be coarsely used as an estimate of the resulting tokenization. One could claim that the higher these values, the more apt the tokenizer is for the task. Indeed, higher average length of a token and number of words per token mean that longer, more semantically charged tokens could be matched for transfer; see \\cite{singh2019bert,aji2020neural,samenko2021fine} for a detailed discussion of various tokenization properties.\n\nWe use this data set to train BERT Classifiers similarly to \\cite{fabien2020bertaa}. Table \\ref{tab:au} shows the validation accuracy for Modern Greek and Multilingual BERTs after MLM on Ancient Greek and 10 epochs of classifier training\\footnote{AdamW, LR = 2e-5, eps = 1e-8,  linear\\_schedule}. Table \\ref{tab:au}  also shows a standard NLTK Naive Bayes Classifier trained on the 2 000 most frequent unigrams as a reference point for authorship attribution accuracy.\n\nSince Ancient Greek was used in various regions and territories, one might expect that the texts also show regional peculiarities. Such peculiarities\nthen should also be detectable by a dedicated regional BERT Classifier. We constructed three coarse regions for the origins of the authors in the data set: a region surrounding Delphi, where Plutarch was working, a region in the proximity of Alexandria, and the region of ancient Ionia, namely the ancient region on the central part of the western coast of modern Anatolia, see Figure \\ref{fig:map}. After balancing texts written by authors in these three regions with the fourth label that includes random sentences from authors outside of these regions, we train another BERT-based classifier to achieve a $0.79$ validation accuracy for the region of the author. Table \\ref{tab:region} shows the results of the obtained classifier on the validation set.\n\nLet us now split these three Pseudo-Plutarchean texts into the separate sentences and apply the author classifier described above to these texts.\nTable \\ref{tab:p} shows the authors that are most frequently attributed within a particular document, along with the share of sentences attributed to them. We have double-checked these results using an alternative scoring method. Instead of classifying every sentence in a document and then averaging the classifier's results across all sentences, one could obtain probability scores for every author that the model estimates for every sentence. Averaging those probabilities throughout the document, one could obtain the three most-probable author candidates. These three most-probable authors turn out to be exactly the same for all three documents as the ones in Table \\ref{tab:p}. Moreover, the resulting probabilities of the authorship are also the same for all three most probable authors across all three documents under examination. \n\nSince we are interested in a particular historical time-span around Plutarch, we did not provide details on other potential applications for the resulting model. However, one could list several further applications that might interest historians and, to our knowledge, are not developed to this day. Stylistic attributes of text include author-specific attributes (see \\cite{xu} or \\cite{Jhamtani} on 'shakespearization'), politeness \\cite{Sennrich}, gender or political slant \\cite{Prabhumoye}, formality of speech \\cite{Rao} but most importantly for the scope of this work the \\textit{'style of the time'} \\cite{Hughes}. Using the provided approach for the historical dating of the documents is a feasible option that might bring new historical insights. It is only the question of data and their quality. For example, one could try to use a corpus spanning several hundreds of years, fine-tune the developed model for the task of date attribution and see if it could work reasonably well. This is a possible further line of work to pursue, yet it is outside of the scope of this contribution.\n\nWe have studied two different ways of data splitting to address possible questions on the validity of the proposed machine learning pipeline. The first preprocessing described above splits the texts into sentences and then randomly splits them into training, validation, and testing. This splitting might create some dependencies in the evaluation sets since sentences from the same text could be in training, validation, and testing sets. This potentially could lead to higher evaluation scores, so here we briefly discuss another way of splitting data that could not be prone to such data' leak'. Let us refine our data set leaving only the authors that have produced several documents. Let's do the test-train split so that whole documents end up in the train or the test set. This split limits the list of authors even further, yet it is not a problem in this context. Table \\ref{appd:allmodels} summarizes the result on the test set.", "table_source": "\\begin{table}[h!]\n\\centering\n\\begin{tabular}{lll}\n\\hline\n                             & Validation Acc. \\\\ \\hline\n\\textbf{Greek BERT with MLM}    &\\textbf{83.8\\%}  \\\\\nGreek BERT without MLM & 83.4\\%  \\\\\n\\textbf{M-BERT with MLM}       & \\textbf{83.8\\%}  \\\\\nM-BERT without MLM     & 81.6\\%  \\\\\nNaive Bayes            & 75.8\\%   \\\\\nRandom Author Assignment  & 11.2\\%   \\\\\\hline\n\\end{tabular}\n\\caption{Accuracy of the authorship classifier on the test set for the document-balanced splitting.}\n\\label{appd:allmodels}\n\\end{table}", "cell_list_gold": [{"value": "83.8", "char_index": [155, 159], "type": "Result", "training data/set": ["xx", "authorship attribution data set"], "test data/set": ["xx", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc."], "experimental settings": {"xx": "yy"}, "model": "Greek BERT with MLM", "model settings": {"xx": "yy"}}, {"value": "83.4", "char_index": [192, 196], "type": "Result", "training data/set": ["xx", "authorship attribution data set"], "test data/set": ["xx", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc."], "experimental settings": {"xx": "yy"}, "model": "Greek BERT without MLM", "model settings": {"xx": "yy"}}, {"value": "83.8", "char_index": [244, 248], "type": "Result", "training data/set": ["xx", "authorship attribution data set"], "test data/set": ["xx", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc."], "experimental settings": {"xx": "yy"}, "model": "M-BERT with MLM", "model settings": {"xx": "yy"}}, {"value": "81.6", "char_index": [281, 285], "type": "Result", "training data/set": ["xx", "authorship attribution data set"], "test data/set": ["xx", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc."], "experimental settings": {"xx": "yy"}, "model": "M-BERT without MLM", "model settings": {"xx": "yy"}}, {"value": "75.8", "char_index": [317, 321], "type": "Result", "training data/set": ["xx", "authorship attribution data set"], "test data/set": ["xx", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc."], "experimental settings": {"xx": "yy"}, "model": "Naive Bayes", "model settings": {"xx": "yy"}}, {"value": "11.2", "char_index": [357, 361], "type": "Result", "training data/set": ["xx", "authorship attribution data set"], "test data/set": ["xx", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc."], "experimental settings": {"xx": "yy"}, "model": "Random Author Assignment", "model settings": {"xx": "yy"}}]}, "2211.05776v1_table0": {"table_code": "\\begin{tabular}{c|ccc}\n             & \\cellcolor{lightgray!30} ann$_{1}$ & \\cellcolor{lightgray!30} ann$_{2}$ & \\cellcolor{lightgray!30} ann$_{3}$\\\\ \\hline\n            \\cellcolor{lightgray!30} ann$_{1}$ & - & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{2}$ & 90.6 & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{3}$ & 91.2 & 92.1 & -\\\\ \\hline\n            \\end{tabular}", "table_label": "{tab:consistency}", "table_numeric_cells": [["90.6", "90.6", 267, 271, 267, 271], ["91.2", "91.2", 332, 336, 332, 336], ["92.1", "92.1", 339, 343, 339, 343]], "text_chunk_selected": "\\author{\nLu Qi$^{1}$\\thanks{Equal contribution.},~\nJason Kuen$^{2*}$,~\nWeidong Guo$^{3*}$,~\nTiancheng Shen$^4$,~\nJiuxiang Gu$^{2}$,~\nWenbo Li$^{4}$,~\\\\\nJiaya Jia$^{4}$~\nZhe Lin$^2$,~\nMing-Hsuan Yang$^{1}$\n\\\\[0.2cm]\n$^1$The University of California, Merced~~\\\\\n$^2$Adobe Research~~\\\\\n$^3$QQ Browser Lab, Tencent~~\\\\\n$^4$The Chinese University of Hong Kong~~\n\\vspace{-0.4in}\n}\n\\maketitle\n\n\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\nMotivated by the above-discussed observations, we propose a new large-scale, high-quality dataset for entity segmentation. Our dataset has three important characteristics: \n1) we collect about 33,227 images from public datasets and the Internet. These images cover various domains such as landscapes, outdoor/indoor scenes, cartoons, graphical illustrations, and computer games; \n2) the annotations are open-world without any predefined class restrictions and densely cover\nthe entire images; \n3) we elevate the segmentation quality objective of entity segmentation~\\cite{qi2021open} to the next level by annotating high-quality masks with fine-grained details on high-resolution images. There are over 70\\% images that \nthat have high image resolutions, spanning from \n2K to 15K, with significantly more accurate boundary annotations compared to existing panoptic datasets. \nAs shown in Figure~\\ref{fig:comp_coco}, our diversified dataset provides fine-grained segmentation masks on high-res images, along with open-world annotations on low-res images from public datasets.\n\n\\section{Related Work}\n\\paragraph{Image Segmentation Dataset}\nNumerous datasets have been proposed for semantic, instance, and panoptic segmentation,~\\emph{e.g}., Microsoft COCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene}, KITTI~\\cite{geiger2012we}, LVIS~\\cite{gupta2019lvis}, PASCAL-VOC~\\cite{everingham2010pascal}, Open-Images~\\cite{kuznetsova2020open},  Cityscapes~\\cite{cordts2016cityscapes},~\\emph{etc}. \nIn addition, there are some datasets designed for specific scenarios, such as amodal segmentation (COCO-Amodal~\\cite{zhu2017semantic}, KINS~\\cite{qi2019amodal}), human segmentation (CelebAMask-HQ~\\cite{CelebAMask-HQ}, LIP~\\cite{gong2017look}, MHP~\\cite{zhao2018understanding}) and domain adaptation (Synscapes~\\cite{wrenninge2018synscapes}, GTA5~\\cite{richter2016playing}). \nDespite significant contributions from these datasets, it is of great interest to fulfill the needs of real-world applications with high-quality images with large diversity. \nFor example, a segmentation model should be robust to high-resolution images from different domains. \nMeanwhile, models should be able to segment unseen objects in the open-world setting. \nMost relevant to our work is the  ADE20K dataset~\\cite{zhou2017scene}, which has large-scale open-vocabulary categories. \nHowever, the collected images in ADE20K are of low resolution and from narrowed domains. \nIn our dataset, we collect the images from multiple image domains, including indoor, outdoor, street scenes, and even cartoon and remote image domains. \nIn addition, over 80\\% of the images resolution\nfall within the high-resolution range of 2000px and 8000 px. Compared to the ADE20K~\\cite{zhou2017places} and LVIS~\\cite{gupta2019lvis} dataset with a predefined category list, our annotation process is different.\nWe first conduct class-agnostic mask annotation on each entity. After that, the category information is then labeled in an open-vocabulary manner.\n\nThe EntitySeg dataset contains 33,227 images with high-quality mask annotations. \nCompared with existing dataets, there are three distinct properties in EntitySeg. \nFirst, 71.25\\% and 86.23\\% of the images are of high resolution with at least 2000px$\\times$2000px and 1000px$\\times$1000px which is more consistent with current digital imaging trends.\nSecond, our dataset is open-world and is not limited to predefined classes. We regard \nevery semantically-coherent region in the images as an entity, even \nif it is a blurred region or it cannot be semantically recognized easily. Third, the mask annotation along the boundaries are more accurate than existing datasets, as shown in Figure~\\ref{fig:comp_coco}. \nIn the following, we describe our EntitySeg dataset and provide  comprehensive analysis.\n\n\\subsection{Image Collection}\nInspired by the collection criterion of COCO~\\cite{lin2014microsoft}, we collect most non-iconic images with more contextual information and non-canonical viewpoints. \nIn addition, the image domains should be as diversified as possible to guarantee substantial domain diversity. Therefore, the image sources of our dataset include several public datasets and the Internet where the images permitted for academic research use. For public dataset sources, we select part of the images from\nCOCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene}, \nPascal VOC~\\cite{everingham2010pascal}, Cityscapes~\\cite{cordts2016cityscapes}, \nMapillary Vistas~\\cite{neuhold2017mapillary}, ImageNet~\\cite{krizhevsky2017imagenet}, \nOpen Images~\\cite{kuznetsova2020open}, DIV2K~\\cite{agustsson2017ntire}, Flick2K~\\cite{wang2019flickr1024}, Clipart~\\cite{inoue2018cross},\nComic~\\cite{inoue2018cross}, DOTA~\\cite{xia2018dota} and some computer game screenshots from Synscapes~\\cite{wrenninge2018synscapes} and GTA5~\\cite{richter2016playing}. From the Internet, we crawled mainly high-resolution images from Pixabay~\\cite{pixabay}, Unsplash~\\cite{Unsplash}, and LAION-400M~\\cite{schuhmann2021laion}. In Figure~\\ref{fig:statistics_01}(c), we show the distribution of image sources in our dataset and see that most of the images are from high-resolution sources like Pixabay, Unsplash, and LAION-400M. In Figure~\\ref{fig:statistics_01}(a), our dataset has more high-resolution images whose resolutions are normally distributed in the resolution range between 0px to 15,000px, whereas all the images of ADE20K~\\cite{zhou2017scene} and COCO~\\cite{lin2014microsoft} are under 1000px.\n\n\\subsection{Image Annotation}\nThe annotation process for our dataset mainly consists of three steps. For an image, we firstly annotate all entities with class-agnostic pixel-level masks where each mask has no overlaps with others. After that, each mask is annotated with a class label based on a large vocabulary class set, which is not fixed but updated continuously over time. There are two special considerations here.\nOn one hand, we annotate the entity as `unknown' or `blurred' if it cannot be name easily or severely blurred. On the other hand, the entity is annotated as a  `\\textit{supercategory}\\_other' if we merely know it is a \\textit{supercategory} but unable to identify its fine-grained class. Our annotation process is the direct opposite of those of the popular segmentation datasets like COCO~\\cite{lin2014microsoft} and ADE20K~\\cite{zhou2017places}, where class sets are predefined first before annotating masks based on the predefined classes. In Table~\\ref{Tab:comp_entity_other}, it shows that our dataset has \na greater proportion of covered image areas and number of masks on average than COCO- and ADE20K-Panoptic. This is by virtue of our novel annotation process that allows us to take into account all the semantically-coherent regions, and then assign class labels to them. \nIn addition, our annotation process is more similar to the the human vision system. This is consistent with human vision system that is inherently class-agnostic and is able to recognize entities without understanding its use and purpose, as explained by Marr~\\cite{Marr1982Vision}.\n\nAnnotation consistency is a crucial aspect of any human-labeled dataset since it tells whether the annotation task is well-defined and reasonable. We randomly selected 500 images from our dataset ($1.5\\%$ of the entire dataset) and asked another two annotators to \nseparately annotate them again after four months. Table~\\ref{tab:consistency}(a) shows the class-agnostic mask mAP in the first step of the annotation process compared to those of the other two annotators. We use one as ground truth, and another one as prediction results. \nIn addition, we evaluate the category consistency by accuracy (ACC) under the same mask annotations in Table~\\ref{tab:consistency}(b). \nThese two tables indicate our dataset has a high degree of annotation consistency both in the mask and category labeling stages.", "table_source": "\\begin{table}[t!]\n\\begin{minipage}{\\textwidth}\n\\begin{minipage}[t]{0.21\\textwidth}\n            \\centering\n            \\footnotesize\n            \\setlength{\\tabcolsep}{2pt}\n            \\begin{tabular}{c|ccc}\n             & \\cellcolor{lightgray!30} ann$_{1}$ & \\cellcolor{lightgray!30} ann$_{2}$ & \\cellcolor{lightgray!30} ann$_{3}$\\\\ \\hline\n            \\cellcolor{lightgray!30} ann$_{1}$ & - & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{2}$ & 90.6 & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{3}$ & 91.2 & 92.1 & -\\\\ \\hline\n            \\end{tabular}\n        \n        (a)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.30\\textwidth}\n        \\centering\n        \\footnotesize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{c|ccc}\n             & \\cellcolor{lightgray!30} ann$_{1}$ & \\cellcolor{lightgray!30} ann$_{2}$ & \\cellcolor{lightgray!30} ann$_{3}$\\\\ \\hline\n            \\cellcolor{lightgray!30} ann$_{1}$ & - & - & -\\\\\n            \\cellcolor{lightgray!30} ann$_{2}$ & 95.4 & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{3}$ & 94.8 & 95.2 & -\\\\ \\hline\n        \\end{tabular}\n        \n    (b)\n        \\end{minipage}\n\\end{minipage}\n\\caption{Annotation consistency of class-agnostic localization and class-aware categories among annotators.}\n\\vspace{-0.1in}\n\\label{tab:consistency}\n\\end{table}", "cell_list_gold": [{"value": "90.6", "char_index": [267, 271], "type": "Other"}, {"value": "91.2", "char_index": [332, 336], "type": "Other"}, {"value": "92.1", "char_index": [339, 343], "type": "Other"}]}, "2211.05776v1_table2": {"table_code": "\\begin{table}[t!]\n\\centering\n\\tiny\n\\begin{tabular}{c|c|c|c|c|c|c}\n\\cellcolor{lightgray!30} Dataset & \\cellcolor{lightgray!30} \\makecell{ImageRes\\\\(avg)$\\uparrow$} & \\cellcolor{lightgray!30} \\cellcolor{lightgray!30} \\makecell{EntityNum\\\\(avg)$\\uparrow$} & \\cellcolor{lightgray!30} \\makecell{EntityNum\\\\(max)$\\uparrow$} & \\cellcolor{lightgray!30} \\makecell{Entity\\\\Complexity$\\downarrow$} &\n\\cellcolor{lightgray!30} \\makecell{Entity\\\\Simplicity$\\downarrow$} &\\cellcolor{lightgray!30} \\makecell{Valid\\\\Area$\\uparrow$}  \\\\ \\hline\nCOCO & 522.5 & 11.2 & 95 & 0.758 & 0.581 & 0.891 \\\\\nADE20K & 461.3& 13.6 & \\textbf{255} & 0.802 & 0.606 & 0.914 \\\\\nEntitySeg & \\textbf{2700.7} & \\textbf{18.1} & 236 & \\textbf{0.719} & \\textbf{0.538} & \\textbf{0.999} \\\\ \\hline\n\\end{tabular}\n\\caption{The statistical comparisons between COCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene} and EntitySeg. `ImageRes (avg)', `EntityNum (avg)', `Valid Area', `Entity Complexity', and `Entity Simplicity' refer to the average value of image resolution size, entity numbers per image, valid area ratio per image, average entity complexity and simplicity respectively. `EntityNum (max)' means the maximum per-image number of entities within each dataset.}\n\\label{Tab:comp_entity_other}\n\\end{table}", "table_label": "{Tab:comp_entity_other}", "table_numeric_cells": [["522.5", "522.5", 533, 538, 533, 538], ["11.2", "11.2", 541, 545, 541, 545], ["95", "95", 548, 550, 548, 550], ["0.758", "0.758", 553, 558, 553, 558], ["0.581", "0.581", 561, 566, 561, 566], ["0.891", "0.891", 569, 574, 569, 574], ["461.3", "461.3", 587, 592, 587, 592], ["13.6", "13.6", 594, 598, 594, 598], ["255", "\\textbf{255}", 609, 612, 601, 613], ["0.802", "0.802", 616, 621, 616, 621], ["0.606", "0.606", 624, 629, 624, 629], ["0.914", "0.914", 632, 637, 632, 637], ["2700.7", "\\textbf{2700.7}", 661, 667, 653, 668], ["18.1", "\\textbf{18.1}", 679, 683, 671, 684], ["236", "236", 687, 690, 687, 690], ["0.719", "\\textbf{0.719}", 701, 706, 693, 707], ["0.538", "\\textbf{0.538}", 718, 723, 710, 724], ["0.999", "\\textbf{0.999}", 735, 740, 727, 741]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\section{Introduction}\nDense image segmentation is an important topic in computer vision with a wide range of applications such as autonomous driving~\\cite{qi2019amodal}, robotics~\\cite{shu2014human}, video surveillance~\\cite{morrison2018cartman}, and image editing tools. \nRecently, entity segmentation~\\cite{qi2021open} has been introduced as a promising dense instance-aware approach for tackling generalization issues caused by narrow image domains and/or predefined class sets in existing panoptic~\\cite{kirillov2019panoptic,kirillov2019panopticfpn,xiong2019upsnet,li2020fully,carion2020end} \nand semantic~\\cite{long2015fully,zhao2017pyramid,chen2017rethinking,chen2018encoder,zhao2018psanet,huang2019ccnet,liu2019auto} segmentation tasks.\nAlthough the early method~\\cite{qi2021open} shows some promising results, it merely borrows existing panoptic segmentation datasets to demonstrate the feasibility and benefits of the entity segmentation. \nHowever, existing panoptic segmentation datasets (\\textit{e.g,} COCO~\\cite{lin2014microsoft}, Cityscapes~\\cite{cordts2016cityscapes}, ADE20K~\\cite{zhou2017scene}) \nsuffer from the same issues that entity segmentation~\\cite{qi2021open} aims to avoid, since those datasets are collected and annotated with \npanoptic segmentation task requirements in mind.\n\n\\section{Related Work}\n\\paragraph{Image Segmentation Dataset}\nNumerous datasets have been proposed for semantic, instance, and panoptic segmentation,~\\emph{e.g}., Microsoft COCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene}, KITTI~\\cite{geiger2012we}, LVIS~\\cite{gupta2019lvis}, PASCAL-VOC~\\cite{everingham2010pascal}, Open-Images~\\cite{kuznetsova2020open},  Cityscapes~\\cite{cordts2016cityscapes},~\\emph{etc}. \nIn addition, there are some datasets designed for specific scenarios, such as amodal segmentation (COCO-Amodal~\\cite{zhu2017semantic}, KINS~\\cite{qi2019amodal}), human segmentation (CelebAMask-HQ~\\cite{CelebAMask-HQ}, LIP~\\cite{gong2017look}, MHP~\\cite{zhao2018understanding}) and domain adaptation (Synscapes~\\cite{wrenninge2018synscapes}, GTA5~\\cite{richter2016playing}). \nDespite significant contributions from these datasets, it is of great interest to fulfill the needs of real-world applications with high-quality images with large diversity. \nFor example, a segmentation model should be robust to high-resolution images from different domains. \nMeanwhile, models should be able to segment unseen objects in the open-world setting. \nMost relevant to our work is the  ADE20K dataset~\\cite{zhou2017scene}, which has large-scale open-vocabulary categories. \nHowever, the collected images in ADE20K are of low resolution and from narrowed domains. \nIn our dataset, we collect the images from multiple image domains, including indoor, outdoor, street scenes, and even cartoon and remote image domains. \nIn addition, over 80\\% of the images resolution\nfall within the high-resolution range of 2000px and 8000 px. Compared to the ADE20K~\\cite{zhou2017places} and LVIS~\\cite{gupta2019lvis} dataset with a predefined category list, our annotation process is different.\nWe first conduct class-agnostic mask annotation on each entity. After that, the category information is then labeled in an open-vocabulary manner.\n\nThe EntitySeg dataset contains 33,227 images with high-quality mask annotations. \nCompared with existing dataets, there are three distinct properties in EntitySeg. \nFirst, 71.25\\% and 86.23\\% of the images are of high resolution with at least 2000px$\\times$2000px and 1000px$\\times$1000px which is more consistent with current digital imaging trends.\nSecond, our dataset is open-world and is not limited to predefined classes. We regard \nevery semantically-coherent region in the images as an entity, even \nif it is a blurred region or it cannot be semantically recognized easily. Third, the mask annotation along the boundaries are more accurate than existing datasets, as shown in Figure~\\ref{fig:comp_coco}. \nIn the following, we describe our EntitySeg dataset and provide  comprehensive analysis.\n\n\\subsection{Image Collection}\nInspired by the collection criterion of COCO~\\cite{lin2014microsoft}, we collect most non-iconic images with more contextual information and non-canonical viewpoints. \nIn addition, the image domains should be as diversified as possible to guarantee substantial domain diversity. Therefore, the image sources of our dataset include several public datasets and the Internet where the images permitted for academic research use. For public dataset sources, we select part of the images from\nCOCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene}, \nPascal VOC~\\cite{everingham2010pascal}, Cityscapes~\\cite{cordts2016cityscapes}, \nMapillary Vistas~\\cite{neuhold2017mapillary}, ImageNet~\\cite{krizhevsky2017imagenet}, \nOpen Images~\\cite{kuznetsova2020open}, DIV2K~\\cite{agustsson2017ntire}, Flick2K~\\cite{wang2019flickr1024}, Clipart~\\cite{inoue2018cross},\nComic~\\cite{inoue2018cross}, DOTA~\\cite{xia2018dota} and some computer game screenshots from Synscapes~\\cite{wrenninge2018synscapes} and GTA5~\\cite{richter2016playing}. From the Internet, we crawled mainly high-resolution images from Pixabay~\\cite{pixabay}, Unsplash~\\cite{Unsplash}, and LAION-400M~\\cite{schuhmann2021laion}. In Figure~\\ref{fig:statistics_01}(c), we show the distribution of image sources in our dataset and see that most of the images are from high-resolution sources like Pixabay, Unsplash, and LAION-400M. In Figure~\\ref{fig:statistics_01}(a), our dataset has more high-resolution images whose resolutions are normally distributed in the resolution range between 0px to 15,000px, whereas all the images of ADE20K~\\cite{zhou2017scene} and COCO~\\cite{lin2014microsoft} are under 1000px.\n\n\\subsection{Image Annotation}\nThe annotation process for our dataset mainly consists of three steps. For an image, we firstly annotate all entities with class-agnostic pixel-level masks where each mask has no overlaps with others. After that, each mask is annotated with a class label based on a large vocabulary class set, which is not fixed but updated continuously over time. There are two special considerations here.\nOn one hand, we annotate the entity as `unknown' or `blurred' if it cannot be name easily or severely blurred. On the other hand, the entity is annotated as a  `\\textit{supercategory}\\_other' if we merely know it is a \\textit{supercategory} but unable to identify its fine-grained class. Our annotation process is the direct opposite of those of the popular segmentation datasets like COCO~\\cite{lin2014microsoft} and ADE20K~\\cite{zhou2017places}, where class sets are predefined first before annotating masks based on the predefined classes. In Table~\\ref{Tab:comp_entity_other}, it shows that our dataset has \na greater proportion of covered image areas and number of masks on average than COCO- and ADE20K-Panoptic. This is by virtue of our novel annotation process that allows us to take into account all the semantically-coherent regions, and then assign class labels to them. \nIn addition, our annotation process is more similar to the the human vision system. This is consistent with human vision system that is inherently class-agnostic and is able to recognize entities without understanding its use and purpose, as explained by Marr~\\cite{Marr1982Vision}.\n\nAnnotation consistency is a crucial aspect of any human-labeled dataset since it tells whether the annotation task is well-defined and reasonable. We randomly selected 500 images from our dataset ($1.5\\%$ of the entire dataset) and asked another two annotators to \nseparately annotate them again after four months. Table~\\ref{tab:consistency}(a) shows the class-agnostic mask mAP in the first step of the annotation process compared to those of the other two annotators. We use one as ground truth, and another one as prediction results. \nIn addition, we evaluate the category consistency by accuracy (ACC) under the same mask annotations in Table~\\ref{tab:consistency}(b). \nThese two tables indicate our dataset has a high degree of annotation consistency both in the mask and category labeling stages.\n\nWe present the quantitative comparison among our dataset, COCO-Panoptic~\\cite{lin2014microsoft}, and ADE20K-Panoptic~\\cite{zhou2017scene} in Table~\\ref{Tab:comp_entity_other}. In the EntitySeg dataset, each image has 18.1 entities on average, which is more than 11.2 and 13.6 entities in COCO and ADE20K. \nThe detailed comparison among those three datasets on the distribution of entity number is shown in Figure~\\ref{fig:statistics_01}(b). Furthermore, the shapes of entities in our dataset are more complex than those COCO~\\cite{lin2014microsoft} and ADE20K~\\cite{zhou2017scene} as represented by the columns `Entity Complexity' and `Entity Simplicity' of Table~\\ref{Tab:comp_entity_other} where an entity with a large convexity and simplicity value means it is a simple shape (and both metrics achieve their maximum value of 1.0 for a circle~\\cite{zhu2017semantic}). \nMore details about how complexity and simplicity are calculated can be found in the supplementary material.", "table_source": "\\begin{table}[t!]\n\\centering\n\\tiny\n\\begin{tabular}{c|c|c|c|c|c|c}\n\\cellcolor{lightgray!30} Dataset & \\cellcolor{lightgray!30} \\makecell{ImageRes\\\\(avg)$\\uparrow$} & \\cellcolor{lightgray!30} \\cellcolor{lightgray!30} \\makecell{EntityNum\\\\(avg)$\\uparrow$} & \\cellcolor{lightgray!30} \\makecell{EntityNum\\\\(max)$\\uparrow$} & \\cellcolor{lightgray!30} \\makecell{Entity\\\\Complexity$\\downarrow$} &\n\\cellcolor{lightgray!30} \\makecell{Entity\\\\Simplicity$\\downarrow$} &\\cellcolor{lightgray!30} \\makecell{Valid\\\\Area$\\uparrow$}  \\\\ \\hline\nCOCO & 522.5 & 11.2 & 95 & 0.758 & 0.581 & 0.891 \\\\\nADE20K & 461.3& 13.6 & \\textbf{255} & 0.802 & 0.606 & 0.914 \\\\\nEntitySeg & \\textbf{2700.7} & \\textbf{18.1} & 236 & \\textbf{0.719} & \\textbf{0.538} & \\textbf{0.999} \\\\ \\hline\n\\end{tabular}\n\\caption{The statistical comparisons between COCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene} and EntitySeg. `ImageRes (avg)', `EntityNum (avg)', `Valid Area', `Entity Complexity', and `Entity Simplicity' refer to the average value of image resolution size, entity numbers per image, valid area ratio per image, average entity complexity and simplicity respectively. `EntityNum (max)' means the maximum per-image number of entities within each dataset.}\n\\label{Tab:comp_entity_other}\n\\end{table}", "cell_list_gold": [{"value": "522.5", "char_index": [533, 538], "type": "Data Stat.", "dataset": "COCO", "attribute name": ["ImageRes (avg)", "average value of image resolution size"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "11.2", "char_index": [541, 545], "type": "Data Stat.", "dataset": "COCO", "attribute name": ["EntityNum (avg)", "entity numbers per image"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "95", "char_index": [548, 550], "type": "Data Stat.", "dataset": "COCO", "attribute name": ["EntityNum (max)", "maximum per-image number of entities"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.758", "char_index": [553, 558], "type": "Data Stat.", "dataset": "COCO", "attribute name": ["Entity Complexity", "average entity complexity"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.581", "char_index": [561, 566], "type": "Data Stat.", "dataset": "COCO", "attribute name": ["Entity Simplicity", "average entity simplicity"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.891", "char_index": [569, 574], "type": "Data Stat.", "dataset": "COCO", "attribute name": ["Valid Area", "valid area ratio per image"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "461.3", "char_index": [587, 592], "type": "Data Stat.", "dataset": "ADE20K", "attribute name": ["ImageRes (avg)", "average value of image resolution size"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "13.6", "char_index": [594, 598], "type": "Data Stat.", "dataset": "ADE20K", "attribute name": ["EntityNum (avg)", "entity numbers per image"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "255", "char_index": [609, 612], "type": "Data Stat.", "dataset": "ADE20K", "attribute name": ["EntityNum (max)", "maximum per-image number of entities"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.802", "char_index": [616, 621], "type": "Data Stat.", "dataset": "ADE20K", "attribute name": ["Entity Complexity", "average entity complexity"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.606", "char_index": [624, 629], "type": "Data Stat.", "dataset": "ADE20K", "attribute name": ["Entity Simplicity", "average entity simplicity"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.914", "char_index": [632, 637], "type": "Data Stat.", "dataset": "ADE20K", "attribute name": ["Valid Area", "valid area ratio per image"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "2700.7", "char_index": [661, 667], "type": "Data Stat.", "dataset": "EntitySeg", "attribute name": ["ImageRes (avg)", "average value of image resolution size"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "18.1", "char_index": [679, 683], "type": "Data Stat.", "dataset": "EntitySeg", "attribute name": ["EntityNum (avg)", "entity numbers per image"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "236", "char_index": [687, 690], "type": "Data Stat.", "dataset": "EntitySeg", "attribute name": ["EntityNum (max)", "maximum per-image number of entities"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.719", "char_index": [701, 706], "type": "Data Stat.", "dataset": "EntitySeg", "attribute name": ["Entity Complexity", "average entity complexity"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.538", "char_index": [718, 723], "type": "Data Stat.", "dataset": "EntitySeg", "attribute name": ["Entity Simplicity", "average entity simplicity"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}, {"value": "0.999", "char_index": [735, 740], "type": "Data Stat.", "dataset": "EntitySeg", "attribute name": ["Valid Area", "valid area ratio per image"], "sub-set/group name": "xx", "dataset features": {"xx": "yy"}}]}, "2211.05776v1_table5": {"table_code": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|ccc}\n\\cellcolor{lightgray!30} Model & \\cellcolor{lightgray!30} Backbone & \\cellcolor{lightgray!30} Pretrain & \\cellcolor{lightgray!30} AP & \\cellcolor{lightgray!30} AP$_{50}$ & \\cellcolor{lightgray!30}AP$_{75}$ \\\\ \\hline\n\\multirow{2}*{Mask-RCNN~\\cite{he2017mask}} & \\multirow{2}*{R-50} & ImageNet & 5.0 & 9.3 & 4.9  \\\\ \n& & COCO-I & 11.9 & 18.9 & 12.4 \\\\ \\hline\n\\multirow{7}*{Mask2Former~\\cite{cheng2022masked}} & \\multirow{2}*{R-50} & ImageNet & 13.0 & 19.6 & 13.3 \\\\ \n& & COCO-I & 20.3 & 29.2 & 21.0 \\\\ \\cline{2-6}\n& \\multirow{3}*{Swin-T} & COCO-E & 20.0 & 28.8 & 20.7 \\\\ \n& & COCO-I & 22.5 & 32.4 & 23.5 \\\\\n& & COCO-P & 22.7 & 32.7 & 23.5\\\\ \\cline{2-6}\n& \\multirow{2}*{Swin-L} & COCO-E & 28.0 & 39.3 & 29.4 \\\\ \n& & COCO-P & 30.3 & 42.3 & 31.6 \\\\ \\hline\n\\end{tabular}\n\\caption{Benchmark on class-aware instance segmentation in Entity Dataset. The `COCO-I' indicates weights trained in\nthe COCO datasets with instance segmentation tasks.}\n\\vspace{-0.1in}\n\\label{Tab:aba_benchmark_classaware_ins}\n\\end{table}", "table_label": "{Tab:aba_benchmark_classaware_ins}", "table_numeric_cells": [["5.0", "5.0", 362, 365, 362, 365], ["9.3", "9.3", 368, 371, 368, 371], ["4.9", "4.9", 374, 377, 374, 377], ["11.9", "11.9", 396, 400, 396, 400], ["18.9", "18.9", 403, 407, 403, 407], ["12.4", "12.4", 410, 414, 410, 414], ["13.0", "13.0", 510, 514, 510, 514], ["19.6", "19.6", 517, 521, 517, 521], ["13.3", "13.3", 524, 528, 524, 528], ["20.3", "20.3", 546, 550, 546, 550], ["29.2", "29.2", 553, 557, 553, 557], ["21.0", "21.0", 560, 564, 560, 564], ["20.0", "20.0", 615, 619, 615, 619], ["28.8", "28.8", 622, 626, 622, 626], ["20.7", "20.7", 629, 633, 629, 633], ["22.5", "22.5", 651, 655, 651, 655], ["32.4", "32.4", 658, 662, 658, 662], ["23.5", "23.5", 665, 669, 665, 669], ["22.7", "22.7", 686, 690, 686, 690], ["32.7", "32.7", 693, 697, 693, 697], ["23.5", "23.5", 700, 704, 700, 704], ["28.0", "28.0", 754, 758, 754, 758], ["39.3", "39.3", 761, 765, 761, 765], ["29.4", "29.4", 768, 772, 768, 772], ["30.3", "30.3", 790, 794, 790, 794], ["42.3", "42.3", 797, 801, 797, 801], ["31.6", "31.6", 804, 808, 804, 808]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\subsection{Image Collection}\nInspired by the collection criterion of COCO~\\cite{lin2014microsoft}, we collect most non-iconic images with more contextual information and non-canonical viewpoints. \nIn addition, the image domains should be as diversified as possible to guarantee substantial domain diversity. Therefore, the image sources of our dataset include several public datasets and the Internet where the images permitted for academic research use. For public dataset sources, we select part of the images from\nCOCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene}, \nPascal VOC~\\cite{everingham2010pascal}, Cityscapes~\\cite{cordts2016cityscapes}, \nMapillary Vistas~\\cite{neuhold2017mapillary}, ImageNet~\\cite{krizhevsky2017imagenet}, \nOpen Images~\\cite{kuznetsova2020open}, DIV2K~\\cite{agustsson2017ntire}, Flick2K~\\cite{wang2019flickr1024}, Clipart~\\cite{inoue2018cross},\nComic~\\cite{inoue2018cross}, DOTA~\\cite{xia2018dota} and some computer game screenshots from Synscapes~\\cite{wrenninge2018synscapes} and GTA5~\\cite{richter2016playing}. From the Internet, we crawled mainly high-resolution images from Pixabay~\\cite{pixabay}, Unsplash~\\cite{Unsplash}, and LAION-400M~\\cite{schuhmann2021laion}. In Figure~\\ref{fig:statistics_01}(c), we show the distribution of image sources in our dataset and see that most of the images are from high-resolution sources like Pixabay, Unsplash, and LAION-400M. In Figure~\\ref{fig:statistics_01}(a), our dataset has more high-resolution images whose resolutions are normally distributed in the resolution range between 0px to 15,000px, whereas all the images of ADE20K~\\cite{zhou2017scene} and COCO~\\cite{lin2014microsoft} are under 1000px.\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Image Encoder and Decoder} Based on learnable image-level queries $\\mathbf{Q}_{i} \\in \\mathbb{R}^{N\\times K}$,  we use image encoder $\\Theta$ and decoder $\\Phi_{i}$ to generate image-level embeddings $\\mathbf{E_i} \\in \\mathbb{R}^{N \\times 2 \\times 1 \\times 1 \\times K}$ for the full input image and its crop. \nFor the image encoder and decoder, we directly adopt the ones in standard image-level Mask2Former~\\cite{cheng2022masked} and do not modify their structures or designs. $\\mathbf{E_i}$ is obtained as follows:\n\n\\subsection{Entity Dataset Benchmark}\n\\label{subsec:entity_dataset_benchmark}\n\\paragraph{Entity Segmentation} We divide the EntitySeg dataset into train and test sets, which have 31,913 and 1,314 images. To obtain a less-biased dataset split, we constructed 20 random dataset splits. We selected the dataset split that is statistically closest to the \nall 20 splits in terms of entity segmentation accuracy, as detailed in the supplementary file.Furthermore, we use class-agnostic metric AP$^\\text{e}$~\\cite{qi2021open} with a strict non-overlapping mask constraint to evaluate the entity segmentation task.\n\nTable~\\ref{Tab:aba_benchmark_entity} shows the benchmark of our dataset with Mask-RCNN~\\cite{he2017mask}, EntityFramework~\\cite{qi2021open}, Mask2Former~\\cite{cheng2022masked}. The first two methods are convolution-based dense prediction methods, whereas the last is a transformer-based query prediction method. In this table, the transformer-based Mask2Former is better than the other two convolution-based methods, demonstrating the advantage of transform-based methods on high-quality mask generation, Moreover, the COCO-E pretrained weights can further boost entity segmentation performance. We also explored the optimal training iterations needed by Mask2Former and found that it performs the best with 3$\\times$ training schedule.\n\n\\vspace{-4mm}\n\\paragraph{Semantic Segmentation} We choose 150 categories with the highest pixel-level frequency as EntitySem for semantic segmentation. EntitySem has 9,729 and 1,444 images for training and testing, respectively. In Table~\\ref{Tab:aba_benchmark_classaware_sem}, we evaluate the two popular semantic segmentation methods DeeplabV3~\\cite{chen2017deeplab} and Mask2Former~\\cite{cheng2022masked} on EntitySem. Semantic segmentation performance is still related to the pretraining weights and network structure. Mask2Former with Swin-L backbone and COCO-E pretrained weights obtains the best mIoU of 50.5 on EntitySem. \n\n\\vspace{-4mm}\n\\paragraph{Instance Segmentation}\nWe select 206 thing categories with the highest object-level frequency in the EntityClass dataset to benchmark instance segmentation. In the EntityIns, 8,993 and 1,498 images for training and testing, respectively. In Table~\\ref{Tab:aba_benchmark_classaware_ins}, we ablate two popular instance segmentation methods including Mask-RCNN~\\cite{he2017mask} and Mask2Former~\\cite{cheng2022masked} on EntityIns. Mask2Former with Swin-L backbone and COCO-P pretrained weights the best AP of 30.3 on EntityIns. ", "table_source": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|ccc}\n\\cellcolor{lightgray!30} Model & \\cellcolor{lightgray!30} Backbone & \\cellcolor{lightgray!30} Pretrain & \\cellcolor{lightgray!30} AP & \\cellcolor{lightgray!30} AP$_{50}$ & \\cellcolor{lightgray!30}AP$_{75}$ \\\\ \\hline\n\\multirow{2}*{Mask-RCNN~\\cite{he2017mask}} & \\multirow{2}*{R-50} & ImageNet & 5.0 & 9.3 & 4.9  \\\\ \n& & COCO-I & 11.9 & 18.9 & 12.4 \\\\ \\hline\n\\multirow{7}*{Mask2Former~\\cite{cheng2022masked}} & \\multirow{2}*{R-50} & ImageNet & 13.0 & 19.6 & 13.3 \\\\ \n& & COCO-I & 20.3 & 29.2 & 21.0 \\\\ \\cline{2-6}\n& \\multirow{3}*{Swin-T} & COCO-E & 20.0 & 28.8 & 20.7 \\\\ \n& & COCO-I & 22.5 & 32.4 & 23.5 \\\\\n& & COCO-P & 22.7 & 32.7 & 23.5\\\\ \\cline{2-6}\n& \\multirow{2}*{Swin-L} & COCO-E & 28.0 & 39.3 & 29.4 \\\\ \n& & COCO-P & 30.3 & 42.3 & 31.6 \\\\ \\hline\n\\end{tabular}\n\\caption{Benchmark on class-aware instance segmentation in Entity Dataset. The `COCO-I' indicates weights trained in\nthe COCO datasets with instance segmentation tasks.}\n\\vspace{-0.1in}\n\\label{Tab:aba_benchmark_classaware_ins}\n\\end{table}", "cell_list_gold": [{"value": "5.0", "char_index": [362, 365], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "9.3", "char_index": [368, 371], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "4.9", "char_index": [374, 377], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "11.9", "char_index": [396, 400], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "18.9", "char_index": [403, 407], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "12.4", "char_index": [410, 414], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "13.0", "char_index": [510, 514], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "19.6", "char_index": [517, 521], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "13.3", "char_index": [524, 528], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "20.3", "char_index": [546, 550], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "29.2", "char_index": [553, 557], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "21.0", "char_index": [560, 564], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "20.0", "char_index": [615, 619], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-E"}}, {"value": "28.8", "char_index": [622, 626], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-E"}}, {"value": "20.7", "char_index": [629, 633], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-E"}}, {"value": "22.5", "char_index": [651, 655], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-I"}}, {"value": "32.4", "char_index": [658, 662], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-I"}}, {"value": "23.5", "char_index": [665, 669], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-I"}}, {"value": "22.7", "char_index": [686, 690], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-P"}}, {"value": "32.7", "char_index": [693, 697], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-P"}}, {"value": "23.5", "char_index": [700, 704], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-P"}}, {"value": "28.0", "char_index": [754, 758], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-E"}}, {"value": "39.3", "char_index": [761, 765], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-E"}}, {"value": "29.4", "char_index": [768, 772], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-E"}}, {"value": "30.3", "char_index": [790, 794], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-P"}}, {"value": "42.3", "char_index": [797, 801], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-P"}}, {"value": "31.6", "char_index": [804, 808], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "instance segmentation", "metric": "AP$_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-P"}}]}, "2211.05776v1_table9": {"table_code": "\\begin{tabular}{c|c}\n            \\cellcolor{lightgray!30} $\\delta$ & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            0.5 & 38.5 \\\\ \n            0.6 & 40.2  \\\\ \n            0.7 & \\textbf{41.0}  \\\\ \n            0.8 & 40.9 \\\\ \\hline\n            \\end{tabular}", "table_label": "{abl_1}", "table_numeric_cells": [["0.5", "0.5", 130, 133, 130, 133], ["38.5", "38.5", 136, 140, 136, 140], ["0.6", "0.6", 157, 160, 157, 160], ["40.2", "40.2", 163, 167, 163, 167], ["0.7", "0.7", 185, 188, 185, 188], ["41.0", "\\textbf{41.0}", 199, 203, 191, 204], ["0.8", "0.8", 222, 225, 222, 225], ["40.9", "40.9", 228, 232, 228, 232]], "text_chunk_selected": "\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Similar to video-level Mask2Former~\\cite{cheng2021mask2former}, the association module aims to generate batch queries $\\mathbf{Q_b}$ that are fully shared by the full image and its crop to represent the same entities consistently. Instead of treating $\\mathbf{Q_b}$ as learnable network parameters, here we generate $\\mathbf{Q_b}$ directly from $\\mathbf{E_i}=\\{\\mathbf{E}_{I^o}, \\mathbf{E}_{I^c})\\}$, considering that $\\mathbf{E_i}$ already contains strong segmentation features. \nIn particular, we use a Transformer architecture with cross attention (XAtt), self-attention (SAtt), and feedforward network (FFN) to obtain $\\mathbf{Q_b}$:\n\n\\vspace{-4mm}\n\\paragraph{Semantic Segmentation} We choose 150 categories with the highest pixel-level frequency as EntitySem for semantic segmentation. EntitySem has 9,729 and 1,444 images for training and testing, respectively. In Table~\\ref{Tab:aba_benchmark_classaware_sem}, we evaluate the two popular semantic segmentation methods DeeplabV3~\\cite{chen2017deeplab} and Mask2Former~\\cite{cheng2022masked} on EntitySem. Semantic segmentation performance is still related to the pretraining weights and network structure. Mask2Former with Swin-L backbone and COCO-E pretrained weights obtains the best mIoU of 50.5 on EntitySem. \n\n\\vspace{-4mm}\n\\paragraph{Panoptic Segmentation} Similar to EntityIns, we select 345 categories, including 274 things and 71 stuff, with the highest entity-level frequency to construct EntityPan. In EntityPan, there are 9,968 and 1,481 images for training and testing. Table~\\ref{Tab:aba_benchmark_classaware_panop} shows the performance of two popular panoptic segmentation methods, including PanopticFPN~\\cite{kirillov2019panopticfpn} and Mask2Former~\\cite{cheng2022masked}. In this table, we see that the PQs are much lower than those of existing panoptic datasets, due to the greater variety of class labels which makes the task more challenging.\nIn addition, current panoptic methods perform worse on EntityPan compared to existing datasets, particularly on the recognition quality (RQ) side which adversely impacts PQ.\n\nTable~\\ref{Tab:aba_ivresults} shows the overall improvement of CropFromer to the Mask2Former~\\cite{cheng2022masked} under 1$\\times$ training setting. The first and second row is our baseline Mask2Former with the single-scale inference (800 and 1040 shortest side) and (1333 and 1732 longest side) for the full images, where 1040=800$\\times\\delta$ and 1732=1333$\\times\\delta$ with $\\delta$ 0.7 in default. For the results in the third and fourth rows, we use test-time bipartite-matching to associate the same entities obtained with multi-scale images and crops. We find no improvements with such inference strategies. Whereas, using the crop output from CropFormer's batch decoder achieves a significant AP$^\\text{e}$ gain as indicated by the second last row. By combining the full image and four crop outputs from CropFormer (final row), we obtain even stronger 1.5 AP$^\\text{e}$ and 1.7 AP$^\\text{e}_{75}$ gain compared to the baseline (first row).\n\n\\vspace{-4mm}\n\\paragraph{Other Batch Fusion Design} Table~\\ref{Tab:aba_fusion_style} shows the ablation study with different fusion designs between original images and cropped patches. Directly using video-level Mask2former or VITA fusion method merely brings marginal performance gains.\n\n\\vspace{-4mm}\n\\paragraph{CropDataloader} Table~\\ref{abl_1}(a) and (b) show the ablation study on the usage of crop ratio and crop type. As indicated by Table~\\ref{abl_1}(a), CropFormer obtains the best performance with a crop ratio $\\delta$ of 0.7. Smaller crop ratios perform worse and it might be a problem caused by queries that are not robust against drastic image transformation, which we leave for future work. Table~\\ref{abl_1}(b) shows the impacts of the type of crop and the number of crops. 4 and 8 fixed crops during training and testing performs the best.\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Table~\\ref{abl_1}(c) shows the ablation study on the structure of the association module. There is a slight performance difference between using self-attention or a feedforward module following cross-attention.", "table_source": "\\begin{table}[t!]\n\\begin{minipage}{\\textwidth}\n\\begin{minipage}[t]{0.08\\textwidth}\n            \\centering\n            \\scriptsize\n             \\setlength{\\tabcolsep}{2pt}\n            \\begin{tabular}{c|c}\n            \\cellcolor{lightgray!30} $\\delta$ & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            0.5 & 38.5 \\\\ \n            0.6 & 40.2  \\\\ \n            0.7 & \\textbf{41.0}  \\\\ \n            0.8 & 40.9 \\\\ \\hline\n            \\end{tabular}\n        \n        (a)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.22\\textwidth}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{c|c|c}\n        \n            \\cellcolor{lightgray!30} Train & \\cellcolor{lightgray!30} Test & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            Random & Fixed (4) & 39.7\\\\ \n            Fixed (4) & Fixed (4)  & 41.0  \\\\\n            Fixed (4) & Fixed (8)  & \\textbf{41.3} \\\\ \n            Fixed (8) & Fixed (8)  & 41.0 \\\\ \\hline\n        \\end{tabular}\n        \n    (b)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.15\\textwidth}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{ccc|c}\n            \\cellcolor{lightgray!30} XAtt & \\cellcolor{lightgray!30} SAtt & \\cellcolor{lightgray!30} FFN & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            \\checkmark & $\\circ$ & $\\circ$ &  40.7 \\\\ \n            \\checkmark & $\\circ$ & \\checkmark & 40.8  \\\\ \n            \\checkmark & \\checkmark & $\\circ$ & 40.8 \\\\\n            \\checkmark & \\checkmark & \\checkmark & \\textbf{41.0} \\\\ \\hline\n        \\end{tabular}\n        \n    (c)\n    \\end{minipage}\n\\end{minipage}\n\\caption{Ablation study on the usage of crop ratio $\\delta$, crop type and association module in CropFormer. In sub-table (b), `Random' indicates random crops and `Fixed (4/8)' indicates 4 or 8 fixed corner crops. In sub-table(c), \\checkmark and $\\circ$ means whether we use the module or not.}\n\\vspace{-0.1in}\n\\label{abl_1}\n\\end{table}", "cell_list_gold": [{"value": "0.5", "char_index": [130, 133], "type": "Hyper-parameter/Architecture", "model": "CropFormer", "parameter/architecture name": "crop ratio", "dataset": "EntitySeg"}, {"value": "38.5", "char_index": [136, 140], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"xx": "yy"}, "model": "CropFormer", "model settings": {"crop ratio": "0.5"}}, {"value": "0.6", "char_index": [157, 160], "type": "Hyper-parameter/Architecture", "model": "CropFormer", "parameter/architecture name": "crop ratio", "dataset": "EntitySeg"}, {"value": "40.2", "char_index": [163, 167], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"xx": "yy"}, "model": "CropFormer", "model settings": {"crop ratio": "0.6"}}, {"value": "0.7", "char_index": [185, 188], "type": "Hyper-parameter/Architecture", "model": "CropFormer", "parameter/architecture name": "crop ratio", "dataset": "EntitySeg"}, {"value": "41.0", "char_index": [199, 203], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"xx": "yy"}, "model": "CropFormer", "model settings": {"crop ratio": "0.7"}}, {"value": "0.8", "char_index": [222, 225], "type": "Hyper-parameter/Architecture", "model": "CropFormer", "parameter/architecture name": "crop ratio", "dataset": "EntitySeg"}, {"value": "40.9", "char_index": [228, 232], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"xx": "yy"}, "model": "CropFormer", "model settings": {"crop ratio": "0.8"}}]}}