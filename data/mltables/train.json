{"2211.05596v1_table1": {"table_code": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}lcc@{}}\n\\toprule\nModel              & SGD  & Assistant \\\\ \\midrule\nBERT-Large         & 0.88 & 0.91      \\\\\\midrule\nMegatron-GPT - 345M & 0.87 & 0.88      \\\\\nMegatron-GPT - 1.3B & 0.91 & 0.92      \\\\ \nMegatron-GPT - 5B   & 0.95 & 0.94      \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Classification Accuracy on test sets of the SGD and Assistant datasets}\n\\label{tab:indomain}\n\\end{table}", "table_label": "{tab:indomain}", "table_numeric_cells": [["0.88", "0.88", 135, 139, 135, 139], ["0.91", "0.91", 142, 146, 142, 146], ["0.87", "0.87", 185, 189, 185, 189], ["0.88", "0.88", 192, 196, 192, 196], ["0.91", "0.91", 227, 231, 227, 231], ["0.92", "0.92", 234, 238, 234, 238], ["0.95", "0.95", 270, 274, 270, 274], ["0.94", "0.94", 277, 281, 277, 281]], "text_chunk_selected": "We observe that using such \\textit{canonical forms} as labels for the intent classification task allows the model to generalize better to domains that are adjacent, but not seen at train time (\\textit{e.g.}, \\textit{Flight Reservations} $\\rightarrow$ \\textit{Bus Bookings}). We also find that it is beneficial to do a two-stage P-tuning for domain adaption, \\textit{i.e.}, once we have a p-tuned large language model on a wide set of domains, we can continue p-tuning this model on a small set of labelled samples from the target domain to allow the model to generalize better. We find that this few-shot approach works very well and this has promising implications for developers for dialogue systems; with minimal effort it would be feasible to adapt an existing model pre-trained on multiple domains to a new domain. In summary, our contributions are:\n\n\\subsection{Datasets}\n\\label{sec:datasets}\nWe consider two widely known datasets in the dialogue community, the Schema Guided Dialogue (SGD) dataset \\citep{sgd-dataset} and the Virtual Assistant dataset \\citep{assistant-dataset}.\\\\\\\\\n\\textbf{Schema Guided Dialogue} - This dataset covers 16 domains and has over 16k annotated conversations. The domains span a variety of user actions, including setting calendars and alarms, travel booking (car rentals, flights, buses and trains), music, weather, movies, and more. The dataset also contains \\textit{multi-domain} dialogues where the utterances switch between domains. For the purpose of our experiments, we consider only the \\textit{single-domain} dialogues with 37 intents across all utterances. \\\\\\\\\n\\textbf{Virtual Assistant Dataset} - This dataset covers 21 domains with 64 intents across all utterances. As the name suggests, the domains relate to user queries over a wide range of topics, including operating smart-home devices, media consumption, weather and travel. It has over 25k annotated user utterances that identify intents and slot values.\n\n\\begin{itemize}\n    \\item Using Fasttext Embeddings \\citep{bojanowski2016enriching}: We take the mean of all the embedding vectors of the generated canonical form and consider the vector obtained to be the representation of the whole sequence. We compute similar vectors for all the canonical form labels and consider the canonical form label that has the maximum cosine similarity with the generated one as the model's prediction. \n    \\item Using Sentence Transformers \\citep{reimers-2019-sentence-bert}: We use the \\textit{miniLM-QA} \\citep{minilm} transformer model that has been pretrained on multiple datasets on the text entailment/semantic search task, \\textit{i.e.}, given a query and a set of keys (documents/labels), it ranks the keys in order of relevance. We give as input to the model the generated canonical form (query) and the list of canonical form labels (keys). The model then returns the closest canonical form label to the generated canonical form which we consider as the prediction.  \n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{BERT-based finetuned model} (Intent Classification): We finetune BERT models on the datasets described in section \\ref{sec:datasets}. While some of the Megatron-GPT models we use are larger than the BERT model in terms of number of parameters, it should be noted that the LM parameters are frozen during the training stage of p-tuning and only the weights of the LSTM (~14M parameters) are updated.\n\\end{itemize}\n\n\\subsubsection{In-Domain}\nThis setting corresponds to the traditional dataset splits where the train and test sets come from similar distributions. We p-tune the Megatron-GPT models on the train set and evaluate them on the test set for intent classification.\n\n\\begin{itemize}\n    \\item \\textbf{Schema Guided Dialogue} (SGD): We hold out utterances corresponding to \\textit{bus bookings} and \\textit{hotel reservations} to form our test set. The train set includes utterances from adjacent domains: flight booking and restaurant reservations. This should be a relatively easy setting for the language model to generalize to.\n    \\item \\textbf{Virtual Assistant}: To make things more challenging, we hold out utterances corresponding to \\textit{operating IOT devices} and \\textit{media consumption commands} (\\textit{e.g.}, commands that are variants of \"play\" - play movie, play audiobook). The train set does not have utterances from similar domains and this setting is more challenging for the model.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Zero-shot}: P-tune the model on the train set and evaluate zero-shot on the unseen domain test set.\n    \\item \\textbf{Few-shot}: After p-tuning on the train set, we do a second stage p-tuning on a set of \\textit{k} samples from the target domain. Unless otherwise noted, \\textit{k} here is 5, 10, 50 or 100 samples. \n\\end{itemize}\n\n\\subsubsection{In-domain}\nWe find that both the p-tuned GPT model as well as the BERT baseline perform very well on the standard in-domain split where both the train and test set come from the same distribution (Table \\ref{tab:indomain}). The classification accuracy of Megatron-GPT increases as we increase the model size. The trend of results remains consistent for both the SGD and Assistant datasets. ", "table_source": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}lcc@{}}\n\\toprule\nModel              & SGD  & Assistant \\\\ \\midrule\nBERT-Large         & 0.88 & 0.91      \\\\\\midrule\nMegatron-GPT - 345M & 0.87 & 0.88      \\\\\nMegatron-GPT - 1.3B & 0.91 & 0.92      \\\\ \nMegatron-GPT - 5B   & 0.95 & 0.94      \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Classification Accuracy on test sets of the SGD and Assistant datasets}\n\\label{tab:indomain}\n\\end{table}", "cell_list_gold": [{"value": "0.88", "char_index": [135, 139], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT-Large", "model settings": {"xx": "yy"}}, {"value": "0.91", "char_index": [142, 146], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant"], "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": "BERT-Large", "model settings": {"xx": "yy"}}, {"value": "0.87", "char_index": [185, 189], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["Megatron-GPT - 345M", "Megatron-GPT"], "model settings": {"model size": "345M"}}, {"value": "0.88", "char_index": [192, 196], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant"], "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["Megatron-GPT - 345M", "Megatron-GPT"], "model settings": {"model size": "345M"}}, {"value": "0.91", "char_index": [227, 231], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["Megatron-GPT - 1.3B", "Megatron-GPT"], "model settings": {"model size": "1.3B"}}, {"value": "0.92", "char_index": [234, 238], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant"], "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["Megatron-GPT - 1.3B", "Megatron-GPT"], "model settings": {"model size": "1.3B"}}, {"value": "0.95", "char_index": [270, 274], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["Megatron-GPT - 5B", "Megatron-GPT"], "model settings": {"model size": "5B"}}, {"value": "0.94", "char_index": [277, 281], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant"], "test data/set": ["Assistant", "Virtual Assistant"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"xx": "yy"}, "model": ["Megatron-GPT - 5B", "Megatron-GPT"], "model settings": {"model size": "5B"}}]}, "2211.05596v1_table3": {"table_code": "\\begin{table}[!htbp]\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}lllllll@{}}\n\\toprule\nMode                    & \\multicolumn{3}{l}{IOT devices} & \\multicolumn{3}{l}{Media Consumption} \\\\ \\midrule\n                        & 345M      & 1.3B     & 5B       & 345M        & 1.3B       & 5B         \\\\ \\midrule\nZero Shot               & 0.096     & 0.011    & 0.022    & 0.037       & 0.008      & 0.012      \\\\\nFS - 10 samples & 0.62      & 0.71     & 0.75     & 0.58            & 0.62           & 0.68            \\\\\nFS - 50 samples & 0.69     & 0.83     & 0.87     & 0.67            & 0.86           & 0.89          \\\\ \\bottomrule     \n\\end{tabular}\n}\n\\caption{Zero-shot and Few Shot (FS) performance on the held out domains of the Assistant dataset. The columns indicate the size of the Megatron-GPT model.}\n\\label{tab:ood-asst}\n\\end{table}", "table_label": "{tab:ood-asst}", "table_numeric_cells": [["345M", "345M", 226, 230, 226, 230], ["1.3B", "1.3B", 238, 242, 238, 242], ["5B", "5B", 249, 251, 249, 251], ["345M", "345M", 260, 264, 260, 264], ["1.3B", "1.3B", 274, 278, 274, 278], ["5B", "5B", 287, 289, 287, 289], ["0.096", "0.096", 336, 341, 336, 341], ["0.011", "0.011", 348, 353, 348, 353], ["0.022", "0.022", 359, 364, 359, 364], ["0.037", "0.037", 370, 375, 370, 375], ["0.008", "0.008", 384, 389, 384, 389], ["0.012", "0.012", 397, 402, 397, 402], ["0.62", "0.62", 429, 433, 429, 433], ["0.71", "0.71", 441, 445, 441, 445], ["0.75", "0.75", 452, 456, 452, 456], ["0.58", "0.58", 463, 467, 463, 467], ["0.62", "0.62", 481, 485, 481, 485], ["0.68", "0.68", 498, 502, 498, 502], ["0.69", "0.69", 535, 539, 535, 539], ["0.83", "0.83", 546, 550, 546, 550], ["0.87", "0.87", 557, 561, 557, 561], ["0.67", "0.67", 568, 572, 568, 572], ["0.86", "0.86", 586, 590, 586, 590], ["0.89", "0.89", 603, 607, 603, 607]], "text_chunk_selected": "\\begin{itemize}\n\\item We cast the problem of intent classification into a generative approach and rewrite intent labels in a more descriptive format (\\textit{canonical forms}).\n    \\item When using such canonical forms, generative approaches with Large Language Models(LLMs) show promising results when compared with traditional methods for intent classification.\n\\item Generative models generalize very well to unseen domains in zero-shot and few-shot settings when compared with BERT-style approaches. \n\\item We demonstrate the sample efficiency of p-tuning LLMs where we can achieve close to full dataset performance with a fraction of the data.\n\\end{itemize}\n\n\\subsection{Datasets}\n\\label{sec:datasets}\nWe consider two widely known datasets in the dialogue community, the Schema Guided Dialogue (SGD) dataset \\citep{sgd-dataset} and the Virtual Assistant dataset \\citep{assistant-dataset}.\\\\\\\\\n\\textbf{Schema Guided Dialogue} - This dataset covers 16 domains and has over 16k annotated conversations. The domains span a variety of user actions, including setting calendars and alarms, travel booking (car rentals, flights, buses and trains), music, weather, movies, and more. The dataset also contains \\textit{multi-domain} dialogues where the utterances switch between domains. For the purpose of our experiments, we consider only the \\textit{single-domain} dialogues with 37 intents across all utterances. \\\\\\\\\n\\textbf{Virtual Assistant Dataset} - This dataset covers 21 domains with 64 intents across all utterances. As the name suggests, the domains relate to user queries over a wide range of topics, including operating smart-home devices, media consumption, weather and travel. It has over 25k annotated user utterances that identify intents and slot values.\n\n\\begin{itemize}\n    \\item Using Fasttext Embeddings \\citep{bojanowski2016enriching}: We take the mean of all the embedding vectors of the generated canonical form and consider the vector obtained to be the representation of the whole sequence. We compute similar vectors for all the canonical form labels and consider the canonical form label that has the maximum cosine similarity with the generated one as the model's prediction. \n    \\item Using Sentence Transformers \\citep{reimers-2019-sentence-bert}: We use the \\textit{miniLM-QA} \\citep{minilm} transformer model that has been pretrained on multiple datasets on the text entailment/semantic search task, \\textit{i.e.}, given a query and a set of keys (documents/labels), it ranks the keys in order of relevance. We give as input to the model the generated canonical form (query) and the list of canonical form labels (keys). The model then returns the closest canonical form label to the generated canonical form which we consider as the prediction.  \n\\end{itemize}\n\n\\subsubsection{Out-of-Domain}\nIn this setting, we aim to explore the generalization capability of LLMs. We hold out certain domains from the train set and use utterances from the held out domains as our test. This helps us understand how well these LLMs can generalize to unseen domains. The held out sets that we consider are:\n\n\\begin{itemize}\n    \\item \\textbf{Schema Guided Dialogue} (SGD): We hold out utterances corresponding to \\textit{bus bookings} and \\textit{hotel reservations} to form our test set. The train set includes utterances from adjacent domains: flight booking and restaurant reservations. This should be a relatively easy setting for the language model to generalize to.\n    \\item \\textbf{Virtual Assistant}: To make things more challenging, we hold out utterances corresponding to \\textit{operating IOT devices} and \\textit{media consumption commands} (\\textit{e.g.}, commands that are variants of \"play\" - play movie, play audiobook). The train set does not have utterances from similar domains and this setting is more challenging for the model.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Zero-shot}: P-tune the model on the train set and evaluate zero-shot on the unseen domain test set.\n    \\item \\textbf{Few-shot}: After p-tuning on the train set, we do a second stage p-tuning on a set of \\textit{k} samples from the target domain. Unless otherwise noted, \\textit{k} here is 5, 10, 50 or 100 samples. \n\\end{itemize}\n\n\\subsubsection{Out-of-Domain}\nThe out-of-domain setting is where the advantage of using a LLM becomes apparent. It is not feasible to expect a finetuned BERT model to generalize to an unseen domain not present in the train set. Such models continue to predict that the intent belongs to one of the intent labels they see during training. The p-tuned Megatron-GPT models, on the other hand, show impressive zero-shot and few-shot generalization capabilities on the SGD dataset (Table \\ref{tab:ood-sgd}). For instance, having seen intents such as \\textit{\"buy flight roundtrip tickets\"} when presented with utterances for \\textit{Flight Reservations} in training, we can expect the model to reasonably generalize to utterances from \\textit{Bus Reservations} with utterances like \"Get me a return trip on the bus\" with the model's prediction for the intent being \\textit{\"buy bus roundtrip tickets\"}.\n\nIn the Assistant dataset, the p-tuned models face the same issue as the BERT models: they struggle to generalize to completely unseen domains and the performance is close to random (Table \\ref{tab:ood-asst}). Unlike in SGD, the held-out domains do not have sufficiently similar domains in training from which to generalize. However, the few-shot setting holds promise as the performance of the models improves with few samples. Since the held out domains have far more intents compared to the held out domains from the SGD dataset, we employ stratified sampling to ensure that the few-shot examples are representative of all intents in the domain. ", "table_source": "\\begin{table}[!htbp]\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}lllllll@{}}\n\\toprule\nMode                    & \\multicolumn{3}{l}{IOT devices} & \\multicolumn{3}{l}{Media Consumption} \\\\ \\midrule\n                        & 345M      & 1.3B     & 5B       & 345M        & 1.3B       & 5B         \\\\ \\midrule\nZero Shot               & 0.096     & 0.011    & 0.022    & 0.037       & 0.008      & 0.012      \\\\\nFS - 10 samples & 0.62      & 0.71     & 0.75     & 0.58            & 0.62           & 0.68            \\\\\nFS - 50 samples & 0.69     & 0.83     & 0.87     & 0.67            & 0.86           & 0.89          \\\\ \\bottomrule     \n\\end{tabular}\n}\n\\caption{Zero-shot and Few Shot (FS) performance on the held out domains of the Assistant dataset. The columns indicate the size of the Megatron-GPT model.}\n\\label{tab:ood-asst}\n\\end{table}", "cell_list_gold": [{"value": "345M", "char_index": [226, 230], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "1.3B", "char_index": [238, 242], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "5B", "char_index": [249, 251], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "345M", "char_index": [260, 264], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "1.3B", "char_index": [274, 278], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "5B", "char_index": [287, 289], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["Assistant", "Virtual Assistant"]}, {"value": "0.096", "char_index": [336, 341], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "Zero Shot", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.011", "char_index": [348, 353], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "Zero Shot", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.022", "char_index": [359, 364], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "Zero Shot", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "0.037", "char_index": [370, 375], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "Zero Shot", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.008", "char_index": [384, 389], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "Zero Shot", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.012", "char_index": [397, 402], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "Zero Shot", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "0.62", "char_index": [429, 433], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 10 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.71", "char_index": [441, 445], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 10 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.75", "char_index": [452, 456], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 10 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "0.58", "char_index": [463, 467], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 10 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.62", "char_index": [481, 485], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 10 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.68", "char_index": [498, 502], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 10 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "0.69", "char_index": [535, 539], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 50 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.83", "char_index": [546, 550], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 50 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.87", "char_index": [557, 561], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant IOT devices", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 50 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "0.67", "char_index": [568, 572], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 50 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.86", "char_index": [586, 590], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 50 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.89", "char_index": [603, 607], "type": "Result", "training data/set": ["Assistant", "Virtual Assistant", "train"], "test data/set": "Assistant Media Consumption", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS - 50 samples", "out-of-domain": "true"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}]}, "2211.05596v1_table4": {"table_code": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}lccc@{}}\n\\toprule\n\\textbf{Mode }         & \\multicolumn{3}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n              & \\textbf{345M }    & \\textbf{1.3B    }&\\textbf{ 5B }     \\\\ \\midrule\nZS - Original & 0.08     & 0.13    & 0.21    \\\\ \nZS - Modified & 0.755    & 0.762   & 0.787   \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Zero shot (ZS) performance on utterances from \\textit{Bus Bookings}. \\textbf{Original} refers to having the canonical form for flight bookings as \\textit{search tickets for flight one way} which led to incorrect generalizations. \\textbf{Modified} refers to having the improved canonical form for flight bookings as \\textit{search for flights one way}.  }\n\\label{tab:ood-sgd-original}\n\\end{table}", "table_label": "{tab:ood-sgd-original}", "table_numeric_cells": [["345M", "\\textbf{345M }", 164, 168, 156, 170], ["1.3B", "\\textbf{1.3B    }", 184, 188, 176, 193], ["5B", "\\textbf{ 5B }", 203, 205, 194, 207], ["0.08", "0.08", 240, 244, 240, 244], ["0.13", "0.13", 251, 255, 251, 255], ["0.21", "0.21", 261, 265, 261, 265], ["0.755", "0.755", 289, 294, 289, 294], ["0.762", "0.762", 300, 305, 300, 305], ["0.787", "0.787", 310, 315, 310, 315]], "text_chunk_selected": "\\subsection{Datasets}\n\\label{sec:datasets}\nWe consider two widely known datasets in the dialogue community, the Schema Guided Dialogue (SGD) dataset \\citep{sgd-dataset} and the Virtual Assistant dataset \\citep{assistant-dataset}.\\\\\\\\\n\\textbf{Schema Guided Dialogue} - This dataset covers 16 domains and has over 16k annotated conversations. The domains span a variety of user actions, including setting calendars and alarms, travel booking (car rentals, flights, buses and trains), music, weather, movies, and more. The dataset also contains \\textit{multi-domain} dialogues where the utterances switch between domains. For the purpose of our experiments, we consider only the \\textit{single-domain} dialogues with 37 intents across all utterances. \\\\\\\\\n\\textbf{Virtual Assistant Dataset} - This dataset covers 21 domains with 64 intents across all utterances. As the name suggests, the domains relate to user queries over a wide range of topics, including operating smart-home devices, media consumption, weather and travel. It has over 25k annotated user utterances that identify intents and slot values.\n\n\\begin{itemize}\n    \\item Using Fasttext Embeddings \\citep{bojanowski2016enriching}: We take the mean of all the embedding vectors of the generated canonical form and consider the vector obtained to be the representation of the whole sequence. We compute similar vectors for all the canonical form labels and consider the canonical form label that has the maximum cosine similarity with the generated one as the model's prediction. \n    \\item Using Sentence Transformers \\citep{reimers-2019-sentence-bert}: We use the \\textit{miniLM-QA} \\citep{minilm} transformer model that has been pretrained on multiple datasets on the text entailment/semantic search task, \\textit{i.e.}, given a query and a set of keys (documents/labels), it ranks the keys in order of relevance. We give as input to the model the generated canonical form (query) and the list of canonical form labels (keys). The model then returns the closest canonical form label to the generated canonical form which we consider as the prediction.  \n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{BERT-based finetuned model} (Intent Classification): We finetune BERT models on the datasets described in section \\ref{sec:datasets}. While some of the Megatron-GPT models we use are larger than the BERT model in terms of number of parameters, it should be noted that the LM parameters are frozen during the training stage of p-tuning and only the weights of the LSTM (~14M parameters) are updated.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Schema Guided Dialogue} (SGD): We hold out utterances corresponding to \\textit{bus bookings} and \\textit{hotel reservations} to form our test set. The train set includes utterances from adjacent domains: flight booking and restaurant reservations. This should be a relatively easy setting for the language model to generalize to.\n    \\item \\textbf{Virtual Assistant}: To make things more challenging, we hold out utterances corresponding to \\textit{operating IOT devices} and \\textit{media consumption commands} (\\textit{e.g.}, commands that are variants of \"play\" - play movie, play audiobook). The train set does not have utterances from similar domains and this setting is more challenging for the model.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Zero-shot}: P-tune the model on the train set and evaluate zero-shot on the unseen domain test set.\n    \\item \\textbf{Few-shot}: After p-tuning on the train set, we do a second stage p-tuning on a set of \\textit{k} samples from the target domain. Unless otherwise noted, \\textit{k} here is 5, 10, 50 or 100 samples. \n\\end{itemize}\n\n\\subsubsection{Out-of-Domain}\nThe out-of-domain setting is where the advantage of using a LLM becomes apparent. It is not feasible to expect a finetuned BERT model to generalize to an unseen domain not present in the train set. Such models continue to predict that the intent belongs to one of the intent labels they see during training. The p-tuned Megatron-GPT models, on the other hand, show impressive zero-shot and few-shot generalization capabilities on the SGD dataset (Table \\ref{tab:ood-sgd}). For instance, having seen intents such as \\textit{\"buy flight roundtrip tickets\"} when presented with utterances for \\textit{Flight Reservations} in training, we can expect the model to reasonably generalize to utterances from \\textit{Bus Reservations} with utterances like \"Get me a return trip on the bus\" with the model's prediction for the intent being \\textit{\"buy bus roundtrip tickets\"}.\n\n\\subsection{How important is framing the right canonical form?}\nThe phrasing of canonical forms has a significant impact on zero-shot cross domain generalization. In our initial experiments, we observed that the language models, especially the smaller ones, sometimes rely on spurious correlations to predict the intent. For instance, if the intent \\textit{SearchFlightOneWay} is mapped to the canonical form \\textit{search tickets for flight one way}, the model correlates the word \\textit{ticket} in both the user utterance and canonical form to identify the intent. When we use this model to predict the intent of user utterances related to \\textit{bus bookings} in a zero-shot manner, the model predicts that that the intent is related to a \\textit{flight booking} as most utterances in the \\textit{bus domain} contain the word \\textit{ticket}. \n\nRephrasing the canonical form for the intent \\textit{SearchFlightOneWay} to \\textit{search for flights one way} helps the model to avoid making the spurious correlation and the performance in the zero-shot setting (Table \\ref{tab:ood-sgd-original}) is significantly improved.", "table_source": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}lccc@{}}\n\\toprule\n\\textbf{Mode }         & \\multicolumn{3}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n              & \\textbf{345M }    & \\textbf{1.3B    }&\\textbf{ 5B }     \\\\ \\midrule\nZS - Original & 0.08     & 0.13    & 0.21    \\\\ \nZS - Modified & 0.755    & 0.762   & 0.787   \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Zero shot (ZS) performance on utterances from \\textit{Bus Bookings}. \\textbf{Original} refers to having the canonical form for flight bookings as \\textit{search tickets for flight one way} which led to incorrect generalizations. \\textbf{Modified} refers to having the improved canonical form for flight bookings as \\textit{search for flights one way}.  }\n\\label{tab:ood-sgd-original}\n\\end{table}", "cell_list_gold": [{"value": "345M", "char_index": [164, 168], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "1.3B", "char_index": [184, 188], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "5B", "char_index": [203, 205], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "0.08", "char_index": [240, 244], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "ZS - Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.13", "char_index": [251, 255], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "ZS - Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.21", "char_index": [261, 265], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "ZS - Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "0.755", "char_index": [289, 294], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "ZS - Modified"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.762", "char_index": [300, 305], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "ZS - Modified"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.787", "char_index": [310, 315], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "ZS - Modified"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}]}, "2211.05596v1_table5": {"table_code": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}lccc@{}}\n\\toprule\n\\textbf{Mode }         & \\multicolumn{2}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n              & \\textbf{345M }    & \\textbf{1.3B    }     \\\\ \\midrule\nZS - Original & 0.08     & 0.13       \\\\ \nFS 10 samples- Original & 0.76     & 0.72       \\\\ \nFS 20 samples - Original & 0.84    & 0.87     \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Zero shot (ZS) performance on utterances from \\textit{Bus Bookings}. \\textbf{Original} refers to having the canonical form for flight bookings as \\textit{search tickets for flight one way} which led to incorrect generalizations. Adding a small number of examples resolves the error.}\n\\label{tab:ood-sgd-original-fs}\n\\end{table}", "table_label": "{tab:ood-sgd-original-fs}", "table_numeric_cells": [["345M", "\\textbf{345M }", 164, 168, 156, 170], ["1.3B", "\\textbf{1.3B    }", 184, 188, 176, 193], ["0.08", "0.08", 226, 230, 226, 230], ["0.13", "0.13", 237, 241, 237, 241], ["0.76", "0.76", 278, 282, 278, 282], ["0.72", "0.72", 289, 293, 289, 293], ["0.84", "0.84", 331, 335, 331, 335], ["0.87", "0.87", 341, 345, 341, 345]], "text_chunk_selected": "\\subsection{Datasets}\n\\label{sec:datasets}\nWe consider two widely known datasets in the dialogue community, the Schema Guided Dialogue (SGD) dataset \\citep{sgd-dataset} and the Virtual Assistant dataset \\citep{assistant-dataset}.\\\\\\\\\n\\textbf{Schema Guided Dialogue} - This dataset covers 16 domains and has over 16k annotated conversations. The domains span a variety of user actions, including setting calendars and alarms, travel booking (car rentals, flights, buses and trains), music, weather, movies, and more. The dataset also contains \\textit{multi-domain} dialogues where the utterances switch between domains. For the purpose of our experiments, we consider only the \\textit{single-domain} dialogues with 37 intents across all utterances. \\\\\\\\\n\\textbf{Virtual Assistant Dataset} - This dataset covers 21 domains with 64 intents across all utterances. As the name suggests, the domains relate to user queries over a wide range of topics, including operating smart-home devices, media consumption, weather and travel. It has over 25k annotated user utterances that identify intents and slot values.\n\n\\begin{itemize}\n    \\item Using Fasttext Embeddings \\citep{bojanowski2016enriching}: We take the mean of all the embedding vectors of the generated canonical form and consider the vector obtained to be the representation of the whole sequence. We compute similar vectors for all the canonical form labels and consider the canonical form label that has the maximum cosine similarity with the generated one as the model's prediction. \n    \\item Using Sentence Transformers \\citep{reimers-2019-sentence-bert}: We use the \\textit{miniLM-QA} \\citep{minilm} transformer model that has been pretrained on multiple datasets on the text entailment/semantic search task, \\textit{i.e.}, given a query and a set of keys (documents/labels), it ranks the keys in order of relevance. We give as input to the model the generated canonical form (query) and the list of canonical form labels (keys). The model then returns the closest canonical form label to the generated canonical form which we consider as the prediction.  \n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Schema Guided Dialogue} (SGD): We hold out utterances corresponding to \\textit{bus bookings} and \\textit{hotel reservations} to form our test set. The train set includes utterances from adjacent domains: flight booking and restaurant reservations. This should be a relatively easy setting for the language model to generalize to.\n    \\item \\textbf{Virtual Assistant}: To make things more challenging, we hold out utterances corresponding to \\textit{operating IOT devices} and \\textit{media consumption commands} (\\textit{e.g.}, commands that are variants of \"play\" - play movie, play audiobook). The train set does not have utterances from similar domains and this setting is more challenging for the model.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Zero-shot}: P-tune the model on the train set and evaluate zero-shot on the unseen domain test set.\n    \\item \\textbf{Few-shot}: After p-tuning on the train set, we do a second stage p-tuning on a set of \\textit{k} samples from the target domain. Unless otherwise noted, \\textit{k} here is 5, 10, 50 or 100 samples. \n\\end{itemize}\n\n\\subsubsection{Out-of-Domain}\nThe out-of-domain setting is where the advantage of using a LLM becomes apparent. It is not feasible to expect a finetuned BERT model to generalize to an unseen domain not present in the train set. Such models continue to predict that the intent belongs to one of the intent labels they see during training. The p-tuned Megatron-GPT models, on the other hand, show impressive zero-shot and few-shot generalization capabilities on the SGD dataset (Table \\ref{tab:ood-sgd}). For instance, having seen intents such as \\textit{\"buy flight roundtrip tickets\"} when presented with utterances for \\textit{Flight Reservations} in training, we can expect the model to reasonably generalize to utterances from \\textit{Bus Reservations} with utterances like \"Get me a return trip on the bus\" with the model's prediction for the intent being \\textit{\"buy bus roundtrip tickets\"}.\n\n\\subsection{How important is framing the right canonical form?}\nThe phrasing of canonical forms has a significant impact on zero-shot cross domain generalization. In our initial experiments, we observed that the language models, especially the smaller ones, sometimes rely on spurious correlations to predict the intent. For instance, if the intent \\textit{SearchFlightOneWay} is mapped to the canonical form \\textit{search tickets for flight one way}, the model correlates the word \\textit{ticket} in both the user utterance and canonical form to identify the intent. When we use this model to predict the intent of user utterances related to \\textit{bus bookings} in a zero-shot manner, the model predicts that that the intent is related to a \\textit{flight booking} as most utterances in the \\textit{bus domain} contain the word \\textit{ticket}. \n\nRephrasing the canonical form for the intent \\textit{SearchFlightOneWay} to \\textit{search for flights one way} helps the model to avoid making the spurious correlation and the performance in the zero-shot setting (Table \\ref{tab:ood-sgd-original}) is significantly improved.\n\nHowever, the few-shot setting (Table \\ref{tab:ood-sgd-original-fs}) alleviates this problem of sensitivity of the model to the canonical form structure. When we provide the model with a few samples from the the target domain, it learns to associate that the important words to distinguish between the domains are \\textit{flight} and \\textit{bus} and not \\textit{ticket}.", "table_source": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{@{}lccc@{}}\n\\toprule\n\\textbf{Mode }         & \\multicolumn{2}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n              & \\textbf{345M }    & \\textbf{1.3B    }     \\\\ \\midrule\nZS - Original & 0.08     & 0.13       \\\\ \nFS 10 samples- Original & 0.76     & 0.72       \\\\ \nFS 20 samples - Original & 0.84    & 0.87     \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Zero shot (ZS) performance on utterances from \\textit{Bus Bookings}. \\textbf{Original} refers to having the canonical form for flight bookings as \\textit{search tickets for flight one way} which led to incorrect generalizations. Adding a small number of examples resolves the error.}\n\\label{tab:ood-sgd-original-fs}\n\\end{table}", "cell_list_gold": [{"value": "345M", "char_index": [164, 168], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "1.3B", "char_index": [184, 188], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "0.08", "char_index": [226, 230], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "ZS - Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.13", "char_index": [237, 241], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "ZS - Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.76", "char_index": [278, 282], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS 10 samples- Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.72", "char_index": [289, 293], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS 10 samples- Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.84", "char_index": [331, 335], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS 20 samples - Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.87", "char_index": [341, 345], "type": "Result", "training data/set": ["SGD", "Schema Guided Dialogue", "flight booking restaurant reservations"], "test data/set": "SGD Bus Bookings", "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"Mode": "FS 20 samples - Original"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}]}, "2211.05596v1_table6": {"table_code": "\\begin{table}[h]\n\\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}l|c|ccc@{}}\n\\toprule\n\\textbf{\\#Samples/Intent} & \\textbf{Train Size} & \\multicolumn{3}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n                          && 345M        & 1.3B        & 5B        \\\\\\midrule\n10                         & 370 & 0.77       & 0.81       & 0.827      \\\\\n20                        & 740 & 0.82        & 0.83        & 0.844      \\\\\n30                        & 1110 & 0.84        & 0.85        & 0.87     \\\\\\bottomrule\n\\end{tabular}\n}\n\\caption{Accuracy on the SGD test set when using only k samples per intent. The columns indicate the size of the Megatron-GPT model used.}\n\\label{tab:sgd-fs-indomain}\n\\end{table}", "table_label": "{tab:sgd-fs-indomain}", "table_numeric_cells": [["345M", "345M", 226, 230, 226, 230], ["1.3B", "1.3B", 240, 244, 240, 244], ["5B", "5B", 254, 256, 254, 256], ["10", "10", 275, 277, 275, 277], ["370", "370", 304, 307, 304, 307], ["0.77", "0.77", 310, 314, 310, 314], ["0.81", "0.81", 323, 327, 323, 327], ["0.827", "0.827", 336, 341, 336, 341], ["20", "20", 350, 352, 350, 352], ["740", "740", 378, 381, 378, 381], ["0.82", "0.82", 384, 388, 384, 388], ["0.83", "0.83", 398, 402, 398, 402], ["0.844", "0.844", 412, 417, 412, 417], ["30", "30", 426, 428, 426, 428], ["1110", "1110", 454, 458, 454, 458], ["0.84", "0.84", 461, 465, 461, 465], ["0.85", "0.85", 475, 479, 475, 479], ["0.87", "0.87", 489, 493, 489, 493]], "text_chunk_selected": "\\begin{itemize}\n    \\item \\textbf{Schema Guided Dialogue} (SGD): We hold out utterances corresponding to \\textit{bus bookings} and \\textit{hotel reservations} to form our test set. The train set includes utterances from adjacent domains: flight booking and restaurant reservations. This should be a relatively easy setting for the language model to generalize to.\n    \\item \\textbf{Virtual Assistant}: To make things more challenging, we hold out utterances corresponding to \\textit{operating IOT devices} and \\textit{media consumption commands} (\\textit{e.g.}, commands that are variants of \"play\" - play movie, play audiobook). The train set does not have utterances from similar domains and this setting is more challenging for the model.\n\\end{itemize}\n\n\\begin{itemize}\n    \\item \\textbf{Zero-shot}: P-tune the model on the train set and evaluate zero-shot on the unseen domain test set.\n    \\item \\textbf{Few-shot}: After p-tuning on the train set, we do a second stage p-tuning on a set of \\textit{k} samples from the target domain. Unless otherwise noted, \\textit{k} here is 5, 10, 50 or 100 samples. \n\\end{itemize}\n\n\\subsubsection{In-domain}\nWe find that both the p-tuned GPT model as well as the BERT baseline perform very well on the standard in-domain split where both the train and test set come from the same distribution (Table \\ref{tab:indomain}). The classification accuracy of Megatron-GPT increases as we increase the model size. The trend of results remains consistent for both the SGD and Assistant datasets. \n\n\\subsubsection{Out-of-Domain}\nThe out-of-domain setting is where the advantage of using a LLM becomes apparent. It is not feasible to expect a finetuned BERT model to generalize to an unseen domain not present in the train set. Such models continue to predict that the intent belongs to one of the intent labels they see during training. The p-tuned Megatron-GPT models, on the other hand, show impressive zero-shot and few-shot generalization capabilities on the SGD dataset (Table \\ref{tab:ood-sgd}). For instance, having seen intents such as \\textit{\"buy flight roundtrip tickets\"} when presented with utterances for \\textit{Flight Reservations} in training, we can expect the model to reasonably generalize to utterances from \\textit{Bus Reservations} with utterances like \"Get me a return trip on the bus\" with the model's prediction for the intent being \\textit{\"buy bus roundtrip tickets\"}.\n\nRephrasing the canonical form for the intent \\textit{SearchFlightOneWay} to \\textit{search for flights one way} helps the model to avoid making the spurious correlation and the performance in the zero-shot setting (Table \\ref{tab:ood-sgd-original}) is significantly improved.\n\nHowever, the few-shot setting (Table \\ref{tab:ood-sgd-original-fs}) alleviates this problem of sensitivity of the model to the canonical form structure. When we provide the model with a few samples from the the target domain, it learns to associate that the important words to distinguish between the domains are \\textit{flight} and \\textit{bus} and not \\textit{ticket}.\n\n\\begin{itemize}\n    \\item \\textbf{Similarity in structure}: Use similar verbs for similar actions/domains, \\textit{e.g.}, \\textbf{book} a flight, \\textbf{book} bus tickets, \\textbf{search} for hotels, \\textbf{search} for restaurant reservations.\n    \\item \\textbf{ Compositional}: Using similar structures for canonical forms in similar domains naturally lends to compositionality. This makes it easier for the model to generalize in the zero-shot/few-shot setting while still allowing the developers to easily map the generations to a supported service on the backend.\n    \\item \\textbf{Looks like natural language}: Since LLMs are pretrained on very large corpora of natural language, the benefit of pre-training is realized when the canonical forms resemble natural language rather than complex semantic forms. Making discrete intents look more like typical verb phrases brings out the expressive nature of language models. \n\\end{itemize}\n\n\\subsection{Do we need the entire training set for p-tuning?}\nWe look for the fewest labelled samples for p-tuning needed to get an accuracy close to accessing the entire train set. We randomly sample \\textit{k} samples per intent ($k \\in {5, 10, 20, 30}$) to form the train set the model is p-tuned on, and evaluate on the same test set as above. The train and test sets are from the in-domain setting for both SGD (Table \\ref{tab:sgd-fs-indomain}) and Assistant (Table \\ref{tab:asst-fs-indomain}) datasets.", "table_source": "\\begin{table}[h]\n\\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}l|c|ccc@{}}\n\\toprule\n\\textbf{\\#Samples/Intent} & \\textbf{Train Size} & \\multicolumn{3}{c}{\\textbf{Accuracy}} \\\\ \\midrule\n                          && 345M        & 1.3B        & 5B        \\\\\\midrule\n10                         & 370 & 0.77       & 0.81       & 0.827      \\\\\n20                        & 740 & 0.82        & 0.83        & 0.844      \\\\\n30                        & 1110 & 0.84        & 0.85        & 0.87     \\\\\\bottomrule\n\\end{tabular}\n}\n\\caption{Accuracy on the SGD test set when using only k samples per intent. The columns indicate the size of the Megatron-GPT model used.}\n\\label{tab:sgd-fs-indomain}\n\\end{table}", "cell_list_gold": [{"value": "345M", "char_index": [226, 230], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "1.3B", "char_index": [240, 244], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "5B", "char_index": [254, 256], "type": "Hyper-parameter/Architecture", "model": "Megatron-GPT", "parameter/architecture name": ["Model Size", "number of parameters"], "dataset": ["SGD", "Schema Guided Dialogue"]}, {"value": "10", "char_index": [275, 277], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "370", "char_index": [304, 307], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples", "size"], "dataset features": {"xx": "yy"}}, {"value": "0.77", "char_index": [310, 314], "type": "Result", "training data/set": "SGD 370 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "10", "train size": "370"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.81", "char_index": [323, 327], "type": "Result", "training data/set": "SGD 370 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "10", "train size": "370"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.827", "char_index": [336, 341], "type": "Result", "training data/set": "SGD 370 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "10", "train size": "370"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "20", "char_index": [350, 352], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "740", "char_index": [378, 381], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples", "size"], "dataset features": {"xx": "yy"}}, {"value": "0.82", "char_index": [384, 388], "type": "Result", "training data/set": "SGD 740 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "20", "train size": "740"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.83", "char_index": [398, 402], "type": "Result", "training data/set": "SGD 740 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "20", "train size": "740"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.844", "char_index": [412, 417], "type": "Result", "training data/set": "SGD 740 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "20", "train size": "740"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}, {"value": "30", "char_index": [426, 428], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples per intent", "# samples / intent"], "dataset features": {"xx": "yy"}}, {"value": "1110", "char_index": [454, 458], "type": "Data Stat.", "dataset": ["SGD", "Schema Guided Dialogue"], "sub-set/group name": "Train", "attribute name": ["number of samples", "size"], "dataset features": {"xx": "yy"}}, {"value": "0.84", "char_index": [461, 465], "type": "Result", "training data/set": "SGD 1110 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "30", "train size": "1110"}, "model": "Megatron-GPT", "model settings": {"Model Size": "345M"}}, {"value": "0.85", "char_index": [475, 479], "type": "Result", "training data/set": "SGD 1110 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "30", "train size": "1110"}, "model": "Megatron-GPT", "model settings": {"Model Size": "1.3B"}}, {"value": "0.87", "char_index": [489, 493], "type": "Result", "training data/set": "SGD 1110 samples", "test data/set": ["SGD", "Schema Guided Dialogue"], "task": "Intent Classification", "metric": "Accuracy", "experimental settings": {"number of samples per intent": "30", "train size": "1110"}, "model": "Megatron-GPT", "model settings": {"Model Size": "5B"}}]}, "2211.05599v1_table3": {"table_code": "\\begin{table}[]\n\\begin{tabular}{lll}\n\\hline\ncoherence (NPMI) & diversity & topics \\\\ \\hline\n-0.25 & 0.97 & 10 \\\\\n-0.27 & 0.97 & 20 \\\\\n-0.32 & 0.96 & 25 \\\\\n-0.32 & 0.97 & 15 \\\\\n-0.35 & 1.00 & 5 \\\\\n-0.38 & 1.00 & 3 \\\\\n-0.38 & 0.97 & 50 \\\\ \\hline\n\\end{tabular}\n\\caption{\n\\label{ctm_training}\nSelecting CTM topic model by evaluating CTM coherence (NPMI) and diversity on different topic numbers parameters.\n}\n\\end{table}", "table_label": "{ctm_training}", "table_numeric_cells": [["-0.25", "-0.25", 92, 97, 92, 97], ["0.97", "0.97", 100, 104, 100, 104], ["10", "10", 107, 109, 107, 109], ["-0.27", "-0.27", 113, 118, 113, 118], ["0.97", "0.97", 121, 125, 121, 125], ["20", "20", 128, 130, 128, 130], ["-0.32", "-0.32", 134, 139, 134, 139], ["0.96", "0.96", 142, 146, 142, 146], ["25", "25", 149, 151, 149, 151], ["-0.32", "-0.32", 155, 160, 155, 160], ["0.97", "0.97", 163, 167, 163, 167], ["15", "15", 170, 172, 170, 172], ["-0.35", "-0.35", 176, 181, 176, 181], ["1.00", "1.00", 184, 188, 184, 188], ["5", "5", 191, 192, 191, 192], ["-0.38", "-0.38", 196, 201, 196, 201], ["1.00", "1.00", 204, 208, 204, 208], ["3", "3", 211, 212, 211, 212], ["-0.38", "-0.38", 216, 221, 216, 221], ["0.97", "0.97", 224, 228, 224, 228], ["50", "50", 231, 233, 231, 233]], "text_chunk_selected": "\\subsection{Topic modeling as clustering and MDS}\nIn order to address the issues presented above, we propose using abstractive MDS as an approach to topic labeling. Topic modeling can be reframed as a set of two tasks: (1) finding meaningful clusters for documents \\citep{sia_tired_2020, zhang_is_2022} and (2) performing MDS on those individual clusters to find meaningful topic labels. In this framework, LDA \\cite{blei_latent_2003} uses document-word distributions to construct clusters and word lists drawn from those clusters as a form of MDS. Since we are looking at abstractive MDS that moves beyond word lists, we propose that the \\textit{topic representation be a sentence or paragraph} but there is no reason why an abstractive MDS can't be trained to generate phrases or even word lists (see \\citet{alokaili_automatic_2020}) since word lists may still be appropriate in some situations.\n\n\\subsection{Evaluation}\nTopic model evaluation is challenging (see \\citet{chang_reading_2009, hoyle_is_2021, doogan_topic_2021}). Traditional metrics like coherence (NPMI), perplexity, and diversity scores are studied in the context of topic word lists and validated with correlation to human ratings of the utility or coherence of those topic word lists. Since we suggest developing abstractive topic representations, we want a way to compare various forms of both abstractive and extractive topic representations presented by the model. Since we are treating representation as a summarization task and this task includes measures that work across extractive and abstractive settings, we suggest that we start with standard summarization metrics such as overlap metrics like Rogue (as used in \\citet{cui_topic-guided_2021} or semantic metrics such as BERTScore \\citep{zhang_is_2022} (as used in \\citet{alokaili_automatic_2020}).\n\n\\section{Case study: how has a scientific document been cited?}\nTo evaluate our proposed method, we chose topic modeling over scientific documents as a setting. While several methods exist for determining citation intent function \\citep{basuki_sdcf_2022, nicholson_scite_2021} and the relationship between two papers \\citep{luu_explaining_2021}, there is very little work on topic models over citations (for some representative work on \"citation summary\" see \\citet{elkiss_blind_2008, wang_generating_2021, zou_citation_2021}). Topic representations of citations are interesting for characterizing trends in how a paper has been cited or helping researchers identify relevant citations to read among potentially thousands of other citations. In this work, we treat topic labels as a \"citation intent\" label and use the proposed approach to understand the utility of MDS for topic modeling in this setting.\n\nFor this study, we used scite.ai \\citep{nicholson_scite_2021} to extract in-text passages which contained citations (citation statements) to the paper \\cite{lau_machine_2014}, a well known paper that introduces the NPMI metric in topic modeling. This resulted in 183 citation statements which is the corpus we will use for topic modeling.\n\nIn order to identify meaningful groups of clusters we use contextualized topic models (CTM) \\citep{bianchi_pre-training_2021} since this method uses contextualized word embeddings (we used SPECTER for constructing embeddings \\citep{cohan-etal-2020-specter}). We selected CTM since we still get word lists as topic labels which we used for evaluation. In order to select the number of topics hyperparameter, we trained CTM several times steadily increasing the number of topics from 3 to 50 and selected the best model according to coherence (NPMI) resulting in a 10 topic model (see Appendix \\ref{ctm} for more details) over 183 citation statements.\n\nThe models selected for generating abstractive MDS are outlined in Table \\ref{models_table}. All MDS models used are based on the longformer architecture \\citep{beltagy_longformer} and used beam search (5 beams) with greedy decoding.\n\nWe randomly sampled 3 topics to explore their representations. As an example, table \\ref{topic_evaluation} shows representations using the multi-lexsum-tiny model (full details are available in Appendix \\ref{outputs}. In representations for topic 0 (Table \\ref{topic_0}), we see there is a general agreement across models that the citing documents are discussing measurement. We can see that the topic representations appear to be split between measuring interpretability (multixscience, multi-lexsum-long) and those discussing the correlation between measures (ms2, multi-lexsum-long) or even potentially an additional topic of describing measures used (multi-lexsum-tiny). Conflicting summaries are not surprising given issues in MDS with regards to summarizing diverse and potentially conflicting documents \\citep{deyoung_ms2_2021}. Table \\ref{topic_0} shows a diversity of topic labels that might be appropriate under different scenarios of applying topic models. Labels like the ones in Table \\ref{topic_evaluation} might be useful for labels that are easy and fast to read while longer summaries in multixscience and multi-lexsum-long might be useful for users who want to engage deeper.\n\nTable \\ref{ctm_training} describes the evaluation of all the CTM \\citep{bianchi_pre-training_2021} models we trained by coherence (NPMI), diversity, and topic number. We trained the model on a dataset of 183 citation statements which are in-text passages from publications citing \\citep{lau_machine_2014} retrieved from scite.ai \\citep{nicholson_scite_2021}. This dataset was also used as the reference corpus for evaluating NPMI in this initial case study. We acknowledge that when building robust topic models a standard reference corpus should be used so results can be comparable in future works.", "table_source": "\\begin{table}[]\n\\begin{tabular}{lll}\n\\hline\ncoherence (NPMI) & diversity & topics \\\\ \\hline\n-0.25 & 0.97 & 10 \\\\\n-0.27 & 0.97 & 20 \\\\\n-0.32 & 0.96 & 25 \\\\\n-0.32 & 0.97 & 15 \\\\\n-0.35 & 1.00 & 5 \\\\\n-0.38 & 1.00 & 3 \\\\\n-0.38 & 0.97 & 50 \\\\ \\hline\n\\end{tabular}\n\\caption{\n\\label{ctm_training}\nSelecting CTM topic model by evaluating CTM coherence (NPMI) and diversity on different topic numbers parameters.\n}\n\\end{table}", "cell_list_gold": [{"value": "-0.25", "char_index": [92, 97], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": ["coherence (NPMI)", "NPMI"], "experimental settings": {"xx": "yy"}, "model": "CTM", "model settings": {"topic number": "10"}}, {"value": "0.97", "char_index": [100, 104], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": "diversity", "experimental settings": {"topic number": "10"}, "model": "CTM", "model settings": {"topic number": "10"}}, {"value": "10", "char_index": [107, 109], "type": "Hyper-parameter/Architecture", "model": "CTM", "parameter/architecture name": "topic number", "dataset": "183 citation statements"}, {"value": "-0.27", "char_index": [113, 118], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": ["coherence (NPMI)", "NPMI"], "experimental settings": {"xx": "yy"}, "model": "CTM", "model settings": {"topic number": "20"}}, {"value": "0.97", "char_index": [121, 125], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": "diversity", "experimental settings": {"topic number": "20"}, "model": "CTM", "model settings": {"topic number": "20"}}, {"value": "20", "char_index": [128, 130], "type": "Hyper-parameter/Architecture", "model": "CTM", "parameter/architecture name": "topic number", "dataset": "183 citation statements"}, {"value": "-0.32", "char_index": [134, 139], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": ["coherence (NPMI)", "NPMI"], "experimental settings": {"xx": "yy"}, "model": "CTM", "model settings": {"topic number": "25"}}, {"value": "0.96", "char_index": [142, 146], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": "diversity", "experimental settings": {"topic number": "25"}, "model": "CTM", "model settings": {"topic number": "25"}}, {"value": "25", "char_index": [149, 151], "type": "Hyper-parameter/Architecture", "model": "CTM", "parameter/architecture name": "topic number", "dataset": "183 citation statements"}, {"value": "-0.32", "char_index": [155, 160], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": ["coherence (NPMI)", "NPMI"], "experimental settings": {"xx": "yy"}, "model": "CTM", "model settings": {"topic number": "15"}}, {"value": "0.97", "char_index": [163, 167], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": "diversity", "experimental settings": {"topic number": "15"}, "model": "CTM", "model settings": {"topic number": "15"}}, {"value": "15", "char_index": [170, 172], "type": "Hyper-parameter/Architecture", "model": "CTM", "parameter/architecture name": "topic number", "dataset": "183 citation statements"}, {"value": "-0.35", "char_index": [176, 181], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": ["coherence (NPMI)", "NPMI"], "experimental settings": {"xx": "yy"}, "model": "CTM", "model settings": {"topic number": "5"}}, {"value": "1.00", "char_index": [184, 188], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": "diversity", "experimental settings": {"topic number": "5"}, "model": "CTM", "model settings": {"topic number": "5"}}, {"value": "5", "char_index": [191, 192], "type": "Hyper-parameter/Architecture", "model": "CTM", "parameter/architecture name": "topic number", "dataset": "183 citation statements"}, {"value": "-0.38", "char_index": [196, 201], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": ["coherence (NPMI)", "NPMI"], "experimental settings": {"xx": "yy"}, "model": "CTM", "model settings": {"topic number": "3"}}, {"value": "1.00", "char_index": [204, 208], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": "diversity", "experimental settings": {"topic number": "3"}, "model": "CTM", "model settings": {"topic number": "3"}}, {"value": "3", "char_index": [211, 212], "type": "Hyper-parameter/Architecture", "model": "CTM", "parameter/architecture name": "topic number", "dataset": "183 citation statements"}, {"value": "-0.38", "char_index": [216, 221], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": ["coherence (NPMI)", "NPMI"], "experimental settings": {"xx": "yy"}, "model": "CTM", "model settings": {"topic number": "50"}}, {"value": "0.97", "char_index": [224, 228], "type": "Result", "training data/set": "183 citation statements", "test data/set": "183 citation statements", "task": "topic modeling", "metric": "diversity", "experimental settings": {"topic number": "50"}, "model": "CTM", "model settings": {"topic number": "50"}}, {"value": "50", "char_index": [231, 233], "type": "Hyper-parameter/Architecture", "model": "CTM", "parameter/architecture name": "topic number", "dataset": "183 citation statements"}]}, "2211.05610v1_table0": {"table_code": "\\begin{table*}[h]\n\\centering\n\\small\n\\begin{tabular}{lp{7.2cm}ccc}\n\\toprule \n  & \\textbf{Instance} & \\textbf{EL2N} & \\textbf{Gold Label} & \\textbf{Prediction}\\\\\n\\midrule \n\\multirow{15}{*}{\\rotatebox{90}{Noise / Hard}}\n& Tom Hauck/Getty Images 1. Rapper Snoop Dogg attended practice Tuesday with No. 1 USC as a guest wide receiver. The Trojans were delighted by the star \\#39;s presence, as were a group of pee-wee football players watching practice that day.\n& 1.413128\n& Business (2)\n& Sports (1)\n\\\\\\cmidrule[0.01em](lr){2-5}\n& Palestinian gunmen kidnap CNN producer GAZA CITY, Gaza Strip -- Palestinian gunmen abducted a CNN producer in Gaza City on Monday, the network said. The network said Riyadh Ali was taken away at gunpoint from a CNN van.\n& 1.412653\n& Sports (1)\n& World (0)\n\\\\\\cmidrule[0.01em](lr){2-5}\n& Names in the Game Dressed in jeans and a white shirt, the men \\#39;s all-around champion in Athens appeared relaxed as he helped promote a 14-city gymnastics exhibition tour that kicks off at the Mohegan Sun Casino on Tuesday.\n& 1.412590\n& Business (2)\n& Sports (1)\n\\\\\\midrule[0.03em]\n\\multirow{14}{*}{\\rotatebox{90}{Easy}}\n& Vikings \\#39; Moss practices despite hamstring injury Eden Prairie, MN (Sports Network) - Minnesota Vikings wide receiver Randy Moss practiced on Wednesday despite nursing a strained right hamstring and is listed as questionable for this Sunday \\#39;s game against the New York Giants.\n& 0.000665\n& Sports (1)\n& Sports (1)\n\\\\\\cmidrule[0.01em](lr){2-5}\n& Cassell a no-show; Wolves sign Griffin Minneapolis, MN (Sports Network) - Minnesota Timberwolves guard Sam Cassell did not show up Tuesday for the first day of training camp.\n& 0.000653\n& Sports (1)\n& Sports (1)\n\\\\\\cmidrule[0.01em](lr){2-5}\n& Rockets activate Lue from injured list Houston, TX (Sports Network) - The Houston Rockets activated guard Tyronn Lue from the injured list prior to Wednesday \\#39;s game against the Hawks.\t\n& 0.000649\n& Sports (1)\n& Sports (1)\n \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\nExamples from AG News belonging to different score regions of EL2N. The highest scoring instances are mostly noisy samples, while the least scoring instances are very easy to learn.\n} \n\\label{tab:examples-agnews}\n\\end{table*}", "table_label": "{tab:examples-agnews}", "table_numeric_cells": [["1.413128", "1.413128", 460, 468, 460, 468], ["1.412653", "1.412653", 750, 758, 750, 758], ["1.412590", "1.412590", 1044, 1052, 1044, 1052], ["0.000665", "0.000665", 1429, 1437, 1429, 1437], ["0.000653", "0.000653", 1672, 1680, 1672, 1680], ["0.000649", "0.000649", 1930, 1938, 1930, 1938]], "text_chunk_selected": "\\section{Introduction}\nLarge datasets have made the phenomenal performance of Transformer-based \\citep{attention-is-all-you-need} pre-trained language models (PLMs) possible \\citep{devlin-etal-2019-bert, sanh2019distilbert, liu-2019-roberta, clark2020electra, bommasani2021fm}. \nHowever, recent studies show that a significant fraction of examples in a training dataset can be omitted without sacrificing test accuracy. \nTo this end, many metrics have been introduced for ranking the examples in a dataset based on their importance. \nOne of these metrics is \\emph{Forgetting Score} \\citep{toneva2018empirical-forgetting, yaghoobzadeh-etal-2021-increasing} which recognizes the examples that are misclassified after being correctly classified during training or are always misclassifed. \n\\emph{Datamap} \\citep{swayamdipta-2020-dataset-cartography} is another technique for diagnosing datasets which uses \\emph{confidence} and \\emph{variability} metrics. \n\n\\section{Background}\nIn this section, we describe the two metrics introduced by \\citet{NEURIPS2021_ac56f8fe_datadiet} for pruning the training data: \\textbf{GraNd} and its estimated variant, \\textbf{EL2N}. \n\\subsection{GraNd}\nConsider $\\mathbf{X} = \\{x_i, y_i\\}_{i=1}^N$ to be a training dataset for a given classification task with $K$ classes, where $x_i$ is the input (i.e. a sequence of tokens) and $y_i$ is its corresponding label. \nTo estimate the importance of each training sample $(x_i, y_i)$,  \\citet{NEURIPS2021_ac56f8fe_datadiet} propose utilizing the expected value of the loss gradient norm denoted as GraNd:\n\n\\subsection{EL2N}\nBy defining $\\psi^{(k)}(x_i) = \\nabla_{\\boldsymbol{w}}f^{(k)}(x_i)$ for the $k^\\text{th}$ logit,\nthe loss gradient ($\\boldsymbol{g}$) can be written as:\n\nSince the $\\mathcal{L}(f(x_i), y_i)$ is a cross entropy loss, $\\nabla_{f^{(k)}}\\mathcal{L}(f(x_i), y_i)^T=p(x_i)^{(k)} - y^{(k)}_i$ , where $p(x_i)$ is the output probability vector of the model. By assuming orthogonality and uniform sizes across logits for $\\psi$ over the training examples, the norm in Eq. \\ref{eq:grand} is approximately equal to $\\left\\|p(x_i) - \\boldsymbol{y}_i\\right\\|_2$ ($\\boldsymbol{y}_i$ is the one-hot vector of the true label). As a result, an estimate of GraND is \\textbf{EL2N}, which is defined as follows:\n\n\\begin{equation}\n\\label{eq:el2n}\n    \\text{EL2N}(x_i, y_i) = \\mathbb{E}_{\\boldsymbol{w}}\\left\\|p(x_i) - \\boldsymbol{y}_i\\right\\|_2\n\\end{equation}\n\n\\section{Experiments}\nIn this section, we verify the effectiveness of GraNd and EL2N \\citep{NEURIPS2021_ac56f8fe_datadiet} in the NLP domain. \nOur experimental setup is similar to \\citet{NEURIPS2021_ac56f8fe_datadiet} for the followings:\n(1) models are trained on the subsets of the data with higher GraNd and EL2N scores;\n(2) based on the expectation over multiple weight initializations mentioned in Eq.~\\ref{eq:grand} and Eq.~\\ref{eq:el2n}, we average the scores over five independent training runs.\\footnote{It is ten independent training runs in \\citet{NEURIPS2021_ac56f8fe_datadiet}}; and,\n(3) for putting our results in context, we utilized random pruning as the baseline\\footnote{Random selection with the same proportion} and the accuracy of training on the whole dataset with no pruning as the target performance.\n\nTwo major differences between these two setups are mentioned here:\n(1) we used a pre-trained model, i.e., BERT \\citep{devlin-etal-2019-bert}, standard in the NLP domain, whereas \\citet{NEURIPS2021_ac56f8fe_datadiet} uses vision models initialized with random weights; and\n(2) as fine-tuning requires few epochs of training, we computed the metrics over fine-grained steps rather than epochs.\n\n\\section{Appendix}\n\\subsection{Dataset samples and their scores}\nExamples with the highest and lowest EL2N scores are provided in Table~\\ref{tab:examples-agnews} and Table~\\ref{tab:examples-mnli} for AG News and MNLI datasets. ", "table_source": "\\begin{table*}[h]\n\\centering\n\\small\n\\begin{tabular}{lp{7.2cm}ccc}\n\\toprule \n  & \\textbf{Instance} & \\textbf{EL2N} & \\textbf{Gold Label} & \\textbf{Prediction}\\\\\n\\midrule \n\\multirow{15}{*}{\\rotatebox{90}{Noise / Hard}}\n& Tom Hauck/Getty Images 1. Rapper Snoop Dogg attended practice Tuesday with No. 1 USC as a guest wide receiver. The Trojans were delighted by the star \\#39;s presence, as were a group of pee-wee football players watching practice that day.\n& 1.413128\n& Business (2)\n& Sports (1)\n\\\\\\cmidrule[0.01em](lr){2-5}\n& Palestinian gunmen kidnap CNN producer GAZA CITY, Gaza Strip -- Palestinian gunmen abducted a CNN producer in Gaza City on Monday, the network said. The network said Riyadh Ali was taken away at gunpoint from a CNN van.\n& 1.412653\n& Sports (1)\n& World (0)\n\\\\\\cmidrule[0.01em](lr){2-5}\n& Names in the Game Dressed in jeans and a white shirt, the men \\#39;s all-around champion in Athens appeared relaxed as he helped promote a 14-city gymnastics exhibition tour that kicks off at the Mohegan Sun Casino on Tuesday.\n& 1.412590\n& Business (2)\n& Sports (1)\n\\\\\\midrule[0.03em]\n\\multirow{14}{*}{\\rotatebox{90}{Easy}}\n& Vikings \\#39; Moss practices despite hamstring injury Eden Prairie, MN (Sports Network) - Minnesota Vikings wide receiver Randy Moss practiced on Wednesday despite nursing a strained right hamstring and is listed as questionable for this Sunday \\#39;s game against the New York Giants.\n& 0.000665\n& Sports (1)\n& Sports (1)\n\\\\\\cmidrule[0.01em](lr){2-5}\n& Cassell a no-show; Wolves sign Griffin Minneapolis, MN (Sports Network) - Minnesota Timberwolves guard Sam Cassell did not show up Tuesday for the first day of training camp.\n& 0.000653\n& Sports (1)\n& Sports (1)\n\\\\\\cmidrule[0.01em](lr){2-5}\n& Rockets activate Lue from injured list Houston, TX (Sports Network) - The Houston Rockets activated guard Tyronn Lue from the injured list prior to Wednesday \\#39;s game against the Hawks.\t\n& 0.000649\n& Sports (1)\n& Sports (1)\n \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\nExamples from AG News belonging to different score regions of EL2N. The highest scoring instances are mostly noisy samples, while the least scoring instances are very easy to learn.\n} \n\\label{tab:examples-agnews}\n\\end{table*}", "cell_list_gold": [{"value": "1.413128", "char_index": [460, 468], "type": "Other"}, {"value": "1.412653", "char_index": [750, 758], "type": "Other"}, {"value": "1.412590", "char_index": [1044, 1052], "type": "Other"}, {"value": "0.000665", "char_index": [1429, 1437], "type": "Other"}, {"value": "0.000653", "char_index": [1672, 1680], "type": "Other"}, {"value": "0.000649", "char_index": [1930, 1938], "type": "Other"}]}, "2211.05610v1_table1": {"table_code": "\\begin{table*}[h]\n\\centering\n\\small\n\\begin{tabular}{lp{2.7cm}p{2.7cm}ccc}\n\\toprule \n  & \\textbf{Premise} & \\textbf{Hypothesis} & \\textbf{EL2N} & \\textbf{Gold Label} & \\textbf{Prediction}\\\\\n\\midrule \n\\multirow{10}{*}{\\rotatebox{90}{Noise / Hard}}\n& Social insurance taxes and contributions paid by Federal employees (575)\t\n& There are no taxes for the Federal employees.\t\n& 1.410452\n& Entailment (0)\n& Contradiction (2)\n\\\\\\cmidrule[0.01em](lr){2-6}\n& um-hum um-hum yep you were very fortunate\n& You were very unfortunate.\t\n& 1.410408\n& Entailment (0)\n& Contradiction (2)\n\\\\\\cmidrule[0.01em](lr){2-6}\n& \"Everyone is chanting for you,\" Nema told him.\n& Everyone was silent.\n& 1.410146\n& Neutral (1)\n& Contradiction (2)\n\\\\\\midrule[0.03em]\n\\multirow{7}{*}{\\rotatebox{90}{Easy}}\n& Many of them did.\t\n& None of them did.\t\n& 0.002198\n& Contradiction (2)\n& Contradiction (2)\n\\\\\\cmidrule[0.01em](lr){2-6}\n& Yes \u201ddoubly careful.\" He turned to me abruptly.\t\n& No, not careful at all. He slunk away.\t\n& 0.002147\n& Contradiction (2)\n& Contradiction (2)\n\\\\\\cmidrule[0.01em](lr){2-6}\n& Many others exist, too.\n& There are no others who exist.\n& 0.002134\n& Contradiction (2)\n& Contradiction (2)\n \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\nExamples from MNLI belonging to different score regions of EL2N. The highest scoring instances are mostly noisy samples, while the least scoring instances are very easy to learn.\n} \n\\label{tab:examples-mnli}\n\\end{table*}", "table_label": "{tab:examples-mnli}", "table_numeric_cells": [["1.410452", "1.410452", 373, 381, 373, 381], ["1.410408", "1.410408", 524, 532, 524, 532], ["1.410146", "1.410146", 673, 681, 673, 681], ["0.002198", "0.002198", 817, 825, 817, 825], ["0.002147", "0.002147", 990, 998, 990, 998], ["0.002134", "0.002134", 1129, 1137, 1129, 1137]], "text_chunk_selected": "\\section{Background}\nIn this section, we describe the two metrics introduced by \\citet{NEURIPS2021_ac56f8fe_datadiet} for pruning the training data: \\textbf{GraNd} and its estimated variant, \\textbf{EL2N}. \n\\subsection{GraNd}\nConsider $\\mathbf{X} = \\{x_i, y_i\\}_{i=1}^N$ to be a training dataset for a given classification task with $K$ classes, where $x_i$ is the input (i.e. a sequence of tokens) and $y_i$ is its corresponding label. \nTo estimate the importance of each training sample $(x_i, y_i)$,  \\citet{NEURIPS2021_ac56f8fe_datadiet} propose utilizing the expected value of the loss gradient norm denoted as GraNd:\n\n\\begin{equation}\n\\label{eq:grand}\n    \\text{GraNd}(x_i, y_i) = \\mathbb{E}_{\\boldsymbol{w}}\\left\\|\\boldsymbol{g}(x_i, y_i)\\right\\|_2\n\\end{equation}\n\n\\subsection{EL2N}\nBy defining $\\psi^{(k)}(x_i) = \\nabla_{\\boldsymbol{w}}f^{(k)}(x_i)$ for the $k^\\text{th}$ logit,\nthe loss gradient ($\\boldsymbol{g}$) can be written as:\n\n\\begin{equation}\n    \\boldsymbol{g}(x_i, y_i) = \\sum_{k=1}^{K}\\nabla_{f^{(k)}}\\mathcal{L}(f(x_i), y_i)^T\\psi^{(k)}(x_i)\n\\end{equation}\n\nSince the $\\mathcal{L}(f(x_i), y_i)$ is a cross entropy loss, $\\nabla_{f^{(k)}}\\mathcal{L}(f(x_i), y_i)^T=p(x_i)^{(k)} - y^{(k)}_i$ , where $p(x_i)$ is the output probability vector of the model. By assuming orthogonality and uniform sizes across logits for $\\psi$ over the training examples, the norm in Eq. \\ref{eq:grand} is approximately equal to $\\left\\|p(x_i) - \\boldsymbol{y}_i\\right\\|_2$ ($\\boldsymbol{y}_i$ is the one-hot vector of the true label). As a result, an estimate of GraND is \\textbf{EL2N}, which is defined as follows:\n\n\\begin{equation}\n\\label{eq:el2n}\n    \\text{EL2N}(x_i, y_i) = \\mathbb{E}_{\\boldsymbol{w}}\\left\\|p(x_i) - \\boldsymbol{y}_i\\right\\|_2\n\\end{equation}\n\n\\section{Experiments}\nIn this section, we verify the effectiveness of GraNd and EL2N \\citep{NEURIPS2021_ac56f8fe_datadiet} in the NLP domain. \nOur experimental setup is similar to \\citet{NEURIPS2021_ac56f8fe_datadiet} for the followings:\n(1) models are trained on the subsets of the data with higher GraNd and EL2N scores;\n(2) based on the expectation over multiple weight initializations mentioned in Eq.~\\ref{eq:grand} and Eq.~\\ref{eq:el2n}, we average the scores over five independent training runs.\\footnote{It is ten independent training runs in \\citet{NEURIPS2021_ac56f8fe_datadiet}}; and,\n(3) for putting our results in context, we utilized random pruning as the baseline\\footnote{Random selection with the same proportion} and the accuracy of training on the whole dataset with no pruning as the target performance.\n\n\\section{Appendix}\n\\subsection{Dataset samples and their scores}\nExamples with the highest and lowest EL2N scores are provided in Table~\\ref{tab:examples-agnews} and Table~\\ref{tab:examples-mnli} for AG News and MNLI datasets. ", "table_source": "\\begin{table*}[h]\n\\centering\n\\small\n\\begin{tabular}{lp{2.7cm}p{2.7cm}ccc}\n\\toprule \n  & \\textbf{Premise} & \\textbf{Hypothesis} & \\textbf{EL2N} & \\textbf{Gold Label} & \\textbf{Prediction}\\\\\n\\midrule \n\\multirow{10}{*}{\\rotatebox{90}{Noise / Hard}}\n& Social insurance taxes and contributions paid by Federal employees (575)\t\n& There are no taxes for the Federal employees.\t\n& 1.410452\n& Entailment (0)\n& Contradiction (2)\n\\\\\\cmidrule[0.01em](lr){2-6}\n& um-hum um-hum yep you were very fortunate\n& You were very unfortunate.\t\n& 1.410408\n& Entailment (0)\n& Contradiction (2)\n\\\\\\cmidrule[0.01em](lr){2-6}\n& \"Everyone is chanting for you,\" Nema told him.\n& Everyone was silent.\n& 1.410146\n& Neutral (1)\n& Contradiction (2)\n\\\\\\midrule[0.03em]\n\\multirow{7}{*}{\\rotatebox{90}{Easy}}\n& Many of them did.\t\n& None of them did.\t\n& 0.002198\n& Contradiction (2)\n& Contradiction (2)\n\\\\\\cmidrule[0.01em](lr){2-6}\n& Yes \u201ddoubly careful.\" He turned to me abruptly.\t\n& No, not careful at all. He slunk away.\t\n& 0.002147\n& Contradiction (2)\n& Contradiction (2)\n\\\\\\cmidrule[0.01em](lr){2-6}\n& Many others exist, too.\n& There are no others who exist.\n& 0.002134\n& Contradiction (2)\n& Contradiction (2)\n \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\nExamples from MNLI belonging to different score regions of EL2N. The highest scoring instances are mostly noisy samples, while the least scoring instances are very easy to learn.\n} \n\\label{tab:examples-mnli}\n\\end{table*}", "cell_list_gold": [{"value": "1.410452", "char_index": [373, 381], "type": "Other"}, {"value": "1.410408", "char_index": [524, 532], "type": "Other"}, {"value": "1.410146", "char_index": [673, 681], "type": "Other"}, {"value": "0.002198", "char_index": [817, 825], "type": "Other"}, {"value": "0.002147", "char_index": [990, 998], "type": "Other"}, {"value": "0.002134", "char_index": [1129, 1137], "type": "Other"}]}, "2211.05673v1_table0": {"table_code": "\\begin{table*}\n\\centering\n\\begin{tabular}{lrrrr}\n\\hline\n              & \\multicolumn{2}{c}{Symbols per Token}                                  & \\multicolumn{2}{c}{Words per Token}                                    \\\\ \\cline{2-5} \nTokenizer     & \\multicolumn{1}{l}{Greek BERT} & \\multicolumn{1}{l}{Multilingual BERT} & \\multicolumn{1}{l}{Greek BERT} & \\multicolumn{1}{l}{Multilingual BERT} \\\\ \\cline{2-5} \nModern Greek  & 4.52                           & 2.55                                  & 0.72                           & 0.41                                  \\\\\nAncient Greek & 2.98                           & 1.9                      & 0.46                           & 0.31                                  \\\\ \\hline\n\\end{tabular}\n\\caption{In comparison with multilingual BERT, Greek BERT tokenizer shows a  higher number of symbols and words per token for both Modern and Ancient Greek} \\label{tab:tok}\n\\end{table*}", "table_label": "{tab:tok}", "table_numeric_cells": [["4.52", "4.52", 424, 428, 424, 428], ["2.55", "2.55", 457, 461, 457, 461], ["0.72", "0.72", 497, 501, 497, 501], ["0.41", "0.41", 530, 534, 530, 534], ["2.98", "2.98", 587, 591, 587, 591], ["1.9", "1.9", 620, 623, 620, 623], ["0.46", "0.46", 647, 651, 647, 651], ["0.31", "0.31", 680, 684, 680, 684]], "text_chunk_selected": "\\section{Introduction}\n\\noindent Authorship attribution through some form of statistical inference dates at least\nhalf of a century back, when Mosteller and Wallace used statistics of short, frequent words to estimate authorship of Federalist Papers, disputed by \\citet{mosteller1963inference}. The physicist \\citet{fucks1968} was the first who systematically developed the methods that relied on statistical patterns like word or sentence length that have been found to be able to distinguish between different authors. While undeniably successful in many cases, they do not require any deeper understanding of the texts in question and therefore have their natural limitations. For a detailed review of various authorship attribution methods developed further, we refer the reader to \\cite{stamatatos2009survey}. These methods rely on statistical patterns like word or sentence length that have been found to be able to distinguish between different authors. While undeniably successful in many cases, they do not require any deeper understanding of the texts in question and therefore have their natural limitations.\n\nTransformer artificial neural networks have shown spectacular success in machine translation and answering search queries by internally extracting, after extensive pretraining, abstract patterns, and long-range dependencies in human language samples through encoder hierarchies \\cite{vaswani2017attention}. Therefore, it seems natural to apply such schemes to other tasks that traditionally depended on human language understanding. Thus, this paper uses versions of BERT to investigate questions of authorship in Ancient Greek literature. Such models have already been used for authorship attribution, but the\nactual number of cases is still limited. \\citet{fabien2020bertaa} demonstrate that fine-tuning of a pretrained BERT  language model with an additional dense layer and a softmax activation allows performing authorship classification. \\citet{polignano2020contextualized} use BERT for author profiling in social media and conclude that despite encouraging results in terms of reliability, the computational power required for running such a model is too demanding for the task. These results show that transformers could be successfully used for authorship attribution, yet the application of these models to historical texts is still limited. Specifically, \\citet{bamman2020latin} develop BERT for Latin and \\cite{assael2019restoring} train an LSTM language model of Ancient Greek. There is also a char-BERT implementation\\footnote{https://github.com/brennannicholson/ancient-greek-char-bert}, but we do not know of any public full BERT model trained to work with Ancient Greek. We are also unaware of any examples when BERT was used successfully for authorship attribution of historical documents. \n\n\\section{Data}\nThe data were taken from the digital history projects at the Chair of\nAncient History of the University of Leipzig. The Plutarch texts were transformed into a digital representation under professional supervision. Data can be obtained from the following free repositories Perseus Digital Library\\footnote{https://github.com/PerseusDL/canonical-greekLit} and First Thousand Years of Greek\\footnote{https://github.com/ThomasK81/TEItoCEX} as part of Open Greek and Latin\\footnote{https://opengreekandlatin.org}. The resulting representation was stored in XML format (TEI guidelines) and enriched with metadata. The XML structure and metadata were removed. The strings were transferred into lowercase letters. The diacritics were removed. We did not touch hyphenation, punctuation, or any multilingual remains, nor did we apply any special language-related transformations.\nThe resulting data set consists of 1 244 documents with 199 809 paragraphs or 14 373 311 words.\n\n\\section{Ancient Greek BERT}\nThe resulting amount of data was too small to train Ancient Greek BERT from scratch. However, data sets of smaller sizes are routinely used for transfer learning and fine-tuning of transformers. Thus, we suggest obtaining BERT for Ancient Greek via transfer learning on a Masked Language Modelling (MLM) task. One could either use Multilingual BERT\\footnote{102 languages including Modern Greek, 110M parameters, see https://github.com/google-research/bert/blob/master/multilingual.md} or Greek BERT\\footnote{Modern Greek language, 110M parameters, see\nhttps://huggingface.co/nlpaueb/bert-base-greek-uncased-v1} as a starting model for knowledge transfer. The resulting model could then further be fine-tuned for the task of authorship attribution in Ancient Greek.\n\n\\subsection{Tokenizers}\nThe tokenization of words into sub-word tokens is a crucial preprocessing step that can affect the performance of the model. Up to this point, we were using \"words\" as a linguistic term, whereas we understand tokens as the output of a tokenization algorithm. Thus, there would be one or more tokens that represent every word. Counting the number of tokens used on average to represent a word or, conversely, an average number of words per token gives an estimate of how fit  the tokenization is for the data set. In particular, the average number of words per token varies from 0 to 1, and the closer it is to 1, the more words are represented with one token. Various researchers have shown that corpus-specific tokenization could be beneficial for an NLP task. For example, \\citet{sennrich-etal-2016-neural} show that optimal vocabulary is dependent on the frequencies of the words in the target corpus. \\citet{lakew2019controlling} and \\citet{aji2020neural} partially discuss the tokenization in the setting of cross-language transfer. Though \\citet{aji2020neural} demonstrate that there is no clear evidence that one parent is better than another for cross-lingual transfer learning, they also show that token matching and joint vocabulary \\cite{nguyen2017transfer} are the best ways to handle the embedding layer during transfer learning. \n\nSince each model has its own specific tokenizer before pretraining, one wants to measure how well each of them works with Ancient Greek. To do that, one could take two sample data sets: a sample of Modern Greek\nWikipedia (referred to in Table \\ref{tab:tok} as \u201cmodern\u201d) and a comparable sample of Ancient Greek (\u201cancient\u201d  in Table \\ref{tab:tok}). Each data sample is tokenized with both the Modern Greek BERT tokenizer\\footnote{https://huggingface.co/nlpaueb/bert-base-greek-uncased-v} and the Multilingual BERT tokenizer\\footnote{https://huggingface.co/bert-base-multilingual-cased}. One can speculate that the model uses shorter tokens to adopt grammatical information and deal with longer, rarely observed words. In contrast, the representations with longer tokens could be useful for semantically intensive problems. These longer, semantically charged tokens may vary significantly on various downstream tasks. Thus, the average length of a token and the average number of words per token, shown in Table \\ref{tab:tok}, could be coarsely used as an estimate of the resulting tokenization. One could claim that the higher these values, the more apt the tokenizer is for the task. Indeed, higher average length of a token and number of words per token mean that longer, more semantically charged tokens could be matched for transfer; see \\cite{singh2019bert,aji2020neural,samenko2021fine} for a detailed discussion of various tokenization properties.\n\nIt is hard to compare the resulting tokenizations that we obtain for the same vocabulary size. Some of the tokens occur in both tokenizations, yet have different frequencies, and some are unique for one of the tokenizations. We want to emphasize that direct token matching would not be relevant for comparing the models. Indeed, a frequent token not matching might significantly influence the downstream performance, while several low-frequency non-matching tokens might not have any noticeable effect on the downstream performance. For a detailed comparison, we publish the resulting vocabularies along with relative frequencies of the tokens obtained\\footnote{https://clck.ru/32HWhK}. \n\nLooking at Table \\ref{tab:tok}, one could conclude that the tokenizer of Modern Greek BERT is a preferable solution for Ancient Greek. However, the higher number of symbols or words per token does not automatically guarantee that the overall performance of the model\nafter fine-tuning would be superior in terms of performance on a downstream task.", "table_source": "\\begin{table*}\n\\centering\n\\begin{tabular}{lrrrr}\n\\hline\n              & \\multicolumn{2}{c}{Symbols per Token}                                  & \\multicolumn{2}{c}{Words per Token}                                    \\\\ \\cline{2-5} \nTokenizer     & \\multicolumn{1}{l}{Greek BERT} & \\multicolumn{1}{l}{Multilingual BERT} & \\multicolumn{1}{l}{Greek BERT} & \\multicolumn{1}{l}{Multilingual BERT} \\\\ \\cline{2-5} \nModern Greek  & 4.52                           & 2.55                                  & 0.72                           & 0.41                                  \\\\\nAncient Greek & 2.98                           & 1.9                      & 0.46                           & 0.31                                  \\\\ \\hline\n\\end{tabular}\n\\caption{In comparison with multilingual BERT, Greek BERT tokenizer shows a  higher number of symbols and words per token for both Modern and Ancient Greek} \\label{tab:tok}\n\\end{table*}", "cell_list_gold": [{"value": "4.52", "char_index": [424, 428], "type": "Data Stat.", "dataset": "Modern Greek", "sub-set/group name": "Greek BERT", "attribute name": "Symbols per Token", "dataset features": {"xx": "yy"}}, {"value": "2.55", "char_index": [457, 461], "type": "Data Stat.", "dataset": "Modern Greek", "sub-set/group name": "Multilingual BERT", "attribute name": "Symbols per Token", "dataset features": {"xx": "yy"}}, {"value": "0.72", "char_index": [497, 501], "type": "Data Stat.", "dataset": "Modern Greek", "sub-set/group name": "Greek BERT", "attribute name": "Words per Token", "dataset features": {"xx": "yy"}}, {"value": "0.41", "char_index": [530, 534], "type": "Data Stat.", "dataset": "Modern Greek", "sub-set/group name": "Multilingual BERT", "attribute name": "Words per Token", "dataset features": {"xx": "yy"}}, {"value": "2.98", "char_index": [587, 591], "type": "Data Stat.", "dataset": "Ancient Greek", "sub-set/group name": "Greek BERT", "attribute name": "Symbols per Token", "dataset features": {"xx": "yy"}}, {"value": "1.9", "char_index": [620, 623], "type": "Data Stat.", "dataset": "Ancient Greek", "sub-set/group name": "Multilingual BERT", "attribute name": "Symbols per Token", "dataset features": {"xx": "yy"}}, {"value": "0.46", "char_index": [647, 651], "type": "Data Stat.", "dataset": "Ancient Greek", "sub-set/group name": "Greek BERT", "attribute name": "Words per Token", "dataset features": {"xx": "yy"}}, {"value": "0.31", "char_index": [680, 684], "type": "Data Stat.", "dataset": "Ancient Greek", "sub-set/group name": "Multilingual BERT", "attribute name": "Words per Token", "dataset features": {"xx": "yy"}}]}, "2211.05673v1_table1": {"table_code": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{lr}\n\\hline\n& Validation\\\\\n& accuracy  \\\\ \n\\hline\nGreek BERT  & \\textbf{80\\%}                                              \\\\\nGreek BERT no MLM-transfer  & 78\\%                                              \\\\\nMultilingual  BERT                 & 78\\%                                                  \\\\ \nNaive Bayes Classifier &   43\\%                                                 \\\\ \nRandom authorship attribution &   6\\%                                                 \\\\ \n\\hline\n\\end{tabular}\n\\caption{After MLM training and ten epoch of fine-tuning for authorship attribution, the validation accuracy of  Modern Greek BERT  is slightly higher than that of the Multilingual BERT after similar fine-tuning procedures. Modern Greek BERT fine-tuned for authorship attribution without MLM transfer learning phase shows lower validation accuracy. All BERT-based classifiers significantly outperform the Naive Bayes Classifier that uses the two thousand most frequent unigrams. Another baseline attributes one of seventeen labels to the text at random.}\n\\label{tab:au}\n\\end{table}", "table_label": "{tab:au}", "table_numeric_cells": [["80", "\\textbf{80\\%}", 115, 117, 107, 120], ["78", "78\\%", 199, 201, 199, 203], ["78", "78\\%", 289, 291, 289, 293], ["43", "43\\%", 374, 376, 374, 378], ["6", "6\\%", 465, 466, 465, 468]], "text_chunk_selected": "Transformer artificial neural networks have shown spectacular success in machine translation and answering search queries by internally extracting, after extensive pretraining, abstract patterns, and long-range dependencies in human language samples through encoder hierarchies \\cite{vaswani2017attention}. Therefore, it seems natural to apply such schemes to other tasks that traditionally depended on human language understanding. Thus, this paper uses versions of BERT to investigate questions of authorship in Ancient Greek literature. Such models have already been used for authorship attribution, but the\nactual number of cases is still limited. \\citet{fabien2020bertaa} demonstrate that fine-tuning of a pretrained BERT  language model with an additional dense layer and a softmax activation allows performing authorship classification. \\citet{polignano2020contextualized} use BERT for author profiling in social media and conclude that despite encouraging results in terms of reliability, the computational power required for running such a model is too demanding for the task. These results show that transformers could be successfully used for authorship attribution, yet the application of these models to historical texts is still limited. Specifically, \\citet{bamman2020latin} develop BERT for Latin and \\cite{assael2019restoring} train an LSTM language model of Ancient Greek. There is also a char-BERT implementation\\footnote{https://github.com/brennannicholson/ancient-greek-char-bert}, but we do not know of any public full BERT model trained to work with Ancient Greek. We are also unaware of any examples when BERT was used successfully for authorship attribution of historical documents. \n\nSince each model has its own specific tokenizer before pretraining, one wants to measure how well each of them works with Ancient Greek. To do that, one could take two sample data sets: a sample of Modern Greek\nWikipedia (referred to in Table \\ref{tab:tok} as \u201cmodern\u201d) and a comparable sample of Ancient Greek (\u201cancient\u201d  in Table \\ref{tab:tok}). Each data sample is tokenized with both the Modern Greek BERT tokenizer\\footnote{https://huggingface.co/nlpaueb/bert-base-greek-uncased-v} and the Multilingual BERT tokenizer\\footnote{https://huggingface.co/bert-base-multilingual-cased}. One can speculate that the model uses shorter tokens to adopt grammatical information and deal with longer, rarely observed words. In contrast, the representations with longer tokens could be useful for semantically intensive problems. These longer, semantically charged tokens may vary significantly on various downstream tasks. Thus, the average length of a token and the average number of words per token, shown in Table \\ref{tab:tok}, could be coarsely used as an estimate of the resulting tokenization. One could claim that the higher these values, the more apt the tokenizer is for the task. Indeed, higher average length of a token and number of words per token mean that longer, more semantically charged tokens could be matched for transfer; see \\cite{singh2019bert,aji2020neural,samenko2021fine} for a detailed discussion of various tokenization properties.\n\nWe use this data set to train BERT Classifiers similarly to \\cite{fabien2020bertaa}. Table \\ref{tab:au} shows the validation accuracy for Modern Greek and Multilingual BERTs after MLM on Ancient Greek and 10 epochs of classifier training\\footnote{AdamW, LR = 2e-5, eps = 1e-8,  linear\\_schedule}. Table \\ref{tab:au}  also shows a standard NLTK Naive Bayes Classifier trained on the 2 000 most frequent unigrams as a reference point for authorship attribution accuracy.\n\nSince Ancient Greek was used in various regions and territories, one might expect that the texts also show regional peculiarities. Such peculiarities\nthen should also be detectable by a dedicated regional BERT Classifier. We constructed three coarse regions for the origins of the authors in the data set: a region surrounding Delphi, where Plutarch was working, a region in the proximity of Alexandria, and the region of ancient Ionia, namely the ancient region on the central part of the western coast of modern Anatolia, see Figure \\ref{fig:map}. After balancing texts written by authors in these three regions with the fourth label that includes random sentences from authors outside of these regions, we train another BERT-based classifier to achieve a $0.79$ validation accuracy for the region of the author. Table \\ref{tab:region} shows the results of the obtained classifier on the validation set.\n\nTable \\ref{tab:p}  demonstrates that the resulting authorship profiles differ for all three documents. However, the sample size for De Fluviis and De Musica is smaller than the sample size for Placita Philosophorum. Plutarch ends up being in the top three of the most frequently predicted authors only in De Musica. For De Fluviis and De Musica Athenaeus ends up being the most frequent guess of the BERT authorship classifier, while for Placita Philosophorum \"other\" is the most frequent attribution. Claudius Ptolemaeus and Sextus Empiricus are, respectively, the second and the third most frequent guesses. \n\nFigure \\ref{fig:reg} shows the regional profile obtained with the BERT-based regional classifier. Once again, all three works show different structural properties. While the model associates every second sentence from De Fluviis and De Musica with the Delphi region, it only attributes $15\\%$ of the sentences from Placita Philosophorum to Delphi. The model is far more uncertain about the third document. Every fourth sentence in Placita Philosophorum is associated with the Alexandrian region, while almost half is labeled as \"other\". \n\nAll in all, the result shows that the Placita Philosophorum is closely related to a philosophical-scientific tradition from the 1st century CE to ca. 220/250 CE. Possibly one could even see an embedding into a specifically Alexandrian context, which despite very different contents of the works (Strabo as a geographer, Ptolemy as mathematician and geographer, Athenaeus as an anthologist, and Sextus as a Skeptic), is related to the Pseudo-Plutarchian Placita. The parallels between Pseudo-Plutarch and Sextus Empiricus have already been pointed out several times in the literature \\cite{mansfeld2020}; connections to Claudius Ptolemaeus have not been considered so far. It is fascinating that in this group of three (Pseudo-Plutarch, Claudius Ptolemaeus, and Sextus\nEmpiricus), a mathematical-cosmologically oriented context for very different topics of philosophy, natural science, and medicine, is the basis. We can thus state a new hypothesis that the Placita Philosophorum presumably originates from an Alexandrian scientific context of the 2nd century CE. We also should notice that De Fluviis and De Musica, and Placita Philosophorum might have three different authors. There are some similarities between De Fluviis and De Musica (both have approximately 50\\% of the text attributed to the Delphi region and approximately 20\\% attributed to Athenaeus). At the same time, one should be cautious in the conclusions since Placita Philosophorum is longer and have a different topical structure than the other two works. This might explain the lower percentage of Delphi-attributed sentences in the document. Athenaeus was allegedly born on the territory of modern Egypt, as well as Claudius Ptolemaeus. \n\nSextus Empiricus lived in different places, however, like the authors of De Fluviis and\nDe Musica, he seems to have been more influenced by the Alexandrian tradition than has been seen so far. Furthermore, the results  shown in Table  \\ref{tab:au}, and in figure \\ref{fig:reg}  indicate that the three works analyzed here (De Fluviis, De Musica, Placita Philosophorum) cannot be securely linked to one identifiable author. Since all three works have a strongly compilatory character due to the many  quotations, references and summaries, this result is quite plausible and confirms the current scholarly opinion against an attribution to Plutarch.", "table_source": "\\begin{table}[h]\n\\centering\n\\begin{tabular}{lr}\n\\hline\n& Validation\\\\\n& accuracy  \\\\ \n\\hline\nGreek BERT  & \\textbf{80\\%}                                              \\\\\nGreek BERT no MLM-transfer  & 78\\%                                              \\\\\nMultilingual  BERT                 & 78\\%                                                  \\\\ \nNaive Bayes Classifier &   43\\%                                                 \\\\ \nRandom authorship attribution &   6\\%                                                 \\\\ \n\\hline\n\\end{tabular}\n\\caption{After MLM training and ten epoch of fine-tuning for authorship attribution, the validation accuracy of  Modern Greek BERT  is slightly higher than that of the Multilingual BERT after similar fine-tuning procedures. Modern Greek BERT fine-tuned for authorship attribution without MLM transfer learning phase shows lower validation accuracy. All BERT-based classifiers significantly outperform the Naive Bayes Classifier that uses the two thousand most frequent unigrams. Another baseline attributes one of seventeen labels to the text at random.}\n\\label{tab:au}\n\\end{table}", "cell_list_gold": [{"value": "80", "char_index": [115, 117], "type": "Result", "training data/set": ["acient greek", "authorship attribution data set"], "test data/set": ["acient greek", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc"], "experimental settings": {"xx": "yy"}, "model": "Greek BERT", "model settings": {"xx": "yy"}}, {"value": "78", "char_index": [199, 201], "type": "Result", "training data/set": ["acient greek", "authorship attribution data set"], "test data/set": ["acient greek", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc"], "experimental settings": {"xx": "yy"}, "model": "Greek BERT no MLM-transfer", "model settings": {"xx": "yy"}}, {"value": "78", "char_index": [289, 291], "type": "Result", "training data/set": ["acient greek", "authorship attribution data set"], "test data/set": ["acient greek", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc"], "experimental settings": {"xx": "yy"}, "model": "Multilingual BERT", "model settings": {"xx": "yy"}}, {"value": "43", "char_index": [374, 376], "type": "Result", "training data/set": ["acient greek", "authorship attribution data set"], "test data/set": ["acient greek", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc"], "experimental settings": {"xx": "yy"}, "model": "Naive Bayes Classifier", "model settings": {"xx": "yy"}}, {"value": "6", "char_index": [465, 466], "type": "Result", "training data/set": ["acient greek", "authorship attribution data set"], "test data/set": ["acient greek", "authorship attribution data set"], "task": "authorship attribution", "metric": ["Accuracy", "Acc"], "experimental settings": {"xx": "yy"}, "model": "Random authorship attribution", "model settings": {"xx": "yy"}}]}, "2211.05673v1_table4": {"table_code": "\\begin{table*}[]\n\\centering\n\\begin{tabular}{lrrrrrrr}\n\\hline\n\t\t& \\multicolumn{1}{l}{Sample} \t& \\multicolumn{1}{l}{Top 1} \t& \\multicolumn{1}{l}{Top 1} \t& \\multicolumn{1}{l}{Top 2} \t& \\multicolumn{1}{l}{Top 2} \t& \\multicolumn{1}{l}{Top 3} \t& \\multicolumn{1}{l}{Top 3} \\\\ \n\t\t& \\multicolumn{1}{l}{Size} \t& \t& \\multicolumn{1}{l}{Share} \t& \t& \\multicolumn{1}{l}{Share} \t& & \\multicolumn{1}{l}{Share} \\\\ \n\n\\hline\nDe Fluviis  & 310 \t\t\t\t\t\t& Athenaeus \t\t\t\t& 22\\% \t\t\t\t\t\t& Others \t\t\t\t\t& 21\\% \t\t\t\t\t\t& Strabo \t\t\t\t\t&  19\\%                                       \\\\\n\\hline\n   & \t\t\t\t\t\t&  \t\t\t\t&  \t\t\t\t\t\t&\t\t\t\t& \t\t\t\t\t& Sextus  \t\t\t&             \\\\ \nDe Musica   & 285 \t\t\t\t\t\t& Athenaeus \t\t\t\t& 21\\% \t\t\t\t\t\t& Plutarch \t\t\t\t& 18\\%\t\t\t\t\t\t& Empiricus \t\t\t&     14\\%         \\\\ \n\\hline\nPlacita  &  \t\t\t\t&  \t\t\t\t\t&\t\t\t\t\t\t& Claudius  \t\t&  \t\t\t\t\t\t& Sextus  \t\t\t&    \\\\\nPhilosophorum  & 928 \t\t\t\t& Others \t\t\t\t\t& 36\\% \t\t\t\t\t\t& Ptolemaeus  \t\t& 20\\% \t\t\t\t\t\t& Empiricus  \t\t\t& 11 \\%   \\\\\n\\hline\n\\end{tabular}\n\\caption{The most frequently attributed authors in the three Pseudo-Plutarchean texts.}\n\\label{tab:p}\n\\end{table*}", "table_label": "{tab:p}", "table_numeric_cells": [["310", "310", 420, 423, 420, 423], ["22", "22\\%", 448, 450, 448, 452], ["21", "21\\%", 475, 477, 475, 479], ["19", "19\\%", 503, 505, 503, 507], ["285", "285", 640, 643, 640, 643], ["21", "21\\%", 668, 670, 668, 672], ["18", "18\\%", 696, 698, 696, 700], ["14", "14\\%", 727, 729, 727, 731], ["928", "928", 843, 846, 843, 846], ["36", "36\\%", 867, 869, 867, 871], ["20", "20\\%", 896, 898, 896, 900], ["11", "11 \\%", 925, 927, 925, 930]], "text_chunk_selected": "\\begin{itemize}\n    \\item we use a transfer learning approach to train BERT for Ancient Greek;\n    \\item we demonstrate that this language model is useful for authorship attribution of Ancient Greek texts;\n    \\item we obtain results that may be used as evidence in the process of authorship attribution of the Pseudo-Plutarchean texts;\n    \\item we obtain new insights into the paths along which the reception of ancient philosophy was developed.\n\\end{itemize}\n\n\\section{Data}\nThe data were taken from the digital history projects at the Chair of\nAncient History of the University of Leipzig. The Plutarch texts were transformed into a digital representation under professional supervision. Data can be obtained from the following free repositories Perseus Digital Library\\footnote{https://github.com/PerseusDL/canonical-greekLit} and First Thousand Years of Greek\\footnote{https://github.com/ThomasK81/TEItoCEX} as part of Open Greek and Latin\\footnote{https://opengreekandlatin.org}. The resulting representation was stored in XML format (TEI guidelines) and enriched with metadata. The XML structure and metadata were removed. The strings were transferred into lowercase letters. The diacritics were removed. We did not touch hyphenation, punctuation, or any multilingual remains, nor did we apply any special language-related transformations.\nThe resulting data set consists of 1 244 documents with 199 809 paragraphs or 14 373 311 words.\n\n\\subsection{Tokenizers}\nThe tokenization of words into sub-word tokens is a crucial preprocessing step that can affect the performance of the model. Up to this point, we were using \"words\" as a linguistic term, whereas we understand tokens as the output of a tokenization algorithm. Thus, there would be one or more tokens that represent every word. Counting the number of tokens used on average to represent a word or, conversely, an average number of words per token gives an estimate of how fit  the tokenization is for the data set. In particular, the average number of words per token varies from 0 to 1, and the closer it is to 1, the more words are represented with one token. Various researchers have shown that corpus-specific tokenization could be beneficial for an NLP task. For example, \\citet{sennrich-etal-2016-neural} show that optimal vocabulary is dependent on the frequencies of the words in the target corpus. \\citet{lakew2019controlling} and \\citet{aji2020neural} partially discuss the tokenization in the setting of cross-language transfer. Though \\citet{aji2020neural} demonstrate that there is no clear evidence that one parent is better than another for cross-lingual transfer learning, they also show that token matching and joint vocabulary \\cite{nguyen2017transfer} are the best ways to handle the embedding layer during transfer learning. \n\nSince each model has its own specific tokenizer before pretraining, one wants to measure how well each of them works with Ancient Greek. To do that, one could take two sample data sets: a sample of Modern Greek\nWikipedia (referred to in Table \\ref{tab:tok} as \u201cmodern\u201d) and a comparable sample of Ancient Greek (\u201cancient\u201d  in Table \\ref{tab:tok}). Each data sample is tokenized with both the Modern Greek BERT tokenizer\\footnote{https://huggingface.co/nlpaueb/bert-base-greek-uncased-v} and the Multilingual BERT tokenizer\\footnote{https://huggingface.co/bert-base-multilingual-cased}. One can speculate that the model uses shorter tokens to adopt grammatical information and deal with longer, rarely observed words. In contrast, the representations with longer tokens could be useful for semantically intensive problems. These longer, semantically charged tokens may vary significantly on various downstream tasks. Thus, the average length of a token and the average number of words per token, shown in Table \\ref{tab:tok}, could be coarsely used as an estimate of the resulting tokenization. One could claim that the higher these values, the more apt the tokenizer is for the task. Indeed, higher average length of a token and number of words per token mean that longer, more semantically charged tokens could be matched for transfer; see \\cite{singh2019bert,aji2020neural,samenko2021fine} for a detailed discussion of various tokenization properties.\n\nWe use this data set to train BERT Classifiers similarly to \\cite{fabien2020bertaa}. Table \\ref{tab:au} shows the validation accuracy for Modern Greek and Multilingual BERTs after MLM on Ancient Greek and 10 epochs of classifier training\\footnote{AdamW, LR = 2e-5, eps = 1e-8,  linear\\_schedule}. Table \\ref{tab:au}  also shows a standard NLTK Naive Bayes Classifier trained on the 2 000 most frequent unigrams as a reference point for authorship attribution accuracy.\n\nSince Ancient Greek was used in various regions and territories, one might expect that the texts also show regional peculiarities. Such peculiarities\nthen should also be detectable by a dedicated regional BERT Classifier. We constructed three coarse regions for the origins of the authors in the data set: a region surrounding Delphi, where Plutarch was working, a region in the proximity of Alexandria, and the region of ancient Ionia, namely the ancient region on the central part of the western coast of modern Anatolia, see Figure \\ref{fig:map}. After balancing texts written by authors in these three regions with the fourth label that includes random sentences from authors outside of these regions, we train another BERT-based classifier to achieve a $0.79$ validation accuracy for the region of the author. Table \\ref{tab:region} shows the results of the obtained classifier on the validation set.\n\nLet us now split these three Pseudo-Plutarchean texts into the separate sentences and apply the author classifier described above to these texts.\nTable \\ref{tab:p} shows the authors that are most frequently attributed within a particular document, along with the share of sentences attributed to them. We have double-checked these results using an alternative scoring method. Instead of classifying every sentence in a document and then averaging the classifier's results across all sentences, one could obtain probability scores for every author that the model estimates for every sentence. Averaging those probabilities throughout the document, one could obtain the three most-probable author candidates. These three most-probable authors turn out to be exactly the same for all three documents as the ones in Table \\ref{tab:p}. Moreover, the resulting probabilities of the authorship are also the same for all three most probable authors across all three documents under examination. \n\nTable \\ref{tab:p}  demonstrates that the resulting authorship profiles differ for all three documents. However, the sample size for De Fluviis and De Musica is smaller than the sample size for Placita Philosophorum. Plutarch ends up being in the top three of the most frequently predicted authors only in De Musica. For De Fluviis and De Musica Athenaeus ends up being the most frequent guess of the BERT authorship classifier, while for Placita Philosophorum \"other\" is the most frequent attribution. Claudius Ptolemaeus and Sextus Empiricus are, respectively, the second and the third most frequent guesses. ", "table_source": "\\begin{table*}[]\n\\centering\n\\begin{tabular}{lrrrrrrr}\n\\hline\n\t\t& \\multicolumn{1}{l}{Sample} \t& \\multicolumn{1}{l}{Top 1} \t& \\multicolumn{1}{l}{Top 1} \t& \\multicolumn{1}{l}{Top 2} \t& \\multicolumn{1}{l}{Top 2} \t& \\multicolumn{1}{l}{Top 3} \t& \\multicolumn{1}{l}{Top 3} \\\\ \n\t\t& \\multicolumn{1}{l}{Size} \t& \t& \\multicolumn{1}{l}{Share} \t& \t& \\multicolumn{1}{l}{Share} \t& & \\multicolumn{1}{l}{Share} \\\\ \n\n\\hline\nDe Fluviis  & 310 \t\t\t\t\t\t& Athenaeus \t\t\t\t& 22\\% \t\t\t\t\t\t& Others \t\t\t\t\t& 21\\% \t\t\t\t\t\t& Strabo \t\t\t\t\t&  19\\%                                       \\\\\n\\hline\n   & \t\t\t\t\t\t&  \t\t\t\t&  \t\t\t\t\t\t&\t\t\t\t& \t\t\t\t\t& Sextus  \t\t\t&             \\\\ \nDe Musica   & 285 \t\t\t\t\t\t& Athenaeus \t\t\t\t& 21\\% \t\t\t\t\t\t& Plutarch \t\t\t\t& 18\\%\t\t\t\t\t\t& Empiricus \t\t\t&     14\\%         \\\\ \n\\hline\nPlacita  &  \t\t\t\t&  \t\t\t\t\t&\t\t\t\t\t\t& Claudius  \t\t&  \t\t\t\t\t\t& Sextus  \t\t\t&    \\\\\nPhilosophorum  & 928 \t\t\t\t& Others \t\t\t\t\t& 36\\% \t\t\t\t\t\t& Ptolemaeus  \t\t& 20\\% \t\t\t\t\t\t& Empiricus  \t\t\t& 11 \\%   \\\\\n\\hline\n\\end{tabular}\n\\caption{The most frequently attributed authors in the three Pseudo-Plutarchean texts.}\n\\label{tab:p}\n\\end{table*}", "cell_list_gold": [{"value": "310", "char_index": [420, 423], "type": "Data Stat.", "dataset": "De Fluviis", "sub-set/group name": "xx", "attribute name": "Sample Size", "dataset features": {"xx": "yy"}}, {"value": "22", "char_index": [448, 450], "type": "Result", "training data/set": "xx", "test data/set": "De Fluviis", "task": "authorship attribution", "metric": "Top 1 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}, {"value": "21", "char_index": [475, 477], "type": "Result", "training data/set": "xx", "test data/set": "De Fluviis", "task": "authorship attribution", "metric": "Top 2 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}, {"value": "19", "char_index": [503, 505], "type": "Result", "training data/set": "xx", "test data/set": "De Fluviis", "task": "authorship attribution", "metric": "Top 3 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}, {"value": "285", "char_index": [640, 643], "type": "Data Stat.", "dataset": "De Musica", "sub-set/group name": "xx", "attribute name": "Sample Size", "dataset features": {"xx": "yy"}}, {"value": "21", "char_index": [668, 670], "type": "Result", "training data/set": "xx", "test data/set": "De Musica", "task": "authorship attribution", "metric": "Top 1 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}, {"value": "18", "char_index": [696, 698], "type": "Result", "training data/set": "xx", "test data/set": "De Musica", "task": "authorship attribution", "metric": "Top 2 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}, {"value": "14", "char_index": [727, 729], "type": "Result", "training data/set": "xx", "test data/set": "De Musica", "task": "authorship attribution", "metric": "Top 3 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}, {"value": "928", "char_index": [843, 846], "type": "Data Stat.", "dataset": "Placita Philosophorum", "sub-set/group name": "xx", "attribute name": "Sample Size", "dataset features": {"xx": "yy"}}, {"value": "36", "char_index": [867, 869], "type": "Result", "training data/set": "xx", "test data/set": "Placita Philosophorum", "task": "authorship attribution", "metric": "Top 1 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}, {"value": "20", "char_index": [896, 898], "type": "Result", "training data/set": "xx", "test data/set": "Placita Philosophorum", "task": "authorship attribution", "metric": "Top 2 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}, {"value": "11", "char_index": [925, 927], "type": "Result", "training data/set": "xx", "test data/set": "Placita Philosophorum", "task": "authorship attribution", "metric": "Top 3 Share", "experimental settings": {"xx": "yy"}, "model": "Ancient Greek BERT", "model settings": {"xx": "yy"}}]}, "2211.05719v1_table0": {"table_code": "\\begin{table}[t!] \n\\centering\n\\resizebox{0.5\\textwidth}{!}{\n\\begin{tabular}{lccc}\n\\toprule\n\nStatistics  & PhotoChat & MMChat & MMDialog \\\\ \\midrule\n\\#Language           & English    & Chinese   & English \\\\\n\\#Open-domain           &   \\ding{56}  & \\ding{52}   & \\ding{52} \\\\ \\midrule\n\\#Dialogues           & 12.29K    & 120.84K   & 1.08M \\\\\n\\#Images        & 10.92K    & 204.32K   & 1.53M \\\\\n\\#Turns         & 156.10K   & 314.13K   & 4.92M \\\\\nAvg.  \\#Turns per Dialogue  & 12.71     & 2.59      & 4.56  \\\\\nAvg. \\#Images per Dialogue & 0.89      & 2.91      & 2.59  \\\\\nAvg.  \\#Tokens per Turn & 6.33      & 8.52      & 15.64 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Statistics of MMDialog~and previous multi-modal dialogue datasets.}\n\\label{tab:statistics}\n\\end{table}", "table_label": "{tab:statistics}", "table_numeric_cells": [["12.29K", "12.29K", 308, 314, 308, 314], ["120.84K", "120.84K", 320, 327, 320, 327], ["1.08M", "1.08M", 332, 337, 332, 337], ["10.92K", "10.92K", 359, 365, 359, 365], ["204.32K", "204.32K", 371, 378, 371, 378], ["1.53M", "1.53M", 383, 388, 383, 388], ["156.10K", "156.10K", 410, 417, 410, 417], ["314.13K", "314.13K", 422, 429, 422, 429], ["4.92M", "4.92M", 434, 439, 434, 439], ["12.71", "12.71", 473, 478, 473, 478], ["2.59", "2.59", 485, 489, 485, 489], ["4.56", "4.56", 497, 501, 497, 501], ["0.89", "0.89", 535, 539, 535, 539], ["2.91", "2.91", 547, 551, 547, 551], ["2.59", "2.59", 559, 563, 559, 563], ["6.33", "6.33", 594, 598, 594, 598], ["8.52", "8.52", 606, 610, 606, 610], ["15.64", "15.64", 618, 623, 618, 623]], "text_chunk_selected": "\\begin{abstract}\nResponding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog~dataset to facilitate multi-modal conversation better. MMDialog~is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog~has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 8x. Second, it contains massive topics to generalize the open domain. To build an engaging dialogue system with this dataset, we propose and normalize two response prediction tasks based on retrieval and generative scenarios. In addition, we build two baselines for the above tasks with state-of-the-art techniques and report their experimental performance. We also propose a novel evaluation metric MM-Relevance to measure the multi-modal responses. Our dataset and scripts are available in \\url{https://github.com/victorsungo/MMDialog}.\n\\end{abstract}\n\nExisting approaches to building multi-modal dialogue systems are primarily data-driven, requiring the collection of a large-scale dataset first. To facilitate this line of research, the community emerges a few dialogue datasets incorporating visual information~\\cite{meng2020openvidial,wang2021openvidial,zang-etal-2021-photochat,zheng-etal-2022-mmchat}. For example, Visual Dialog~\\cite{das2017visual} is set up for visual question answering involving image inputs. IGC~\\cite{mostafazadeh-etal-2017-image} and Image-Chat~\\cite{shuster-etal-2020-image} are constructed in a crowd-sourcing method in which annotators are employed to chat about given images. PhotoChat~\\cite{zang-etal-2021-photochat} is also built via crowd-sourcing but contains sharing photos in conversations. MMChat~\\cite{zheng-etal-2022-mmchat} is collected from real conversations on Chinese social media.\n\nDespite the diversity of multi-modal dialogue corpora, these datasets still have limitations. Firstly, several corpora, including Visual Dialog, IGC and Image-Chat, are derived from crowd-sourcing dialogues talking about given images. The topics of human utterances in a dialogue session are often triggered and grounded by these images, which is inconsistent with our daily communications, where the utterances are not always image-related~\\cite{zheng-etal-2022-mmchat}. Secondly, other groups of datasets, such as OpenViDial 1.0/2.0~\\cite{meng2020openvidial,wang2021openvidial} and dialogues collected by \\citet{lee-etal-2021-constructing}, are not originated from a real multi-modal conversation scenario. The former directly extracts dialogues and their visual contexts from movies and TV series, and the latter replaces some utterances with retrieved relevant images. Both methods artificially construct images from the multi-turn conversation to simulate multi-modal dialogues. Finally, some recently proposed multi-modal dialogue data like PhotoChat and MMChat introduce real human-human conversations. They are still limited by their small scale or lack of domain diversity, impeding the further explorations on multi-modal dialogue modeling.  \nTo address the aforementioned issue, we present MMDialog, a large-scale multi-turn dialogue dataset containing multi-modal open-domain conversations derived from real human-human chat content in social media. MMDialog~contains 1.08M dialogue sessions and 1.53M associated images. We elaborately design a series of data filtering processes during the data collection phase. On average, one dialogue session has 2.59 images, which can be located anywhere at any conversation turn.  {Figure~\\ref{fig:intro_case} depicts an example of human conversations in our MMDialog~dataset.} To the best of our knowledge, this is the first million-scale open-domain multi-modal dialogue corpora. We hope the large amount of dialogues and images can shed light on this line of research.\n\n\\begin{itemize}\n\\setlength{\\itemsep}{0pt}\n    \\item We construct a novel multi-turn dialogue dataset \\textbf{MMDialog}~that contains 1.08M multi-modal open-domain conversations and 1.53M associated images derived from social media and conduct data filtering and post-processing elaborately. To the best of our knowledge, this is the first million-scale multi-turn open-domain multi-modal dialogue corpus.\n    \\item We propose two benchmark tasks on MMDialog~that are essential for building more engaging multi-modal dialogue systems.\n    \\item We propose a novel evaluation metric \\textbf{MM-Relevance} measuring the relevance between generated multi-modal response and ground-truth response. It builds upon the large-scale pre-trained multi-modal CLIP model, which can specifically mitigate the modal misalignment issues.\n    \\item We design two baselines for corresponding tasks to promote future research on this dataset and achieve considerable performance on generation and retrieval tasks of both modalities. We also give comprehensive analysis to provide more insights into multi-modal dialogue modeling.\n\\end{itemize}\n\n\\section{Related Works}\n\\subsection{Multi-Modal Dialogue Datasets}\nRecently has witnessed a rapid development of visual-language modeling and emerged several multi-modal datasets. The multi-modal datasets include MSCOCO Image Caption dataset~\\cite{chen2015microsoft} for image captioning and image generation task; VQAv2 dataset~\\cite{goyal2017making} for visual question answering task; SNLI-VE dataset~\\cite{xie2019visual} for visual entailment task; RefCOCO~\\cite{yu2016modeling}, RefCOCO+~\\cite{yu2016modeling} and RefCOCOg~\\cite{mao2016generation} for referring expression comprehension task.\n\nConcurrent with the above works, several dialogue-related tasks have also been explored.~\\citet{das2017visual} introduced the task of Visual Dialog, which requires an AI agent to hold a meaningful dialogue with humans in natural, conversational language about visual content.~\\citet{mostafazadeh-etal-2017-image} proposed IGC, which contains 4K dialogues where each includes an image with a textual description, along with the questions and responses around the image. However, IGC is usually used for evaluation due to its small scale.~\\citet{shuster-etal-2020-image} released Image-Chat that is larger than IGC and consists of 202K image-grounded dialogues. However, the above three datasets were created by asking the crowd workers to talk about a shared image to generate the conversation. Therefore, the utterances are often triggered and grounded by these images. In contrast, human daily communication utterances are not always image-related~\\cite{zheng-etal-2022-mmchat}, which retain gaps with open-domain multi-modal conversation scenarios. Then, other groups of works proposed to derive the images from the multi-turn conversations:~\\citet{meng2020openvidial,wang2021openvidial} constructed OpenViDial 1.0/2.0 by directly extracting dialogues and their visual contexts from movies and TV series.~\\citet{lee-etal-2021-constructing} also built a multi-modal dialogue dataset by replacing the selected utterances with retrieved relevant images. However, although these corpora were constructed from open-domain conversations with images, they did not originate from a real multi-modal conversation scenario. Therefore, recently some researchers begin to introduce real human-human conversations.~\\citet{zang-etal-2021-photochat} created the first human-human dialogue dataset with photo-sharing acts via crowd-sourcing.~\\citet{zheng-etal-2022-mmchat} collected multi-modal dialogues from real conversations on social media. Nevertheless, they were still limited by their small scale or lack of domain diversity, which may hinder further explorations on multi-modal dialogue modeling. To address the aforementioned issue, we make the first attempt to construct a million-scale multi-turn dialogue dataset, namely \\textbf{MMDialog}, derived from social media and conduct data filtering and post-processing elaborately.\n\n\\subsection{Multi-Modal Dialogue Modeling}\nBased on the aforementioned multi-modal dialogue datasets, many advanced works have been proposed. Several modeling works~\\citep{qi2020two,niu2019recursive,gan2019multi} investigate how to escalate the performance of conversational agents in image-grounded dialogue. Afterward, researchers~\\citep{yang2021open,liang2021maria} explore enriching textual expressions of generated dialogue responses through associative vision scenes.~\\citet{zang-etal-2021-photochat} proposes two tasks, including photo-sharing intent prediction to predict whether model should intend to share the photo in the next dialogue turn and a dialogue-based image retrieval task to retrieve the most proper photo given the dialogue context. They also propose a dual-encoder model that uses object labels to encode image features, which achieves the best performance among all the models w/o cross-attention mechanisms. However, the authors do not conduct textual response retrieval tasks.~\\citet{zheng-etal-2022-mmchat} proposes a multi-modal dialogue generation model based on Seq2Seq architecture, which was proved to be superior to textual Seq2Seq model. However, this model can only generate plain textual responses, which is not in line with the open domain multi-modal response generation scenario. Recently,~\\citet{sun-etal-2022-multimodal} make the first attempt to build a multi-modal dialogue response generation model named Divter that can effectively understand multi-modal dialogue context and generate informative text and high-resolution image responses. As advanced works on dialogue systems include retrieval-based methods~\\citep{wu2017sequential,zhou2018multi,whang2020effective,li2021small} and generative methods~\\citep{li2015diversity,serban2016building,zhang2020dialogpt}. Therefore, we adapt Divter~\\cite{sun-etal-2022-multimodal} to our multi-modal response generation settings and extend the dual-encoder~\\cite{zang-etal-2021-photochat} to the retrieval-based scenarios as baselines.\n\n\\section{Corpus Statistics}\nMMDialog~consists of 1,079,117 unique dialogues and 1,531,339 images. The statistics of several multi-modal open-domain dialogue corpora are shown in Table~\\ref{tab:statistics}. On average one dialogue session has 2.59 images and 4.56 turns, and the images can be located anywhere in any turns of the conversation. We believe that in daily life, people are free to choose any modalities of conversational expressions at any stage of the conversation, and our dialogue data reflects this organizing style. Compared to the recently released multi-modal open domain dialogue dataset PhotoChat~\\cite{zang-etal-2021-photochat} and MMChat~\\cite{zheng-etal-2022-mmchat}, MMDialog~enjoys a significantly larger scale of dialogue data and more visual objects, especially that the volume of dialogue sessions has reached million-level. Since conversations originate from a wide range of hashtags presenting broad domains, the dialogues in MMDialog~are open-domain and cover diverse topics, which can shed light on research of multi-modal dialogue modeling. Besides, on average each dialogue turn in MMDialog~contains more text tokens than PhotoChat and MMChat, demonstrating that our proposed data may convey more semantic information in textual utterances.", "table_source": "\\begin{table}[t!] \n\\centering\n\\resizebox{0.5\\textwidth}{!}{\n\\begin{tabular}{lccc}\n\\toprule\n\nStatistics  & PhotoChat & MMChat & MMDialog \\\\ \\midrule\n\\#Language           & English    & Chinese   & English \\\\\n\\#Open-domain           &   \\ding{56}  & \\ding{52}   & \\ding{52} \\\\ \\midrule\n\\#Dialogues           & 12.29K    & 120.84K   & 1.08M \\\\\n\\#Images        & 10.92K    & 204.32K   & 1.53M \\\\\n\\#Turns         & 156.10K   & 314.13K   & 4.92M \\\\\nAvg.  \\#Turns per Dialogue  & 12.71     & 2.59      & 4.56  \\\\\nAvg. \\#Images per Dialogue & 0.89      & 2.91      & 2.59  \\\\\nAvg.  \\#Tokens per Turn & 6.33      & 8.52      & 15.64 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Statistics of MMDialog~and previous multi-modal dialogue datasets.}\n\\label{tab:statistics}\n\\end{table}", "cell_list_gold": [{"value": "12.29K", "char_index": [308, 314], "type": "Data Stat.", "dataset": "PhotoChat", "attribute name": ["# dialogues", "number of dialogues"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "false", "Language": "English"}}, {"value": "120.84K", "char_index": [320, 327], "type": "Data Stat.", "dataset": "MMChat", "attribute name": ["# dialogues", "number of dialogues"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "Chinese"}}, {"value": "1.08M", "char_index": [332, 337], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# dialogues", "number of dialogues"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "English"}}, {"value": "10.92K", "char_index": [359, 365], "type": "Data Stat.", "dataset": "PhotoChat", "attribute name": ["# images", "number of images"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "false", "Language": "English"}}, {"value": "204.32K", "char_index": [371, 378], "type": "Data Stat.", "dataset": "MMChat", "attribute name": ["# images", "number of images"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "Chinese"}}, {"value": "1.53M", "char_index": [383, 388], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# images", "number of images"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "English"}}, {"value": "156.10K", "char_index": [410, 417], "type": "Data Stat.", "dataset": "PhotoChat", "attribute name": ["# turns", "number of turns"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "false", "Language": "English"}}, {"value": "314.13K", "char_index": [422, 429], "type": "Data Stat.", "dataset": "MMChat", "attribute name": ["# turns", "number of turns"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "Chinese"}}, {"value": "4.92M", "char_index": [434, 439], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# turns", "number of turns"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "English"}}, {"value": "12.71", "char_index": [473, 478], "type": "Data Stat.", "dataset": "PhotoChat", "attribute name": ["avg. # turns per dialogue", "average number of turns per dialogue"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "false", "Language": "English"}}, {"value": "2.59", "char_index": [485, 489], "type": "Data Stat.", "dataset": "MMChat", "attribute name": ["avg. # turns per dialogue", "average number of turns per dialogue"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "Chinese"}}, {"value": "4.56", "char_index": [497, 501], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # turns per dialogue", "average number of turns per dialogue"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "English"}}, {"value": "0.89", "char_index": [535, 539], "type": "Data Stat.", "dataset": "PhotoChat", "attribute name": ["avg. # images per dialogue", "average number of images per dialogue"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "false", "Language": "English"}}, {"value": "2.91", "char_index": [547, 551], "type": "Data Stat.", "dataset": "MMChat", "attribute name": ["avg. # images per dialogue", "average number of images per dialogue"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "Chinese"}}, {"value": "2.59", "char_index": [559, 563], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # images per dialogue", "average number of images per dialogue"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "English"}}, {"value": "6.33", "char_index": [594, 598], "type": "Data Stat.", "dataset": "PhotoChat", "attribute name": ["avg. # tokens per turn", "average number of tokens per turn"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "false", "Language": "English"}}, {"value": "8.52", "char_index": [606, 610], "type": "Data Stat.", "dataset": "MMChat", "attribute name": ["avg. # tokens per turn", "average number of tokens per turn"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "Chinese"}}, {"value": "15.64", "char_index": [618, 623], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # tokens per turn", "average number of tokens per turn"], "sub-set/group name": "xx", "dataset features": {"Open-domain": "true", "Language": "English"}}]}, "2211.05719v1_table1": {"table_code": "\\begin{table}[t!] \n\\centering\n\\resizebox{0.5\\textwidth}{!}{\n\\begin{tabular}{lccc}\n\\toprule\n\nStatistics  & Train & Validation & Test \\\\ \\midrule\n\\#Dialogues           & 1,059,117    & 10,000   &  10,000 \\\\\n\\#Images        & 1,509,288    & 23,812   & 23,766 \\\\\n\\#Turns         & 4,825,054   & 45,382   & 45,798 \\\\\nAvg. \\#Turns per Dialogue  & 4.56    & 4.54     & 4.58  \\\\\nAvg. \\#Images per Dialogue & 2.59      & 2.58      & 2.61  \\\\\nAvg. \\#Tokens per Turn & 15.64     & 15.71      & 15.57 \\\\ \\midrule\nAvg. \\#(Neg. Images) per Dialogue & - & 999 & 999 \\\\\nAvg. \\#(Neg. Utterances) per Dialogue & - & 999 & 999 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Statistics of our train, validation, and test sets.}\n\\label{tab:statistics_split}\n\\end{table}", "table_label": "{tab:statistics_split}", "table_numeric_cells": [["1,059,117", "1,059,117", 168, 177, 168, 177], ["10,000", "10,000", 183, 189, 183, 189], ["10,000", "10,000", 195, 201, 195, 201], ["1,509,288", "1,509,288", 223, 232, 223, 232], ["23,812", "23,812", 238, 244, 238, 244], ["23,766", "23,766", 249, 255, 249, 255], ["4,825,054", "4,825,054", 277, 286, 277, 286], ["45,382", "45,382", 291, 297, 291, 297], ["45,798", "45,798", 302, 308, 302, 308], ["4.56", "4.56", 341, 345, 341, 345], ["4.54", "4.54", 351, 355, 351, 355], ["4.58", "4.58", 362, 366, 362, 366], ["2.59", "2.59", 400, 404, 400, 404], ["2.58", "2.58", 412, 416, 412, 416], ["2.61", "2.61", 424, 428, 424, 428], ["15.64", "15.64", 458, 463, 458, 463], ["15.71", "15.71", 470, 475, 470, 475], ["15.57", "15.57", 483, 488, 483, 488], ["999", "999", 541, 544, 541, 544], ["999", "999", 547, 550, 547, 550], ["999", "999", 598, 601, 598, 601], ["999", "999", 604, 607, 604, 607]], "text_chunk_selected": "\\begin{abstract}\nResponding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog~dataset to facilitate multi-modal conversation better. MMDialog~is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog~has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 8x. Second, it contains massive topics to generalize the open domain. To build an engaging dialogue system with this dataset, we propose and normalize two response prediction tasks based on retrieval and generative scenarios. In addition, we build two baselines for the above tasks with state-of-the-art techniques and report their experimental performance. We also propose a novel evaluation metric MM-Relevance to measure the multi-modal responses. Our dataset and scripts are available in \\url{https://github.com/victorsungo/MMDialog}.\n\\end{abstract}\n\nFurthermore, we define the multi-modal response generation and retrieval tasks based on MMDialog~that are essential for building a more engaging multi-modal dialogue agent. We build baseline models and conduct several analyses of their performance. For the generative task, we follow~\\citet{sun-etal-2022-multimodal} and implement the models for multi-modal response generation. For the retrieval task, we also propose a CLIP-based dual-encoder for retrieval tasks inspired by~\\citet{zang-etal-2021-photochat}. Since in our multi-modal response prediction settings, the modality orders of generated responses may not be aligned with the ground-truth responses. Thus, it is non-trivial to conduct evaluation on cross-modal response elements. To tackle the above challenges, we propose a novel evaluation metric named MM-Relevance, which performs visual-language matching based on the large-scale pre-trained multi-modal CLIP model~\\cite{radford2021learning}. Evaluation results on MMDialog~demonstrate that our designed baselines can achieve considerable performance on generation and retrieval tasks of both modalities.\n\n\\subsection{Multi-Modal Dialogue Modeling}\nBased on the aforementioned multi-modal dialogue datasets, many advanced works have been proposed. Several modeling works~\\citep{qi2020two,niu2019recursive,gan2019multi} investigate how to escalate the performance of conversational agents in image-grounded dialogue. Afterward, researchers~\\citep{yang2021open,liang2021maria} explore enriching textual expressions of generated dialogue responses through associative vision scenes.~\\citet{zang-etal-2021-photochat} proposes two tasks, including photo-sharing intent prediction to predict whether model should intend to share the photo in the next dialogue turn and a dialogue-based image retrieval task to retrieve the most proper photo given the dialogue context. They also propose a dual-encoder model that uses object labels to encode image features, which achieves the best performance among all the models w/o cross-attention mechanisms. However, the authors do not conduct textual response retrieval tasks.~\\citet{zheng-etal-2022-mmchat} proposes a multi-modal dialogue generation model based on Seq2Seq architecture, which was proved to be superior to textual Seq2Seq model. However, this model can only generate plain textual responses, which is not in line with the open domain multi-modal response generation scenario. Recently,~\\citet{sun-etal-2022-multimodal} make the first attempt to build a multi-modal dialogue response generation model named Divter that can effectively understand multi-modal dialogue context and generate informative text and high-resolution image responses. As advanced works on dialogue systems include retrieval-based methods~\\citep{wu2017sequential,zhou2018multi,whang2020effective,li2021small} and generative methods~\\citep{li2015diversity,serban2016building,zhang2020dialogpt}. Therefore, we adapt Divter~\\cite{sun-etal-2022-multimodal} to our multi-modal response generation settings and extend the dual-encoder~\\cite{zang-etal-2021-photochat} to the retrieval-based scenarios as baselines.\n\n\\paragraph{Task-2: Multi-modal Response Retrieval}\nAs for the retrieval-based models, each dialogue example $(U, R)$ additionally provides a series of negative multi-modal elements as distractions. We compose the ground-truth elements $\\{ r^m_l\\}_{l=1}^{L}$ in $R$ and the including distracting elements into an element candidate set $C = \\{ r^{m}_z\\}_{z=1}^{Z}$ for retrieval where $n_z$ is the total number of candidate elements including ground-truth and distracting ones. Meanwhile, we also assign corresponding element labels $\\{l^{m}_z\\}_{z=1}^{Z} \\in \\{0,1\\}$ with $l^{m}_z = 1$ indicating $r^{m}_z$ a proper element of $m$ modal type for $U$ and otherwise $l^{m}_z = 0$. Thus, the goal of a response retrieval model is to extract an element from a given element candidate set $C$ step by step while predicting each element $r^m_z$ in $R$ . Through such an element retrieval process in an auto-regressive style, we can obtain a multi-modal response consisting of retrieved elements, including textual utterances or visual images, or both of them.\n\nTo tackle the above issue, we propose a novel evaluation metric, named MM-Relevance, which performs visual-language matching based on the large-scale pre-trained multi-modal CLIP model~\\cite{radford2021learning} for multi-modal dialogue response generation task. CLIP is trained on a vast corpus of image-caption pairs from the Web. It learns to bring the embeddings of both modalities (visual and textual) together via a contrastive objective. Therefore, we utilize this model to assess the relevance between the generated responses and the ground-truth responses to mitigate modal misalignment issues. In specific, suppose we obtain a generated multi-modal response $\\Tilde{R}=\\{ \\Tilde{r}^m_k\\}_{k=1}^{n_{\\Tilde{r}}}$ where $n_{\\Tilde{r}}$ is the number of elements in $\\Tilde{R}$, and the corresponding ground-truth response $R = \\{ r^m_k\\}_{k=1}^{n_r}$. We first align the two sequences from the left. Then, the representation vector of each element is obtained by encoding the textual response or visual image through text encoder or image encoder pre-trained by CLIP respectively. We denote the encoded vectors of two responses as: $E=\\{ e^m_k\\}_{k=1}^{n_r}$ and $\\Tilde{E}=\\{ \\Tilde{e}^m_k\\}_{k=1}^{n_{\\Tilde{r}}}$. Then, we compute the CLIP scores of the two elements position by position until they cannot be aligned:\n\n\\begin{equation}\n\\begin{aligned}\n    \\text{MM}_{\\text{Rel}}(R, \\Tilde{R}) &= \\sum_{k=1}^{K}(e^m_k)^T \\cdot \\Tilde{e}^m_k \\\\\n    K & = \\min\\{n_r, n_{\\Tilde{r}} \\}\n\\end{aligned}\n\\end{equation}\n\n\\begin{equation}\n\\begin{aligned}\n    \\text{P}_{\\text{MM}} &= \\frac{\\text{MM}_{\\text{Rel}}(R, \\Tilde{R})}{n_{\\Tilde{r}}} \\\\ \n    \\text{R}_{\\text{MM}} &= \\frac{\\text{MM}_{\\text{Rel}}(R, \\Tilde{R})}{n_r} \\\\\n    \\text{F1}_{\\text{MM}} &= \\frac{2\\text{P}_{\\text{MM}}\\text{R}_{\\text{MM}}}{\\text{P}_{\\text{MM}}+\\text{R}_{\\text{MM}}} \\\\\n\\end{aligned} \n\\end{equation}\n\n\\subsection{Experimental Setup}\nWe first sample 10K and 10K dialogues sessions for validation and testing respectively. The detailed statistics are presented in Table~\\ref{tab:statistics_split}. For retrieval tasks, we randomly sample 999 negative textual utterances and 999 negative visual images from the same split set for each dialogue, maintaining the total number of candidate elements at 1K. While in training phase, the negative ones are in-batch sampled similar to~\\citet{radford2021learning}. For the textual dialogue response generator, we fine-tune DialoGPT~\\cite{zhang2020dialogpt} with \\textit{transformers} library provided by huggingface\\footnote{\\url{https://github.com/huggingface/transformers}} using the version ``DialoGPT-medium'' consistent with~\\citet{sun-etal-2022-multimodal}. For the description-to-image translator, we implement DALL-E~\\cite{ramesh2021zero} using the code of ``mega'' version in \\url{https://github.com/borisdayma/dalle-mini}, which also has the same model settings with~\\citet{sun-etal-2022-multimodal}. We fine-tune DALL-E mega for one epoch with initial learning rate 1e-7 and mini-batch size of 64. We process all images into 256 $\\times$ 256 RGB format for DALL-E. To obtain the description of images in MMDialog, we adopt OFA-huge~\\cite{wang2022ofa} using the code \\url{https://github.com/OFA-Sys/OFA/tree/feature/add_transformers} for image captioning. All version of CLIP models we leveraged in this paper are ``openai/clip-vit-base-patch32'' in \\url{https://huggingface.co/openai/clip-vit-base-patch32}. When implementing Divter, we follow the same experimental configuration. As for the retrieval baseline, the representation vectors for both modality are obtained by CLIP model and fixed during training. The transformers used in retrieval tasks consist of 4 Transformer layers with a hidden size of 512 and 8 heads. We train the retrieval models with an initial learning rate of 5e-7 and mini-batch size of 512. For all baselines, early stopping on the validation set is adopted as a regularization strategy and the best model is selected based on the validation performance. The training of both tasks is conducted on 8 Nvidia Tesla A100 80G GPU cards.", "table_source": "\\begin{table}[t!] \n\\centering\n\\resizebox{0.5\\textwidth}{!}{\n\\begin{tabular}{lccc}\n\\toprule\n\nStatistics  & Train & Validation & Test \\\\ \\midrule\n\\#Dialogues           & 1,059,117    & 10,000   &  10,000 \\\\\n\\#Images        & 1,509,288    & 23,812   & 23,766 \\\\\n\\#Turns         & 4,825,054   & 45,382   & 45,798 \\\\\nAvg. \\#Turns per Dialogue  & 4.56    & 4.54     & 4.58  \\\\\nAvg. \\#Images per Dialogue & 2.59      & 2.58      & 2.61  \\\\\nAvg. \\#Tokens per Turn & 15.64     & 15.71      & 15.57 \\\\ \\midrule\nAvg. \\#(Neg. Images) per Dialogue & - & 999 & 999 \\\\\nAvg. \\#(Neg. Utterances) per Dialogue & - & 999 & 999 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Statistics of our train, validation, and test sets.}\n\\label{tab:statistics_split}\n\\end{table}", "cell_list_gold": [{"value": "1,059,117", "char_index": [168, 177], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# dialogues", "number of dialogues"], "sub-set/group name": "train", "dataset features": {"xx": "yy"}}, {"value": "1,509,288", "char_index": [223, 232], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# images", "number of images"], "sub-set/group name": "train", "dataset features": {"xx": "yy"}}, {"value": "4,825,054", "char_index": [277, 286], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# turns", "number of turns"], "sub-set/group name": "train", "dataset features": {"xx": "yy"}}, {"value": "4.56", "char_index": [341, 345], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # turns per dialogue", "average number of turns per dialogue"], "sub-set/group name": "train", "dataset features": {"xx": "yy"}}, {"value": "2.59", "char_index": [400, 404], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # images per dialogue", "average number of images per dialogue"], "sub-set/group name": "train", "dataset features": {"xx": "yy"}}, {"value": "15.64", "char_index": [458, 463], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # tokens per turn", "average number of tokens per turn"], "sub-set/group name": "train", "dataset features": {"xx": "yy"}}, {"value": "10,000", "char_index": [183, 189], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# dialogues", "number of dialogues"], "sub-set/group name": "validation", "dataset features": {"xx": "yy"}}, {"value": "23,812", "char_index": [238, 244], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# images", "number of images"], "sub-set/group name": "validation", "dataset features": {"xx": "yy"}}, {"value": "45,382", "char_index": [291, 297], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# turns", "number of turns"], "sub-set/group name": "validation", "dataset features": {"xx": "yy"}}, {"value": "4.54", "char_index": [351, 355], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # turns per dialogue", "average number of turns per dialogue"], "sub-set/group name": "validation", "dataset features": {"xx": "yy"}}, {"value": "2.58", "char_index": [412, 416], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # images per dialogue", "average number of images per dialogue"], "sub-set/group name": "validation", "dataset features": {"xx": "yy"}}, {"value": "15.71", "char_index": [470, 475], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # tokens per turn", "average number of tokens per turn"], "sub-set/group name": "validation", "dataset features": {"xx": "yy"}}, {"value": "999", "char_index": [541, 544], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # (neg. images) per dialogue", "average number of negative images per dialogue"], "sub-set/group name": "validation", "dataset features": {"xx": "yy"}}, {"value": "999", "char_index": [598, 601], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # (neg. utterances) per dialogue", "average number of negative utterances per dialogue"], "sub-set/group name": "validation", "dataset features": {"xx": "yy"}}, {"value": "10,000", "char_index": [195, 201], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# dialogues", "number of dialogues"], "sub-set/group name": "test", "dataset features": {"xx": "yy"}}, {"value": "23,766", "char_index": [249, 255], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# images", "number of images"], "sub-set/group name": "test", "dataset features": {"xx": "yy"}}, {"value": "45,798", "char_index": [302, 308], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["# turns", "number of turns"], "sub-set/group name": "test", "dataset features": {"xx": "yy"}}, {"value": "4.58", "char_index": [362, 366], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # turns per dialogue", "average number of turns per dialogue"], "sub-set/group name": "test", "dataset features": {"xx": "yy"}}, {"value": "2.61", "char_index": [424, 428], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # images per dialogue", "average number of images per dialogue"], "sub-set/group name": "test", "dataset features": {"xx": "yy"}}, {"value": "15.57", "char_index": [483, 488], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # tokens per turn", "average number of tokens per turn"], "sub-set/group name": "test", "dataset features": {"xx": "yy"}}, {"value": "999", "char_index": [547, 550], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # (neg. images) per dialogue", "average number of negative images per dialogue"], "sub-set/group name": "test", "dataset features": {"xx": "yy"}}, {"value": "999", "char_index": [604, 607], "type": "Data Stat.", "dataset": "MMDialog", "attribute name": ["avg. # (neg. utterances) per dialogue", "average number of negative utterances per dialogue"], "sub-set/group name": "test", "dataset features": {"xx": "yy"}}]}, "2211.05719v1_table2": {"table_code": "\\begin{table*}[ht!] \n\\centering\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\multirow{2}{*}{Models} &  \\multicolumn{1}{c}{Intent}  & \\multicolumn{1}{c}{Image Generation} &  \\multicolumn{3}{c}{Textual Response Generation} & \\multicolumn{1}{c}{Multi-Modal Generation} \\\\ \n\\cmidrule(lr){2-2}  \\cmidrule(lr){3-3} \\cmidrule(lr){4-6} \\cmidrule(lr){7-7} \n                            &  F1      & IS$\\uparrow$  & BLEU-1 & BLEU-2 & ROUGE-L &  MM-Relevance$\\uparrow$   \\\\ \\midrule\nDivter~\\cite{sun-etal-2022-multimodal} &  75.56     &  21.84 $\\pm$ 0.81  &   8.78    & 6.86 &   11.57   &  68.40   \\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\caption{Automatic evaluation results of the generative baseline on the test set of MMDialog. All numbers except ``IS'' and ``MM-Relevance'' are in percentage.}\n\\label{tab:main_gen}\n\\end{table*}", "table_label": "{tab:main_gen}", "table_numeric_cells": [["75.56", "75.56", 535, 540, 535, 540], ["21.84", "21.84 $\\pm$ 0.81", 548, 553, 548, 564], ["8.78", "8.78", 570, 574, 570, 574], ["6.86", "6.86", 580, 584, 580, 584], ["11.57", "11.57", 589, 594, 589, 594], ["68.40", "68.40", 600, 605, 600, 605]], "text_chunk_selected": "Furthermore, we define the multi-modal response generation and retrieval tasks based on MMDialog~that are essential for building a more engaging multi-modal dialogue agent. We build baseline models and conduct several analyses of their performance. For the generative task, we follow~\\citet{sun-etal-2022-multimodal} and implement the models for multi-modal response generation. For the retrieval task, we also propose a CLIP-based dual-encoder for retrieval tasks inspired by~\\citet{zang-etal-2021-photochat}. Since in our multi-modal response prediction settings, the modality orders of generated responses may not be aligned with the ground-truth responses. Thus, it is non-trivial to conduct evaluation on cross-modal response elements. To tackle the above challenges, we propose a novel evaluation metric named MM-Relevance, which performs visual-language matching based on the large-scale pre-trained multi-modal CLIP model~\\cite{radford2021learning}. Evaluation results on MMDialog~demonstrate that our designed baselines can achieve considerable performance on generation and retrieval tasks of both modalities.\n\n\\begin{itemize}\n\\setlength{\\itemsep}{0pt}\n    \\item We construct a novel multi-turn dialogue dataset \\textbf{MMDialog}~that contains 1.08M multi-modal open-domain conversations and 1.53M associated images derived from social media and conduct data filtering and post-processing elaborately. To the best of our knowledge, this is the first million-scale multi-turn open-domain multi-modal dialogue corpus.\n    \\item We propose two benchmark tasks on MMDialog~that are essential for building more engaging multi-modal dialogue systems.\n    \\item We propose a novel evaluation metric \\textbf{MM-Relevance} measuring the relevance between generated multi-modal response and ground-truth response. It builds upon the large-scale pre-trained multi-modal CLIP model, which can specifically mitigate the modal misalignment issues.\n    \\item We design two baselines for corresponding tasks to promote future research on this dataset and achieve considerable performance on generation and retrieval tasks of both modalities. We also give comprehensive analysis to provide more insights into multi-modal dialogue modeling.\n\\end{itemize}\n\n\\subsection{Multi-Modal Dialogue Modeling}\nBased on the aforementioned multi-modal dialogue datasets, many advanced works have been proposed. Several modeling works~\\citep{qi2020two,niu2019recursive,gan2019multi} investigate how to escalate the performance of conversational agents in image-grounded dialogue. Afterward, researchers~\\citep{yang2021open,liang2021maria} explore enriching textual expressions of generated dialogue responses through associative vision scenes.~\\citet{zang-etal-2021-photochat} proposes two tasks, including photo-sharing intent prediction to predict whether model should intend to share the photo in the next dialogue turn and a dialogue-based image retrieval task to retrieve the most proper photo given the dialogue context. They also propose a dual-encoder model that uses object labels to encode image features, which achieves the best performance among all the models w/o cross-attention mechanisms. However, the authors do not conduct textual response retrieval tasks.~\\citet{zheng-etal-2022-mmchat} proposes a multi-modal dialogue generation model based on Seq2Seq architecture, which was proved to be superior to textual Seq2Seq model. However, this model can only generate plain textual responses, which is not in line with the open domain multi-modal response generation scenario. Recently,~\\citet{sun-etal-2022-multimodal} make the first attempt to build a multi-modal dialogue response generation model named Divter that can effectively understand multi-modal dialogue context and generate informative text and high-resolution image responses. As advanced works on dialogue systems include retrieval-based methods~\\citep{wu2017sequential,zhou2018multi,whang2020effective,li2021small} and generative methods~\\citep{li2015diversity,serban2016building,zhang2020dialogpt}. Therefore, we adapt Divter~\\cite{sun-etal-2022-multimodal} to our multi-modal response generation settings and extend the dual-encoder~\\cite{zang-etal-2021-photochat} to the retrieval-based scenarios as baselines.\n\nTo tackle the above issue, we propose a novel evaluation metric, named MM-Relevance, which performs visual-language matching based on the large-scale pre-trained multi-modal CLIP model~\\cite{radford2021learning} for multi-modal dialogue response generation task. CLIP is trained on a vast corpus of image-caption pairs from the Web. It learns to bring the embeddings of both modalities (visual and textual) together via a contrastive objective. Therefore, we utilize this model to assess the relevance between the generated responses and the ground-truth responses to mitigate modal misalignment issues. In specific, suppose we obtain a generated multi-modal response $\\Tilde{R}=\\{ \\Tilde{r}^m_k\\}_{k=1}^{n_{\\Tilde{r}}}$ where $n_{\\Tilde{r}}$ is the number of elements in $\\Tilde{R}$, and the corresponding ground-truth response $R = \\{ r^m_k\\}_{k=1}^{n_r}$. We first align the two sequences from the left. Then, the representation vector of each element is obtained by encoding the textual response or visual image through text encoder or image encoder pre-trained by CLIP respectively. We denote the encoded vectors of two responses as: $E=\\{ e^m_k\\}_{k=1}^{n_r}$ and $\\Tilde{E}=\\{ \\Tilde{e}^m_k\\}_{k=1}^{n_{\\Tilde{r}}}$. Then, we compute the CLIP scores of the two elements position by position until they cannot be aligned:\n\n\\begin{equation}\n\\begin{aligned}\n    \\text{MM}_{\\text{Rel}}(R, \\Tilde{R}) &= \\sum_{k=1}^{K}(e^m_k)^T \\cdot \\Tilde{e}^m_k \\\\\n    K & = \\min\\{n_r, n_{\\Tilde{r}} \\}\n\\end{aligned}\n\\end{equation}\n\n\\begin{equation}\n\\begin{aligned}\n    \\text{P}_{\\text{MM}} &= \\frac{\\text{MM}_{\\text{Rel}}(R, \\Tilde{R})}{n_{\\Tilde{r}}} \\\\ \n    \\text{R}_{\\text{MM}} &= \\frac{\\text{MM}_{\\text{Rel}}(R, \\Tilde{R})}{n_r} \\\\\n    \\text{F1}_{\\text{MM}} &= \\frac{2\\text{P}_{\\text{MM}}\\text{R}_{\\text{MM}}}{\\text{P}_{\\text{MM}}+\\text{R}_{\\text{MM}}} \\\\\n\\end{aligned} \n\\end{equation}\n\n\\subsection{Multi-modal Response Generation Model}\nWe consider to implement the state-of-the-art multi-modal dialogue response generation model Divter (Figure \\ref{fig:model}a) proposed by~\\citet{sun-etal-2022-multimodal}, which consists of two components: a textual dialogue response generator $\\mathcal{G}$ and a  description-to-image translator $\\mathcal{F}$. \n\n\\subsection{Results of Multi-modal Response Generation Model}\nTable~\\ref{tab:main_gen} reports the evaluation results of multi-modal response generation baseline. Follow~\\citet{sun-etal-2022-multimodal}, we evaluate the textual response generation, image generation and intent prediction tasks. Firstly, we can find that the state-of-the-art model Divter achieves relatively low textual response generation performance (8.78 on BLEU-1 and 11.57 on ROUGE-L) on our proposed MMDialog, which validates the difficulty of multi-modal response generation tasks and also demonstrates the necessity of constructing a large-scale multi-modal dialogue dataset for building data-driven models. Secondly, compared with the results on text generation, we are surprised to find that the model achieves better performance on the image generation task and reaches 21.84 on IS. Thirdly, we observe that the baseline achieve a 75.56 F1 score on intent prediction task, indicating that the model has a considerable ability to determine whether to generate text or images during the conversation. Finally, we also leverage the proposed MM-Relevance to evaluate the overall relevance degree between the generated multi-modal dialogue responses and ground-truth ones and our baseline achieves a score of 68.40.", "table_source": "\\begin{table*}[ht!] \n\\centering\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\multirow{2}{*}{Models} &  \\multicolumn{1}{c}{Intent}  & \\multicolumn{1}{c}{Image Generation} &  \\multicolumn{3}{c}{Textual Response Generation} & \\multicolumn{1}{c}{Multi-Modal Generation} \\\\ \n\\cmidrule(lr){2-2}  \\cmidrule(lr){3-3} \\cmidrule(lr){4-6} \\cmidrule(lr){7-7} \n                            &  F1      & IS$\\uparrow$  & BLEU-1 & BLEU-2 & ROUGE-L &  MM-Relevance$\\uparrow$   \\\\ \\midrule\nDivter~\\cite{sun-etal-2022-multimodal} &  75.56     &  21.84 $\\pm$ 0.81  &   8.78    & 6.86 &   11.57   &  68.40   \\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\caption{Automatic evaluation results of the generative baseline on the test set of MMDialog. All numbers except ``IS'' and ``MM-Relevance'' are in percentage.}\n\\label{tab:main_gen}\n\\end{table*}", "cell_list_gold": [{"value": "75.56", "char_index": [535, 540], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Intent Prediction", "metric": "F1", "experimental settings": {"xx": "yy"}, "model": "Divter", "model settings": {"xx": "yy"}}, {"value": "21.84", "char_index": [548, 553], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Image Generation", "metric": "IS", "experimental settings": {"xx": "yy"}, "model": "Divter", "model settings": {"xx": "yy"}}, {"value": "8.78", "char_index": [570, 574], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Textual Response Generation", "metric": "BLEU-1", "experimental settings": {"xx": "yy"}, "model": "Divter", "model settings": {"xx": "yy"}}, {"value": "6.86", "char_index": [580, 584], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Textual Response Generation", "metric": "BLEU-2", "experimental settings": {"xx": "yy"}, "model": "Divter", "model settings": {"xx": "yy"}}, {"value": "11.57", "char_index": [589, 594], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Textual Response Generation", "metric": "ROUGE-L", "experimental settings": {"xx": "yy"}, "model": "Divter", "model settings": {"xx": "yy"}}, {"value": "68.40", "char_index": [600, 605], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Multi-Modal Generation", "metric": "MM-Relevance", "experimental settings": {"xx": "yy"}, "model": "Divter", "model settings": {"xx": "yy"}}]}, "2211.05719v1_table3": {"table_code": "\\begin{table*}[ht!]\n\\centering\n\\resizebox{0.69\\textwidth}{!}{\n\\begin{tabular}{lccccccc}\n\\toprule\n\\multirow{2}{*}{Models} &  \\multicolumn{1}{c}{Intent} & \\multicolumn{4}{c}{Image Retrieval}  \n\\\\ \\cmidrule(lr){2-2} \\cmidrule(lr){3-6}\n\n      &  F1  & R@1   & R@5   & R@10 & Sum(R@1,5,10)  \\\\ \\midrule \nDE++~\\cite{zang-etal-2021-photochat}     & 59.04  & 29.55     &  45.14   & 53.61    &  128.30  \\\\ \n\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Automatic evaluation results of the retrieval baselines on the test set of MMDialog. All numbers are in percentage.}\n\\label{tab:main_ret}\n\\end{table*}", "table_label": "{tab:main_ret}", "table_numeric_cells": [["59.04", "59.04", 342, 347, 342, 347], ["29.55", "29.55", 351, 356, 351, 356], ["45.14", "45.14", 364, 369, 364, 369], ["53.61", "53.61", 374, 379, 374, 379], ["128.30", "128.30", 386, 392, 386, 392]], "text_chunk_selected": "\\subsection{Multi-Modal Dialogue Modeling}\nBased on the aforementioned multi-modal dialogue datasets, many advanced works have been proposed. Several modeling works~\\citep{qi2020two,niu2019recursive,gan2019multi} investigate how to escalate the performance of conversational agents in image-grounded dialogue. Afterward, researchers~\\citep{yang2021open,liang2021maria} explore enriching textual expressions of generated dialogue responses through associative vision scenes.~\\citet{zang-etal-2021-photochat} proposes two tasks, including photo-sharing intent prediction to predict whether model should intend to share the photo in the next dialogue turn and a dialogue-based image retrieval task to retrieve the most proper photo given the dialogue context. They also propose a dual-encoder model that uses object labels to encode image features, which achieves the best performance among all the models w/o cross-attention mechanisms. However, the authors do not conduct textual response retrieval tasks.~\\citet{zheng-etal-2022-mmchat} proposes a multi-modal dialogue generation model based on Seq2Seq architecture, which was proved to be superior to textual Seq2Seq model. However, this model can only generate plain textual responses, which is not in line with the open domain multi-modal response generation scenario. Recently,~\\citet{sun-etal-2022-multimodal} make the first attempt to build a multi-modal dialogue response generation model named Divter that can effectively understand multi-modal dialogue context and generate informative text and high-resolution image responses. As advanced works on dialogue systems include retrieval-based methods~\\citep{wu2017sequential,zhou2018multi,whang2020effective,li2021small} and generative methods~\\citep{li2015diversity,serban2016building,zhang2020dialogpt}. Therefore, we adapt Divter~\\cite{sun-etal-2022-multimodal} to our multi-modal response generation settings and extend the dual-encoder~\\cite{zang-etal-2021-photochat} to the retrieval-based scenarios as baselines.\n\n\\begin{equation}\n\\begin{aligned}\n    \\text{MM}_{\\text{Rel}}(R, \\Tilde{R}) &= \\sum_{k=1}^{K}(e^m_k)^T \\cdot \\Tilde{e}^m_k \\\\\n    K & = \\min\\{n_r, n_{\\Tilde{r}} \\}\n\\end{aligned}\n\\end{equation}\n\n\\begin{equation}\n\\begin{aligned}\n    \\text{P}_{\\text{MM}} &= \\frac{\\text{MM}_{\\text{Rel}}(R, \\Tilde{R})}{n_{\\Tilde{r}}} \\\\ \n    \\text{R}_{\\text{MM}} &= \\frac{\\text{MM}_{\\text{Rel}}(R, \\Tilde{R})}{n_r} \\\\\n    \\text{F1}_{\\text{MM}} &= \\frac{2\\text{P}_{\\text{MM}}\\text{R}_{\\text{MM}}}{\\text{P}_{\\text{MM}}+\\text{R}_{\\text{MM}}} \\\\\n\\end{aligned} \n\\end{equation}\n\nAs for the retrieval tasks, we also include ranking-based evaluation metrics recall $n$ at $k$ including R@1, R@5 and R@10 consistent with previous works~\\cite{zang-etal-2021-photochat}, measuring if the ground-truth textual or visual outputs can be ranked in top $k \\in \\{1, 5, 10\\}$ positions among $n$ element candidates. With regard to intent prediction, we follow~\\citet{zang-etal-2021-photochat} and adopt F1 score as the evaluation metrics that measures the accuracy of the model's prediction of the modality order for a dialogue turn. \n\n\\subsection{Multi-modal Response Retrieval Model}\nInspired by~\\citet{parekh-etal-2021-crisscrossed} and \\citet{zang-etal-2021-photochat}, we also build a retrieval model $\\mathcal{R}$ named DE++ which consists of a modality intent prediction module $\\mathcal{R}_\\alpha$ and a ranking module $\\mathcal{R}_\\beta$. As shown in Figure \\ref{fig:model}b, before each ranking action, $\\mathcal{R}_\\alpha$ firstly takes the dialogue context $U$ and previous retrieved response elements $\\Tilde{R}_{<z}$ before $z$-th step as inputs and predicts i) the response is completed and model should stop retrieving new elements. or ii) the modality of next elements. If i), the $\\mathcal{R}_\\alpha$ will take $U$, $\\Tilde{R}_{<z}$ as input to predicts the intention $\\mathcal{I} ([U,\\Tilde{R}_{<z}])$; if ii), $\\mathcal{R}_\\beta$ will calculate the relevance score  $\\mathcal{S} ([U,\\Tilde{R}_{<z}], r)$. In the same light,  $\\mathcal{R}_\\beta$ measures all candidates in  $\\{r^{m}_z\\}_{z=1}^{Z}$ and selects the one with highest relevance score as the final response.\n\n\\subsection{Experimental Setup}\nWe first sample 10K and 10K dialogues sessions for validation and testing respectively. The detailed statistics are presented in Table~\\ref{tab:statistics_split}. For retrieval tasks, we randomly sample 999 negative textual utterances and 999 negative visual images from the same split set for each dialogue, maintaining the total number of candidate elements at 1K. While in training phase, the negative ones are in-batch sampled similar to~\\citet{radford2021learning}. For the textual dialogue response generator, we fine-tune DialoGPT~\\cite{zhang2020dialogpt} with \\textit{transformers} library provided by huggingface\\footnote{\\url{https://github.com/huggingface/transformers}} using the version ``DialoGPT-medium'' consistent with~\\citet{sun-etal-2022-multimodal}. For the description-to-image translator, we implement DALL-E~\\cite{ramesh2021zero} using the code of ``mega'' version in \\url{https://github.com/borisdayma/dalle-mini}, which also has the same model settings with~\\citet{sun-etal-2022-multimodal}. We fine-tune DALL-E mega for one epoch with initial learning rate 1e-7 and mini-batch size of 64. We process all images into 256 $\\times$ 256 RGB format for DALL-E. To obtain the description of images in MMDialog, we adopt OFA-huge~\\cite{wang2022ofa} using the code \\url{https://github.com/OFA-Sys/OFA/tree/feature/add_transformers} for image captioning. All version of CLIP models we leveraged in this paper are ``openai/clip-vit-base-patch32'' in \\url{https://huggingface.co/openai/clip-vit-base-patch32}. When implementing Divter, we follow the same experimental configuration. As for the retrieval baseline, the representation vectors for both modality are obtained by CLIP model and fixed during training. The transformers used in retrieval tasks consist of 4 Transformer layers with a hidden size of 512 and 8 heads. We train the retrieval models with an initial learning rate of 5e-7 and mini-batch size of 512. For all baselines, early stopping on the validation set is adopted as a regularization strategy and the best model is selected based on the validation performance. The training of both tasks is conducted on 8 Nvidia Tesla A100 80G GPU cards.\n\n\\subsection{Results of Multi-modal Response Generation Model}\nTable~\\ref{tab:main_gen} reports the evaluation results of multi-modal response generation baseline. Follow~\\citet{sun-etal-2022-multimodal}, we evaluate the textual response generation, image generation and intent prediction tasks. Firstly, we can find that the state-of-the-art model Divter achieves relatively low textual response generation performance (8.78 on BLEU-1 and 11.57 on ROUGE-L) on our proposed MMDialog, which validates the difficulty of multi-modal response generation tasks and also demonstrates the necessity of constructing a large-scale multi-modal dialogue dataset for building data-driven models. Secondly, compared with the results on text generation, we are surprised to find that the model achieves better performance on the image generation task and reaches 21.84 on IS. Thirdly, we observe that the baseline achieve a 75.56 F1 score on intent prediction task, indicating that the model has a considerable ability to determine whether to generate text or images during the conversation. Finally, we also leverage the proposed MM-Relevance to evaluate the overall relevance degree between the generated multi-modal dialogue responses and ground-truth ones and our baseline achieves a score of 68.40.\n\n\\subsection{Results of Multi-modal Response Retrieval Model}\nWe also conduct the retrieval baselines and show the results in Table~\\ref{tab:main_ret}. Our proposed baseline DE++ achieves the image retrieval performance with 29.55\\% R@1, 45.14\\% R@5 and 53.61\\% R@10, which demonstrating the capacity of multi-modal retrieval model and the effectiveness of CLIP representation. As for the intent prediction, the F1 score is 59.04 which is inferior to the counterpart in generative baseline Divter. This may be due to the fact that 24 layers of transformer (i.e., DialoGPT-medium) is used to encode the context in Divter but only 4 layers are used in DE++.", "table_source": "\\begin{table*}[ht!]\n\\centering\n\\resizebox{0.69\\textwidth}{!}{\n\\begin{tabular}{lccccccc}\n\\toprule\n\\multirow{2}{*}{Models} &  \\multicolumn{1}{c}{Intent} & \\multicolumn{4}{c}{Image Retrieval}  \n\\\\ \\cmidrule(lr){2-2} \\cmidrule(lr){3-6}\n\n      &  F1  & R@1   & R@5   & R@10 & Sum(R@1,5,10)  \\\\ \\midrule \nDE++~\\cite{zang-etal-2021-photochat}     & 59.04  & 29.55     &  45.14   & 53.61    &  128.30  \\\\ \n\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Automatic evaluation results of the retrieval baselines on the test set of MMDialog. All numbers are in percentage.}\n\\label{tab:main_ret}\n\\end{table*}", "cell_list_gold": [{"value": "59.04", "char_index": [342, 347], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Intent Prediction", "metric": "F1", "experimental settings": {"xx": "yy"}, "model": "DE++", "model settings": {"xx": "yy"}}, {"value": "29.55", "char_index": [351, 356], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Image Retrieval", "metric": "R@1", "experimental settings": {"xx": "yy"}, "model": "DE++", "model settings": {"xx": "yy"}}, {"value": "45.14", "char_index": [364, 369], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Image Retrieval", "metric": "R@5", "experimental settings": {"xx": "yy"}, "model": "DE++", "model settings": {"xx": "yy"}}, {"value": "53.61", "char_index": [374, 379], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Image Retrieval", "metric": "R@10", "experimental settings": {"xx": "yy"}, "model": "DE++", "model settings": {"xx": "yy"}}, {"value": "128.30", "char_index": [386, 392], "type": "Result", "training data/set": "MMDialog", "test data/set": "MMDialog", "task": "Image Retrieval", "metric": "Sum(R@1,5,10)", "experimental settings": {"xx": "yy"}, "model": "DE++", "model settings": {"xx": "yy"}}]}, "2211.05739v1_table0": {"table_code": "\\begin{table}[t]\n\\centering\n\\begin{adjustbox}{width=\\columnwidth,  center}\n\\begin{tabular}{|c|ccc|cc|}\n\\hline\n\\multirow{2}{*}{\\textbf{Dataset}} & \\multicolumn{3}{c|}{\\textbf{Hyperparameters}}                                                           & \\multicolumn{2}{c|}{\\textbf{Number of Training Rounds}}                             \\\\ \\cline{2-6} \n                                  & \\multicolumn{1}{c|}{\\textbf{Epochs}} & \\multicolumn{1}{c|}{\\textbf{Batch Size}} & \\textbf{Learning Rate} & \\multicolumn{1}{c|}{\\textbf{Standard}} & \\textbf{Straggler (\\%)} \\\\ \\hline\nMNIST                             & \\multicolumn{1}{c|}{5}               & \\multicolumn{1}{c|}{10}                  & $1e-3$                 & \\multicolumn{1}{c|}{60}                & 60                      \\\\ \\hline\nFEMNIST                           & \\multicolumn{1}{c|}{5}               & \\multicolumn{1}{c|}{10}                  & $1e-3$                  & \\multicolumn{1}{c|}{40}                & 40                      \\\\ \\hline\nShakespeare                       & \\multicolumn{1}{c|}{1}               & \\multicolumn{1}{c|}{32}                  & $0.8$                 & \\multicolumn{1}{c|}{25}                & 25                      \\\\ \\hline\nSpeech Command                    & \\multicolumn{1}{c|}{5}               & \\multicolumn{1}{c|}{5}                   & $1e-3$                  & \\multicolumn{1}{c|}{35}                & 60                      \\\\ \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Experiment hyperparameters for the different datasets (\\S\\ref{sec:datasets}) and experiment scenarios (\\S\\ref{sec:scenarios}).}\n\n\\vspace{-5mm}\n\\label{tab:experiment-params}\n\\end{table}", "table_label": "{tab:experiment-params}", "table_numeric_cells": [["5", "\\multicolumn{1}{c|}{5}", 626, 627, 606, 628], ["10", "\\multicolumn{1}{c|}{10}", 665, 667, 645, 668], ["1e-3", "$1e-3$", 689, 693, 688, 694], ["60", "\\multicolumn{1}{c|}{60}", 733, 735, 713, 736], ["60", "60", 754, 756, 754, 756], ["5", "\\multicolumn{1}{c|}{5}", 844, 845, 824, 846], ["10", "\\multicolumn{1}{c|}{10}", 883, 885, 863, 886], ["1e-3", "$1e-3$", 907, 911, 906, 912], ["40", "\\multicolumn{1}{c|}{40}", 952, 954, 932, 955], ["40", "40", 973, 975, 973, 975], ["1", "\\multicolumn{1}{c|}{1}", 1063, 1064, 1043, 1065], ["32", "\\multicolumn{1}{c|}{32}", 1102, 1104, 1082, 1105], ["0.8", "$0.8$", 1126, 1129, 1125, 1130], ["25", "\\multicolumn{1}{c|}{25}", 1169, 1171, 1149, 1172], ["25", "25", 1190, 1192, 1190, 1192], ["5", "\\multicolumn{1}{c|}{5}", 1280, 1281, 1260, 1282], ["5", "\\multicolumn{1}{c|}{5}", 1319, 1320, 1299, 1321], ["1e-3", "$1e-3$", 1343, 1347, 1342, 1348], ["35", "\\multicolumn{1}{c|}{35}", 1388, 1390, 1368, 1391], ["60", "60", 1409, 1411, 1409, 1411]], "text_chunk_selected": "The different proposed approaches for straggler mitigation in FL can be divided into three categories, i.e., \\textit{synchronous}~\\cite{fedprox}, \\textit{asynchronous}~\\cite{asofed,fed_async}, and \\textit{semi-asynchronous}~\\cite{fedAt, safa, csafl}. One of the most popular synchronous approaches is \\texttt{FedProx}~\\cite{fedprox}, which is based on \\texttt{FedAvg}~\\cite{mcmahan2017communication} with two minor differences. First, a custom loss function for client-side training that limits the varying effects of local model updates. Second, toleration for partial work, i.e., clients can perform a variable amount of work to accommodate constraints in terms of hardware and network. However, incorporating partial work requires tailoring the number of local epochs for each client individually, which might be infeasible for a significantly large number of clients. Moreover, in contrast to \\texttt{FedLesScan} it relies on random selection of clients which makes it sensitive to stragglers. In~\\cite{fed_async}, Xie et al. propose an asynchronous federated optimization algorithm called \\texttt{FedAsync} that relies on a parameter server architecture to invoke and synchronize FL clients. The server consists of two parallel threads, i.e., a \\textit{scheduler} and an \\textit{updater}. The scheduler thread periodically triggers the FL clients to perform training using the latest global model, while the updater thread receives the client model updates and directly aggregates them to the global\nmodel. However, in the context of serverless, this approach has significantly higher resource and communication costs (\\S\\ref{sec:faas}, \\S\\ref{sec:servfl}) since it would require a function invocation to perform aggregation with the global model after each individual client update.\n\n\\subsection{Gathering Behavioral Data}\n\\label{sec:behavedata}\nOur client selection strategy (\\S\\ref{sec:ClientSelection}) depends on the data collected from the clients' behavior in previous training rounds. Towards this, for each client we collect three attributes, i.e., \\textit{training time}, \\textit{missed rounds}, and \\textit{cooldown}. \\textit{Training time} is the time taken by the client to complete local model training. \\textit{Missed rounds} is a list that contains the round numbers of the rounds missed by a client. We use this to calculate a client's penalty as described in \\S\\ref{sec:ClientSelection}. \\textit{Cooldown} represents the number of rounds a client has to stay in the last tier and cannot participate in clustering (\\S\\ref{sec:tiers}). We evaluate the cooldown period from the client\u2019s last missed round using Equation~\\ref{eq: cooldown}. For instance, if a client missed round $2$, the cooldown is set to $1$. Moreover, if the same client missed round $4$, the cooldown is multiplied by two. As a result, the clients with \\textit{cooldown} greater than zero are characterized as \\textit{stragglers}. Towards this, using \\textit{cooldown} can reduce the impact of temporarily slow or unavailable clients by lowering their priority for a certain number of rounds (\\S\\ref{sec:tiers}, \\S\\ref{sec:ClientSelection}).\n\n\\subsection{Staleness-aware Aggregation}\n\\label{sec:stalelessaware}\nAlthough our intelligent client selection strategy (\\S\\ref{sec:ClientSelection}) improves the efficiency of the system, stragglers are not completely eliminated. Stragglers might push their local model updates to the parameter server after the completion of an FL training round. Moreover, these updates might contain valuable information that can improve the performance of the global model. Towards this, we aggregate the delayed updates of the clients with a dampening effect asynchronously, i.e., delayed updates are considered the next time the aggregation function is called. \nEquation~\\ref{eq:fedless-aggr} shows our updated aggregation function used to include stale updates. $w^k_{t_k}$ is the local model of client \\textit{k} at round $t_k$ and $w_{t+1}$ is the global model after aggregation at round $t$. Furthermore, $n_k$ represents the cardinality of the dataset at client $k$ while $n$ is the total cardinality of the aggregated clients.  If the updates arrive at the same round ($t_k = t$), the equation becomes similar to \\texttt{FedAvg}~\\cite{mcmahan2017communication}. On the other hand, older updates ($t_k < t$), are dampened by $\\frac{t_k}{t}$. To avoid obsolete updates from affecting the training, the aggregator uses a parameter $\\tau$ to dictate the maximum age of updates included in the aggregation. Updates with $t - t_k \\ge \\tau$ are discarded by the aggregator. In our experiments, we use a value of two for $\\tau$.\n\nFor MNIST, we use a 2-layer Convolutional Neural Network (CNN) with a 5x5 kernel. Each convolutional layer is followed by a 2x2 max-pooling layer.  The model ends with a fully-connected layer with 512 neurons and a ten-neuron output layer. Similarly, for FEMNIST we use a 2-layer CNN with a 5x5 kernel in which each convolutional layer is followed by a 2x2 max-pooling layer. The CNN layers are followed by a fully-connected layer with 2048 neurons, which is followed by an output layer with 62 neurons. For the Shakespeare dataset, we use a Long Short Term Memory (LSTM) recurrent neural network~\\cite{hochreiter1997long}. The model consists of an embedding layer of size eight followed by two LSTM layers with 256 units and an output layer of size 82. Our model architecture for Google Speech consists of two identical blocks followed by an average pooling layer and an output layer with 35 neurons. A block contains two convolutional layers with a 3x3 kernel followed by a max-pooling layer. A dropout layer follows the max-pooling layer with a rate of \\texttt{0.25} to avoid overfitting. For MNIST, FEMNIST, and Google Speech, we use Adam~\\cite{adam} as the optimizer with a learning rate of $1e-3$. On the other hand, for Shakespeare, we use SGD~\\cite{sgd} with a learning rate of $0.8$. The clients for the MNIST, FEMNIST, and Speech datasets train for five local epochs with a batch size of $10$, $10$, and five respectively. Due to the limitations of the current commercial FaaS platforms (\\S\\ref{sec:faas}), the clients for Shakespeare train for one local epoch with a batch size of $32$. The hyperparameters for the different datasets are shown in Table~\\ref{tab:experiment-params}.\n\n\\textbf{Straggler (\\%) Scenario}. In this, we simulate varying percentages of stragglers in the FL system.  Although there might be different reasons for FaaS client failures, such as memory limit, function timeout, or communication issues, these failures can only have one of two effects on the clients. Clients can either push their updates after the training round is finished (slow updates) or can completely crash (not push their updates). To simulate slow updates, we limit the training round time to only fit clients with no issues or delays. Meaning, that clients which experience cold starts, bandwidth limitations, or communication delays do not finish the round in time; therefore, pushing their updates later. On the other hand, to simulate failures, we randomly select a specific ratio of clients to fail their training at the beginning of each experiment. We perform four different experiments for each dataset with 10\\%, 30\\%, 50\\%, and 70\\% stragglers in the system. Note that, in all our experiments, stragglers are different from \\textit{malicious clients} found in some FL settings~\\cite{li2020learning, zhang2022fldetector}. While malicious clients can act as stragglers to deliberately slow training or hinder model performance, they can perform more powerful attacks such as data or model poisoning~\\cite{fl_challenges,fl_data_poisoning} to skew the model's performance. In our work, we focus on the problem of stragglers and their behavior rather than the clients' intentions. The number of training rounds for the different datasets and experiment scenarios is shown in Table~\\ref{tab:experiment-params}.\n\nThe obtained accuracy and EUR values across all our experiments is summarized in Table~\\ref{tab:acc-all}. For the \\textit{standard scenarios} (\\S\\ref{sec:scenarios}), we obtained better accuracy and round utlization for our strategy as compared to \\texttt{FedAvg} and \\texttt{FedProx} across all datasets except Shakespeare. This is because with the Shakespeare dataset, some of the clients contribute to the model accuracy more than others, especially clients with longer training times. Moreover, due to budget constraints and significantly high training costs, we train for a slightly less number of rounds, i.e., 25 for Shakespeare. However, we argue that in a more realistic scenario with more number of rounds, the difference in accuracy will decrease. This is because our strategy utilizes clients more efficiently, i.e., higher EUR. As a result, with more rounds the number of invocations per client will increase and more clients will contribute to the global model, leading to a higher accuracy. Similarly, for the \\textit{straggler (\\%) scenarios}, we obtained better results for accuracy with our strategy as compared to the other two for the MNIST, FEMNIST, and the Google Speech datasets. On the Shakespeare dataset, our strategy outperformed \\texttt{FedAvg} and \\texttt{FedProx} in scenarios with 30\\%, 50\\%, and 70\\% stragglers. In terms of EUR, our strategy constantly outperforms the other two strategies across all scenarios and datasets since they use random selection for selecting clients for an FL training round.\n\nTo offer detailed insights, we present results for the Google Speech dataset~\\cite{speech} wrt the metrics accuracy and EUR, across the FL training session as shown in Figures \\ref{fig:acc-speech} and \\ref{fig:eur-speech} respectively. For the \\textit{standard scenarios}, we ran the FL training session for $35$ rounds, while for the \\textit{straggler (\\%) scenarios}, we ran the experiments for $60$ rounds. For the standard scenario, our strategy reached an accuracy of 79.4\\% as compared to 76.6\\% and 77.4\\% for \\texttt{FedAvg} and \\texttt{FedProx} respectively. Furthermore, our strategy showed faster convergence by reaching an accuracy of 70\\% in 19 rounds as compared to 21 and 22 for \\texttt{FedAvg} and \\texttt{FedProx} respectively. With 10\\% stragglers, our strategy and \\texttt{FedAvg} had a similar convergence rate, while \\texttt{FedProx} was slightly behind. Moreover, our strategy reached an end accuracy of 76\\% which is a 6\\% and 10\\% increase over \\texttt{FedAvg} and \\texttt{FedProx} respectively. For an FL system with 30\\% stragglers, our strategy consistently outperforms \\texttt{FedAvg} by around 8\\% towards the end of the training, while outperforming \\texttt{FedProx} by a smaller margin of 1\\%. We observe a similar trend for our experiments with 50\\% and 70\\% stragglers in the system, where our strategy outperforms the other two. From our experiments, we observe that the presence of stragglers affects the convergence speed of the DNN models in an FL training session. DNN models trained in \\textit{standard scenarios} converge faster as compared to the models trained with \\textit{straggler (\\%) scenarios} as shown in Figure~\\ref{fig:acc-speech} (Table~\\ref{tab:experiment-params}). We observe a similar behaviour for the different datasets (\\S\\ref{sec:datasets}). Moreover, for the \\textit{straggler (\\%) scenarios}, increasing the percentage of stragglers in the system does not consistently decrease the accuracy of the trained DNN model as shown in Table~\\ref{tab:acc-all}. This is especially true for experiments with a higher number of stragglers, i.e., $70\\%$. This behavior is not exclusive to our experiments and was also reported by Li et al.~\\cite{fedprox} and Wu et al.~\\cite{safa}. While the authors do not provide a clear explanation for this behaviour, we argue that due to the non-IID nature of the data, clients do not contribute evenly to the test accuracy. In addition, having fewer number of reliable clients reduces the varying effect of local model updates and local model deviations from the global model, making it easier to reach a consensus on the global model. Therefore, we can reach situations where a system with more stragglers reaches higher overall accuracy at the end. ", "table_source": "\\begin{table}[t]\n\\centering\n\\begin{adjustbox}{width=\\columnwidth,  center}\n\\begin{tabular}{|c|ccc|cc|}\n\\hline\n\\multirow{2}{*}{\\textbf{Dataset}} & \\multicolumn{3}{c|}{\\textbf{Hyperparameters}}                                                           & \\multicolumn{2}{c|}{\\textbf{Number of Training Rounds}}                             \\\\ \\cline{2-6} \n                                  & \\multicolumn{1}{c|}{\\textbf{Epochs}} & \\multicolumn{1}{c|}{\\textbf{Batch Size}} & \\textbf{Learning Rate} & \\multicolumn{1}{c|}{\\textbf{Standard}} & \\textbf{Straggler (\\%)} \\\\ \\hline\nMNIST                             & \\multicolumn{1}{c|}{5}               & \\multicolumn{1}{c|}{10}                  & $1e-3$                 & \\multicolumn{1}{c|}{60}                & 60                      \\\\ \\hline\nFEMNIST                           & \\multicolumn{1}{c|}{5}               & \\multicolumn{1}{c|}{10}                  & $1e-3$                  & \\multicolumn{1}{c|}{40}                & 40                      \\\\ \\hline\nShakespeare                       & \\multicolumn{1}{c|}{1}               & \\multicolumn{1}{c|}{32}                  & $0.8$                 & \\multicolumn{1}{c|}{25}                & 25                      \\\\ \\hline\nSpeech Command                    & \\multicolumn{1}{c|}{5}               & \\multicolumn{1}{c|}{5}                   & $1e-3$                  & \\multicolumn{1}{c|}{35}                & 60                      \\\\ \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Experiment hyperparameters for the different datasets (\\S\\ref{sec:datasets}) and experiment scenarios (\\S\\ref{sec:scenarios}).}\n\n\\vspace{-5mm}\n\\label{tab:experiment-params}\n\\end{table}", "cell_list_gold": [{"value": "5", "char_index": [626, 627], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "epochs", "dataset": "MNIST"}, {"value": "10", "char_index": [665, 667], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "batch size", "dataset": "MNIST"}, {"value": "1e-3", "char_index": [689, 693], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "learning rate", "dataset": "MNIST"}, {"value": "60", "char_index": [733, 735], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "number of training rounds standard", "dataset": "MNIST"}, {"value": "60", "char_index": [754, 756], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "number of training rounds straggler", "dataset": "MNIST"}, {"value": "5", "char_index": [844, 845], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "epochs", "dataset": "FEMNIST"}, {"value": "10", "char_index": [883, 885], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "batch size", "dataset": "FEMNIST"}, {"value": "1e-3", "char_index": [907, 911], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "learning rate", "dataset": "FEMNIST"}, {"value": "40", "char_index": [952, 954], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "number of training rounds standard", "dataset": "FEMNIST"}, {"value": "40", "char_index": [973, 975], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "number of training rounds straggler", "dataset": "FEMNIST"}, {"value": "1", "char_index": [1063, 1064], "type": "Hyper-parameter/Architecture", "model": ["LSTM", "Long Short Term Memory"], "parameter/architecture name": "epochs", "dataset": "Shakespeare"}, {"value": "32", "char_index": [1102, 1104], "type": "Hyper-parameter/Architecture", "model": ["LSTM", "Long Short Term Memory"], "parameter/architecture name": "batch size", "dataset": "Shakespeare"}, {"value": "0.8", "char_index": [1126, 1129], "type": "Hyper-parameter/Architecture", "model": ["LSTM", "Long Short Term Memory"], "parameter/architecture name": "learning rate", "dataset": "Shakespeare"}, {"value": "25", "char_index": [1169, 1171], "type": "Hyper-parameter/Architecture", "model": ["LSTM", "Long Short Term Memory"], "parameter/architecture name": "number of training rounds standard", "dataset": "Shakespeare"}, {"value": "25", "char_index": [1190, 1192], "type": "Hyper-parameter/Architecture", "model": ["LSTM", "Long Short Term Memory"], "parameter/architecture name": "number of training rounds straggler", "dataset": "Shakespeare"}, {"value": "5", "char_index": [1280, 1281], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "epochs", "dataset": "Speech Command"}, {"value": "5", "char_index": [1319, 1320], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "batch size", "dataset": "Speech Command"}, {"value": "1e-3", "char_index": [1343, 1347], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "learning rate", "dataset": "Speech Command"}, {"value": "35", "char_index": [1388, 1390], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "number of training rounds standard", "dataset": "Speech Command"}, {"value": "60", "char_index": [1409, 1411], "type": "Hyper-parameter/Architecture", "model": ["CNN", "Convolutional Neural Network"], "parameter/architecture name": "number of training rounds straggler", "dataset": "Speech Command"}]}, "2211.05739v1_table1": {"table_code": "\\begin{table}\n\\centering\n\n\\begin{adjustbox}{width=8.8cm,  center}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} \n\\hline\n\\multirow{3}{*}{\\textbf{ Dataset}} & \\multirow{3}{*}{\\textbf{ Strategy}} & \\multicolumn{10}{c|}{\\textbf{Ratio of Stragglers}}                                                                                                                                          \\\\ \n\\cline{3-12}\n                                   &                                     & \\multicolumn{2}{c|}{\\textbf{Baseline}} & \\multicolumn{2}{c|}{\\textbf{10\\%}} & \\multicolumn{2}{c|}{\\textbf{30\\%}} & \\multicolumn{2}{c|}{\\textbf{50\\%}} & \\multicolumn{2}{c|}{\\textbf{70\\%}}  \\\\ \n\\cline{3-12}\n                                   &                                     & \\textbf{Acc}   & \\textbf{EUR}          & \\textbf{Acc}   & \\textbf{EUR}      & \\textbf{Acc}   & \\textbf{EUR}      & \\textbf{Acc}   & \\textbf{EUR}      & \\textbf{Acc}   & \\textbf{EUR}       \\\\ \n\\hline\n\\multirow{3}{*}{MNIST}             & FedAvg                              & 0.981          & 0.99                  & \\textbf{0.983} & 0.89              & \\textbf{0.972} & 0.70               & 0.971          & 0.49              & 0.977          & 0.31               \\\\ \n\\cline{2-12}\n                                   & FedProx                             & 0.935          & 0.98                  & 0.972          & 0.88              & 0.960           & 0.69              & 0.962          & 0.49              & 0.967          & 0.29               \\\\ \n\\cline{2-12}\n                                   & FedLesScan                          & \\textbf{0.985} & \\textbf{0.99}         & 0.976          & \\textbf{0.98}     & \\textbf{0.972} & \\textbf{0.96}     & \\textbf{0.974} & \\textbf{0.74}     & \\textbf{0.979}  & \\textbf{0.44}      \\\\ \n\\hline\n\\multirow{3}{*}{FEMNIST}           & FedAvg                              & 0.756          & 0.99                  & 0.765          & 0.89              & 0.790           & 0.69              & 0.779         & 0.49              & 0.744           & 0.3                \\\\ \n\\cline{2-12}\n                                   & FedProx                             & 0.753          & 0.96                  & \\textbf{0.778} & 0.80              & 0.756          & 0.64              & \\textbf{0.785}  & 0.46              & 0.731           & 0.28               \\\\ \n\\cline{2-12}\n                                   & FedLesScan                          & \\textbf{0.776} & \\textbf{0.99}         & 0.770          & \\textbf{0.97}     & \\textbf{0.785} & \\textbf{0.93}     & 0.774          & \\textbf{0.80}      & \\textbf{0.753} & \\textbf{0.50}       \\\\ \n\\hline\n\\multirow{3}{*}{Shakespeare}       & FedAvg                              & \\textbf{0.434} & 0.87                  & \\textbf{0.430}  & 0.80              & \\textbf{0.401}   & 0.60               & 0.402   & 0.41              & 0.428           & 0.31               \\\\ \n\\cline{2-12}\n                                   & FedProx                             & 0.400          & 0.86                  & 0.399          & 0.78              & 0.396          & 0.58              & 0.345          & 0.40               & 0.38           & 0.29               \\\\ \n\\cline{2-12}\n                                   & FedLesScan                          & 0.388          & \\textbf{0.94}         & 0.408          & \\textbf{0.90}      & 0.400          & \\textbf{0.86}     & \\textbf{0.403}   & \\textbf{0.72}     & \\textbf{0.432}  & \\textbf{0.53}      \\\\ \n\\hline\n\\multirow{3}{*}{Google Speech}     & FedAvg                              & 0.766           & \\textbf{0.99}        & 0.699            & 0.90               & 0.639           & 0.70               & 0.645           & 0.50               & 0.804           & 0.30                \\\\ \n\\cline{2-12}\n                                   & FedProx                             & 0.774          & \\textbf{0.99}         & 0.664           & 0.89              & 0.709           & 0.70               & 0.694           & 0.49              & 0.759           & 0.29               \\\\ \n\\cline{2-12}\n                                   & FedLesScan                          & \\textbf{0.794} & \\textbf{0.99}         & \\textbf{0.762}   & \\textbf{0.97}     & \\textbf{0.719}  & \\textbf{0.90}      & \\textbf{0.720}  & \\textbf{0.86}     & \\textbf{0.824}  & \\textbf{0.74}      \\\\\n\\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Accuracy and EUR values for the three strategies across different scenarios (\\S\\ref{sec:scenarios}) and datasets (\\S\\ref{sec:datasets}). The highlighted values represent the highest accuracy and EUR values for a particular dataset, strategy and scenario.}\n\\label{tab:acc-all}\n\\vspace{-5mm}\n\\vspace{-1mm}\n\\end{table}", "table_label": "{tab:acc-all}", "table_numeric_cells": [["10", "\\multicolumn{2}{c|}{\\textbf{10\\%}}", 541, 543, 513, 547], ["30", "\\multicolumn{2}{c|}{\\textbf{30\\%}}", 578, 580, 550, 584], ["50", "\\multicolumn{2}{c|}{\\textbf{50\\%}}", 615, 617, 587, 621], ["70", "\\multicolumn{2}{c|}{\\textbf{70\\%}}", 652, 654, 624, 658], ["0.981", "0.981", 1026, 1031, 1026, 1031], ["0.99", "0.99", 1043, 1047, 1043, 1047], ["0.983", "\\textbf{0.983}", 1075, 1080, 1067, 1081], ["0.89", "0.89", 1084, 1088, 1084, 1088], ["0.972", "\\textbf{0.972}", 1112, 1117, 1104, 1118], ["0.70", "0.70", 1121, 1125, 1121, 1125], ["0.971", "0.971", 1142, 1147, 1142, 1147], ["0.49", "0.49", 1159, 1163, 1159, 1163], ["0.977", "0.977", 1179, 1184, 1179, 1184], ["0.31", "0.31", 1196, 1200, 1196, 1200], ["0.935", "0.935", 1307, 1312, 1307, 1312], ["0.98", "0.98", 1324, 1328, 1324, 1328], ["0.972", "0.972", 1348, 1353, 1348, 1353], ["0.88", "0.88", 1365, 1369, 1365, 1369], ["0.960", "0.960", 1385, 1390, 1385, 1390], ["0.69", "0.69", 1403, 1407, 1403, 1407], ["0.962", "0.962", 1423, 1428, 1423, 1428], ["0.49", "0.49", 1440, 1444, 1440, 1444], ["0.967", "0.967", 1460, 1465, 1460, 1465], ["0.29", "0.29", 1477, 1481, 1477, 1481], ["0.985", "\\textbf{0.985}", 1596, 1601, 1588, 1602], ["0.99", "\\textbf{0.99}", 1613, 1617, 1605, 1618], ["0.976", "0.976", 1629, 1634, 1629, 1634], ["0.98", "\\textbf{0.98}", 1654, 1658, 1646, 1659], ["0.972", "\\textbf{0.972}", 1674, 1679, 1666, 1680], ["0.96", "\\textbf{0.96}", 1691, 1695, 1683, 1696], ["0.974", "\\textbf{0.974}", 1711, 1716, 1703, 1717], ["0.74", "\\textbf{0.74}", 1728, 1732, 1720, 1733], ["0.979", "\\textbf{0.979}", 1748, 1753, 1740, 1754], ["0.44", "\\textbf{0.44}", 1766, 1770, 1758, 1771], ["0.756", "0.756", 1863, 1868, 1863, 1868], ["0.99", "0.99", 1880, 1884, 1880, 1884], ["0.765", "0.765", 1904, 1909, 1904, 1909], ["0.89", "0.89", 1921, 1925, 1921, 1925], ["0.790", "0.790", 1941, 1946, 1941, 1946], ["0.69", "0.69", 1959, 1963, 1959, 1963], ["0.779", "0.779", 1979, 1984, 1979, 1984], ["0.49", "0.49", 1995, 1999, 1995, 1999], ["0.744", "0.744", 2015, 2020, 2015, 2020], ["0.3", "0.3", 2033, 2036, 2033, 2036], ["0.753", "0.753", 2144, 2149, 2144, 2149], ["0.96", "0.96", 2161, 2165, 2161, 2165], ["0.778", "\\textbf{0.778}", 2193, 2198, 2185, 2199], ["0.80", "0.80", 2202, 2206, 2202, 2206], ["0.756", "0.756", 2222, 2227, 2222, 2227], ["0.64", "0.64", 2239, 2243, 2239, 2243], ["0.785", "\\textbf{0.785}", 2267, 2272, 2259, 2273], ["0.46", "0.46", 2277, 2281, 2277, 2281], ["0.731", "0.731", 2297, 2302, 2297, 2302], ["0.28", "0.28", 2315, 2319, 2315, 2319], ["0.776", "\\textbf{0.776}", 2434, 2439, 2426, 2440], ["0.99", "\\textbf{0.99}", 2451, 2455, 2443, 2456], ["0.770", "0.770", 2467, 2472, 2467, 2472], ["0.97", "\\textbf{0.97}", 2492, 2496, 2484, 2497], ["0.785", "\\textbf{0.785}", 2512, 2517, 2504, 2518], ["0.93", "\\textbf{0.93}", 2529, 2533, 2521, 2534], ["0.774", "0.774", 2541, 2546, 2541, 2546], ["0.80", "\\textbf{0.80}", 2566, 2570, 2558, 2571], ["0.753", "\\textbf{0.753}", 2587, 2592, 2579, 2593], ["0.50", "\\textbf{0.50}", 2604, 2608, 2596, 2609], ["0.434", "\\textbf{0.434}", 2710, 2715, 2702, 2716], ["0.87", "0.87", 2719, 2723, 2719, 2723], ["0.430", "\\textbf{0.430}", 2751, 2756, 2743, 2757], ["0.80", "0.80", 2761, 2765, 2761, 2765], ["0.401", "\\textbf{0.401}", 2789, 2794, 2781, 2795], ["0.60", "0.60", 2800, 2804, 2800, 2804], ["0.402", "0.402", 2821, 2826, 2821, 2826], ["0.41", "0.41", 2831, 2835, 2831, 2835], ["0.428", "0.428", 2851, 2856, 2851, 2856], ["0.31", "0.31", 2869, 2873, 2869, 2873], ["0.400", "0.400", 2980, 2985, 2980, 2985], ["0.86", "0.86", 2997, 3001, 2997, 3001], ["0.399", "0.399", 3021, 3026, 3021, 3026], ["0.78", "0.78", 3038, 3042, 3038, 3042], ["0.396", "0.396", 3058, 3063, 3058, 3063], ["0.58", "0.58", 3075, 3079, 3075, 3079], ["0.345", "0.345", 3095, 3100, 3095, 3100], ["0.40", "0.40", 3112, 3116, 3112, 3116], ["0.38", "0.38", 3133, 3137, 3133, 3137], ["0.29", "0.29", 3150, 3154, 3150, 3154], ["0.388", "0.388", 3261, 3266, 3261, 3266], ["0.94", "\\textbf{0.94}", 3286, 3290, 3278, 3291], ["0.408", "0.408", 3302, 3307, 3302, 3307], ["0.90", "\\textbf{0.90}", 3327, 3331, 3319, 3332], ["0.400", "0.400", 3340, 3345, 3340, 3345], ["0.86", "\\textbf{0.86}", 3365, 3369, 3357, 3370], ["0.403", "\\textbf{0.403}", 3385, 3390, 3377, 3391], ["0.72", "\\textbf{0.72}", 3404, 3408, 3396, 3409], ["0.432", "\\textbf{0.432}", 3424, 3429, 3416, 3430], ["0.53", "\\textbf{0.53}", 3442, 3446, 3434, 3447], ["0.766", "0.766", 3539, 3544, 3539, 3544], ["0.99", "\\textbf{0.99}", 3565, 3569, 3557, 3570], ["0.699", "0.699", 3580, 3585, 3580, 3585], ["0.90", "0.90", 3599, 3603, 3599, 3603], ["0.639", "0.639", 3620, 3625, 3620, 3625], ["0.70", "0.70", 3638, 3642, 3638, 3642], ["0.645", "0.645", 3659, 3664, 3659, 3664], ["0.50", "0.50", 3677, 3681, 3677, 3681], ["0.804", "0.804", 3698, 3703, 3698, 3703], ["0.30", "0.30", 3716, 3720, 3716, 3720], ["0.774", "0.774", 3828, 3833, 3828, 3833], ["0.99", "\\textbf{0.99}", 3853, 3857, 3845, 3858], ["0.664", "0.664", 3869, 3874, 3869, 3874], ["0.89", "0.89", 3887, 3891, 3887, 3891], ["0.709", "0.709", 3907, 3912, 3907, 3912], ["0.70", "0.70", 3925, 3929, 3925, 3929], ["0.694", "0.694", 3946, 3951, 3946, 3951], ["0.49", "0.49", 3964, 3968, 3964, 3968], ["0.759", "0.759", 3984, 3989, 3984, 3989], ["0.29", "0.29", 4002, 4006, 4002, 4006], ["0.794", "\\textbf{0.794}", 4121, 4126, 4113, 4127], ["0.99", "\\textbf{0.99}", 4138, 4142, 4130, 4143], ["0.762", "\\textbf{0.762}", 4162, 4167, 4154, 4168], ["0.97", "\\textbf{0.97}", 4181, 4185, 4173, 4186], ["0.719", "\\textbf{0.719}", 4201, 4206, 4193, 4207], ["0.90", "\\textbf{0.90}", 4219, 4223, 4211, 4224], ["0.720", "\\textbf{0.720}", 4240, 4245, 4232, 4246], ["0.86", "\\textbf{0.86}", 4258, 4262, 4250, 4263], ["0.824", "\\textbf{0.824}", 4278, 4283, 4270, 4284], ["0.74", "\\textbf{0.74}", 4296, 4300, 4288, 4301]], "text_chunk_selected": "The different proposed approaches for straggler mitigation in FL can be divided into three categories, i.e., \\textit{synchronous}~\\cite{fedprox}, \\textit{asynchronous}~\\cite{asofed,fed_async}, and \\textit{semi-asynchronous}~\\cite{fedAt, safa, csafl}. One of the most popular synchronous approaches is \\texttt{FedProx}~\\cite{fedprox}, which is based on \\texttt{FedAvg}~\\cite{mcmahan2017communication} with two minor differences. First, a custom loss function for client-side training that limits the varying effects of local model updates. Second, toleration for partial work, i.e., clients can perform a variable amount of work to accommodate constraints in terms of hardware and network. However, incorporating partial work requires tailoring the number of local epochs for each client individually, which might be infeasible for a significantly large number of clients. Moreover, in contrast to \\texttt{FedLesScan} it relies on random selection of clients which makes it sensitive to stragglers. In~\\cite{fed_async}, Xie et al. propose an asynchronous federated optimization algorithm called \\texttt{FedAsync} that relies on a parameter server architecture to invoke and synchronize FL clients. The server consists of two parallel threads, i.e., a \\textit{scheduler} and an \\textit{updater}. The scheduler thread periodically triggers the FL clients to perform training using the latest global model, while the updater thread receives the client model updates and directly aggregates them to the global\nmodel. However, in the context of serverless, this approach has significantly higher resource and communication costs (\\S\\ref{sec:faas}, \\S\\ref{sec:servfl}) since it would require a function invocation to perform aggregation with the global model after each individual client update.\n\n\\section{Experimental Results}\n\\label{sec:results}\nIn this section, we first describe the datasets and model architectures used for evaluating our strategy. Following this, we describe the configuration for \\emph{FedLess} and the FaaS client functions. Finally, we evaluate the performance of our strategy as compared to \\texttt{FedAvg}~\\cite{mcmahan2017communication} and \\texttt{FedProx}~\\cite{fedprox} wrt several metrics for varying number of stragglers. We chose these strategies since they are robust and work well in a serverless setting. For all our experiments, we follow best practices while reporting results and repeat them three times~\\cite{8758926}. More extensive experimental results can be found here~\\cite{thesismohamed}.\n\n\\textbf{Straggler (\\%) Scenario}. In this, we simulate varying percentages of stragglers in the FL system.  Although there might be different reasons for FaaS client failures, such as memory limit, function timeout, or communication issues, these failures can only have one of two effects on the clients. Clients can either push their updates after the training round is finished (slow updates) or can completely crash (not push their updates). To simulate slow updates, we limit the training round time to only fit clients with no issues or delays. Meaning, that clients which experience cold starts, bandwidth limitations, or communication delays do not finish the round in time; therefore, pushing their updates later. On the other hand, to simulate failures, we randomly select a specific ratio of clients to fail their training at the beginning of each experiment. We perform four different experiments for each dataset with 10\\%, 30\\%, 50\\%, and 70\\% stragglers in the system. Note that, in all our experiments, stragglers are different from \\textit{malicious clients} found in some FL settings~\\cite{li2020learning, zhang2022fldetector}. While malicious clients can act as stragglers to deliberately slow training or hinder model performance, they can perform more powerful attacks such as data or model poisoning~\\cite{fl_challenges,fl_data_poisoning} to skew the model's performance. In our work, we focus on the problem of stragglers and their behavior rather than the clients' intentions. The number of training rounds for the different datasets and experiment scenarios is shown in Table~\\ref{tab:experiment-params}.\n\n\\subsubsection{Metrics}\n\\label{sec:metrics}\nFor comparing the different strategies, we use the standard metrics \\textit{accuracy}, \\textit{experiment duration}, and \\textit{cost}. For calculating accuracy of the trained global model, we randomly choose a set of clients and evaluate it on their test datasets. Following this, we multiply the obtained accuracy for a particular client by the ratio of its test set cardinality to the total cardinality of the test dataset. The final accuracy value is obtained by averaging the obtained accuracy values. While accuracy describes model performance, it does not provide insights about the performance of the strategy and the contributions of the clients to the global model. Towards this, we also use the metrics \\textit{Effective Update Ratio} (EUR)~\\cite{safa} and \\textit{Bias}~\\cite{safa}. EUR is defined as the ratio between the successful clients and the subset of selected clients. It shows the effect of stragglers on round utilization. A higher value of EUR represents less wasted resources since clients requested to participate in a certain round end up contributing to the global model. Furthermore, to provide insights into the bias of the client selection schemes, we use variance plots. This is done by showing the frequency of selection for each client across the FL training process. Bias is defined as the difference between the frequency of the least called client and the most called client~\\cite{safa}. For scenarios with low number of stragglers, we target low bias, while for scenarios with high number of stragglers the bias should be higher due to the prioritization of reliable clients in training. Experiment duration represents the total time required for training the model. For computing training costs, we use the cost computation model~\\cite{gcf_cost} used by Google to estimate the cost for each client function based on the number of invocations, allocated memory, and execution duration.\n\nThe obtained accuracy and EUR values across all our experiments is summarized in Table~\\ref{tab:acc-all}. For the \\textit{standard scenarios} (\\S\\ref{sec:scenarios}), we obtained better accuracy and round utlization for our strategy as compared to \\texttt{FedAvg} and \\texttt{FedProx} across all datasets except Shakespeare. This is because with the Shakespeare dataset, some of the clients contribute to the model accuracy more than others, especially clients with longer training times. Moreover, due to budget constraints and significantly high training costs, we train for a slightly less number of rounds, i.e., 25 for Shakespeare. However, we argue that in a more realistic scenario with more number of rounds, the difference in accuracy will decrease. This is because our strategy utilizes clients more efficiently, i.e., higher EUR. As a result, with more rounds the number of invocations per client will increase and more clients will contribute to the global model, leading to a higher accuracy. Similarly, for the \\textit{straggler (\\%) scenarios}, we obtained better results for accuracy with our strategy as compared to the other two for the MNIST, FEMNIST, and the Google Speech datasets. On the Shakespeare dataset, our strategy outperformed \\texttt{FedAvg} and \\texttt{FedProx} in scenarios with 30\\%, 50\\%, and 70\\% stragglers. In terms of EUR, our strategy constantly outperforms the other two strategies across all scenarios and datasets since they use random selection for selecting clients for an FL training round.\n\nFor most \\textit{standard scenarios} (Table~\\ref{tab:acc-all}), \\texttt{FedLesScan} obtained better accuracy as compared to the other two strategies due to the better distribution of client invocations. This is because our strategy prioritizes clients with the least number of invocations in a selected client cluster (\\S\\ref{sec:ClientSelection}) leading to more balanced contributions among the participating clients. On the other hand, with \\textit{straggler (\\%) scenario}, our strategy reached better accuracies by relying more on robust and reliable clients. Furthermore, the utilization of a staleness-aware aggregation scheme (\\S\\ref{sec:stalelessaware}) avoids wasting valuable contributions, which in turn increases accuracy.\n\nTo offer detailed insights, we present results for the Google Speech dataset~\\cite{speech} wrt the metrics accuracy and EUR, across the FL training session as shown in Figures \\ref{fig:acc-speech} and \\ref{fig:eur-speech} respectively. For the \\textit{standard scenarios}, we ran the FL training session for $35$ rounds, while for the \\textit{straggler (\\%) scenarios}, we ran the experiments for $60$ rounds. For the standard scenario, our strategy reached an accuracy of 79.4\\% as compared to 76.6\\% and 77.4\\% for \\texttt{FedAvg} and \\texttt{FedProx} respectively. Furthermore, our strategy showed faster convergence by reaching an accuracy of 70\\% in 19 rounds as compared to 21 and 22 for \\texttt{FedAvg} and \\texttt{FedProx} respectively. With 10\\% stragglers, our strategy and \\texttt{FedAvg} had a similar convergence rate, while \\texttt{FedProx} was slightly behind. Moreover, our strategy reached an end accuracy of 76\\% which is a 6\\% and 10\\% increase over \\texttt{FedAvg} and \\texttt{FedProx} respectively. For an FL system with 30\\% stragglers, our strategy consistently outperforms \\texttt{FedAvg} by around 8\\% towards the end of the training, while outperforming \\texttt{FedProx} by a smaller margin of 1\\%. We observe a similar trend for our experiments with 50\\% and 70\\% stragglers in the system, where our strategy outperforms the other two. From our experiments, we observe that the presence of stragglers affects the convergence speed of the DNN models in an FL training session. DNN models trained in \\textit{standard scenarios} converge faster as compared to the models trained with \\textit{straggler (\\%) scenarios} as shown in Figure~\\ref{fig:acc-speech} (Table~\\ref{tab:experiment-params}). We observe a similar behaviour for the different datasets (\\S\\ref{sec:datasets}). Moreover, for the \\textit{straggler (\\%) scenarios}, increasing the percentage of stragglers in the system does not consistently decrease the accuracy of the trained DNN model as shown in Table~\\ref{tab:acc-all}. This is especially true for experiments with a higher number of stragglers, i.e., $70\\%$. This behavior is not exclusive to our experiments and was also reported by Li et al.~\\cite{fedprox} and Wu et al.~\\cite{safa}. While the authors do not provide a clear explanation for this behaviour, we argue that due to the non-IID nature of the data, clients do not contribute evenly to the test accuracy. In addition, having fewer number of reliable clients reduces the varying effect of local model updates and local model deviations from the global model, making it easier to reach a consensus on the global model. Therefore, we can reach situations where a system with more stragglers reaches higher overall accuracy at the end. ", "table_source": "\\begin{table}\n\\centering\n\n\\begin{adjustbox}{width=8.8cm,  center}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|} \n\\hline\n\\multirow{3}{*}{\\textbf{ Dataset}} & \\multirow{3}{*}{\\textbf{ Strategy}} & \\multicolumn{10}{c|}{\\textbf{Ratio of Stragglers}}                                                                                                                                          \\\\ \n\\cline{3-12}\n                                   &                                     & \\multicolumn{2}{c|}{\\textbf{Baseline}} & \\multicolumn{2}{c|}{\\textbf{10\\%}} & \\multicolumn{2}{c|}{\\textbf{30\\%}} & \\multicolumn{2}{c|}{\\textbf{50\\%}} & \\multicolumn{2}{c|}{\\textbf{70\\%}}  \\\\ \n\\cline{3-12}\n                                   &                                     & \\textbf{Acc}   & \\textbf{EUR}          & \\textbf{Acc}   & \\textbf{EUR}      & \\textbf{Acc}   & \\textbf{EUR}      & \\textbf{Acc}   & \\textbf{EUR}      & \\textbf{Acc}   & \\textbf{EUR}       \\\\ \n\\hline\n\\multirow{3}{*}{MNIST}             & FedAvg                              & 0.981          & 0.99                  & \\textbf{0.983} & 0.89              & \\textbf{0.972} & 0.70               & 0.971          & 0.49              & 0.977          & 0.31               \\\\ \n\\cline{2-12}\n                                   & FedProx                             & 0.935          & 0.98                  & 0.972          & 0.88              & 0.960           & 0.69              & 0.962          & 0.49              & 0.967          & 0.29               \\\\ \n\\cline{2-12}\n                                   & FedLesScan                          & \\textbf{0.985} & \\textbf{0.99}         & 0.976          & \\textbf{0.98}     & \\textbf{0.972} & \\textbf{0.96}     & \\textbf{0.974} & \\textbf{0.74}     & \\textbf{0.979}  & \\textbf{0.44}      \\\\ \n\\hline\n\\multirow{3}{*}{FEMNIST}           & FedAvg                              & 0.756          & 0.99                  & 0.765          & 0.89              & 0.790           & 0.69              & 0.779         & 0.49              & 0.744           & 0.3                \\\\ \n\\cline{2-12}\n                                   & FedProx                             & 0.753          & 0.96                  & \\textbf{0.778} & 0.80              & 0.756          & 0.64              & \\textbf{0.785}  & 0.46              & 0.731           & 0.28               \\\\ \n\\cline{2-12}\n                                   & FedLesScan                          & \\textbf{0.776} & \\textbf{0.99}         & 0.770          & \\textbf{0.97}     & \\textbf{0.785} & \\textbf{0.93}     & 0.774          & \\textbf{0.80}      & \\textbf{0.753} & \\textbf{0.50}       \\\\ \n\\hline\n\\multirow{3}{*}{Shakespeare}       & FedAvg                              & \\textbf{0.434} & 0.87                  & \\textbf{0.430}  & 0.80              & \\textbf{0.401}   & 0.60               & 0.402   & 0.41              & 0.428           & 0.31               \\\\ \n\\cline{2-12}\n                                   & FedProx                             & 0.400          & 0.86                  & 0.399          & 0.78              & 0.396          & 0.58              & 0.345          & 0.40               & 0.38           & 0.29               \\\\ \n\\cline{2-12}\n                                   & FedLesScan                          & 0.388          & \\textbf{0.94}         & 0.408          & \\textbf{0.90}      & 0.400          & \\textbf{0.86}     & \\textbf{0.403}   & \\textbf{0.72}     & \\textbf{0.432}  & \\textbf{0.53}      \\\\ \n\\hline\n\\multirow{3}{*}{Google Speech}     & FedAvg                              & 0.766           & \\textbf{0.99}        & 0.699            & 0.90               & 0.639           & 0.70               & 0.645           & 0.50               & 0.804           & 0.30                \\\\ \n\\cline{2-12}\n                                   & FedProx                             & 0.774          & \\textbf{0.99}         & 0.664           & 0.89              & 0.709           & 0.70               & 0.694           & 0.49              & 0.759           & 0.29               \\\\ \n\\cline{2-12}\n                                   & FedLesScan                          & \\textbf{0.794} & \\textbf{0.99}         & \\textbf{0.762}   & \\textbf{0.97}     & \\textbf{0.719}  & \\textbf{0.90}      & \\textbf{0.720}  & \\textbf{0.86}     & \\textbf{0.824}  & \\textbf{0.74}      \\\\\n\\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Accuracy and EUR values for the three strategies across different scenarios (\\S\\ref{sec:scenarios}) and datasets (\\S\\ref{sec:datasets}). The highlighted values represent the highest accuracy and EUR values for a particular dataset, strategy and scenario.}\n\\label{tab:acc-all}\n\\vspace{-5mm}\n\\vspace{-1mm}\n\\end{table}", "cell_list_gold": [{"value": "10", "char_index": [541, 543], "type": "Other"}, {"value": "30", "char_index": [578, 580], "type": "Other"}, {"value": "50", "char_index": [615, 617], "type": "Other"}, {"value": "70", "char_index": [652, 654], "type": "Other"}, {"value": "0.981", "char_index": [1026, 1031], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [1043, 1047], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.983", "char_index": [1075, 1080], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.89", "char_index": [1084, 1088], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.972", "char_index": [1112, 1117], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.70", "char_index": [1121, 1125], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.971", "char_index": [1142, 1147], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.49", "char_index": [1159, 1163], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.977", "char_index": [1179, 1184], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.31", "char_index": [1196, 1200], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.935", "char_index": [1307, 1312], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.98", "char_index": [1324, 1328], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.972", "char_index": [1348, 1353], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.88", "char_index": [1365, 1369], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.960", "char_index": [1385, 1390], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.69", "char_index": [1403, 1407], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.962", "char_index": [1423, 1428], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.49", "char_index": [1440, 1444], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.967", "char_index": [1460, 1465], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.29", "char_index": [1477, 1481], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.985", "char_index": [1596, 1601], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [1613, 1617], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.976", "char_index": [1629, 1634], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.98", "char_index": [1654, 1658], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.972", "char_index": [1674, 1679], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.96", "char_index": [1691, 1695], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.974", "char_index": [1711, 1716], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.74", "char_index": [1728, 1732], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.979", "char_index": [1748, 1753], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.44", "char_index": [1766, 1770], "type": "Result", "training data/set": "MNIST", "test data/set": "MNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.756", "char_index": [1863, 1868], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [1880, 1884], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.765", "char_index": [1904, 1909], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.89", "char_index": [1921, 1925], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.790", "char_index": [1941, 1946], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.69", "char_index": [1959, 1963], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.779", "char_index": [1979, 1984], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.49", "char_index": [1995, 1999], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.744", "char_index": [2015, 2020], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.3", "char_index": [2033, 2036], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.753", "char_index": [2144, 2149], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.96", "char_index": [2161, 2165], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.778", "char_index": [2193, 2198], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.80", "char_index": [2202, 2206], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.756", "char_index": [2222, 2227], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.64", "char_index": [2239, 2243], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.785", "char_index": [2267, 2272], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.46", "char_index": [2277, 2281], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.731", "char_index": [2297, 2302], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.28", "char_index": [2315, 2319], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.776", "char_index": [2434, 2439], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [2451, 2455], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.770", "char_index": [2467, 2472], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.97", "char_index": [2492, 2496], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.785", "char_index": [2512, 2517], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.93", "char_index": [2529, 2533], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.774", "char_index": [2541, 2546], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.80", "char_index": [2566, 2570], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.753", "char_index": [2587, 2592], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.50", "char_index": [2604, 2608], "type": "Result", "training data/set": "FEMNIST", "test data/set": "FEMNIST", "task": "Image Classification", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.434", "char_index": [2710, 2715], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "Baseline"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.87", "char_index": [2719, 2723], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "Baseline"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.430", "char_index": [2751, 2756], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "10%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.80", "char_index": [2761, 2765], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "10%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.401", "char_index": [2789, 2794], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "30%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.60", "char_index": [2800, 2804], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "30%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.402", "char_index": [2821, 2826], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "50%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.41", "char_index": [2831, 2835], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "50%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.428", "char_index": [2851, 2856], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "70%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.31", "char_index": [2869, 2873], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "70%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.400", "char_index": [2980, 2985], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "Baseline"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.86", "char_index": [2997, 3001], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "Baseline"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.399", "char_index": [3021, 3026], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "10%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.78", "char_index": [3038, 3042], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "10%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.396", "char_index": [3058, 3063], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "30%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.58", "char_index": [3075, 3079], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "30%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.345", "char_index": [3095, 3100], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "50%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.40", "char_index": [3112, 3116], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "50%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.38", "char_index": [3133, 3137], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "70%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.29", "char_index": [3150, 3154], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "70%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.388", "char_index": [3261, 3266], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "Baseline"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.94", "char_index": [3286, 3290], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "Baseline"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.408", "char_index": [3302, 3307], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "10%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.90", "char_index": [3327, 3331], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "10%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.400", "char_index": [3340, 3345], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "30%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.86", "char_index": [3365, 3369], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "30%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.403", "char_index": [3385, 3390], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "50%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.72", "char_index": [3404, 3408], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "50%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.432", "char_index": [3424, 3429], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "70%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.53", "char_index": [3442, 3446], "type": "Result", "training data/set": "Shakespeare", "test data/set": "Shakespeare", "task": "Language Modeling", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "70%"}, "model": ["LSTM", "Long Short Term Memory"], "model settings": {"xx": "yy"}}, {"value": "0.766", "char_index": [3539, 3544], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [3565, 3569], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.699", "char_index": [3580, 3585], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.90", "char_index": [3599, 3603], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.639", "char_index": [3620, 3625], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.70", "char_index": [3638, 3642], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.645", "char_index": [3659, 3664], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.50", "char_index": [3677, 3681], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.804", "char_index": [3698, 3703], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.30", "char_index": [3716, 3720], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedAvg", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.774", "char_index": [3828, 3833], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [3853, 3857], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.664", "char_index": [3869, 3874], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.89", "char_index": [3887, 3891], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.709", "char_index": [3907, 3912], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.70", "char_index": [3925, 3929], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.694", "char_index": [3946, 3951], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.49", "char_index": [3964, 3968], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.759", "char_index": [3984, 3989], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.29", "char_index": [4002, 4006], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedProx", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.794", "char_index": [4121, 4126], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.99", "char_index": [4138, 4142], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "Baseline"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.762", "char_index": [4162, 4167], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.97", "char_index": [4181, 4185], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "10%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.719", "char_index": [4201, 4206], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.90", "char_index": [4219, 4223], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "30%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.720", "char_index": [4240, 4245], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.86", "char_index": [4258, 4262], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "50%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.824", "char_index": [4278, 4283], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["Acc", "Accuracy"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}, {"value": "0.74", "char_index": [4296, 4300], "type": "Result", "training data/set": "Google Speech", "test data/set": "Google Speech", "task": "Speech Recognition", "metric": ["EUR", "Effective Update Ratio"], "experimental settings": {"Strategy": "FedLesScan", "Ratio of Stragglers": "70%"}, "model": ["CNN", "Convolutional Neural Network"], "model settings": {"xx": "yy"}}]}, "2211.05739v1_table2": {"table_code": "\\begin{table}[t]\n\\centering\n\\begin{adjustbox}{width=8.5cm,  center}\n\\begin{tabular}{|c|c|c|c|c|c|c|} \n\\hline\n\\multirow{2}{*}{\\textbf{ Dataset}} & \\multirow{2}{*}{\\textbf{ Strategy}} & \\multicolumn{5}{c|}{\\textbf{Experiment Time (mins)}}                               \\\\ \n\\cline{3-7}\n                                   &                                     & \\textbf{Standard} & \\textbf{10\\%} & \\textbf{30\\%} & \\textbf{50\\%} & \\textbf{70\\%}  \\\\ \n\\hline\n\\multirow{3}{*}{MNIST}             & FedAvg                              & 39.7              & 40.2          & 40.0            & \\textbf{40.0}            & \\textbf{40.0}             \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 40.0                & 40.3          & 40.0          & \\textbf{40.0}            & \\textbf{40.0}             \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{23.7}              & \\textbf{28.6}          & \\textbf{27.3}          & \\textbf{40.0}            & \\textbf{40.0}             \\\\ \n\\hline\n\\multirow{3}{*}{FEMNIST}           & FedAvg                              & 75.5              & 86.7          & 86.9          & \\textbf{86.7}          & \\textbf{86.7}          \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 112.4               & 88.2            & 88.1            & 87.8            & 87.4             \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{70.9}              & \\textbf{75.6}          & \\textbf{82.8}          & 86.8          & \\textbf{86.7}           \\\\ \n\\hline\n\\multirow{3}{*}{Shakespeare}       & FedAvg                              & 217.0           & 217.0        & 217.0         & 216.9         & 216.8          \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 217.0             & 217.0         & 217.0        & 217.0        & 216.7          \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{185.5}             &\\textbf{ 215.5  }         & \\textbf{205.2}           & \\textbf{216.8}           & \\textbf{216.6}            \\\\ \n\\hline\n\\multirow{3}{*}{Google Speech}    & FedAvg                              & 20.3                & 40.1            & 40.1            & 40.0           & \\textbf{40.0}             \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 21.5              & 40.0           & 40.0           & 40.0           & \\textbf{40.0}             \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{15.1}                & \\textbf{31.1}            & \\textbf{28.8}            & \\textbf{33.3}            & \\textbf{40.0}             \\\\\n\\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Comparing total time for the three strategies across different scenarios (\\S\\ref{sec:scenarios}) and datasets (\\S\\ref{sec:datasets}). The highlighted values represent the minimum training time for each experiment.}\n\\label{tab:total-time}\n\\vspace{-5mm}\n\\end{table}", "table_label": "{tab:total-time}", "table_numeric_cells": [["10", "\\textbf{10\\%}", 386, 388, 378, 391], ["30", "\\textbf{30\\%}", 402, 404, 394, 407], ["50", "\\textbf{50\\%}", 418, 420, 410, 423], ["70", "\\textbf{70\\%}", 434, 436, 426, 439], ["39.7", "39.7", 527, 531, 527, 531], ["40.2", "40.2", 547, 551, 547, 551], ["40.0", "40.0", 563, 567, 563, 567], ["40.0", "\\textbf{40.0}", 589, 593, 581, 594], ["40.0", "\\textbf{40.0}", 616, 620, 608, 621], ["40.0", "40.0", 725, 729, 725, 729], ["40.3", "40.3", 747, 751, 747, 751], ["40.0", "40.0", 763, 767, 763, 767], ["40.0", "\\textbf{40.0}", 787, 791, 779, 792], ["40.0", "\\textbf{40.0}", 814, 818, 806, 819], ["23.7", "\\textbf{23.7}", 931, 935, 923, 936], ["28.6", "\\textbf{28.6}", 960, 964, 952, 965], ["27.3", "\\textbf{27.3}", 985, 989, 977, 990], ["40.0", "\\textbf{40.0}", 1010, 1014, 1002, 1015], ["40.0", "\\textbf{40.0}", 1037, 1041, 1029, 1042], ["75.5", "75.5", 1141, 1145, 1141, 1145], ["86.7", "86.7", 1161, 1165, 1161, 1165], ["86.9", "86.9", 1177, 1181, 1177, 1181], ["86.7", "\\textbf{86.7}", 1201, 1205, 1193, 1206], ["86.7", "\\textbf{86.7}", 1226, 1230, 1218, 1231], ["112.4", "112.4", 1332, 1337, 1332, 1337], ["88.2", "88.2", 1354, 1358, 1354, 1358], ["88.1", "88.1", 1372, 1376, 1372, 1376], ["87.8", "87.8", 1390, 1394, 1390, 1394], ["87.4", "87.4", 1408, 1412, 1408, 1412], ["70.9", "\\textbf{70.9}", 1524, 1528, 1516, 1529], ["75.6", "\\textbf{75.6}", 1553, 1557, 1545, 1558], ["82.8", "\\textbf{82.8}", 1578, 1582, 1570, 1583], ["86.8", "86.8", 1595, 1599, 1595, 1599], ["86.7", "\\textbf{86.7}", 1619, 1623, 1611, 1624], ["217.0", "217.0", 1721, 1726, 1721, 1726], ["217.0", "217.0", 1739, 1744, 1739, 1744], ["217.0", "217.0", 1754, 1759, 1754, 1759], ["216.9", "216.9", 1770, 1775, 1770, 1775], ["216.8", "216.8", 1786, 1791, 1786, 1791], ["217.0", "217.0", 1892, 1897, 1892, 1897], ["217.0", "217.0", 1912, 1917, 1912, 1917], ["217.0", "217.0", 1928, 1933, 1928, 1933], ["217.0", "217.0", 1943, 1948, 1943, 1948], ["216.7", "216.7", 1958, 1963, 1958, 1963], ["185.5", "\\textbf{185.5}", 2072, 2077, 2064, 2078], ["215.5", "\\textbf{ 215.5  }", 2101, 2106, 2092, 2109], ["205.2", "\\textbf{205.2}", 2128, 2133, 2120, 2134], ["216.8", "\\textbf{216.8}", 2155, 2160, 2147, 2161], ["216.6", "\\textbf{216.6}", 2182, 2187, 2174, 2188], ["20.3", "20.3", 2285, 2289, 2285, 2289], ["40.1", "40.1", 2307, 2311, 2307, 2311], ["40.1", "40.1", 2325, 2329, 2325, 2329], ["40.0", "40.0", 2343, 2347, 2343, 2347], ["40.0", "\\textbf{40.0}", 2368, 2372, 2360, 2373], ["21.5", "21.5", 2477, 2481, 2477, 2481], ["40.0", "40.0", 2497, 2501, 2497, 2501], ["40.0", "40.0", 2514, 2518, 2514, 2518], ["40.0", "40.0", 2531, 2535, 2531, 2535], ["40.0", "\\textbf{40.0}", 2556, 2560, 2548, 2561], ["15.1", "\\textbf{15.1}", 2673, 2677, 2665, 2678], ["31.1", "\\textbf{31.1}", 2704, 2708, 2696, 2709], ["28.8", "\\textbf{28.8}", 2731, 2735, 2723, 2736], ["33.3", "\\textbf{33.3}", 2758, 2762, 2750, 2763], ["40.0", "\\textbf{40.0}", 2785, 2789, 2777, 2790]], "text_chunk_selected": "The different proposed approaches for straggler mitigation in FL can be divided into three categories, i.e., \\textit{synchronous}~\\cite{fedprox}, \\textit{asynchronous}~\\cite{asofed,fed_async}, and \\textit{semi-asynchronous}~\\cite{fedAt, safa, csafl}. One of the most popular synchronous approaches is \\texttt{FedProx}~\\cite{fedprox}, which is based on \\texttt{FedAvg}~\\cite{mcmahan2017communication} with two minor differences. First, a custom loss function for client-side training that limits the varying effects of local model updates. Second, toleration for partial work, i.e., clients can perform a variable amount of work to accommodate constraints in terms of hardware and network. However, incorporating partial work requires tailoring the number of local epochs for each client individually, which might be infeasible for a significantly large number of clients. Moreover, in contrast to \\texttt{FedLesScan} it relies on random selection of clients which makes it sensitive to stragglers. In~\\cite{fed_async}, Xie et al. propose an asynchronous federated optimization algorithm called \\texttt{FedAsync} that relies on a parameter server architecture to invoke and synchronize FL clients. The server consists of two parallel threads, i.e., a \\textit{scheduler} and an \\textit{updater}. The scheduler thread periodically triggers the FL clients to perform training using the latest global model, while the updater thread receives the client model updates and directly aggregates them to the global\nmodel. However, in the context of serverless, this approach has significantly higher resource and communication costs (\\S\\ref{sec:faas}, \\S\\ref{sec:servfl}) since it would require a function invocation to perform aggregation with the global model after each individual client update.\n\n\\subsection{Gathering Behavioral Data}\n\\label{sec:behavedata}\nOur client selection strategy (\\S\\ref{sec:ClientSelection}) depends on the data collected from the clients' behavior in previous training rounds. Towards this, for each client we collect three attributes, i.e., \\textit{training time}, \\textit{missed rounds}, and \\textit{cooldown}. \\textit{Training time} is the time taken by the client to complete local model training. \\textit{Missed rounds} is a list that contains the round numbers of the rounds missed by a client. We use this to calculate a client's penalty as described in \\S\\ref{sec:ClientSelection}. \\textit{Cooldown} represents the number of rounds a client has to stay in the last tier and cannot participate in clustering (\\S\\ref{sec:tiers}). We evaluate the cooldown period from the client\u2019s last missed round using Equation~\\ref{eq: cooldown}. For instance, if a client missed round $2$, the cooldown is set to $1$. Moreover, if the same client missed round $4$, the cooldown is multiplied by two. As a result, the clients with \\textit{cooldown} greater than zero are characterized as \\textit{stragglers}. Towards this, using \\textit{cooldown} can reduce the impact of temporarily slow or unavailable clients by lowering their priority for a certain number of rounds (\\S\\ref{sec:tiers}, \\S\\ref{sec:ClientSelection}).\n\n\\textbf{Straggler (\\%) Scenario}. In this, we simulate varying percentages of stragglers in the FL system.  Although there might be different reasons for FaaS client failures, such as memory limit, function timeout, or communication issues, these failures can only have one of two effects on the clients. Clients can either push their updates after the training round is finished (slow updates) or can completely crash (not push their updates). To simulate slow updates, we limit the training round time to only fit clients with no issues or delays. Meaning, that clients which experience cold starts, bandwidth limitations, or communication delays do not finish the round in time; therefore, pushing their updates later. On the other hand, to simulate failures, we randomly select a specific ratio of clients to fail their training at the beginning of each experiment. We perform four different experiments for each dataset with 10\\%, 30\\%, 50\\%, and 70\\% stragglers in the system. Note that, in all our experiments, stragglers are different from \\textit{malicious clients} found in some FL settings~\\cite{li2020learning, zhang2022fldetector}. While malicious clients can act as stragglers to deliberately slow training or hinder model performance, they can perform more powerful attacks such as data or model poisoning~\\cite{fl_challenges,fl_data_poisoning} to skew the model's performance. In our work, we focus on the problem of stragglers and their behavior rather than the clients' intentions. The number of training rounds for the different datasets and experiment scenarios is shown in Table~\\ref{tab:experiment-params}.\n\nThe obtained accuracy and EUR values across all our experiments is summarized in Table~\\ref{tab:acc-all}. For the \\textit{standard scenarios} (\\S\\ref{sec:scenarios}), we obtained better accuracy and round utlization for our strategy as compared to \\texttt{FedAvg} and \\texttt{FedProx} across all datasets except Shakespeare. This is because with the Shakespeare dataset, some of the clients contribute to the model accuracy more than others, especially clients with longer training times. Moreover, due to budget constraints and significantly high training costs, we train for a slightly less number of rounds, i.e., 25 for Shakespeare. However, we argue that in a more realistic scenario with more number of rounds, the difference in accuracy will decrease. This is because our strategy utilizes clients more efficiently, i.e., higher EUR. As a result, with more rounds the number of invocations per client will increase and more clients will contribute to the global model, leading to a higher accuracy. Similarly, for the \\textit{straggler (\\%) scenarios}, we obtained better results for accuracy with our strategy as compared to the other two for the MNIST, FEMNIST, and the Google Speech datasets. On the Shakespeare dataset, our strategy outperformed \\texttt{FedAvg} and \\texttt{FedProx} in scenarios with 30\\%, 50\\%, and 70\\% stragglers. In terms of EUR, our strategy constantly outperforms the other two strategies across all scenarios and datasets since they use random selection for selecting clients for an FL training round.\n\nFor most \\textit{standard scenarios} (Table~\\ref{tab:acc-all}), \\texttt{FedLesScan} obtained better accuracy as compared to the other two strategies due to the better distribution of client invocations. This is because our strategy prioritizes clients with the least number of invocations in a selected client cluster (\\S\\ref{sec:ClientSelection}) leading to more balanced contributions among the participating clients. On the other hand, with \\textit{straggler (\\%) scenario}, our strategy reached better accuracies by relying more on robust and reliable clients. Furthermore, the utilization of a staleness-aware aggregation scheme (\\S\\ref{sec:stalelessaware}) avoids wasting valuable contributions, which in turn increases accuracy.\n\nTo offer detailed insights, we present results for the Google Speech dataset~\\cite{speech} wrt the metrics accuracy and EUR, across the FL training session as shown in Figures \\ref{fig:acc-speech} and \\ref{fig:eur-speech} respectively. For the \\textit{standard scenarios}, we ran the FL training session for $35$ rounds, while for the \\textit{straggler (\\%) scenarios}, we ran the experiments for $60$ rounds. For the standard scenario, our strategy reached an accuracy of 79.4\\% as compared to 76.6\\% and 77.4\\% for \\texttt{FedAvg} and \\texttt{FedProx} respectively. Furthermore, our strategy showed faster convergence by reaching an accuracy of 70\\% in 19 rounds as compared to 21 and 22 for \\texttt{FedAvg} and \\texttt{FedProx} respectively. With 10\\% stragglers, our strategy and \\texttt{FedAvg} had a similar convergence rate, while \\texttt{FedProx} was slightly behind. Moreover, our strategy reached an end accuracy of 76\\% which is a 6\\% and 10\\% increase over \\texttt{FedAvg} and \\texttt{FedProx} respectively. For an FL system with 30\\% stragglers, our strategy consistently outperforms \\texttt{FedAvg} by around 8\\% towards the end of the training, while outperforming \\texttt{FedProx} by a smaller margin of 1\\%. We observe a similar trend for our experiments with 50\\% and 70\\% stragglers in the system, where our strategy outperforms the other two. From our experiments, we observe that the presence of stragglers affects the convergence speed of the DNN models in an FL training session. DNN models trained in \\textit{standard scenarios} converge faster as compared to the models trained with \\textit{straggler (\\%) scenarios} as shown in Figure~\\ref{fig:acc-speech} (Table~\\ref{tab:experiment-params}). We observe a similar behaviour for the different datasets (\\S\\ref{sec:datasets}). Moreover, for the \\textit{straggler (\\%) scenarios}, increasing the percentage of stragglers in the system does not consistently decrease the accuracy of the trained DNN model as shown in Table~\\ref{tab:acc-all}. This is especially true for experiments with a higher number of stragglers, i.e., $70\\%$. This behavior is not exclusive to our experiments and was also reported by Li et al.~\\cite{fedprox} and Wu et al.~\\cite{safa}. While the authors do not provide a clear explanation for this behaviour, we argue that due to the non-IID nature of the data, clients do not contribute evenly to the test accuracy. In addition, having fewer number of reliable clients reduces the varying effect of local model updates and local model deviations from the global model, making it easier to reach a consensus on the global model. Therefore, we can reach situations where a system with more stragglers reaches higher overall accuracy at the end. \n\n\\subsection{Comparing time and cost}\n\\label{sec:timecost}\nAlthough model accuracy is an important metric, fast convergence wrt the number of training rounds does not provide a complete picture of the efficiency of the system. Towards this, in this section, we provide a collective analysis of all experiments wrt total duration and costs. To compute the total experiment duration, we aggregate the total round time during training. For all the three strategies, the round time is determined by the slowest invoked client. As a result, the round time depends on either the response of the slowest client or a predetermined timeout (\\S\\ref{sec:scenarios}). Furthermore, for \\textit{straggler (\\%) scenarios}, we simulate real-world behavior by assuming that the stragglers will not respond, thus forcing the controller (\\S\\ref{sec:sysdesign}) to wait until the round timeout. \n\nTable~\\ref{tab:total-time} shows the total aggregated time per experiment. For the \\textit{standard scenario}, training a model with our strategy is significantly faster as compared to \\texttt{FedAvg} and \\texttt{FedProx} across all datasets. For instance, for the MNIST dataset, our strategy takes 40\\% less time as compared to the other strategies. For the \\textit{straggler (\\%) scenarios}, we see the effect of stragglers on experiment duration. In the scenarios with 10\\% and 30\\% stragglers, we observe that \n\\texttt{FedLesScan} maintains a lower duration across all experiments. However, when the number of stragglers in the system is significantly higher, they must be invoked in almost all training rounds to meet the minimum number of clients required per round. We notice this behavior for our strategy with greater than 50\\% stragglers in the system for the MNIST, FEMNIST, and the Shakespeare dataset. However, for the Google Speech dataset, our strategy still has an 18\\% lower experiment duration as compared to the other two. This is because the total number of clients for the Google Speech dataset is 542 with 200 concurrent clients participating in a training round (\\S\\ref{sec:expconfig}). For the scenario with 70\\% stragglers in the system, all approaches have similar experiment times across all datasets.", "table_source": "\\begin{table}[t]\n\\centering\n\\begin{adjustbox}{width=8.5cm,  center}\n\\begin{tabular}{|c|c|c|c|c|c|c|} \n\\hline\n\\multirow{2}{*}{\\textbf{ Dataset}} & \\multirow{2}{*}{\\textbf{ Strategy}} & \\multicolumn{5}{c|}{\\textbf{Experiment Time (mins)}}                               \\\\ \n\\cline{3-7}\n                                   &                                     & \\textbf{Standard} & \\textbf{10\\%} & \\textbf{30\\%} & \\textbf{50\\%} & \\textbf{70\\%}  \\\\ \n\\hline\n\\multirow{3}{*}{MNIST}             & FedAvg                              & 39.7              & 40.2          & 40.0            & \\textbf{40.0}            & \\textbf{40.0}             \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 40.0                & 40.3          & 40.0          & \\textbf{40.0}            & \\textbf{40.0}             \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{23.7}              & \\textbf{28.6}          & \\textbf{27.3}          & \\textbf{40.0}            & \\textbf{40.0}             \\\\ \n\\hline\n\\multirow{3}{*}{FEMNIST}           & FedAvg                              & 75.5              & 86.7          & 86.9          & \\textbf{86.7}          & \\textbf{86.7}          \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 112.4               & 88.2            & 88.1            & 87.8            & 87.4             \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{70.9}              & \\textbf{75.6}          & \\textbf{82.8}          & 86.8          & \\textbf{86.7}           \\\\ \n\\hline\n\\multirow{3}{*}{Shakespeare}       & FedAvg                              & 217.0           & 217.0        & 217.0         & 216.9         & 216.8          \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 217.0             & 217.0         & 217.0        & 217.0        & 216.7          \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{185.5}             &\\textbf{ 215.5  }         & \\textbf{205.2}           & \\textbf{216.8}           & \\textbf{216.6}            \\\\ \n\\hline\n\\multirow{3}{*}{Google Speech}    & FedAvg                              & 20.3                & 40.1            & 40.1            & 40.0           & \\textbf{40.0}             \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 21.5              & 40.0           & 40.0           & 40.0           & \\textbf{40.0}             \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{15.1}                & \\textbf{31.1}            & \\textbf{28.8}            & \\textbf{33.3}            & \\textbf{40.0}             \\\\\n\\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Comparing total time for the three strategies across different scenarios (\\S\\ref{sec:scenarios}) and datasets (\\S\\ref{sec:datasets}). The highlighted values represent the minimum training time for each experiment.}\n\\label{tab:total-time}\n\\vspace{-5mm}\n\\end{table}", "cell_list_gold": [{"value": "10", "char_index": [386, 388], "type": "Other"}, {"value": "30", "char_index": [402, 404], "type": "Other"}, {"value": "50", "char_index": [418, 420], "type": "Other"}, {"value": "70", "char_index": [434, 436], "type": "Other"}, {"value": "39.7", "char_index": [527, 531], "type": "Other"}, {"value": "40.2", "char_index": [547, 551], "type": "Other"}, {"value": "40.0", "char_index": [563, 567], "type": "Other"}, {"value": "40.0", "char_index": [589, 593], "type": "Other"}, {"value": "40.0", "char_index": [616, 620], "type": "Other"}, {"value": "40.0", "char_index": [725, 729], "type": "Other"}, {"value": "40.3", "char_index": [747, 751], "type": "Other"}, {"value": "40.0", "char_index": [763, 767], "type": "Other"}, {"value": "40.0", "char_index": [787, 791], "type": "Other"}, {"value": "40.0", "char_index": [814, 818], "type": "Other"}, {"value": "23.7", "char_index": [931, 935], "type": "Other"}, {"value": "28.6", "char_index": [960, 964], "type": "Other"}, {"value": "27.3", "char_index": [985, 989], "type": "Other"}, {"value": "40.0", "char_index": [1010, 1014], "type": "Other"}, {"value": "40.0", "char_index": [1037, 1041], "type": "Other"}, {"value": "75.5", "char_index": [1141, 1145], "type": "Other"}, {"value": "86.7", "char_index": [1161, 1165], "type": "Other"}, {"value": "86.9", "char_index": [1177, 1181], "type": "Other"}, {"value": "86.7", "char_index": [1201, 1205], "type": "Other"}, {"value": "86.7", "char_index": [1226, 1230], "type": "Other"}, {"value": "112.4", "char_index": [1332, 1337], "type": "Other"}, {"value": "88.2", "char_index": [1354, 1358], "type": "Other"}, {"value": "88.1", "char_index": [1372, 1376], "type": "Other"}, {"value": "87.8", "char_index": [1390, 1394], "type": "Other"}, {"value": "87.4", "char_index": [1408, 1412], "type": "Other"}, {"value": "70.9", "char_index": [1524, 1528], "type": "Other"}, {"value": "75.6", "char_index": [1553, 1557], "type": "Other"}, {"value": "82.8", "char_index": [1578, 1582], "type": "Other"}, {"value": "86.8", "char_index": [1595, 1599], "type": "Other"}, {"value": "86.7", "char_index": [1619, 1623], "type": "Other"}, {"value": "217.0", "char_index": [1721, 1726], "type": "Other"}, {"value": "217.0", "char_index": [1739, 1744], "type": "Other"}, {"value": "217.0", "char_index": [1754, 1759], "type": "Other"}, {"value": "216.9", "char_index": [1770, 1775], "type": "Other"}, {"value": "216.8", "char_index": [1786, 1791], "type": "Other"}, {"value": "217.0", "char_index": [1892, 1897], "type": "Other"}, {"value": "217.0", "char_index": [1912, 1917], "type": "Other"}, {"value": "217.0", "char_index": [1928, 1933], "type": "Other"}, {"value": "217.0", "char_index": [1943, 1948], "type": "Other"}, {"value": "216.7", "char_index": [1958, 1963], "type": "Other"}, {"value": "185.5", "char_index": [2072, 2077], "type": "Other"}, {"value": "215.5", "char_index": [2101, 2106], "type": "Other"}, {"value": "205.2", "char_index": [2128, 2133], "type": "Other"}, {"value": "216.8", "char_index": [2155, 2160], "type": "Other"}, {"value": "216.6", "char_index": [2182, 2187], "type": "Other"}, {"value": "20.3", "char_index": [2285, 2289], "type": "Other"}, {"value": "40.1", "char_index": [2307, 2311], "type": "Other"}, {"value": "40.1", "char_index": [2325, 2329], "type": "Other"}, {"value": "40.0", "char_index": [2343, 2347], "type": "Other"}, {"value": "40.0", "char_index": [2368, 2372], "type": "Other"}, {"value": "21.5", "char_index": [2477, 2481], "type": "Other"}, {"value": "40.0", "char_index": [2497, 2501], "type": "Other"}, {"value": "40.0", "char_index": [2514, 2518], "type": "Other"}, {"value": "40.0", "char_index": [2531, 2535], "type": "Other"}, {"value": "40.0", "char_index": [2556, 2560], "type": "Other"}, {"value": "15.1", "char_index": [2673, 2677], "type": "Other"}, {"value": "31.1", "char_index": [2704, 2708], "type": "Other"}, {"value": "28.8", "char_index": [2731, 2735], "type": "Other"}, {"value": "33.3", "char_index": [2758, 2762], "type": "Other"}, {"value": "40.0", "char_index": [2785, 2789], "type": "Other"}]}, "2211.05739v1_table3": {"table_code": "\\begin{table}[t]\n\\centering\n\\begin{adjustbox}{width=8.5cm,  center}\n\\begin{tabular}{|c|c|c|c|c|c|c|} \n\\hline\n\\multirow{2}{*}{\\textbf{ Dataset}} & \\multirow{2}{*}{\\textbf{ Strategy}} & \\multicolumn{5}{c|}{\\textbf{Experiment Cost (\\$)}}                                 \\\\ \n\\cline{3-7}\n                                   &                                     & \\textbf{Standard} & \\textbf{10\\%} & \\textbf{30\\%} & \\textbf{50\\%} & \\textbf{70\\%}  \\\\ \n\\hline\n\\multirow{3}{*}{MNIST}             & FedAvg                              & 2.90               & 3.90           & 6.00             & 8.00             & 10.40           \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 5.50               & 6.40           & 7.60           & 9.21          & 11.03          \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{2.70}               & \\textbf{3.86}           & \\textbf{4.00}             & \\textbf{5.99}             &\\textbf{9.2}           \\\\ \n\\hline\n\\multirow{3}{*}{FEMNIST}           & FedAvg                              & 13.50              & 16.19          & 17.87         & 20.54         & 24.70           \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 16.67              & 17.29          & 19.40          & 22.42         & 25.80           \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{13.17}              & \\textbf{14.58}         & \\textbf{14.40}          & \\textbf{14.81}         & \\textbf{20.60}           \\\\ \n\\hline\n\\multirow{3}{*}{Shakespeare}       & FedAvg                              & 5.40               & 6.60           & 9.21           & 12.50          & 15.40           \\\\ \n\\cline{2-7}\n                                   & FedProx                             & \\textbf{5.12}              & 6.72          & 9.00             & 12.20          & 15.40           \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & 5.33              & \\textbf{5.50}           & \\textbf{6.75}          &\\textbf{ 8.46}          & \\textbf{12.00}             \\\\ \n\\hline\n\\multirow{3}{*}{Google Speech}    & FedAvg                              & 1.98              & 3.90         & 6.40         & 8.30         & 10.50           \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 2.39             & 4.60         & 6.77          & 8.70          & 10.80           \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{1.73}              & \\textbf{2.70}         & \\textbf{3.68}          & \\textbf{4.20}          & \\textbf{5.50}           \\\\\n\\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Comparing training costs for the three strategies across different scenarios (\\S\\ref{sec:scenarios}) and datasets (\\S\\ref{sec:datasets}). The highlighted values represent the minimum experiment cost.}\n\\label{tab:cost-analysis}\n\\vspace{-5mm}\n\\end{table}", "table_label": "{tab:cost-analysis}", "table_numeric_cells": [["10", "\\textbf{10\\%}", 386, 388, 378, 391], ["30", "\\textbf{30\\%}", 402, 404, 394, 407], ["50", "\\textbf{50\\%}", 418, 420, 410, 423], ["70", "\\textbf{70\\%}", 434, 436, 426, 439], ["2.90", "2.90", 527, 531, 527, 531], ["3.90", "3.90", 548, 552, 548, 552], ["6.00", "6.00", 565, 569, 565, 569], ["8.00", "8.00", 584, 588, 584, 588], ["10.40", "10.40", 603, 608, 603, 608], ["5.50", "5.50", 710, 714, 710, 714], ["6.40", "6.40", 731, 735, 731, 735], ["7.60", "7.60", 748, 752, 748, 752], ["9.21", "9.21", 765, 769, 765, 769], ["11.03", "11.03", 781, 786, 781, 786], ["2.70", "\\textbf{2.70}", 895, 899, 887, 900], ["3.86", "\\textbf{3.86}", 925, 929, 917, 930], ["4.00", "\\textbf{4.00}", 951, 955, 943, 956], ["5.99", "\\textbf{5.99}", 979, 983, 971, 984], ["9.2", "\\textbf{9.2}", 1006, 1009, 998, 1010], ["13.50", "13.50", 1107, 1112, 1107, 1112], ["16.19", "16.19", 1128, 1133, 1128, 1133], ["17.87", "17.87", 1145, 1150, 1145, 1150], ["20.54", "20.54", 1161, 1166, 1161, 1166], ["24.70", "24.70", 1177, 1182, 1177, 1182], ["16.67", "16.67", 1284, 1289, 1284, 1289], ["17.29", "17.29", 1305, 1310, 1305, 1310], ["19.40", "19.40", 1322, 1327, 1322, 1327], ["22.42", "22.42", 1339, 1344, 1339, 1344], ["25.80", "25.80", 1355, 1360, 1355, 1360], ["13.17", "\\textbf{13.17}", 1470, 1475, 1462, 1476], ["14.58", "\\textbf{14.58}", 1500, 1505, 1492, 1506], ["14.40", "\\textbf{14.40}", 1525, 1530, 1517, 1531], ["14.81", "\\textbf{14.81}", 1551, 1556, 1543, 1557], ["20.60", "\\textbf{20.60}", 1576, 1581, 1568, 1582], ["5.40", "5.40", 1679, 1683, 1679, 1683], ["6.60", "6.60", 1700, 1704, 1700, 1704], ["9.21", "9.21", 1717, 1721, 1717, 1721], ["12.50", "12.50", 1734, 1739, 1734, 1739], ["15.40", "15.40", 1751, 1756, 1751, 1756], ["5.12", "\\textbf{5.12}", 1866, 1870, 1858, 1871], ["6.72", "6.72", 1887, 1891, 1887, 1891], ["9.00", "9.00", 1903, 1907, 1903, 1907], ["12.20", "12.20", 1922, 1927, 1922, 1927], ["15.40", "15.40", 1939, 1944, 1939, 1944], ["5.33", "5.33", 2046, 2050, 2046, 2050], ["5.50", "\\textbf{5.50}", 2074, 2078, 2066, 2079], ["6.75", "\\textbf{6.75}", 2100, 2104, 2092, 2105], ["8.46", "\\textbf{ 8.46}", 2125, 2129, 2116, 2130], ["12.00", "\\textbf{12.00}", 2150, 2155, 2142, 2156], ["1.98", "1.98", 2254, 2258, 2254, 2258], ["3.90", "3.90", 2274, 2278, 2274, 2278], ["6.40", "6.40", 2289, 2293, 2289, 2293], ["8.30", "8.30", 2304, 2308, 2304, 2308], ["10.50", "10.50", 2319, 2324, 2319, 2324], ["2.39", "2.39", 2426, 2430, 2426, 2430], ["4.60", "4.60", 2445, 2449, 2445, 2449], ["6.77", "6.77", 2460, 2464, 2460, 2464], ["8.70", "8.70", 2476, 2480, 2476, 2480], ["10.80", "10.80", 2492, 2497, 2492, 2497], ["1.73", "\\textbf{1.73}", 2607, 2611, 2599, 2612], ["2.70", "\\textbf{2.70}", 2636, 2640, 2628, 2641], ["3.68", "\\textbf{3.68}", 2660, 2664, 2652, 2665], ["4.20", "\\textbf{4.20}", 2685, 2689, 2677, 2690], ["5.50", "\\textbf{5.50}", 2710, 2714, 2702, 2715]], "text_chunk_selected": "The different proposed approaches for straggler mitigation in FL can be divided into three categories, i.e., \\textit{synchronous}~\\cite{fedprox}, \\textit{asynchronous}~\\cite{asofed,fed_async}, and \\textit{semi-asynchronous}~\\cite{fedAt, safa, csafl}. One of the most popular synchronous approaches is \\texttt{FedProx}~\\cite{fedprox}, which is based on \\texttt{FedAvg}~\\cite{mcmahan2017communication} with two minor differences. First, a custom loss function for client-side training that limits the varying effects of local model updates. Second, toleration for partial work, i.e., clients can perform a variable amount of work to accommodate constraints in terms of hardware and network. However, incorporating partial work requires tailoring the number of local epochs for each client individually, which might be infeasible for a significantly large number of clients. Moreover, in contrast to \\texttt{FedLesScan} it relies on random selection of clients which makes it sensitive to stragglers. In~\\cite{fed_async}, Xie et al. propose an asynchronous federated optimization algorithm called \\texttt{FedAsync} that relies on a parameter server architecture to invoke and synchronize FL clients. The server consists of two parallel threads, i.e., a \\textit{scheduler} and an \\textit{updater}. The scheduler thread periodically triggers the FL clients to perform training using the latest global model, while the updater thread receives the client model updates and directly aggregates them to the global\nmodel. However, in the context of serverless, this approach has significantly higher resource and communication costs (\\S\\ref{sec:faas}, \\S\\ref{sec:servfl}) since it would require a function invocation to perform aggregation with the global model after each individual client update.\n\n\\section{Experimental Results}\n\\label{sec:results}\nIn this section, we first describe the datasets and model architectures used for evaluating our strategy. Following this, we describe the configuration for \\emph{FedLess} and the FaaS client functions. Finally, we evaluate the performance of our strategy as compared to \\texttt{FedAvg}~\\cite{mcmahan2017communication} and \\texttt{FedProx}~\\cite{fedprox} wrt several metrics for varying number of stragglers. We chose these strategies since they are robust and work well in a serverless setting. For all our experiments, we follow best practices while reporting results and repeat them three times~\\cite{8758926}. More extensive experimental results can be found here~\\cite{thesismohamed}.\n\n\\textbf{Straggler (\\%) Scenario}. In this, we simulate varying percentages of stragglers in the FL system.  Although there might be different reasons for FaaS client failures, such as memory limit, function timeout, or communication issues, these failures can only have one of two effects on the clients. Clients can either push their updates after the training round is finished (slow updates) or can completely crash (not push their updates). To simulate slow updates, we limit the training round time to only fit clients with no issues or delays. Meaning, that clients which experience cold starts, bandwidth limitations, or communication delays do not finish the round in time; therefore, pushing their updates later. On the other hand, to simulate failures, we randomly select a specific ratio of clients to fail their training at the beginning of each experiment. We perform four different experiments for each dataset with 10\\%, 30\\%, 50\\%, and 70\\% stragglers in the system. Note that, in all our experiments, stragglers are different from \\textit{malicious clients} found in some FL settings~\\cite{li2020learning, zhang2022fldetector}. While malicious clients can act as stragglers to deliberately slow training or hinder model performance, they can perform more powerful attacks such as data or model poisoning~\\cite{fl_challenges,fl_data_poisoning} to skew the model's performance. In our work, we focus on the problem of stragglers and their behavior rather than the clients' intentions. The number of training rounds for the different datasets and experiment scenarios is shown in Table~\\ref{tab:experiment-params}.\n\nThe obtained accuracy and EUR values across all our experiments is summarized in Table~\\ref{tab:acc-all}. For the \\textit{standard scenarios} (\\S\\ref{sec:scenarios}), we obtained better accuracy and round utlization for our strategy as compared to \\texttt{FedAvg} and \\texttt{FedProx} across all datasets except Shakespeare. This is because with the Shakespeare dataset, some of the clients contribute to the model accuracy more than others, especially clients with longer training times. Moreover, due to budget constraints and significantly high training costs, we train for a slightly less number of rounds, i.e., 25 for Shakespeare. However, we argue that in a more realistic scenario with more number of rounds, the difference in accuracy will decrease. This is because our strategy utilizes clients more efficiently, i.e., higher EUR. As a result, with more rounds the number of invocations per client will increase and more clients will contribute to the global model, leading to a higher accuracy. Similarly, for the \\textit{straggler (\\%) scenarios}, we obtained better results for accuracy with our strategy as compared to the other two for the MNIST, FEMNIST, and the Google Speech datasets. On the Shakespeare dataset, our strategy outperformed \\texttt{FedAvg} and \\texttt{FedProx} in scenarios with 30\\%, 50\\%, and 70\\% stragglers. In terms of EUR, our strategy constantly outperforms the other two strategies across all scenarios and datasets since they use random selection for selecting clients for an FL training round.\n\nFor most \\textit{standard scenarios} (Table~\\ref{tab:acc-all}), \\texttt{FedLesScan} obtained better accuracy as compared to the other two strategies due to the better distribution of client invocations. This is because our strategy prioritizes clients with the least number of invocations in a selected client cluster (\\S\\ref{sec:ClientSelection}) leading to more balanced contributions among the participating clients. On the other hand, with \\textit{straggler (\\%) scenario}, our strategy reached better accuracies by relying more on robust and reliable clients. Furthermore, the utilization of a staleness-aware aggregation scheme (\\S\\ref{sec:stalelessaware}) avoids wasting valuable contributions, which in turn increases accuracy.\n\nTo offer detailed insights, we present results for the Google Speech dataset~\\cite{speech} wrt the metrics accuracy and EUR, across the FL training session as shown in Figures \\ref{fig:acc-speech} and \\ref{fig:eur-speech} respectively. For the \\textit{standard scenarios}, we ran the FL training session for $35$ rounds, while for the \\textit{straggler (\\%) scenarios}, we ran the experiments for $60$ rounds. For the standard scenario, our strategy reached an accuracy of 79.4\\% as compared to 76.6\\% and 77.4\\% for \\texttt{FedAvg} and \\texttt{FedProx} respectively. Furthermore, our strategy showed faster convergence by reaching an accuracy of 70\\% in 19 rounds as compared to 21 and 22 for \\texttt{FedAvg} and \\texttt{FedProx} respectively. With 10\\% stragglers, our strategy and \\texttt{FedAvg} had a similar convergence rate, while \\texttt{FedProx} was slightly behind. Moreover, our strategy reached an end accuracy of 76\\% which is a 6\\% and 10\\% increase over \\texttt{FedAvg} and \\texttt{FedProx} respectively. For an FL system with 30\\% stragglers, our strategy consistently outperforms \\texttt{FedAvg} by around 8\\% towards the end of the training, while outperforming \\texttt{FedProx} by a smaller margin of 1\\%. We observe a similar trend for our experiments with 50\\% and 70\\% stragglers in the system, where our strategy outperforms the other two. From our experiments, we observe that the presence of stragglers affects the convergence speed of the DNN models in an FL training session. DNN models trained in \\textit{standard scenarios} converge faster as compared to the models trained with \\textit{straggler (\\%) scenarios} as shown in Figure~\\ref{fig:acc-speech} (Table~\\ref{tab:experiment-params}). We observe a similar behaviour for the different datasets (\\S\\ref{sec:datasets}). Moreover, for the \\textit{straggler (\\%) scenarios}, increasing the percentage of stragglers in the system does not consistently decrease the accuracy of the trained DNN model as shown in Table~\\ref{tab:acc-all}. This is especially true for experiments with a higher number of stragglers, i.e., $70\\%$. This behavior is not exclusive to our experiments and was also reported by Li et al.~\\cite{fedprox} and Wu et al.~\\cite{safa}. While the authors do not provide a clear explanation for this behaviour, we argue that due to the non-IID nature of the data, clients do not contribute evenly to the test accuracy. In addition, having fewer number of reliable clients reduces the varying effect of local model updates and local model deviations from the global model, making it easier to reach a consensus on the global model. Therefore, we can reach situations where a system with more stragglers reaches higher overall accuracy at the end. \n\nTable~\\ref{tab:total-time} shows the total aggregated time per experiment. For the \\textit{standard scenario}, training a model with our strategy is significantly faster as compared to \\texttt{FedAvg} and \\texttt{FedProx} across all datasets. For instance, for the MNIST dataset, our strategy takes 40\\% less time as compared to the other strategies. For the \\textit{straggler (\\%) scenarios}, we see the effect of stragglers on experiment duration. In the scenarios with 10\\% and 30\\% stragglers, we observe that \n\\texttt{FedLesScan} maintains a lower duration across all experiments. However, when the number of stragglers in the system is significantly higher, they must be invoked in almost all training rounds to meet the minimum number of clients required per round. We notice this behavior for our strategy with greater than 50\\% stragglers in the system for the MNIST, FEMNIST, and the Shakespeare dataset. However, for the Google Speech dataset, our strategy still has an 18\\% lower experiment duration as compared to the other two. This is because the total number of clients for the Google Speech dataset is 542 with 200 concurrent clients participating in a training round (\\S\\ref{sec:expconfig}). For the scenario with 70\\% stragglers in the system, all approaches have similar experiment times across all datasets.\n\nTable~\\ref{tab:cost-analysis} shows the cost for the different strategies, datasets, and scenarios. For the \\textit{standard scenario} with MNIST, we observe a 6.8\\% and 50\\% cost reduction for our strategy as compared to \\texttt{FedAvg} and \\texttt{FedProx}. Similarly for the FEMNIST and Google Speech datasets in the \\textit{standard scenario}, we observe cost reductions of about 2\\%, 20\\% and 12\\%, 27\\% as compared to \\texttt{FedAvg} and \\texttt{FedProx} respectively. On the other hand, for the Shakespeare dataset, \\texttt{FedProx} performed better than our strategy and \\texttt{FedAvg} by 4\\% and 6\\% respectively. We observe that the efficiency of our strategy becomes more visible as the number of stragglers in the system increases. For all the scenarios with a varying number of stragglers, our strategy has the minimum cost as compared to the other two strategies. For the \\textit{straggler (\\%) scenarios}, our strategy consistently achieved lower experiment cost across all datasets with an average cost reduction of 25\\% and 32\\% as compared to \\texttt{FedAvg} and \\texttt{FedProx} respectively.", "table_source": "\\begin{table}[t]\n\\centering\n\\begin{adjustbox}{width=8.5cm,  center}\n\\begin{tabular}{|c|c|c|c|c|c|c|} \n\\hline\n\\multirow{2}{*}{\\textbf{ Dataset}} & \\multirow{2}{*}{\\textbf{ Strategy}} & \\multicolumn{5}{c|}{\\textbf{Experiment Cost (\\$)}}                                 \\\\ \n\\cline{3-7}\n                                   &                                     & \\textbf{Standard} & \\textbf{10\\%} & \\textbf{30\\%} & \\textbf{50\\%} & \\textbf{70\\%}  \\\\ \n\\hline\n\\multirow{3}{*}{MNIST}             & FedAvg                              & 2.90               & 3.90           & 6.00             & 8.00             & 10.40           \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 5.50               & 6.40           & 7.60           & 9.21          & 11.03          \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{2.70}               & \\textbf{3.86}           & \\textbf{4.00}             & \\textbf{5.99}             &\\textbf{9.2}           \\\\ \n\\hline\n\\multirow{3}{*}{FEMNIST}           & FedAvg                              & 13.50              & 16.19          & 17.87         & 20.54         & 24.70           \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 16.67              & 17.29          & 19.40          & 22.42         & 25.80           \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{13.17}              & \\textbf{14.58}         & \\textbf{14.40}          & \\textbf{14.81}         & \\textbf{20.60}           \\\\ \n\\hline\n\\multirow{3}{*}{Shakespeare}       & FedAvg                              & 5.40               & 6.60           & 9.21           & 12.50          & 15.40           \\\\ \n\\cline{2-7}\n                                   & FedProx                             & \\textbf{5.12}              & 6.72          & 9.00             & 12.20          & 15.40           \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & 5.33              & \\textbf{5.50}           & \\textbf{6.75}          &\\textbf{ 8.46}          & \\textbf{12.00}             \\\\ \n\\hline\n\\multirow{3}{*}{Google Speech}    & FedAvg                              & 1.98              & 3.90         & 6.40         & 8.30         & 10.50           \\\\ \n\\cline{2-7}\n                                   & FedProx                             & 2.39             & 4.60         & 6.77          & 8.70          & 10.80           \\\\ \n\\cline{2-7}\n                                   & FedLesScan                          & \\textbf{1.73}              & \\textbf{2.70}         & \\textbf{3.68}          & \\textbf{4.20}          & \\textbf{5.50}           \\\\\n\\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Comparing training costs for the three strategies across different scenarios (\\S\\ref{sec:scenarios}) and datasets (\\S\\ref{sec:datasets}). The highlighted values represent the minimum experiment cost.}\n\\label{tab:cost-analysis}\n\\vspace{-5mm}\n\\end{table}", "cell_list_gold": [{"value": "10", "char_index": [386, 388], "type": "Other"}, {"value": "30", "char_index": [402, 404], "type": "Other"}, {"value": "50", "char_index": [418, 420], "type": "Other"}, {"value": "70", "char_index": [434, 436], "type": "Other"}, {"value": "2.90", "char_index": [527, 531], "type": "Other"}, {"value": "3.90", "char_index": [548, 552], "type": "Other"}, {"value": "6.00", "char_index": [565, 569], "type": "Other"}, {"value": "8.00", "char_index": [584, 588], "type": "Other"}, {"value": "10.40", "char_index": [603, 608], "type": "Other"}, {"value": "5.50", "char_index": [710, 714], "type": "Other"}, {"value": "6.40", "char_index": [731, 735], "type": "Other"}, {"value": "7.60", "char_index": [748, 752], "type": "Other"}, {"value": "9.21", "char_index": [765, 769], "type": "Other"}, {"value": "11.03", "char_index": [781, 786], "type": "Other"}, {"value": "2.70", "char_index": [895, 899], "type": "Other"}, {"value": "3.86", "char_index": [925, 929], "type": "Other"}, {"value": "4.00", "char_index": [951, 955], "type": "Other"}, {"value": "5.99", "char_index": [979, 983], "type": "Other"}, {"value": "9.2", "char_index": [1006, 1009], "type": "Other"}, {"value": "13.50", "char_index": [1107, 1112], "type": "Other"}, {"value": "16.19", "char_index": [1128, 1133], "type": "Other"}, {"value": "17.87", "char_index": [1145, 1150], "type": "Other"}, {"value": "20.54", "char_index": [1161, 1166], "type": "Other"}, {"value": "24.70", "char_index": [1177, 1182], "type": "Other"}, {"value": "16.67", "char_index": [1284, 1289], "type": "Other"}, {"value": "17.29", "char_index": [1305, 1310], "type": "Other"}, {"value": "19.40", "char_index": [1322, 1327], "type": "Other"}, {"value": "22.42", "char_index": [1339, 1344], "type": "Other"}, {"value": "25.80", "char_index": [1355, 1360], "type": "Other"}, {"value": "13.17", "char_index": [1470, 1475], "type": "Other"}, {"value": "14.58", "char_index": [1500, 1505], "type": "Other"}, {"value": "14.40", "char_index": [1525, 1530], "type": "Other"}, {"value": "14.81", "char_index": [1551, 1556], "type": "Other"}, {"value": "20.60", "char_index": [1576, 1581], "type": "Other"}, {"value": "5.40", "char_index": [1679, 1683], "type": "Other"}, {"value": "6.60", "char_index": [1700, 1704], "type": "Other"}, {"value": "9.21", "char_index": [1717, 1721], "type": "Other"}, {"value": "12.50", "char_index": [1734, 1739], "type": "Other"}, {"value": "15.40", "char_index": [1751, 1756], "type": "Other"}, {"value": "5.12", "char_index": [1866, 1870], "type": "Other"}, {"value": "6.72", "char_index": [1887, 1891], "type": "Other"}, {"value": "9.00", "char_index": [1903, 1907], "type": "Other"}, {"value": "12.20", "char_index": [1922, 1927], "type": "Other"}, {"value": "15.40", "char_index": [1939, 1944], "type": "Other"}, {"value": "5.33", "char_index": [2046, 2050], "type": "Other"}, {"value": "5.50", "char_index": [2074, 2078], "type": "Other"}, {"value": "6.75", "char_index": [2100, 2104], "type": "Other"}, {"value": "8.46", "char_index": [2125, 2129], "type": "Other"}, {"value": "12.00", "char_index": [2150, 2155], "type": "Other"}, {"value": "1.98", "char_index": [2254, 2258], "type": "Other"}, {"value": "3.90", "char_index": [2274, 2278], "type": "Other"}, {"value": "6.40", "char_index": [2289, 2293], "type": "Other"}, {"value": "8.30", "char_index": [2304, 2308], "type": "Other"}, {"value": "10.50", "char_index": [2319, 2324], "type": "Other"}, {"value": "2.39", "char_index": [2426, 2430], "type": "Other"}, {"value": "4.60", "char_index": [2445, 2449], "type": "Other"}, {"value": "6.77", "char_index": [2460, 2464], "type": "Other"}, {"value": "8.70", "char_index": [2476, 2480], "type": "Other"}, {"value": "10.80", "char_index": [2492, 2497], "type": "Other"}, {"value": "1.73", "char_index": [2607, 2611], "type": "Other"}, {"value": "2.70", "char_index": [2636, 2640], "type": "Other"}, {"value": "3.68", "char_index": [2660, 2664], "type": "Other"}, {"value": "4.20", "char_index": [2685, 2689], "type": "Other"}, {"value": "5.50", "char_index": [2710, 2714], "type": "Other"}]}, "2211.05756v1_table0": {"table_code": "\\begin{table}[ht]\n\\vspace{-0.3cm}\n\\centering\n\\caption{Average WER(\\%) on test \\textit{vid-clean} and \\textit{vid-noisy}.}\n\\begin{tabular}[t]{ l  c  c }\n \\hline\n \\textbf{Model} & \\textbf{\\texttt{vid-}} & \\textbf{\\texttt{vid-}} \\\\ \n & \\textbf{\\texttt{clean}} & \\textbf{\\texttt{noisy}} \\\\ \\hline \\hline\n Monolingual {(MO)}& 19.2 & 20.2 \\\\ \n \\hline\n Multilingual &  &  \\\\  \n  \\quad w/ shared char {(ML-SC)} & 20.6 & 21.4 \\\\\n  \\quad w/ shared char + subw. {(ML-SCW)}& 16.9 & 17.8 \\\\\n  \\quad w/ lang. spec. in\\&out  {(ML-IO)}& 16.2 & 17.4 \\\\\n \\hline\n\\end{tabular}\n\\vspace{-0.3cm}\n\\label{tab:summary}\n\\end{table}", "table_label": "{tab:summary}", "table_numeric_cells": [["19.2", "19.2", 321, 325, 321, 325], ["20.2", "20.2", 328, 332, 328, 332], ["20.6", "20.6", 405, 409, 405, 409], ["21.4", "21.4", 412, 416, 412, 416], ["16.9", "16.9", 463, 467, 463, 467], ["17.8", "17.8", 470, 474, 470, 474], ["16.2", "16.2", 521, 525, 521, 525], ["17.4", "17.4", 528, 532, 528, 532]], "text_chunk_selected": "\\begin{abstract}\nEnd-to-end multilingual ASR has become more appealing because of several reasons such as simplifying the training and deployment process and positive performance transfer from high-resource to low-resource languages. However, scaling up the number of languages, total hours, and number of unique tokens is not a trivial task. This paper explores large-scale multilingual ASR models on 70 languages. We inspect two architectures: (1) Shared embedding and output and (2) Multiple embedding and output model. In the shared model experiments, we show the importance of tokenization strategy across different languages. Later, we use our optimal tokenization strategy to train multiple embedding and output model to further improve our result. Our multilingual ASR achieves 13.9\\%-15.6\\% average WER relative improvement compared to monolingual models. We show that our multilingual ASR generalizes well on an unseen dataset and domain, achieving 9.5\\% and 7.5\\% WER on Multilingual Librispeech (MLS) with zero-shot and finetuning, respectively.\n\\end{abstract}\n\n\\subsection{Model Architecture}\nOur multilingual model is based on an end-to-end Transducer model \\cite{graves2012sequence} that is composed by encoder, predictor, and joiner modules. Let $\\mathbf{x} = [x_1,..,x_T]$ be an input speech features with length $T$ and $\\mathbf{y}=[y_1,...y_U]$ be an output token sequence with length $U$. The encoder model process $\\mathbf{x}$ and produce higher level acoustic representation $\\mathbf{h^{enc}}=[h_1^{enc},..,h_T^{enc}]=enc(\\mathbf{x})$. The prediction network is (usually) an autoregressive decoder that produces hidden states $\\mathbf{h^{pred}} = [h_1^{pred},..,h_U^{pred}]$. Each hidden states $h_{u}^{pred} = pred(y_{1},..,y_{u-1})$ conditioned on previous output tokens $y_{<u}$. The joiner module combines both encoder $h_t^{enc}$ and predictor $h_u^{pred}$ representation to calculate the logits $z_{t,u} = joiner(h_t^{enc}, h_u^{pred})$. Lastly, we apply softmax function to calculate the probability given input and previous output tokens $P(y_{t,u} | \\mathbf{x}_{[1..t]}, y_{<u}) = softmax(z_{t,u})$. Transducer loss is defined as a negative log-likelihood of output sequence given the input features $\\mathcal{L(\\mathbf{x}, \\mathbf{y}; \\theta)}=-\\log P_{\\theta}(\\mathbf{y}|\\mathbf{x})$ where $\\theta = \\{enc, pred, joiner\\}$ parameters.\n\n\\subsection{Multiple Embedding and Output Architecture}\nWe explore another architecture with language-specific input embedding and output linear layer. Figure \\ref{fig:shared_lang_rnnt}b shows the architecture differences compared to the shared language architecture. Let $d_{emb}$ be the embedding output dimension, $d_{hid}$ be the LSTM hidden dimension, and $\\hat{\\mathcal{V}}_{l}=\\mathcal{V}_{l} \\cup \\O$ be the tokens for language $l$ plus a blank token $\\O$. Then we have multiple embedding weights $\\mathbf{E} = [E_1,..E_L]$ where $ E_{l} \\in \\mathbb R^{|\\hat{\\mathcal{V}}_{l}| \\times d_{emb}}$ and multiple output linear weights $\\mathbf{W} = [W_1,..,W_{L}]$ where $ W_{l} \\in \\mathbb R^{d_{hid} \\times |\\hat{\\mathcal{V}}_{l}|}$ and $\\mathbf{b} = [b_1,..,b_{L}]$ where $ b_{l} \\in \\mathbb R^{|\\hat{\\mathcal{V}}_{l}|}$. For each language input embedding and output layer, we use characters if that language contains $>512$ unique characters, otherwise, we use subwords with 512 tokens. One advantage of this language-specific architecture is that we could represent the same token between different languages with different embedding and weight matrices. Thus, we could disambiguate characters and subwords that look the same in the written space but sound different (e.g., `a' is spelled as `\\textipa{eI}' in English and `\\textipa{a}' in Indonesian).\n\n\\section{Experimental Setup}\n\\subsection{Dataset}\nWe conduct the experiments on our in-house datasets. All in-house datasets are de-identified public videos with no personally identifiable information (PII). Figure \\ref{fig:plot_hours} shows the amount of hours training data for each language. Overall, we have around 150,000 hours across 70 languages.\n\nWe run data alignment and segmentation pipelines to remove long silences or low-quality transcription and segment original speech and transcripts into maximum of 10-second chunks.  To increase the amount of training data, we apply speed perturbation \\cite{ko15_interspeech} with speed factors of 0.9x and 1.1x. \nSince there are some languages with low amount of data, we apply batch re-sampling \\cite{babu22_interspeech} by sampling from $p_l \\sim \\left( \\frac{n_l}{N}\\right)^{\\alpha}$ with $\\alpha = 0.5$.\nWe do not apply any extra processing for the transcript and keep all casings and punctuations for our model to learn them in an end-to-end fashion.\n\nWe evaluate our model with test data on two different domains, \\textit{vid-clean} and \\textit{vid-noisy}, for every language. The main difference between these two domains is that the data from \\textit{vid-noisy} are more acoustically challenging compared to \\textit{vid-clean}. The amount of data for each language ranges from 5-50 hours for \\textit{vid-clean} and 12-50 hours for \\textit{vid-noisy}.\n\nOur multilingual models have around 1 billion parameters for all scenarios (with little variation depending on the vocabulary size). To improve training throughput and memory efficiency, we use several tricks following \\cite{zheng22d_interspeech} such as: fully sharded data parallel (FSDP) \\cite{FairScale2021}, activation checkpointing \\cite{chen2016training} and mixed-precision training \\cite{micikevicius2017mixed}. We use Adam optimizer \\cite{kingma2014adam} with peak $lr=4e-4, \\beta_1=0.9, \\beta_2=0.98$. We train our model for up to 700,000 updates with 64 GPUs. We apply $lr$ warm-up for the first 20,000 updates and exponentially decay for the rest of the remaining updates into $0.1$ from the peak $lr$.\n\nWe report our results on in-house datasets \\textit{vid-clean} and \\textit{vid-noisy} in Table \\ref{tab:summary}. The multilingual model with shared character strategy {ML-SC} shows inferior results with relative WER degradation of 5.9\\%-7.3\\% compared to {MO}. By using the shared char+subword strategy, {ML-SCW} shows relative WER improvement of 16.8\\%-18\\% compared to {ML-SC} and 11.9\\%-12\\% compared to {MO}. On top of that, by adding language-specific embedding and output layer, {ML-IO} achieved relative WER improvement of 2.2\\%-4.1\\% compared to {ML-SCW} and 13.9\\%-15.6\\% compared to {MO}. ", "table_source": "\\begin{table}[ht]\n\\vspace{-0.3cm}\n\\centering\n\\caption{Average WER(\\%) on test \\textit{vid-clean} and \\textit{vid-noisy}.}\n\\begin{tabular}[t]{ l  c  c }\n \\hline\n \\textbf{Model} & \\textbf{\\texttt{vid-}} & \\textbf{\\texttt{vid-}} \\\\ \n & \\textbf{\\texttt{clean}} & \\textbf{\\texttt{noisy}} \\\\ \\hline \\hline\n Monolingual {(MO)}& 19.2 & 20.2 \\\\ \n \\hline\n Multilingual &  &  \\\\  \n  \\quad w/ shared char {(ML-SC)} & 20.6 & 21.4 \\\\\n  \\quad w/ shared char + subw. {(ML-SCW)}& 16.9 & 17.8 \\\\\n  \\quad w/ lang. spec. in\\&out  {(ML-IO)}& 16.2 & 17.4 \\\\\n \\hline\n\\end{tabular}\n\\vspace{-0.3cm}\n\\label{tab:summary}\n\\end{table}", "cell_list_gold": [{"value": "19.2", "char_index": [321, 325], "type": "Result", "training data/set": "in-house datasets", "test data/set": "vid-clean", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "experimental settings": {"xx": "yy"}, "model": ["Transducer", "Monolingual", "MO"], "model settings": {"xx": "yy"}}, {"value": "20.2", "char_index": [328, 332], "type": "Result", "training data/set": "in-house datasets", "test data/set": "vid-noisy", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "experimental settings": {"xx": "yy"}, "model": ["Transducer", "Monolingual", "MO"], "model settings": {"xx": "yy"}}, {"value": "20.6", "char_index": [405, 409], "type": "Result", "training data/set": "in-house datasets", "test data/set": "vid-clean", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "experimental settings": {"xx": "yy"}, "model": ["Transducer", "Multilingual w/ shared char", "ML-SC"], "model settings": {"xx": "yy"}}, {"value": "21.4", "char_index": [412, 416], "type": "Result", "training data/set": "in-house datasets", "test data/set": "vid-noisy", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "experimental settings": {"xx": "yy"}, "model": ["Transducer", "Multilingual w/ shared char", "ML-SC"], "model settings": {"xx": "yy"}}, {"value": "16.9", "char_index": [463, 467], "type": "Result", "training data/set": "in-house datasets", "test data/set": "vid-clean", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "experimental settings": {"xx": "yy"}, "model": ["Transducer", "Multilingual w/ shared char + subw.", "ML-SCW"], "model settings": {"xx": "yy"}}, {"value": "17.8", "char_index": [470, 474], "type": "Result", "training data/set": "in-house datasets", "test data/set": "vid-noisy", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "experimental settings": {"xx": "yy"}, "model": ["Transducer", "Multilingual w/ shared char + subw.", "ML-SCW"], "model settings": {"xx": "yy"}}, {"value": "16.2", "char_index": [521, 525], "type": "Result", "training data/set": "in-house datasets", "test data/set": "vid-clean", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "experimental settings": {"xx": "yy"}, "model": ["Transducer", "Multilingual w/ lang. spec. in&out", "ML-IO"], "model settings": {"xx": "yy"}}, {"value": "17.4", "char_index": [528, 532], "type": "Result", "training data/set": "in-house datasets", "test data/set": "vid-noisy", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "experimental settings": {"xx": "yy"}, "model": ["Transducer", "Multilingual w/ lang. spec. in&out", "ML-IO"], "model settings": {"xx": "yy"}}]}, "2211.05756v1_table1": {"table_code": "\\begin{table}[ht]\n\\centering\n\\vspace{-0.3cm}\n\n\\caption{Average WER(\\%) on MLS test sets.}\n\\begin{tabular}[t]{ l  c }\n \n \\hline\n \\textbf{Model} & \\textbf{WER (\\%)} \\\\ \\hline \\hline\n \n \\textbf{Prior works}  \\\\ \n Monolingual CTC \\cite{Pratap2020} & 11.8 \\\\\n Monolingual CTC w/ 5-gram LM \\cite{Pratap2020} & 10.7 \\\\\n XLS-R (ft. 10h) \\cite{babu22_interspeech} & 13.8 \\\\\n RNN-T 1B (ft. all) \\cite{li2021scaling} & 7.9 \\\\\n \\hline\n \\textbf{Zero-shot} \\\\\n {MO} & 13.7 \\\\ \n {ML-SCW} & 9.8  \\\\  \n {ML-IO} & 9.5  \\\\ \n \\hline\n \\textbf{Finetune all}  \\\\  \n {ML-SCW} & 7.7  \\\\  \n {ML-IO} & 7.5  \\\\ \n \\hline\n\\end{tabular}\n\\label{tab:mls}\n\\end{table}", "table_label": "{tab:mls}", "table_numeric_cells": [["11.8", "11.8", 246, 250, 246, 250], ["10.7", "10.7", 304, 308, 304, 308], ["13.8", "13.8", 357, 361, 357, 361], ["7.9", "7.9", 408, 411, 408, 411], ["13.7", "13.7", 454, 458, 454, 458], ["9.8", "9.8", 475, 478, 475, 478], ["9.5", "9.5", 496, 499, 496, 499], ["7.7", "7.7", 554, 557, 554, 557], ["7.5", "7.5", 575, 578, 575, 578]], "text_chunk_selected": "\\begin{abstract}\nEnd-to-end multilingual ASR has become more appealing because of several reasons such as simplifying the training and deployment process and positive performance transfer from high-resource to low-resource languages. However, scaling up the number of languages, total hours, and number of unique tokens is not a trivial task. This paper explores large-scale multilingual ASR models on 70 languages. We inspect two architectures: (1) Shared embedding and output and (2) Multiple embedding and output model. In the shared model experiments, we show the importance of tokenization strategy across different languages. Later, we use our optimal tokenization strategy to train multiple embedding and output model to further improve our result. Our multilingual ASR achieves 13.9\\%-15.6\\% average WER relative improvement compared to monolingual models. We show that our multilingual ASR generalizes well on an unseen dataset and domain, achieving 9.5\\% and 7.5\\% WER on Multilingual Librispeech (MLS) with zero-shot and finetuning, respectively.\n\\end{abstract}\n\nOne key challenge of multilingual ASR in practice is when we scale up the number of languages, our vocabulary size grows larger. Prior works \\cite{li2021scaling} trained a multilingual ASR for 15 languages by simply combining all graphemes together. \\cite{joshi21_interspeech} proposed multilingual with multi-decoder output for 4 languages. \\cite{Pratap2020} built multilingual ASR on top of 50 languages with total 16,000 hours training dataset. To the best of our knowledge, most of the existing works have less amount of languages and smaller unique graphemes size compared to our in-house video dataset.\n\n\\subsection{Model Architecture}\nOur multilingual model is based on an end-to-end Transducer model \\cite{graves2012sequence} that is composed by encoder, predictor, and joiner modules. Let $\\mathbf{x} = [x_1,..,x_T]$ be an input speech features with length $T$ and $\\mathbf{y}=[y_1,...y_U]$ be an output token sequence with length $U$. The encoder model process $\\mathbf{x}$ and produce higher level acoustic representation $\\mathbf{h^{enc}}=[h_1^{enc},..,h_T^{enc}]=enc(\\mathbf{x})$. The prediction network is (usually) an autoregressive decoder that produces hidden states $\\mathbf{h^{pred}} = [h_1^{pred},..,h_U^{pred}]$. Each hidden states $h_{u}^{pred} = pred(y_{1},..,y_{u-1})$ conditioned on previous output tokens $y_{<u}$. The joiner module combines both encoder $h_t^{enc}$ and predictor $h_u^{pred}$ representation to calculate the logits $z_{t,u} = joiner(h_t^{enc}, h_u^{pred})$. Lastly, we apply softmax function to calculate the probability given input and previous output tokens $P(y_{t,u} | \\mathbf{x}_{[1..t]}, y_{<u}) = softmax(z_{t,u})$. Transducer loss is defined as a negative log-likelihood of output sequence given the input features $\\mathcal{L(\\mathbf{x}, \\mathbf{y}; \\theta)}=-\\log P_{\\theta}(\\mathbf{y}|\\mathbf{x})$ where $\\theta = \\{enc, pred, joiner\\}$ parameters.\n\n\\subsection{Multiple Embedding and Output Architecture}\nWe explore another architecture with language-specific input embedding and output linear layer. Figure \\ref{fig:shared_lang_rnnt}b shows the architecture differences compared to the shared language architecture. Let $d_{emb}$ be the embedding output dimension, $d_{hid}$ be the LSTM hidden dimension, and $\\hat{\\mathcal{V}}_{l}=\\mathcal{V}_{l} \\cup \\O$ be the tokens for language $l$ plus a blank token $\\O$. Then we have multiple embedding weights $\\mathbf{E} = [E_1,..E_L]$ where $ E_{l} \\in \\mathbb R^{|\\hat{\\mathcal{V}}_{l}| \\times d_{emb}}$ and multiple output linear weights $\\mathbf{W} = [W_1,..,W_{L}]$ where $ W_{l} \\in \\mathbb R^{d_{hid} \\times |\\hat{\\mathcal{V}}_{l}|}$ and $\\mathbf{b} = [b_1,..,b_{L}]$ where $ b_{l} \\in \\mathbb R^{|\\hat{\\mathcal{V}}_{l}|}$. For each language input embedding and output layer, we use characters if that language contains $>512$ unique characters, otherwise, we use subwords with 512 tokens. One advantage of this language-specific architecture is that we could represent the same token between different languages with different embedding and weight matrices. Thus, we could disambiguate characters and subwords that look the same in the written space but sound different (e.g., `a' is spelled as `\\textipa{eI}' in English and `\\textipa{a}' in Indonesian).\n\nWe run data alignment and segmentation pipelines to remove long silences or low-quality transcription and segment original speech and transcripts into maximum of 10-second chunks.  To increase the amount of training data, we apply speed perturbation \\cite{ko15_interspeech} with speed factors of 0.9x and 1.1x. \nSince there are some languages with low amount of data, we apply batch re-sampling \\cite{babu22_interspeech} by sampling from $p_l \\sim \\left( \\frac{n_l}{N}\\right)^{\\alpha}$ with $\\alpha = 0.5$.\nWe do not apply any extra processing for the transcript and keep all casings and punctuations for our model to learn them in an end-to-end fashion.\n\nOur multilingual models have around 1 billion parameters for all scenarios (with little variation depending on the vocabulary size). To improve training throughput and memory efficiency, we use several tricks following \\cite{zheng22d_interspeech} such as: fully sharded data parallel (FSDP) \\cite{FairScale2021}, activation checkpointing \\cite{chen2016training} and mixed-precision training \\cite{micikevicius2017mixed}. We use Adam optimizer \\cite{kingma2014adam} with peak $lr=4e-4, \\beta_1=0.9, \\beta_2=0.98$. We train our model for up to 700,000 updates with 64 GPUs. We apply $lr$ warm-up for the first 20,000 updates and exponentially decay for the rest of the remaining updates into $0.1$ from the peak $lr$.\n\nWe report our results on in-house datasets \\textit{vid-clean} and \\textit{vid-noisy} in Table \\ref{tab:summary}. The multilingual model with shared character strategy {ML-SC} shows inferior results with relative WER degradation of 5.9\\%-7.3\\% compared to {MO}. By using the shared char+subword strategy, {ML-SCW} shows relative WER improvement of 16.8\\%-18\\% compared to {ML-SC} and 11.9\\%-12\\% compared to {MO}. On top of that, by adding language-specific embedding and output layer, {ML-IO} achieved relative WER improvement of 2.2\\%-4.1\\% compared to {ML-SCW} and 13.9\\%-15.6\\% compared to {MO}. \n\n\\subsection{Results on MLS: Zero-shot and Finetuning}\nHere, we want to observe how our multilingual model generalizes on new unseen domains and datasets. Therefore, we perform a zero-shot task to see how our model performs directly without any additional training steps and adaptation. After that, we also want to measure how good our model is to be adapted into a new dataset by using it as a seed model. For this purpose, we use Multilingual Librispeech (MLS) \\cite{pratap2020mls} dataset that consisted of 8 languages (English, German, Dutch, French, Spanish, Italian, Portuguese, and Polish). Table \\ref{tab:mls} shows MLS average WER from prior works and our experiment. We achieve 9.5\\% WER on zero-shot and 7.5\\% WER after finetuning. To the best of our knowledge, our result is competitive with the state-of-the-art performance on MLS dataset.", "table_source": "\\begin{table}[ht]\n\\centering\n\\vspace{-0.3cm}\n\n\\caption{Average WER(\\%) on MLS test sets.}\n\\begin{tabular}[t]{ l  c }\n \n \\hline\n \\textbf{Model} & \\textbf{WER (\\%)} \\\\ \\hline \\hline\n \n \\textbf{Prior works}  \\\\ \n Monolingual CTC \\cite{Pratap2020} & 11.8 \\\\\n Monolingual CTC w/ 5-gram LM \\cite{Pratap2020} & 10.7 \\\\\n XLS-R (ft. 10h) \\cite{babu22_interspeech} & 13.8 \\\\\n RNN-T 1B (ft. all) \\cite{li2021scaling} & 7.9 \\\\\n \\hline\n \\textbf{Zero-shot} \\\\\n {MO} & 13.7 \\\\ \n {ML-SCW} & 9.8  \\\\  \n {ML-IO} & 9.5  \\\\ \n \\hline\n \\textbf{Finetune all}  \\\\  \n {ML-SCW} & 7.7  \\\\  \n {ML-IO} & 7.5  \\\\ \n \\hline\n\\end{tabular}\n\\label{tab:mls}\n\\end{table}", "cell_list_gold": [{"value": "11.8", "char_index": [246, 250], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "MLS", "test data/set": "MLS", "experimental settings": {"xx": "yy"}, "model": "Monolingual CTC", "model settings": {"xx": "yy"}}, {"value": "10.7", "char_index": [304, 308], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "MLS", "test data/set": "MLS", "experimental settings": {"xx": "yy"}, "model": "Monolingual CTC w/ 5-gram LM", "model settings": {"xx": "yy"}}, {"value": "13.8", "char_index": [357, 361], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "MLS", "test data/set": "MLS", "experimental settings": {"xx": "yy"}, "model": "XLS-R (ft. 10h)", "model settings": {"xx": "yy"}}, {"value": "7.9", "char_index": [408, 411], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "MLS", "test data/set": "MLS", "experimental settings": {"xx": "yy"}, "model": "RNN-T 1B (ft. all)", "model settings": {"xx": "yy"}}, {"value": "13.7", "char_index": [454, 458], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "in-house datasets", "test data/set": "MLS", "experimental settings": {"Zero-shot": "true"}, "model": "MO", "model settings": {"xx": "yy"}}, {"value": "9.8", "char_index": [475, 478], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "in-house datasets", "test data/set": "MLS", "experimental settings": {"Zero-shot": "true"}, "model": "ML-SCW", "model settings": {"xx": "yy"}}, {"value": "9.5", "char_index": [496, 499], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "in-house datasets", "test data/set": "MLS", "experimental settings": {"Zero-shot": "true"}, "model": "ML-IO", "model settings": {"xx": "yy"}}, {"value": "7.7", "char_index": [554, 557], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "MLS", "test data/set": "MLS", "experimental settings": {"Finetune all": "true"}, "model": "ML-SCW", "model settings": {"xx": "yy"}}, {"value": "7.5", "char_index": [575, 578], "type": "Result", "task": ["ASR", "automatic speech recognition"], "metric": ["WER", "word error rate"], "training data/set": "MLS", "test data/set": "MLS", "experimental settings": {"Finetune all": "true"}, "model": "ML-IO", "model settings": {"xx": "yy"}}]}, "2211.05770v1_table0": {"table_code": "\\begin{table*}[!ht]\n    \\centering\n    \\begin{tabular}{l|l|cc|cc}\n        \\toprule\n\n        {\\multirow{2}{*}{\\textbf{Architecture}}} &\n        {\\multirow{2}{*}{\\textbf{Model}}} & \\multicolumn{2}{|c|}{\\textbf{FFHQ FID} $\\downarrow$} & \\multicolumn{2}{|c}{\\textbf{Usage Metrics (256)}}\\\\ \n        & & \\textbf{256} & \\textbf{1024} & \\textbf{Throughput (img/s)} & \\# \\textbf{Params (M)} \\\\ \\hline\n        {\\multirow{6}{*}{Convolutions}}\n        &SWAGAN-Bi~\\cite{gal2021swagan} & 5.22 & 4.06 & - & -\\\\ \n        &StyleGAN2~\\cite{Karras2020AnalyzingAI} & 3.83 & 2.84 & 95.79 & 30.03 \\\\\n        &StyleGAN3-T~\\cite{Karras2021AliasFreeGA} & - & 2.70 & - & - \\\\\n        &Projected GAN~\\cite{Sauer2021NEURIPS} & 3.39 & - & - & -\\\\\n        &INSGAN~\\cite{yang2021insgen} & 3.31 & - & - & -\\\\\n        &StyleGAN-XL~\\cite{Sauer2021ARXIV} & 2.19 & 2.02 & 14.29 & 67.93\\\\\n        \\hline\n        {\\multirow{7}{*}{Transformers}}\n        &GANFormer~\\cite{hudson2021ganformer} & 7.42 & - & - & -\\\\\n        &GANFormer2~\\cite{hudson2021ganformer2} & 7.77 & - & - & -\\\\\n        &StyleSwin~\\cite{zhang2022styleswin} & 2.81 & 5.07 & 30.20 & 48.93 \\\\\n        &HiT-L~\\cite{zhao2021improved} & 2.58 & 6.37 & 20.67 & 97.46 \\\\\n        &StyleNAT (ours) & 2.05 & 4.17 & 32.56 & 48.92\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{FID50k results based on training with entire 70k FFHQ dataset. A random 50k samples are used for each FID evaluation. \n    We separate convolutional based methods from transformer based using a horizontal line.\n    ``Usage Metrics'' are evaluated on the $256\\times256$ resolution images for best comparison to other models. \n    We also note that none of our StyleNAT results (on any dataset) utilize fidelity increasing techniques such as the truncation trick~\\cite{Kynkaanniemi2019}.\n    }\n    \\label{table:ffhq}\n\\end{table*}", "table_label": "{table:ffhq}", "table_numeric_cells": [["256", "\\textbf{256}", 307, 310, 299, 311], ["1024", "\\textbf{1024}", 322, 326, 314, 327], ["5.22", "5.22", 475, 479, 475, 479], ["4.06", "4.06", 482, 486, 482, 486], ["3.83", "3.83", 548, 552, 548, 552], ["2.84", "2.84", 555, 559, 555, 559], ["95.79", "95.79", 562, 567, 562, 567], ["30.03", "30.03", 570, 575, 570, 575], ["2.70", "2.70", 635, 639, 635, 639], ["3.39", "3.39", 700, 704, 700, 704], ["3.31", "3.31", 759, 763, 759, 763], ["2.19", "2.19", 823, 827, 823, 827], ["2.02", "2.02", 830, 834, 830, 834], ["14.29", "14.29", 837, 842, 837, 842], ["67.93", "67.93", 845, 850, 845, 850], ["7.42", "7.42", 956, 960, 956, 960], ["7.77", "7.77", 1025, 1029, 1025, 1029], ["2.81", "2.81", 1091, 1095, 1091, 1095], ["5.07", "5.07", 1098, 1102, 1098, 1102], ["30.20", "30.20", 1105, 1110, 1105, 1110], ["48.93", "48.93", 1113, 1118, 1113, 1118], ["2.58", "2.58", 1163, 1167, 1163, 1167], ["6.37", "6.37", 1170, 1174, 1170, 1174], ["20.67", "20.67", 1177, 1182, 1177, 1182], ["97.46", "97.46", 1185, 1190, 1185, 1190], ["2.05", "2.05", 1221, 1225, 1221, 1225], ["4.17", "4.17", 1228, 1232, 1228, 1232], ["32.56", "32.56", 1235, 1240, 1235, 1240], ["48.92", "48.92", 1243, 1248, 1243, 1248]], "text_chunk_selected": "\\author{\n    Steven Walton\\textsuperscript{1},\n    Ali Hassani\\textsuperscript{1},\n    Xingqian Xu\\textsuperscript{1},\n    Zhangyang Wang\\textsuperscript{2,3},\n    Humphrey Shi\\textsuperscript{1,3} \\\\\n{\\small \\textsuperscript{1}SHI Lab @ U of Oregon \\& UIUC, \\textsuperscript{2}UT Austin, \\textsuperscript{3}Picsart AI Research (PAIR)}\\\\\n{\\small \\textbf{\\url{https://github.com/SHI-Labs/StyleNAT}}}\n}\n\\twocolumn[{\n\\renewcommand\\twocolumn[1][]{#1}\n\\maketitle\n\n\\subsection{Attention-based Models}\nThe Transformer~\\cite{vaswani2017attention} is arguably one of the most prevalent architectures in language processing. The architecture simply consists of linear projections and dot product attention, along with normalization layers and skip connections. Other than large language models directly using this architecture~\\cite{radford2018improving,devlin2019bert}, the work inspired research into models built with dot product self attention~\\cite{parmar2018image,ramachandran2019stand}. Later in 2020, Vision Transformer~\\cite{dosovitskiy2020image} applied a plain transformer encoder to image classification, which was outperformed existing CNNs at large scale classification. \nThis inspired many researchers to study Transformers as direct competitors to CNNs in different settings~\\cite{touvron2020training,touvron2021going,hassani2021escaping} and across different vision tasks~\\cite{li2022exploring,he2022masked}.\n\n\\subsubsection{Swin Transformer}\nLiu et al.~\\cite{liu2021swin} proposed Window Self Attention (WSA) and Shifted Window Self Attention (SWSA), both of which partition feature maps into windows of fixed size, and apply self attention to each window separately. The difference between the regular and shifted variants is that the latter shifts the partition boundaries by shifting pixels, therefore allowing out-of-window interactions and receptive field growth. Because of the fixed window size, self attention's quadratic complexity drops to a linear complexity. \nThrough these mechanisms, they propose a hierarchical transformer model, which is a stack of 4 transformer encoders with self attention replaced with WSA and SWSA (every other layer uses the shifted variant).\nTheir model was studied on a variety of vision tasks, beyond image classification, and became the state of the art on COCO object detection and instance segmentation, as well as on ADE20K semantic segmentation.\nIt also inspired follow up works in using their architecture and WSA/SWSA attention patterns in image restoration~\\cite{liang2021swinir}, masked image modeling~\\cite{xie2022simmim}, video classification~\\cite{liu2022video}, and generation~\\cite{zhang2022styleswin}.\n\n\\subsubsection{Neighborhood Attention Transformer}\nNeighborhood Attention (NA)~\\cite{hassani2022neighborhood} was proposed as a direct restriction of self attention to local windows.\nThe key difference between NA and WSA is that the restriction is pixel-wise, leading to each pixel attending to only its nearest-neighboring pixels.\nThe resulting attention spans would be in theory similar to how convolutions apply weights, with the exception of how cornering pixels are handled.\nCompared to WSA, in addition to introducing locality~\\cite{liu2021swin}, NA also maintains translational equivariance.\nNA also approaches self attention itself as its window size grows, and unlike WSA, would not need pixel shifts as it is a dynamic operation.\nSimilar operations, in which self attention is restricted in a token-wise manner, had been investigated prior to this work~\\cite{ramachandran2019stand}, but were less actively studied due to implementation difficulties~\\cite{ramachandran2019stand,vaswani2021scaling,liu2021swin}.\nTo that end, Neighborhood Attention Extension (NATTEN)~\\cite{hassani2022neighborhood} was created as an extension to PyTorch, with efficient CUDA kernels, which allow NA to run even faster than WSA, while using less memory.\nThe model built with NA, Neighborhood Attention Transformer (NAT)~\\cite{hassani2022neighborhood}, was shown to have superior classification performance compared to Swin, and competitive object detection, instance segmentation, and semantic segmentation performance.\n\n\\subsubsection{Dilated Neighborhood Attention Transformer}\nDilated Neighborhood Attention Transformer (DiNAT)~\\cite{hassani2022dilated} followed NAT~\\cite{hassani2022neighborhood} by extending NA to Dilated NA (DiNA), which allows models to use extended receptive fields\nand capture more global context, all with no additional computational cost.\nDiNA was added to the existing NATTEN package and the underlying CUDA kernels, which allowed the easy utilization of this attention pattern.\nBy simply stacking NA and DiNA layers in the same manner that Swin stacks WSA and SWSA, DiNAT models significantly outperformed Swin Transformer~\\cite{liu2021swin}, as well as NAT~\\cite{hassani2022neighborhood} across multiple vision tasks, especially downstream tasks. \nDiNAT also outperformed Swin's convolutional competitor, ConvNeXt~\\cite{liu2022convnet}, in object detection, instance segmentation, and semantic segmentation.\n\n\\subsubsection{StyleGAN}\nKarras et al. first introduced the idea of using progressive generating using convolutions where the one would begin with a small latent representation, process each layer, and then scale up and repeat the process until the desired image size is achieved~\\cite{Karras2018ProgressiveGO}.\nThis architecture has major advantages in that it reduces the size of the latent representation, and thus the complexity of data that the model needs to learn.\nThis comes with the obvious drawbacks that we make a trade-off of diversity for fidelity and computational simplicity.\nKarras et al. later produced several papers that built on this idea~\\cite{karras2019a,Karras2020AnalyzingAI,Karras2020TrainingGA,Karras2021AliasFreeGA}, improving the progressive network, but also introducing a sub-network, called the ``style network'', that allows one to control the latent representations at each of these levels.\nThis innovation dramatically increased the usability of this type of network architecture because it allows for simpler feature interpolation, which is often difficult in implicit density models.\nBecause of the high fidelity, flexibility, and the ability to perform feature interpolation, this style of network has become the de facto choice for many researchers~\\cite{Abdal2021StyleFlowAE,humayun2022magnet,humayun2022polarity,karnewar2020msg}. \n\\subsubsection{StyleSwin}\nStyleSwin~\\cite{zhang2022styleswin} introduced Swin based transformers into the StyleGAN framework, beating other transformer based models, but still not achieving any state of the art results.\nThis work did show the power of Swin and how localized attention mechanisms are able to out perform many CNN based architectures and did extend the state of the art for style based networks.\nStyleSwin achieved this not only by replacing the CNN layers with Swin layers, but also partitioned the transformer heads into two, so that each half could operate with a different kernel.\nThis was key to improving their generative quality, allowing the framework to increase flexibility and pay attention to distinct parts of the scene at the same time. \nStyleSwin also included a wavelet based discriminator, as was introduced in SWAGAN~\\cite{gal2021swagan} and also used by StyleGAN3~\\cite{Karras2021AliasFreeGA}, which all show improvements for addressing high-frequency content that causes artifacts.\nStyleSwin uses a constant window size of 8 for all latents larger than the initial $4\\times4$, wherein they use a window size of 4.\nWe follow a similar structure except replace the initial layer with MHSA for efficiency. \n\nTo accurately judge the quality and usability of StyleNAT we investigate the performance on FFHQ (\\cref{sec:ffhq}) and LSUN Churches (\\cref{sec:church}). \nWe also perform an ablation study compared to StyleSwin (\\cref{sec:ablation}) to demonstrate the effects of the improvements we have made.\nThese datasets are common evaluation benchmarks that can help us understand how the generator works on more regular data as well as irregular data.\nAll arguments for experiments are saved alongside the checkpoints for verification. \nFor best comparisons we keep hyper-parameters similar to StyleSwin, unless otherwise specified.\nThis means that we use the Two Time-Scale Update Rule (TTUR)~\\cite{Heusel2017GANsTB} training method, have a discriminator learning rate of $2\\times 10^{-4}$, use Balanced Consistency Regularization (bCR)~\\cite{Zhao2021ImprovedCR}, r1 regularization~\\cite{Mescheder2018WhichTM}, and a learning rate decay (specified per dataset).\n\nOur second experiment is with LSUN~\\cite{yu2015lsun} Church.\nThis dataset is uni-modal, as all images are of churches, but is less structural than FFHQ where images have much higher variance from one another.\nThere is a higher variability in designs as some churches are quite simple and others are complex and have other scene information like people in the foreground/background.\nThis task demonstrates the ability to perform on diverse and complicated distributions of images.\nWe found that with just the two partition design that our models would converge to a FID of around 20.\nThis shows that the simple two partition does not incorporate enough information to build the more complex structures of this distribution.\nIntroducing the four head partition we found that we could achieve a significant improvement in performance, and this dataset represented the major motivation to expanding the capabilities of our architecture. \nLooking across generative literature it is clear that there is a pattern that models perform better on more structured data or less structured data.\nWe show our results in~\\cref{tab:church}, but note that not all models from~\\cref{table:ffhq} also tested on Church.\nComputational complexity and throughput will be identical to those of the FFHQ-256 cases as the image sizes are the same.\nThe number of partitions has almost no affect on throughput, slight due to lack of CUDA optimization, and does not affect MACs.", "table_source": "\\begin{table*}[!ht]\n    \\centering\n    \\begin{tabular}{l|l|cc|cc}\n        \\toprule\n\n        {\\multirow{2}{*}{\\textbf{Architecture}}} &\n        {\\multirow{2}{*}{\\textbf{Model}}} & \\multicolumn{2}{|c|}{\\textbf{FFHQ FID} $\\downarrow$} & \\multicolumn{2}{|c}{\\textbf{Usage Metrics (256)}}\\\\ \n        & & \\textbf{256} & \\textbf{1024} & \\textbf{Throughput (img/s)} & \\# \\textbf{Params (M)} \\\\ \\hline\n        {\\multirow{6}{*}{Convolutions}}\n        &SWAGAN-Bi~\\cite{gal2021swagan} & 5.22 & 4.06 & - & -\\\\ \n        &StyleGAN2~\\cite{Karras2020AnalyzingAI} & 3.83 & 2.84 & 95.79 & 30.03 \\\\\n        &StyleGAN3-T~\\cite{Karras2021AliasFreeGA} & - & 2.70 & - & - \\\\\n        &Projected GAN~\\cite{Sauer2021NEURIPS} & 3.39 & - & - & -\\\\\n        &INSGAN~\\cite{yang2021insgen} & 3.31 & - & - & -\\\\\n        &StyleGAN-XL~\\cite{Sauer2021ARXIV} & 2.19 & 2.02 & 14.29 & 67.93\\\\\n        \\hline\n        {\\multirow{7}{*}{Transformers}}\n        &GANFormer~\\cite{hudson2021ganformer} & 7.42 & - & - & -\\\\\n        &GANFormer2~\\cite{hudson2021ganformer2} & 7.77 & - & - & -\\\\\n        &StyleSwin~\\cite{zhang2022styleswin} & 2.81 & 5.07 & 30.20 & 48.93 \\\\\n        &HiT-L~\\cite{zhao2021improved} & 2.58 & 6.37 & 20.67 & 97.46 \\\\\n        &StyleNAT (ours) & 2.05 & 4.17 & 32.56 & 48.92\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{FID50k results based on training with entire 70k FFHQ dataset. A random 50k samples are used for each FID evaluation. \n    We separate convolutional based methods from transformer based using a horizontal line.\n    ``Usage Metrics'' are evaluated on the $256\\times256$ resolution images for best comparison to other models. \n    We also note that none of our StyleNAT results (on any dataset) utilize fidelity increasing techniques such as the truncation trick~\\cite{Kynkaanniemi2019}.\n    }\n    \\label{table:ffhq}\n\\end{table*}", "cell_list_gold": [{"value": "256", "char_index": [307, 310], "type": "Other"}, {"value": "1024", "char_index": [322, 326], "type": "Other"}, {"value": "5.22", "char_index": [475, 479], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "SWAGAN-Bi", "model settings": {"Architecture": "Convolutions"}}, {"value": "4.06", "char_index": [482, 486], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "1024"}, "model": "SWAGAN-Bi", "model settings": {"Architecture": "Convolutions"}}, {"value": "3.83", "char_index": [548, 552], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "StyleGAN2", "model settings": {"Architecture": "Convolutions"}}, {"value": "2.84", "char_index": [555, 559], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "1024"}, "model": "StyleGAN2", "model settings": {"Architecture": "Convolutions"}}, {"value": "95.79", "char_index": [562, 567], "type": "Other"}, {"value": "30.03", "char_index": [570, 575], "type": "Hyper-parameter/Architecture", "model": "StyleGAN2", "parameter/architecture name": ["number of parameters", "# Params (M)"], "dataset": "FFHQ"}, {"value": "2.70", "char_index": [635, 639], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "1024"}, "model": "StyleGAN3-T", "model settings": {"Architecture": "Convolutions"}}, {"value": "3.39", "char_index": [700, 704], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "Projected GAN", "model settings": {"Architecture": "Convolutions"}}, {"value": "3.31", "char_index": [759, 763], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "INSGAN", "model settings": {"Architecture": "Convolutions"}}, {"value": "2.19", "char_index": [823, 827], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "StyleGAN-XL", "model settings": {"Architecture": "Convolutions"}}, {"value": "2.02", "char_index": [830, 834], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "1024"}, "model": "StyleGAN-XL", "model settings": {"Architecture": "Convolutions"}}, {"value": "14.29", "char_index": [837, 842], "type": "Other"}, {"value": "67.93", "char_index": [845, 850], "type": "Hyper-parameter/Architecture", "model": "StyleGAN-XL", "parameter/architecture name": ["number of parameters", "# Params (M)"], "dataset": "FFHQ"}, {"value": "7.42", "char_index": [956, 960], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "GANFormer", "model settings": {"Architecture": "Transformers"}}, {"value": "7.77", "char_index": [1025, 1029], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "GANFormer2", "model settings": {"Architecture": "Transformers"}}, {"value": "2.81", "char_index": [1091, 1095], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "StyleSwin", "model settings": {"Architecture": "Transformers"}}, {"value": "5.07", "char_index": [1098, 1102], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "1024"}, "model": "StyleSwin", "model settings": {"Architecture": "Transformers"}}, {"value": "30.20", "char_index": [1105, 1110], "type": "Other"}, {"value": "48.93", "char_index": [1113, 1118], "type": "Hyper-parameter/Architecture", "model": "StyleSwin", "parameter/architecture name": ["number of parameters", "# Params (M)"], "dataset": "FFHQ"}, {"value": "2.58", "char_index": [1163, 1167], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "HiT-L", "model settings": {"Architecture": "Transformers"}}, {"value": "6.37", "char_index": [1170, 1174], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "1024"}, "model": "HiT-L", "model settings": {"Architecture": "Transformers"}}, {"value": "20.67", "char_index": [1177, 1182], "type": "Other"}, {"value": "97.46", "char_index": [1185, 1190], "type": "Hyper-parameter/Architecture", "model": "HiT-L", "parameter/architecture name": ["number of parameters", "# Params (M)"], "dataset": "FFHQ"}, {"value": "2.05", "char_index": [1221, 1225], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "StyleNAT", "model settings": {"Architecture": "Transformers"}}, {"value": "4.17", "char_index": [1228, 1232], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "1024"}, "model": "StyleNAT", "model settings": {"Architecture": "Transformers"}}, {"value": "32.56", "char_index": [1235, 1240], "type": "Other"}, {"value": "48.92", "char_index": [1243, 1248], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": ["number of parameters", "# Params (M)"], "dataset": "FFHQ"}]}, "2211.05770v1_table1": {"table_code": "\\begin{table}[!ht]\n    \\centering\n    \\begin{tabular}{l|l|c}\n    \\toprule\n    \\textbf{Arch} & \\textbf{Model} & \\textbf{FID $\\downarrow$} \\\\\n    \\hline\n    {\\multirow{2}{*}{Diffusion}}\n    & Diffusion StyleGAN2~\\cite{wang2022diffusiongan} & 3.17 \\\\\n    & Diffusion ProjectedGAN~\\cite{wang2022diffusiongan} & 1.85 \\\\\n    \\hline\n    {\\multirow{3}{*}{Convs}}\n    & SWAGAN-Bi~\\cite{gal2021swagan} & 4.97 \\\\\n    & StyleGAN2~\\cite{Karras2020AnalyzingAI} & 3.86 \\\\\n    & Projected GAN~\\cite{Sauer2021NEURIPS} & 1.59 \\\\\n    \\hline\n    {\\multirow{4}{*}{Transformer}}\n    & TransGAN~\\cite{Jiang2021TransGANTP} & 8.94\\\\\n    & Unleashing~\\cite{bond2021unleashing} & 4.07\\\\\n    & StyleSwin~\\cite{zhang2022styleswin} & 2.95 \\\\\n    & StyleNAT(ours) &  3.40\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{FID50k results for LSUN-Church Outdoor (256) dataset.}\n    \\label{tab:church}\n\\end{table}", "table_label": "{tab:church}", "table_numeric_cells": [["3.17", "3.17", 240, 244, 240, 244], ["1.85", "1.85", 307, 311, 307, 311], ["4.97", "4.97", 394, 398, 394, 398], ["3.86", "3.86", 449, 453, 449, 453], ["1.59", "1.59", 503, 507, 503, 507], ["8.94", "8.94", 601, 605, 601, 605], ["4.07", "4.07", 653, 657, 653, 657], ["2.95", "2.95", 704, 708, 704, 708], ["3.40", "3.40", 736, 740, 736, 740]], "text_chunk_selected": "\\author{\n    Steven Walton\\textsuperscript{1},\n    Ali Hassani\\textsuperscript{1},\n    Xingqian Xu\\textsuperscript{1},\n    Zhangyang Wang\\textsuperscript{2,3},\n    Humphrey Shi\\textsuperscript{1,3} \\\\\n{\\small \\textsuperscript{1}SHI Lab @ U of Oregon \\& UIUC, \\textsuperscript{2}UT Austin, \\textsuperscript{3}Picsart AI Research (PAIR)}\\\\\n{\\small \\textbf{\\url{https://github.com/SHI-Labs/StyleNAT}}}\n}\n\\twocolumn[{\n\\renewcommand\\twocolumn[1][]{#1}\n\\maketitle\n\nAdversarially trained generative image modeling has been long dominated by Convolutional Neural Networks (CNNs). \nMore recently, however, there have been strides in shifting towards new architectures, such as Transformer-based GANs~\\cite{zhang2022styleswin,zhao2021improved} and Diffusion-based GANs~\\cite{wang2022diffusiongan}.\nIn addition to this, other generative modeling methods, such as pure Diffusion~\\cite{Dhariwal2021DiffusionMB,Song2021DenoisingDI} and Variational Autoencoders~\\cite{Child2021VeryDV,hazami2022efficient}, are catching up to the standards set by GANs.\nDiffusion and VAEs have some advantages in that they are able to approximate the densities of the training distribution, and thus can incorporate more features and can have a higher recall.\nBut these methods also have a major disadvantage in that they need to process substantially more information, compared to GANs, and thus can be quite computationally expensive and difficult to train.\n\n\\subsection{Attention-based Models}\nThe Transformer~\\cite{vaswani2017attention} is arguably one of the most prevalent architectures in language processing. The architecture simply consists of linear projections and dot product attention, along with normalization layers and skip connections. Other than large language models directly using this architecture~\\cite{radford2018improving,devlin2019bert}, the work inspired research into models built with dot product self attention~\\cite{parmar2018image,ramachandran2019stand}. Later in 2020, Vision Transformer~\\cite{dosovitskiy2020image} applied a plain transformer encoder to image classification, which was outperformed existing CNNs at large scale classification. \nThis inspired many researchers to study Transformers as direct competitors to CNNs in different settings~\\cite{touvron2020training,touvron2021going,hassani2021escaping} and across different vision tasks~\\cite{li2022exploring,he2022masked}.\n\n\\subsubsection{Neighborhood Attention Transformer}\nNeighborhood Attention (NA)~\\cite{hassani2022neighborhood} was proposed as a direct restriction of self attention to local windows.\nThe key difference between NA and WSA is that the restriction is pixel-wise, leading to each pixel attending to only its nearest-neighboring pixels.\nThe resulting attention spans would be in theory similar to how convolutions apply weights, with the exception of how cornering pixels are handled.\nCompared to WSA, in addition to introducing locality~\\cite{liu2021swin}, NA also maintains translational equivariance.\nNA also approaches self attention itself as its window size grows, and unlike WSA, would not need pixel shifts as it is a dynamic operation.\nSimilar operations, in which self attention is restricted in a token-wise manner, had been investigated prior to this work~\\cite{ramachandran2019stand}, but were less actively studied due to implementation difficulties~\\cite{ramachandran2019stand,vaswani2021scaling,liu2021swin}.\nTo that end, Neighborhood Attention Extension (NATTEN)~\\cite{hassani2022neighborhood} was created as an extension to PyTorch, with efficient CUDA kernels, which allow NA to run even faster than WSA, while using less memory.\nThe model built with NA, Neighborhood Attention Transformer (NAT)~\\cite{hassani2022neighborhood}, was shown to have superior classification performance compared to Swin, and competitive object detection, instance segmentation, and semantic segmentation performance.\n\n\\subsubsection{Dilated Neighborhood Attention Transformer}\nDilated Neighborhood Attention Transformer (DiNAT)~\\cite{hassani2022dilated} followed NAT~\\cite{hassani2022neighborhood} by extending NA to Dilated NA (DiNA), which allows models to use extended receptive fields\nand capture more global context, all with no additional computational cost.\nDiNA was added to the existing NATTEN package and the underlying CUDA kernels, which allowed the easy utilization of this attention pattern.\nBy simply stacking NA and DiNA layers in the same manner that Swin stacks WSA and SWSA, DiNAT models significantly outperformed Swin Transformer~\\cite{liu2021swin}, as well as NAT~\\cite{hassani2022neighborhood} across multiple vision tasks, especially downstream tasks. \nDiNAT also outperformed Swin's convolutional competitor, ConvNeXt~\\cite{liu2022convnet}, in object detection, instance segmentation, and semantic segmentation.\n\n\\subsubsection{StyleGAN}\nKarras et al. first introduced the idea of using progressive generating using convolutions where the one would begin with a small latent representation, process each layer, and then scale up and repeat the process until the desired image size is achieved~\\cite{Karras2018ProgressiveGO}.\nThis architecture has major advantages in that it reduces the size of the latent representation, and thus the complexity of data that the model needs to learn.\nThis comes with the obvious drawbacks that we make a trade-off of diversity for fidelity and computational simplicity.\nKarras et al. later produced several papers that built on this idea~\\cite{karras2019a,Karras2020AnalyzingAI,Karras2020TrainingGA,Karras2021AliasFreeGA}, improving the progressive network, but also introducing a sub-network, called the ``style network'', that allows one to control the latent representations at each of these levels.\nThis innovation dramatically increased the usability of this type of network architecture because it allows for simpler feature interpolation, which is often difficult in implicit density models.\nBecause of the high fidelity, flexibility, and the ability to perform feature interpolation, this style of network has become the de facto choice for many researchers~\\cite{Abdal2021StyleFlowAE,humayun2022magnet,humayun2022polarity,karnewar2020msg}. \n\\subsubsection{StyleSwin}\nStyleSwin~\\cite{zhang2022styleswin} introduced Swin based transformers into the StyleGAN framework, beating other transformer based models, but still not achieving any state of the art results.\nThis work did show the power of Swin and how localized attention mechanisms are able to out perform many CNN based architectures and did extend the state of the art for style based networks.\nStyleSwin achieved this not only by replacing the CNN layers with Swin layers, but also partitioned the transformer heads into two, so that each half could operate with a different kernel.\nThis was key to improving their generative quality, allowing the framework to increase flexibility and pay attention to distinct parts of the scene at the same time. \nStyleSwin also included a wavelet based discriminator, as was introduced in SWAGAN~\\cite{gal2021swagan} and also used by StyleGAN3~\\cite{Karras2021AliasFreeGA}, which all show improvements for addressing high-frequency content that causes artifacts.\nStyleSwin uses a constant window size of 8 for all latents larger than the initial $4\\times4$, wherein they use a window size of 4.\nWe follow a similar structure except replace the initial layer with MHSA for efficiency. \n\nTo accurately judge the quality and usability of StyleNAT we investigate the performance on FFHQ (\\cref{sec:ffhq}) and LSUN Churches (\\cref{sec:church}). \nWe also perform an ablation study compared to StyleSwin (\\cref{sec:ablation}) to demonstrate the effects of the improvements we have made.\nThese datasets are common evaluation benchmarks that can help us understand how the generator works on more regular data as well as irregular data.\nAll arguments for experiments are saved alongside the checkpoints for verification. \nFor best comparisons we keep hyper-parameters similar to StyleSwin, unless otherwise specified.\nThis means that we use the Two Time-Scale Update Rule (TTUR)~\\cite{Heusel2017GANsTB} training method, have a discriminator learning rate of $2\\times 10^{-4}$, use Balanced Consistency Regularization (bCR)~\\cite{Zhao2021ImprovedCR}, r1 regularization~\\cite{Mescheder2018WhichTM}, and a learning rate decay (specified per dataset).\n\nOur second experiment is with LSUN~\\cite{yu2015lsun} Church.\nThis dataset is uni-modal, as all images are of churches, but is less structural than FFHQ where images have much higher variance from one another.\nThere is a higher variability in designs as some churches are quite simple and others are complex and have other scene information like people in the foreground/background.\nThis task demonstrates the ability to perform on diverse and complicated distributions of images.\nWe found that with just the two partition design that our models would converge to a FID of around 20.\nThis shows that the simple two partition does not incorporate enough information to build the more complex structures of this distribution.\nIntroducing the four head partition we found that we could achieve a significant improvement in performance, and this dataset represented the major motivation to expanding the capabilities of our architecture. \nLooking across generative literature it is clear that there is a pattern that models perform better on more structured data or less structured data.\nWe show our results in~\\cref{tab:church}, but note that not all models from~\\cref{table:ffhq} also tested on Church.\nComputational complexity and throughput will be identical to those of the FFHQ-256 cases as the image sizes are the same.\nThe number of partitions has almost no affect on throughput, slight due to lack of CUDA optimization, and does not affect MACs.", "table_source": "\\begin{table}[!ht]\n    \\centering\n    \\begin{tabular}{l|l|c}\n    \\toprule\n    \\textbf{Arch} & \\textbf{Model} & \\textbf{FID $\\downarrow$} \\\\\n    \\hline\n    {\\multirow{2}{*}{Diffusion}}\n    & Diffusion StyleGAN2~\\cite{wang2022diffusiongan} & 3.17 \\\\\n    & Diffusion ProjectedGAN~\\cite{wang2022diffusiongan} & 1.85 \\\\\n    \\hline\n    {\\multirow{3}{*}{Convs}}\n    & SWAGAN-Bi~\\cite{gal2021swagan} & 4.97 \\\\\n    & StyleGAN2~\\cite{Karras2020AnalyzingAI} & 3.86 \\\\\n    & Projected GAN~\\cite{Sauer2021NEURIPS} & 1.59 \\\\\n    \\hline\n    {\\multirow{4}{*}{Transformer}}\n    & TransGAN~\\cite{Jiang2021TransGANTP} & 8.94\\\\\n    & Unleashing~\\cite{bond2021unleashing} & 4.07\\\\\n    & StyleSwin~\\cite{zhang2022styleswin} & 2.95 \\\\\n    & StyleNAT(ours) &  3.40\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{FID50k results for LSUN-Church Outdoor (256) dataset.}\n    \\label{tab:church}\n\\end{table}", "cell_list_gold": [{"value": "3.17", "char_index": [240, 244], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "Diffusion StyleGAN2", "model settings": {"Arch": "Diffusion"}}, {"value": "1.85", "char_index": [307, 311], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "Diffusion ProjectedGAN", "model settings": {"Arch": "Diffusion"}}, {"value": "4.97", "char_index": [394, 398], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "SWAGAN-Bi", "model settings": {"Arch": "Convs"}}, {"value": "3.86", "char_index": [449, 453], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "StyleGAN2", "model settings": {"Arch": "Convs"}}, {"value": "1.59", "char_index": [503, 507], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "Projected GAN", "model settings": {"Arch": "Convs"}}, {"value": "8.94", "char_index": [601, 605], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "TransGAN", "model settings": {"Arch": "Transformer"}}, {"value": "4.07", "char_index": [653, 657], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "Unleashing", "model settings": {"Arch": "Transformer"}}, {"value": "2.95", "char_index": [704, 708], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "StyleSwin", "model settings": {"Arch": "Transformer"}}, {"value": "3.40", "char_index": [736, 740], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church Outdoor", "test data/set": "LSUN Church Outdoor", "experimental settings": {"resolution": "256"}, "model": "StyleNAT", "model settings": {"Arch": "Transformer"}}]}, "2211.05770v1_table2": {"table_code": "\\begin{table}[!ht]\n    \\centering\n    \\begin{tabular}{l|c|r}\n    \\toprule\n    Ablation & FID $\\downarrow$ & diff $\\downarrow$\\\\\n    \\hline\n    StyleSwin & 2.81 & --\\phantom{00} \\\\\n    + NA & 2.74 &  \\gain{-0.07}\\\\\n    + DiNA & 2.24 & \\gain{-0.50}\\\\\n    + Flips & \\textbf{2.05} & \\gain{-0.19}\\\\\n    + Prog Di (4) & 2.55 & \\loss{+0.50}\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Ablation study comparing models on FFHQ-256 dataset. We start with StyleSwin, replace the Swin transformer with a Neighborhood Attention Transformer, then introduce dilation, then introduce the horizontal flip augmentation, and finally show a result with a 4 partition design with progressive dilation.}\n    \\label{tab:ablation}\n\\end{table}", "table_label": "{tab:ablation}", "table_numeric_cells": [["2.81", "2.81", 155, 159, 155, 159], ["2.74", "2.74", 191, 195, 191, 195], ["2.24", "2.24", 227, 231, 227, 231], ["2.05", "\\textbf{2.05}", 271, 275, 263, 276], ["2.55", "2.55", 314, 318, 314, 318]], "text_chunk_selected": "\\subsubsection{StyleGAN}\nKarras et al. first introduced the idea of using progressive generating using convolutions where the one would begin with a small latent representation, process each layer, and then scale up and repeat the process until the desired image size is achieved~\\cite{Karras2018ProgressiveGO}.\nThis architecture has major advantages in that it reduces the size of the latent representation, and thus the complexity of data that the model needs to learn.\nThis comes with the obvious drawbacks that we make a trade-off of diversity for fidelity and computational simplicity.\nKarras et al. later produced several papers that built on this idea~\\cite{karras2019a,Karras2020AnalyzingAI,Karras2020TrainingGA,Karras2021AliasFreeGA}, improving the progressive network, but also introducing a sub-network, called the ``style network'', that allows one to control the latent representations at each of these levels.\nThis innovation dramatically increased the usability of this type of network architecture because it allows for simpler feature interpolation, which is often difficult in implicit density models.\nBecause of the high fidelity, flexibility, and the ability to perform feature interpolation, this style of network has become the de facto choice for many researchers~\\cite{Abdal2021StyleFlowAE,humayun2022magnet,humayun2022polarity,karnewar2020msg}. \n\\subsubsection{StyleSwin}\nStyleSwin~\\cite{zhang2022styleswin} introduced Swin based transformers into the StyleGAN framework, beating other transformer based models, but still not achieving any state of the art results.\nThis work did show the power of Swin and how localized attention mechanisms are able to out perform many CNN based architectures and did extend the state of the art for style based networks.\nStyleSwin achieved this not only by replacing the CNN layers with Swin layers, but also partitioned the transformer heads into two, so that each half could operate with a different kernel.\nThis was key to improving their generative quality, allowing the framework to increase flexibility and pay attention to distinct parts of the scene at the same time. \nStyleSwin also included a wavelet based discriminator, as was introduced in SWAGAN~\\cite{gal2021swagan} and also used by StyleGAN3~\\cite{Karras2021AliasFreeGA}, which all show improvements for addressing high-frequency content that causes artifacts.\nStyleSwin uses a constant window size of 8 for all latents larger than the initial $4\\times4$, wherein they use a window size of 4.\nWe follow a similar structure except replace the initial layer with MHSA for efficiency. \n\nFor FFHQ-256 experiments we trained our networks for 1M iterations with a batch size of 8 (per GPU), for a total of 64 million images seen.\nWe also used a LR decay starting at 775k iterations and achieve our best score at 940k iterations with a FID50k of 2.046.\nFor our best result we also include a random horizontal flip, which will double the number of potential images.\nAs we are compute limited, it is possible that these scores could be improved by better hyper-parameter tuning and likely could be trained for longer.\nFor this task we found that it was unnecessary to go beyond 2 partitions, and that we got significant improvements with just two partitions with sparse dilations.\nWe show both of these results in our ablation study, \\cref{tab:ablation}.\nOur results represent the current state of the art for FFHQ-256 image generation. \n\nFor FFHQ-1024 we use a batch size of 4 (per GPU), 1M iterations, horizontal flips, and we start our LR-Decay at 500k iterations.\nOur best score is reached at 900k iterations with FID50k of 4.174.\nThis model still significantly benefits from just two partitions but due to the substantial increase in computation required for these larger images, we have yet to be able to train other architectures (this is the only one we tried).\nWe do believe that a larger partition scheme would likely increase the results here, specifically with more partitions in the later layers.\nNote that training on a single A100 node takes approximately a month and that writing 50k images to disk (to calculate FID) can take over 5.5hrs alone\\footnote{This time should not be used to calculate throughput, but can be used as a rough estimate. Proper throughput calculations only measure inference as a means to standardize against disk speeds. See our implementation for more details.}.\nStill, our results reflect the state of the art for transformer based GANs.\nIn our FFHQ results we notice that our results frequently capture the long range features that we are seeking.\nThis provides evidence that our hypothesis about including non-local features increases the quality of the results.\nWe show some samples of our FFHQ-256 model in~\\cref{fig:ffhq256}.\n\nOur second experiment is with LSUN~\\cite{yu2015lsun} Church.\nThis dataset is uni-modal, as all images are of churches, but is less structural than FFHQ where images have much higher variance from one another.\nThere is a higher variability in designs as some churches are quite simple and others are complex and have other scene information like people in the foreground/background.\nThis task demonstrates the ability to perform on diverse and complicated distributions of images.\nWe found that with just the two partition design that our models would converge to a FID of around 20.\nThis shows that the simple two partition does not incorporate enough information to build the more complex structures of this distribution.\nIntroducing the four head partition we found that we could achieve a significant improvement in performance, and this dataset represented the major motivation to expanding the capabilities of our architecture. \nLooking across generative literature it is clear that there is a pattern that models perform better on more structured data or less structured data.\nWe show our results in~\\cref{tab:church}, but note that not all models from~\\cref{table:ffhq} also tested on Church.\nComputational complexity and throughput will be identical to those of the FFHQ-256 cases as the image sizes are the same.\nThe number of partitions has almost no affect on throughput, slight due to lack of CUDA optimization, and does not affect MACs.\n\nStarting with StyleSwin, introduce Neighborhood Attention into the model, simply by replacing both WSA and SWSA modules in the generator with Neighborhood Attention (+NA). \nNext, we introduce Dilated NA (DiNA), replacing only one split of the attention heads with the dilated variant. In other words, we replace WSA with NA and SWSA with DiNA (+DiNA).\nThird, we introduce the random horizontal flip to demonstrate the effect of augmentation (+Flips).\nThis is not something StyleSwin tried on their FFHQ training but did with other datasets.\nFinally, we introduce our Hydra-NA into the model, with four partitions; the first partition with dilation 1 (NA) and the last partition with maximum dilation according to feature map size, and the partitions in between adjusted accordingly (+Prog Di).\nAll results are tested on FFHQ-256 and shown in~\\cref{tab:ablation}.\n\nIn addition to the above ablation we include one for our search space on LSUN Church in an effort to demonstrate the effectiveness of the number of heads and partitions.\nBecause this data is more complex, a simple two partition design was not enough.\nIn an indirect way, the number of partitions required to achieve good results provides insights into the complexity of a dataset.\nThe number of partitions do not change the computational or memory complexity of the model but there is a slight increase in computation as we handle the complexity.\nAll runs use a progressive style of dilation as described above.\nBelow we report the best scores achieved by the number of partitions.\nThese models were trained until they converged or diverged and in all cases we present the best evaluation.\nOur best runs with 2 and 4 partitions were achieved with r1 set to 20 and were achieved with fewer than 200k iterations.\nThe difference between 2 and 4 partitions represents the largest difference in FID scores between models, demonstrating that for more complex datasets we need more information.\nFor our 6 partition design we made an additional modification to the generative network.\nThe minimum number of heads in the previous setups was 4, which only applied to sizes $>$128, and we changed this minimum to 8.\nThis change allowed for our usage of 6 partitions but also represents a slight increase in computation and memory, as complexity is influenced by the number of heads.\nSee~\\cref{tab:church_heads} for ablation on the number of partitions. \nAn uneven partition results in the tail heads evenly absorbing the remainder.\n\nWe also investigated replacement of StyleSwin where individual layers were replaced with pure NA layers, but did not run these to convergence.\nEach swap resulted in a minor improvement in the initial loss curves but not by significant margins.\nA version where we replaced all version of swin layers can be seen in our ablation study (\\cref{tab:ablation}, +NAT) and we note that NAT alone only provides minor improvements from Swin, but that dilation (DiNA) offers significant improvements. \nWe remind the reader here that using two NA partitions with the same kernel and dilation -- as was done here -- is equivalent to using a single partition. \n\nThis flexibility is important for adapting to different datasets with different complexities.\nWe demonstrate this with our two ablations.\nIn our ablation on FFHQ, \\cref{tab:ablation}, we do not see continual improvements from added partitions (+ Prog Di (4)).\nBut in our LSUN Church ablations, \\cref{tab:church_heads}, we see that this partitioning is essential.\nWe also show that the gain in fidelity is not due purely to the minimum number of heads as there is a much larger gain when moving to 8 partitions rather than 6 partitions, despite that both models have the same minimums.\nWe will continue expanding upon these studies but with increased flexibility comes a larger solution space.", "table_source": "\\begin{table}[!ht]\n    \\centering\n    \\begin{tabular}{l|c|r}\n    \\toprule\n    Ablation & FID $\\downarrow$ & diff $\\downarrow$\\\\\n    \\hline\n    StyleSwin & 2.81 & --\\phantom{00} \\\\\n    + NA & 2.74 &  \\gain{-0.07}\\\\\n    + DiNA & 2.24 & \\gain{-0.50}\\\\\n    + Flips & \\textbf{2.05} & \\gain{-0.19}\\\\\n    + Prog Di (4) & 2.55 & \\loss{+0.50}\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Ablation study comparing models on FFHQ-256 dataset. We start with StyleSwin, replace the Swin transformer with a Neighborhood Attention Transformer, then introduce dilation, then introduce the horizontal flip augmentation, and finally show a result with a 4 partition design with progressive dilation.}\n    \\label{tab:ablation}\n\\end{table}", "cell_list_gold": [{"value": "2.81", "char_index": [155, 159], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": "StyleSwin", "model settings": {"xx": "yy"}}, {"value": "2.74", "char_index": [191, 195], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": ["StyleSwin", "StyleNAT"], "model settings": {"NA": "true"}}, {"value": "-0.07", "char_index": [205, 210], "type": "Other"}, {"value": "2.24", "char_index": [227, 231], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": ["StyleSwin", "StyleNAT"], "model settings": {"NA": "true", "DiNA": "true"}}, {"value": "-0.50", "char_index": [240, 245], "type": "Other"}, {"value": "2.05", "char_index": [271, 275], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": ["StyleSwin", "StyleNAT"], "model settings": {"NA": "true", "DiNA": "true", "Flips": "true"}}, {"value": "-0.19", "char_index": [285, 290], "type": "Other"}, {"value": "2.55", "char_index": [314, 318], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "FFHQ", "test data/set": "FFHQ", "experimental settings": {"resolution": "256"}, "model": ["StyleSwin", "StyleNAT"], "model settings": {"NA": "true", "DiNA": "true", "Flips": "true", "Prog Di": "4"}}, {"value": "+0.50", "char_index": [327, 332], "type": "Other"}]}, "2211.05770v1_table3": {"table_code": "\\begin{table}[!ht]\n    \\centering\n    \\begin{tabular}{l|c|c|r}\n    \\toprule\n    \\textbf{Partitions} & \\textbf{Min Heads} & \\textbf{FID} $\\downarrow$ & \\textbf{Diff} $\\downarrow$\\\\\n    \\hline\n    2 & 4 & 23.33 & --\\phantom{00} \\\\\n    4 & 4 & 6.08 &  \\gain{-17.25}\\\\\n    6 & 8 & 5.50 & \\gain{-0.58}\\\\\n    8 & 8 & 3.40 & \\gain{-2.10} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Comparison for number of head partitions when learning LSUN Church. Min heads represents the minimum number of heads in our transformer.}\n    \\label{tab:church_heads}\n\\end{table}", "table_label": "{tab:church_heads}", "table_numeric_cells": [["2", "2", 195, 196, 195, 196], ["4", "4", 199, 200, 199, 200], ["23.33", "23.33", 203, 208, 203, 208], ["4", "4", 233, 234, 233, 234], ["4", "4", 237, 238, 237, 238], ["6.08", "6.08", 241, 245, 241, 245], ["6", "6", 269, 270, 269, 270], ["8", "8", 273, 274, 273, 274], ["5.50", "5.50", 277, 281, 277, 281], ["8", "8", 303, 304, 303, 304], ["8", "8", 307, 308, 307, 308], ["3.40", "3.40", 311, 315, 311, 315]], "text_chunk_selected": "\\begin{abstract}\nImage generation has been a long sought-after but challenging task, and performing the generation task in an efficient manner is similarly difficult.\nOften researchers attempt to create a ``one size fits all\" generator, where there are few differences in the parameter space for drastically different datasets.\nHerein, we present a new transformer-based framework, dubbed StyleNAT, targeting high-quality image generation with superior efficiency and flexibility. \nAt the core of our model, is a carefully designed framework that partitions attention heads to capture local and global information, which is achieved through using Neighborhood Attention (NA).\nWith different heads able to pay attention to varying receptive fields, the model is able to better combine this information, and adapt, in a highly flexible manner, to the data at hand.\nStyleNAT attains a new SOTA  FID score on FFHQ-256 with 2.046, beating prior arts with convolutional models such as StyleGAN-XL and transformers such as HIT and StyleSwin, and a new transformer SOTA on FFHQ-1024 with an FID score of 4.174.\nThese results show a 6.4\\% improvement on FFHQ-256 scores when compared to StyleGAN-XL with a 28\\% reduction in the number of parameters and 56\\% improvement in sampling throughput. \nCode and models will be open-sourced at \\href{https://github.com/SHI-Labs/StyleNAT}{https://github.com/SHI-Labs/StyleNAT}.\n\\end{abstract}\n\n\\subsubsection{StyleGAN}\nKarras et al. first introduced the idea of using progressive generating using convolutions where the one would begin with a small latent representation, process each layer, and then scale up and repeat the process until the desired image size is achieved~\\cite{Karras2018ProgressiveGO}.\nThis architecture has major advantages in that it reduces the size of the latent representation, and thus the complexity of data that the model needs to learn.\nThis comes with the obvious drawbacks that we make a trade-off of diversity for fidelity and computational simplicity.\nKarras et al. later produced several papers that built on this idea~\\cite{karras2019a,Karras2020AnalyzingAI,Karras2020TrainingGA,Karras2021AliasFreeGA}, improving the progressive network, but also introducing a sub-network, called the ``style network'', that allows one to control the latent representations at each of these levels.\nThis innovation dramatically increased the usability of this type of network architecture because it allows for simpler feature interpolation, which is often difficult in implicit density models.\nBecause of the high fidelity, flexibility, and the ability to perform feature interpolation, this style of network has become the de facto choice for many researchers~\\cite{Abdal2021StyleFlowAE,humayun2022magnet,humayun2022polarity,karnewar2020msg}. \n\\subsubsection{StyleSwin}\nStyleSwin~\\cite{zhang2022styleswin} introduced Swin based transformers into the StyleGAN framework, beating other transformer based models, but still not achieving any state of the art results.\nThis work did show the power of Swin and how localized attention mechanisms are able to out perform many CNN based architectures and did extend the state of the art for style based networks.\nStyleSwin achieved this not only by replacing the CNN layers with Swin layers, but also partitioned the transformer heads into two, so that each half could operate with a different kernel.\nThis was key to improving their generative quality, allowing the framework to increase flexibility and pay attention to distinct parts of the scene at the same time. \nStyleSwin also included a wavelet based discriminator, as was introduced in SWAGAN~\\cite{gal2021swagan} and also used by StyleGAN3~\\cite{Karras2021AliasFreeGA}, which all show improvements for addressing high-frequency content that causes artifacts.\nStyleSwin uses a constant window size of 8 for all latents larger than the initial $4\\times4$, wherein they use a window size of 4.\nWe follow a similar structure except replace the initial layer with MHSA for efficiency. \n\nThe \\textbf{key design philosophy} of StyleNAT is then conceptually simple yet novel to our best knowledge: \\textit{partitioning our attention heads into different groups which can utilize different dilated forms of localized attention}. \nMultiple-headed attention extends the power of the standard attention mechanism by allowing each head to operate on a different set of learnable features.\nTo extend the capabilities of these heads we create a mechanism in which they can have different vantage points to the latent structures.\nIn our framework we allow heads to utilize different NA kernel sizes and different DiNA dilations.\nThrough this, some heads can focus on features within a small and local receptive field, other heads are able to pay attention to medium-sized receptive fields (with low sparsity), while other heads are able to pay attention to large receptive fields (with proportional sparsity).\nWe depict this structure on the right side of~\\cref{fig:architecture}.\nWhile the figure shows three partitions, any number may be used as long as it does not exceed the number of heads.\n\nTo accurately judge the quality and usability of StyleNAT we investigate the performance on FFHQ (\\cref{sec:ffhq}) and LSUN Churches (\\cref{sec:church}). \nWe also perform an ablation study compared to StyleSwin (\\cref{sec:ablation}) to demonstrate the effects of the improvements we have made.\nThese datasets are common evaluation benchmarks that can help us understand how the generator works on more regular data as well as irregular data.\nAll arguments for experiments are saved alongside the checkpoints for verification. \nFor best comparisons we keep hyper-parameters similar to StyleSwin, unless otherwise specified.\nThis means that we use the Two Time-Scale Update Rule (TTUR)~\\cite{Heusel2017GANsTB} training method, have a discriminator learning rate of $2\\times 10^{-4}$, use Balanced Consistency Regularization (bCR)~\\cite{Zhao2021ImprovedCR}, r1 regularization~\\cite{Mescheder2018WhichTM}, and a learning rate decay (specified per dataset).\n\nFor FFHQ-1024 we use a batch size of 4 (per GPU), 1M iterations, horizontal flips, and we start our LR-Decay at 500k iterations.\nOur best score is reached at 900k iterations with FID50k of 4.174.\nThis model still significantly benefits from just two partitions but due to the substantial increase in computation required for these larger images, we have yet to be able to train other architectures (this is the only one we tried).\nWe do believe that a larger partition scheme would likely increase the results here, specifically with more partitions in the later layers.\nNote that training on a single A100 node takes approximately a month and that writing 50k images to disk (to calculate FID) can take over 5.5hrs alone\\footnote{This time should not be used to calculate throughput, but can be used as a rough estimate. Proper throughput calculations only measure inference as a means to standardize against disk speeds. See our implementation for more details.}.\nStill, our results reflect the state of the art for transformer based GANs.\nIn our FFHQ results we notice that our results frequently capture the long range features that we are seeking.\nThis provides evidence that our hypothesis about including non-local features increases the quality of the results.\nWe show some samples of our FFHQ-256 model in~\\cref{fig:ffhq256}.\n\nOur second experiment is with LSUN~\\cite{yu2015lsun} Church.\nThis dataset is uni-modal, as all images are of churches, but is less structural than FFHQ where images have much higher variance from one another.\nThere is a higher variability in designs as some churches are quite simple and others are complex and have other scene information like people in the foreground/background.\nThis task demonstrates the ability to perform on diverse and complicated distributions of images.\nWe found that with just the two partition design that our models would converge to a FID of around 20.\nThis shows that the simple two partition does not incorporate enough information to build the more complex structures of this distribution.\nIntroducing the four head partition we found that we could achieve a significant improvement in performance, and this dataset represented the major motivation to expanding the capabilities of our architecture. \nLooking across generative literature it is clear that there is a pattern that models perform better on more structured data or less structured data.\nWe show our results in~\\cref{tab:church}, but note that not all models from~\\cref{table:ffhq} also tested on Church.\nComputational complexity and throughput will be identical to those of the FFHQ-256 cases as the image sizes are the same.\nThe number of partitions has almost no affect on throughput, slight due to lack of CUDA optimization, and does not affect MACs.\n\nIn addition to the above ablation we include one for our search space on LSUN Church in an effort to demonstrate the effectiveness of the number of heads and partitions.\nBecause this data is more complex, a simple two partition design was not enough.\nIn an indirect way, the number of partitions required to achieve good results provides insights into the complexity of a dataset.\nThe number of partitions do not change the computational or memory complexity of the model but there is a slight increase in computation as we handle the complexity.\nAll runs use a progressive style of dilation as described above.\nBelow we report the best scores achieved by the number of partitions.\nThese models were trained until they converged or diverged and in all cases we present the best evaluation.\nOur best runs with 2 and 4 partitions were achieved with r1 set to 20 and were achieved with fewer than 200k iterations.\nThe difference between 2 and 4 partitions represents the largest difference in FID scores between models, demonstrating that for more complex datasets we need more information.\nFor our 6 partition design we made an additional modification to the generative network.\nThe minimum number of heads in the previous setups was 4, which only applied to sizes $>$128, and we changed this minimum to 8.\nThis change allowed for our usage of 6 partitions but also represents a slight increase in computation and memory, as complexity is influenced by the number of heads.\nSee~\\cref{tab:church_heads} for ablation on the number of partitions. \nAn uneven partition results in the tail heads evenly absorbing the remainder.\n\nThis flexibility is important for adapting to different datasets with different complexities.\nWe demonstrate this with our two ablations.\nIn our ablation on FFHQ, \\cref{tab:ablation}, we do not see continual improvements from added partitions (+ Prog Di (4)).\nBut in our LSUN Church ablations, \\cref{tab:church_heads}, we see that this partitioning is essential.\nWe also show that the gain in fidelity is not due purely to the minimum number of heads as there is a much larger gain when moving to 8 partitions rather than 6 partitions, despite that both models have the same minimums.\nWe will continue expanding upon these studies but with increased flexibility comes a larger solution space.", "table_source": "\\begin{table}[!ht]\n    \\centering\n    \\begin{tabular}{l|c|c|r}\n    \\toprule\n    \\textbf{Partitions} & \\textbf{Min Heads} & \\textbf{FID} $\\downarrow$ & \\textbf{Diff} $\\downarrow$\\\\\n    \\hline\n    2 & 4 & 23.33 & --\\phantom{00} \\\\\n    4 & 4 & 6.08 &  \\gain{-17.25}\\\\\n    6 & 8 & 5.50 & \\gain{-0.58}\\\\\n    8 & 8 & 3.40 & \\gain{-2.10} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Comparison for number of head partitions when learning LSUN Church. Min heads represents the minimum number of heads in our transformer.}\n    \\label{tab:church_heads}\n\\end{table}", "cell_list_gold": [{"value": "2", "char_index": [195, 196], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Partitions", "dataset": "LSUN Church"}, {"value": "4", "char_index": [199, 200], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Min Heads", "dataset": "LSUN Church"}, {"value": "23.33", "char_index": [203, 208], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church", "test data/set": "LSUN Church", "experimental settings": {"xx": "yy"}, "model": "StyleNAT", "model settings": {"Partitions": "2", "Min Heads": "4"}}, {"value": "4", "char_index": [233, 234], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Partitions", "dataset": "LSUN Church"}, {"value": "4", "char_index": [237, 238], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Min Heads", "dataset": "LSUN Church"}, {"value": "6.08", "char_index": [241, 245], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church", "test data/set": "LSUN Church", "experimental settings": {"xx": "yy"}, "model": "StyleNAT", "model settings": {"Partitions": "4", "Min Heads": "4"}}, {"value": "-17.25", "char_index": [255, 261], "type": "Other"}, {"value": "6", "char_index": [269, 270], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Partitions", "dataset": "LSUN Church"}, {"value": "8", "char_index": [273, 274], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Min Heads", "dataset": "LSUN Church"}, {"value": "5.50", "char_index": [277, 281], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church", "test data/set": "LSUN Church", "experimental settings": {"xx": "yy"}, "model": "StyleNAT", "model settings": {"Partitions": "6", "Min Heads": "8"}}, {"value": "-0.58", "char_index": [290, 295], "type": "Other"}, {"value": "8", "char_index": [303, 304], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Partitions", "dataset": "LSUN Church"}, {"value": "8", "char_index": [307, 308], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Min Heads", "dataset": "LSUN Church"}, {"value": "3.40", "char_index": [311, 315], "type": "Result", "task": "Image Generation", "metric": "FID", "training data/set": "LSUN Church", "test data/set": "LSUN Church", "experimental settings": {"xx": "yy"}, "model": "StyleNAT", "model settings": {"Partitions": "8", "Min Heads": "8"}}, {"value": "-2.10", "char_index": [324, 329], "type": "Other"}]}, "2211.05770v1_table4": {"table_code": "\\begin{table}[ht]\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n    Level & Kernel Size & Dilation & Dilated Size \\\\\n    \\hline\n    $4$ & -  & -  & - \\\\\n    $8$ & 7 & 1 & 7 \\\\\n    $16$ & 7 & 2 & 14\\\\\n    $32$ & 7 & 4 & 28 \\\\\n    $64$ & 7 & 8 & 56\\\\\n    $128$ & 7 & 16 & 112 \\\\\n    $256$ & 7 & 32 & 224 \\\\\n    $512$ & 7 & 64 & 448\\\\\n    $1024$ & 7 & 128 & 896\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{StyleNAT 2-Split Model Architecture. First level uses Multi-headed Self Attention and not DiNA. This model is used for all FFHQ results.}\n    \\label{tab:2split_model_arch}\n\\end{table}", "table_label": "{tab:2split_model_arch}", "table_numeric_cells": [["4", "$4$", 144, 145, 143, 146], ["8", "$8$", 169, 170, 168, 171], ["7", "7", 174, 175, 174, 175], ["1", "1", 178, 179, 178, 179], ["7", "7", 182, 183, 182, 183], ["16", "$16$", 192, 194, 191, 195], ["7", "7", 198, 199, 198, 199], ["2", "2", 202, 203, 202, 203], ["14", "14", 206, 208, 206, 208], ["32", "$32$", 216, 218, 215, 219], ["7", "7", 222, 223, 222, 223], ["4", "4", 226, 227, 226, 227], ["28", "28", 230, 232, 230, 232], ["64", "$64$", 241, 243, 240, 244], ["7", "7", 247, 248, 247, 248], ["8", "8", 251, 252, 251, 252], ["56", "56", 255, 257, 255, 257], ["128", "$128$", 265, 268, 264, 269], ["7", "7", 272, 273, 272, 273], ["16", "16", 276, 278, 276, 278], ["112", "112", 281, 284, 281, 284], ["256", "$256$", 293, 296, 292, 297], ["7", "7", 300, 301, 300, 301], ["32", "32", 304, 306, 304, 306], ["224", "224", 309, 312, 309, 312], ["512", "$512$", 321, 324, 320, 325], ["7", "7", 328, 329, 328, 329], ["64", "64", 332, 334, 332, 334], ["448", "448", 337, 340, 337, 340], ["1024", "$1024$", 348, 352, 347, 353], ["7", "7", 356, 357, 356, 357], ["128", "128", 360, 363, 360, 363], ["896", "896", 366, 369, 366, 369]], "text_chunk_selected": "\\author{\n    Steven Walton\\textsuperscript{1},\n    Ali Hassani\\textsuperscript{1},\n    Xingqian Xu\\textsuperscript{1},\n    Zhangyang Wang\\textsuperscript{2,3},\n    Humphrey Shi\\textsuperscript{1,3} \\\\\n{\\small \\textsuperscript{1}SHI Lab @ U of Oregon \\& UIUC, \\textsuperscript{2}UT Austin, \\textsuperscript{3}Picsart AI Research (PAIR)}\\\\\n{\\small \\textbf{\\url{https://github.com/SHI-Labs/StyleNAT}}}\n}\n\\twocolumn[{\n\\renewcommand\\twocolumn[1][]{#1}\n\\maketitle\n\n\\begin{abstract}\nImage generation has been a long sought-after but challenging task, and performing the generation task in an efficient manner is similarly difficult.\nOften researchers attempt to create a ``one size fits all\" generator, where there are few differences in the parameter space for drastically different datasets.\nHerein, we present a new transformer-based framework, dubbed StyleNAT, targeting high-quality image generation with superior efficiency and flexibility. \nAt the core of our model, is a carefully designed framework that partitions attention heads to capture local and global information, which is achieved through using Neighborhood Attention (NA).\nWith different heads able to pay attention to varying receptive fields, the model is able to better combine this information, and adapt, in a highly flexible manner, to the data at hand.\nStyleNAT attains a new SOTA  FID score on FFHQ-256 with 2.046, beating prior arts with convolutional models such as StyleGAN-XL and transformers such as HIT and StyleSwin, and a new transformer SOTA on FFHQ-1024 with an FID score of 4.174.\nThese results show a 6.4\\% improvement on FFHQ-256 scores when compared to StyleGAN-XL with a 28\\% reduction in the number of parameters and 56\\% improvement in sampling throughput. \nCode and models will be open-sourced at \\href{https://github.com/SHI-Labs/StyleNAT}{https://github.com/SHI-Labs/StyleNAT}.\n\\end{abstract}\n\n\\begin{itemize}\n    \\item We introduce Hydra-NA, which extends NA/DiNA and provides a flexible design to combine local and long-range receptive fields via different attention heads.\n    \\item We propose StyleNAT, an efficient and flexible image generation framework that is not only memory and compute efficient but also adaptive to different datasets and tasks via neighborhood attention and its dilated variants with our Hydra-NA design.\n     \\item We achieve new state of the arts on FFHQ-256 (among all GANs) and FFHQ-1024 (among all transformer-based GANs) with our StyleNAT.\n\\end{itemize}\n\n\\subsubsection{Dilated Neighborhood Attention Transformer}\nDilated Neighborhood Attention Transformer (DiNAT)~\\cite{hassani2022dilated} followed NAT~\\cite{hassani2022neighborhood} by extending NA to Dilated NA (DiNA), which allows models to use extended receptive fields\nand capture more global context, all with no additional computational cost.\nDiNA was added to the existing NATTEN package and the underlying CUDA kernels, which allowed the easy utilization of this attention pattern.\nBy simply stacking NA and DiNA layers in the same manner that Swin stacks WSA and SWSA, DiNAT models significantly outperformed Swin Transformer~\\cite{liu2021swin}, as well as NAT~\\cite{hassani2022neighborhood} across multiple vision tasks, especially downstream tasks. \nDiNAT also outperformed Swin's convolutional competitor, ConvNeXt~\\cite{liu2022convnet}, in object detection, instance segmentation, and semantic segmentation.\n\n\\subsubsection{StyleGAN}\nKarras et al. first introduced the idea of using progressive generating using convolutions where the one would begin with a small latent representation, process each layer, and then scale up and repeat the process until the desired image size is achieved~\\cite{Karras2018ProgressiveGO}.\nThis architecture has major advantages in that it reduces the size of the latent representation, and thus the complexity of data that the model needs to learn.\nThis comes with the obvious drawbacks that we make a trade-off of diversity for fidelity and computational simplicity.\nKarras et al. later produced several papers that built on this idea~\\cite{karras2019a,Karras2020AnalyzingAI,Karras2020TrainingGA,Karras2021AliasFreeGA}, improving the progressive network, but also introducing a sub-network, called the ``style network'', that allows one to control the latent representations at each of these levels.\nThis innovation dramatically increased the usability of this type of network architecture because it allows for simpler feature interpolation, which is often difficult in implicit density models.\nBecause of the high fidelity, flexibility, and the ability to perform feature interpolation, this style of network has become the de facto choice for many researchers~\\cite{Abdal2021StyleFlowAE,humayun2022magnet,humayun2022polarity,karnewar2020msg}. \n\\subsubsection{StyleSwin}\nStyleSwin~\\cite{zhang2022styleswin} introduced Swin based transformers into the StyleGAN framework, beating other transformer based models, but still not achieving any state of the art results.\nThis work did show the power of Swin and how localized attention mechanisms are able to out perform many CNN based architectures and did extend the state of the art for style based networks.\nStyleSwin achieved this not only by replacing the CNN layers with Swin layers, but also partitioned the transformer heads into two, so that each half could operate with a different kernel.\nThis was key to improving their generative quality, allowing the framework to increase flexibility and pay attention to distinct parts of the scene at the same time. \nStyleSwin also included a wavelet based discriminator, as was introduced in SWAGAN~\\cite{gal2021swagan} and also used by StyleGAN3~\\cite{Karras2021AliasFreeGA}, which all show improvements for addressing high-frequency content that causes artifacts.\nStyleSwin uses a constant window size of 8 for all latents larger than the initial $4\\times4$, wherein they use a window size of 4.\nWe follow a similar structure except replace the initial layer with MHSA for efficiency. \n\nTo accurately judge the quality and usability of StyleNAT we investigate the performance on FFHQ (\\cref{sec:ffhq}) and LSUN Churches (\\cref{sec:church}). \nWe also perform an ablation study compared to StyleSwin (\\cref{sec:ablation}) to demonstrate the effects of the improvements we have made.\nThese datasets are common evaluation benchmarks that can help us understand how the generator works on more regular data as well as irregular data.\nAll arguments for experiments are saved alongside the checkpoints for verification. \nFor best comparisons we keep hyper-parameters similar to StyleSwin, unless otherwise specified.\nThis means that we use the Two Time-Scale Update Rule (TTUR)~\\cite{Heusel2017GANsTB} training method, have a discriminator learning rate of $2\\times 10^{-4}$, use Balanced Consistency Regularization (bCR)~\\cite{Zhao2021ImprovedCR}, r1 regularization~\\cite{Mescheder2018WhichTM}, and a learning rate decay (specified per dataset).\n\nIn addition to the above ablation we include one for our search space on LSUN Church in an effort to demonstrate the effectiveness of the number of heads and partitions.\nBecause this data is more complex, a simple two partition design was not enough.\nIn an indirect way, the number of partitions required to achieve good results provides insights into the complexity of a dataset.\nThe number of partitions do not change the computational or memory complexity of the model but there is a slight increase in computation as we handle the complexity.\nAll runs use a progressive style of dilation as described above.\nBelow we report the best scores achieved by the number of partitions.\nThese models were trained until they converged or diverged and in all cases we present the best evaluation.\nOur best runs with 2 and 4 partitions were achieved with r1 set to 20 and were achieved with fewer than 200k iterations.\nThe difference between 2 and 4 partitions represents the largest difference in FID scores between models, demonstrating that for more complex datasets we need more information.\nFor our 6 partition design we made an additional modification to the generative network.\nThe minimum number of heads in the previous setups was 4, which only applied to sizes $>$128, and we changed this minimum to 8.\nThis change allowed for our usage of 6 partitions but also represents a slight increase in computation and memory, as complexity is influenced by the number of heads.\nSee~\\cref{tab:church_heads} for ablation on the number of partitions. \nAn uneven partition results in the tail heads evenly absorbing the remainder.\n\nWe include a table for the StyleNAT architecture for added clarity and reproducibility.\nEach level has 2 NAT layers which are split heads.\nHalf the heads takes the normal NA kernel and the other half takes DiNA kernels.\nDilation follows the pattern of using powers of 2 and starts when we start using the NA kernels.\n\\cref{tab:2split_model_arch} shows the configurations used for both FFHQ results.", "table_source": "\\begin{table}[ht]\n    \\centering\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n    Level & Kernel Size & Dilation & Dilated Size \\\\\n    \\hline\n    $4$ & -  & -  & - \\\\\n    $8$ & 7 & 1 & 7 \\\\\n    $16$ & 7 & 2 & 14\\\\\n    $32$ & 7 & 4 & 28 \\\\\n    $64$ & 7 & 8 & 56\\\\\n    $128$ & 7 & 16 & 112 \\\\\n    $256$ & 7 & 32 & 224 \\\\\n    $512$ & 7 & 64 & 448\\\\\n    $1024$ & 7 & 128 & 896\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{StyleNAT 2-Split Model Architecture. First level uses Multi-headed Self Attention and not DiNA. This model is used for all FFHQ results.}\n    \\label{tab:2split_model_arch}\n\\end{table}", "cell_list_gold": [{"value": "4", "char_index": [144, 145], "type": "Other"}, {"value": "8", "char_index": [169, 170], "type": "Other"}, {"value": "7", "char_index": [174, 175], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "1", "char_index": [178, 179], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilation", "dataset": "FFHQ"}, {"value": "7", "char_index": [182, 183], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilated Size", "dataset": "FFHQ"}, {"value": "16", "char_index": [192, 194], "type": "Other"}, {"value": "7", "char_index": [198, 199], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "2", "char_index": [202, 203], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilation", "dataset": "FFHQ"}, {"value": "14", "char_index": [206, 208], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilated Size", "dataset": "FFHQ"}, {"value": "32", "char_index": [216, 218], "type": "Other"}, {"value": "7", "char_index": [222, 223], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "4", "char_index": [226, 227], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilation", "dataset": "FFHQ"}, {"value": "28", "char_index": [230, 232], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilated Size", "dataset": "FFHQ"}, {"value": "64", "char_index": [241, 243], "type": "Other"}, {"value": "7", "char_index": [247, 248], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "8", "char_index": [251, 252], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilation", "dataset": "FFHQ"}, {"value": "56", "char_index": [255, 257], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilated Size", "dataset": "FFHQ"}, {"value": "128", "char_index": [265, 268], "type": "Other"}, {"value": "7", "char_index": [272, 273], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "16", "char_index": [276, 278], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilation", "dataset": "FFHQ"}, {"value": "112", "char_index": [281, 284], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilated Size", "dataset": "FFHQ"}, {"value": "256", "char_index": [293, 296], "type": "Other"}, {"value": "7", "char_index": [300, 301], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "32", "char_index": [304, 306], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilation", "dataset": "FFHQ"}, {"value": "224", "char_index": [309, 312], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilated Size", "dataset": "FFHQ"}, {"value": "512", "char_index": [321, 324], "type": "Other"}, {"value": "7", "char_index": [328, 329], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "64", "char_index": [332, 334], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilation", "dataset": "FFHQ"}, {"value": "448", "char_index": [337, 340], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilated Size", "dataset": "FFHQ"}, {"value": "1024", "char_index": [348, 352], "type": "Other"}, {"value": "7", "char_index": [356, 357], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "128", "char_index": [360, 363], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilation", "dataset": "FFHQ"}, {"value": "896", "char_index": [366, 369], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilated Size", "dataset": "FFHQ"}]}, "2211.05770v1_table5": {"table_code": "\\begin{table}[ht]\n    \\centering\n    \\begin{tabular}{c|c|l}\n    \\toprule\n    Level & Kernel Size & Dilations \\\\\n    \\hline\n    $4$    & -  & -  \\\\\n    $8$    & 7 & 1 \\\\\n    $16$   & 7 & 1,2\\\\ \n    $32$   & 7 & 1,2,4\\\\ \n    $64$   & 7 & 1,2,4,8\\\\ \n    $128$  & 7 & 1,2,4,8,16\\\\ \n    $256$  & 7 & 1,2,4,8,16,32\\\\ \n    $512$  & 7 & 1,2,4,8,16,32,64\\\\ \n    $1024$ & 7 & 1,2,4,8,16,32,64,128\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Example of progressive dilation with 8 heads, referred to ``pyramid dilation.''}\n    \\label{tab:pyramid_model_arch}\n\\end{table}", "table_label": "{tab:pyramid_model_arch}", "table_numeric_cells": [["4", "$4$", 128, 129, 127, 130], ["8", "$8$", 152, 153, 151, 154], ["7", "7", 160, 161, 160, 161], ["1", "1", 164, 165, 164, 165], ["16", "$16$", 174, 176, 173, 177], ["7", "7", 182, 183, 182, 183], ["32", "$32$", 198, 200, 197, 201], ["7", "7", 206, 207, 206, 207], ["64", "$64$", 224, 226, 223, 227], ["7", "7", 232, 233, 232, 233], ["128", "$128$", 252, 255, 251, 256], ["7", "7", 260, 261, 260, 261], ["256", "$256$", 283, 286, 282, 287], ["7", "7", 291, 292, 291, 292], ["512", "$512$", 317, 320, 316, 321], ["7", "7", 325, 326, 325, 326], ["1024", "$1024$", 354, 358, 353, 359], ["7", "7", 362, 363, 362, 363]], "text_chunk_selected": "\\author{\n    Steven Walton\\textsuperscript{1},\n    Ali Hassani\\textsuperscript{1},\n    Xingqian Xu\\textsuperscript{1},\n    Zhangyang Wang\\textsuperscript{2,3},\n    Humphrey Shi\\textsuperscript{1,3} \\\\\n{\\small \\textsuperscript{1}SHI Lab @ U of Oregon \\& UIUC, \\textsuperscript{2}UT Austin, \\textsuperscript{3}Picsart AI Research (PAIR)}\\\\\n{\\small \\textbf{\\url{https://github.com/SHI-Labs/StyleNAT}}}\n}\n\\twocolumn[{\n\\renewcommand\\twocolumn[1][]{#1}\n\\maketitle\n\n\\begin{itemize}\n    \\item We introduce Hydra-NA, which extends NA/DiNA and provides a flexible design to combine local and long-range receptive fields via different attention heads.\n    \\item We propose StyleNAT, an efficient and flexible image generation framework that is not only memory and compute efficient but also adaptive to different datasets and tasks via neighborhood attention and its dilated variants with our Hydra-NA design.\n     \\item We achieve new state of the arts on FFHQ-256 (among all GANs) and FFHQ-1024 (among all transformer-based GANs) with our StyleNAT.\n\\end{itemize}\n\n\\section{Related Works}\n\\label{sec:related}\nIn this section, we will briefly introduce attention modules introduced in Swin Transformer~\\cite{liu2021swin}, NAT~\\cite{hassani2022neighborhood}, and DiNAT~\\cite{hassani2022dilated}.\nWe will then move on to style-based generative adversarial models, particularly StyleGAN~\\cite{karras2019a} and StyleSwin~\\cite{zhang2022styleswin}.\n\n\\subsubsection{StyleGAN}\nKarras et al. first introduced the idea of using progressive generating using convolutions where the one would begin with a small latent representation, process each layer, and then scale up and repeat the process until the desired image size is achieved~\\cite{Karras2018ProgressiveGO}.\nThis architecture has major advantages in that it reduces the size of the latent representation, and thus the complexity of data that the model needs to learn.\nThis comes with the obvious drawbacks that we make a trade-off of diversity for fidelity and computational simplicity.\nKarras et al. later produced several papers that built on this idea~\\cite{karras2019a,Karras2020AnalyzingAI,Karras2020TrainingGA,Karras2021AliasFreeGA}, improving the progressive network, but also introducing a sub-network, called the ``style network'', that allows one to control the latent representations at each of these levels.\nThis innovation dramatically increased the usability of this type of network architecture because it allows for simpler feature interpolation, which is often difficult in implicit density models.\nBecause of the high fidelity, flexibility, and the ability to perform feature interpolation, this style of network has become the de facto choice for many researchers~\\cite{Abdal2021StyleFlowAE,humayun2022magnet,humayun2022polarity,karnewar2020msg}. \n\\subsubsection{StyleSwin}\nStyleSwin~\\cite{zhang2022styleswin} introduced Swin based transformers into the StyleGAN framework, beating other transformer based models, but still not achieving any state of the art results.\nThis work did show the power of Swin and how localized attention mechanisms are able to out perform many CNN based architectures and did extend the state of the art for style based networks.\nStyleSwin achieved this not only by replacing the CNN layers with Swin layers, but also partitioned the transformer heads into two, so that each half could operate with a different kernel.\nThis was key to improving their generative quality, allowing the framework to increase flexibility and pay attention to distinct parts of the scene at the same time. \nStyleSwin also included a wavelet based discriminator, as was introduced in SWAGAN~\\cite{gal2021swagan} and also used by StyleGAN3~\\cite{Karras2021AliasFreeGA}, which all show improvements for addressing high-frequency content that causes artifacts.\nStyleSwin uses a constant window size of 8 for all latents larger than the initial $4\\times4$, wherein they use a window size of 4.\nWe follow a similar structure except replace the initial layer with MHSA for efficiency. \n\nTo accurately judge the quality and usability of StyleNAT we investigate the performance on FFHQ (\\cref{sec:ffhq}) and LSUN Churches (\\cref{sec:church}). \nWe also perform an ablation study compared to StyleSwin (\\cref{sec:ablation}) to demonstrate the effects of the improvements we have made.\nThese datasets are common evaluation benchmarks that can help us understand how the generator works on more regular data as well as irregular data.\nAll arguments for experiments are saved alongside the checkpoints for verification. \nFor best comparisons we keep hyper-parameters similar to StyleSwin, unless otherwise specified.\nThis means that we use the Two Time-Scale Update Rule (TTUR)~\\cite{Heusel2017GANsTB} training method, have a discriminator learning rate of $2\\times 10^{-4}$, use Balanced Consistency Regularization (bCR)~\\cite{Zhao2021ImprovedCR}, r1 regularization~\\cite{Mescheder2018WhichTM}, and a learning rate decay (specified per dataset).\n\nIn addition to the above ablation we include one for our search space on LSUN Church in an effort to demonstrate the effectiveness of the number of heads and partitions.\nBecause this data is more complex, a simple two partition design was not enough.\nIn an indirect way, the number of partitions required to achieve good results provides insights into the complexity of a dataset.\nThe number of partitions do not change the computational or memory complexity of the model but there is a slight increase in computation as we handle the complexity.\nAll runs use a progressive style of dilation as described above.\nBelow we report the best scores achieved by the number of partitions.\nThese models were trained until they converged or diverged and in all cases we present the best evaluation.\nOur best runs with 2 and 4 partitions were achieved with r1 set to 20 and were achieved with fewer than 200k iterations.\nThe difference between 2 and 4 partitions represents the largest difference in FID scores between models, demonstrating that for more complex datasets we need more information.\nFor our 6 partition design we made an additional modification to the generative network.\nThe minimum number of heads in the previous setups was 4, which only applied to sizes $>$128, and we changed this minimum to 8.\nThis change allowed for our usage of 6 partitions but also represents a slight increase in computation and memory, as complexity is influenced by the number of heads.\nSee~\\cref{tab:church_heads} for ablation on the number of partitions. \nAn uneven partition results in the tail heads evenly absorbing the remainder.\n\n\\definecolor{codegreen}{rgb}{0,0.6,0}\n\\definecolor{codepurple}{rgb}{0.58,0,0.82}\n\\definecolor{codeblue}{rgb}{0, 0, 0.6}\n\\definecolor{codered}{rgb}{0.6, 0, 0}\n\\definecolor{codeyellow}{rgb}{0.6, 0.6, 0}\n\\definecolor{background}{rgb}{0.9, 0.9, 0.9}\n\n\\lstdefinelanguage{pytorch}\n{\n    morekeywords={class, def, super, len, for, range, len, if, else, zip, apply, return, __init__, append},\n    keywordstyle=\\color{codepurple}\\textbf,\n    keywords=[2]{self,},\n    keywordstyle=[2]\\color{codegreen}\\textbf,\n    keywords=[3]{in, True, False, None, or, \\%, },\n    keywordstyle=[3]\\color{codeblue}\\textbf,\n    keywords=[4]{nn, torch, Module, Linear, cat, Parameter, ParameterList, chunk, shape, zeros, Dropout, squeeze, reshape, permute, trunc_normal_, softmax, forward, dim},\n    keywordstyle=[4]\\color{codered}\\textbf,\n    keywords=[5]{natten2dqkrpb,natten2dav,HydraNeighborhoodAttention},\n    keywordstyle=[5]\\color{codeyellow}\\textbf,\n}", "table_source": "\\begin{table}[ht]\n    \\centering\n    \\begin{tabular}{c|c|l}\n    \\toprule\n    Level & Kernel Size & Dilations \\\\\n    \\hline\n    $4$    & -  & -  \\\\\n    $8$    & 7 & 1 \\\\\n    $16$   & 7 & 1,2\\\\ \n    $32$   & 7 & 1,2,4\\\\ \n    $64$   & 7 & 1,2,4,8\\\\ \n    $128$  & 7 & 1,2,4,8,16\\\\ \n    $256$  & 7 & 1,2,4,8,16,32\\\\ \n    $512$  & 7 & 1,2,4,8,16,32,64\\\\ \n    $1024$ & 7 & 1,2,4,8,16,32,64,128\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Example of progressive dilation with 8 heads, referred to ``pyramid dilation.''}\n    \\label{tab:pyramid_model_arch}\n\\end{table}", "cell_list_gold": [{"value": "4", "char_index": [128, 129], "type": "Other"}, {"value": "8", "char_index": [152, 153], "type": "Other"}, {"value": "7", "char_index": [160, 161], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "1", "char_index": [164, 165], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Dilations", "dataset": "FFHQ"}, {"value": "16", "char_index": [174, 176], "type": "Other"}, {"value": "7", "char_index": [182, 183], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "32", "char_index": [198, 200], "type": "Other"}, {"value": "7", "char_index": [206, 207], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "64", "char_index": [224, 226], "type": "Other"}, {"value": "7", "char_index": [232, 233], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "128", "char_index": [252, 255], "type": "Other"}, {"value": "7", "char_index": [260, 261], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "256", "char_index": [283, 286], "type": "Other"}, {"value": "7", "char_index": [291, 292], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "512", "char_index": [317, 320], "type": "Other"}, {"value": "7", "char_index": [325, 326], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}, {"value": "1024", "char_index": [354, 358], "type": "Other"}, {"value": "7", "char_index": [362, 363], "type": "Hyper-parameter/Architecture", "model": "StyleNAT", "parameter/architecture name": "Kernel Size", "dataset": "FFHQ"}]}, "2211.05776v1_table10": {"table_code": "\\begin{tabular}{c|c|c}\n        \n            \\cellcolor{lightgray!30} Train & \\cellcolor{lightgray!30} Test & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            Random & Fixed (4) & 39.7\\\\ \n            Fixed (4) & Fixed (4)  & 41.0  \\\\\n            Fixed (4) & Fixed (8)  & \\textbf{41.3} \\\\ \n            Fixed (8) & Fixed (8)  & 41.0 \\\\ \\hline\n        \\end{tabular}", "table_label": "{abl_1}", "table_numeric_cells": [["39.7", "39.7", 191, 195, 191, 195], ["41.0", "41.0", 236, 240, 236, 240], ["41.3", "\\textbf{41.3}", 290, 294, 282, 295], ["41.0", "41.0", 337, 341, 337, 341]], "text_chunk_selected": "\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\nwhere $I^c$ is the crop. Compared to the strategy of random cropping in state-of-the-art video instance segmentation methods SeqFormer~\\cite{seqformer} and VITA~\\cite{heo2022vita}, we adopt a stricter fixed crop strategy to keep training and inference consistent. This provides a strong inference efficiency because it is sufficient to evaluate a fixed number of crops during inference, rather than many random crops.\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Similar to video-level Mask2Former~\\cite{cheng2021mask2former}, the association module aims to generate batch queries $\\mathbf{Q_b}$ that are fully shared by the full image and its crop to represent the same entities consistently. Instead of treating $\\mathbf{Q_b}$ as learnable network parameters, here we generate $\\mathbf{Q_b}$ directly from $\\mathbf{E_i}=\\{\\mathbf{E}_{I^o}, \\mathbf{E}_{I^c})\\}$, considering that $\\mathbf{E_i}$ already contains strong segmentation features. \nIn particular, we use a Transformer architecture with cross attention (XAtt), self-attention (SAtt), and feedforward network (FFN) to obtain $\\mathbf{Q_b}$:\n\n\\vspace{-4mm}\n\\paragraph{Inference} An ensemble of predictions from multiple image views (or TTA) is a common practice in deep learning, but has never been explored in the context of query-based segmentation architectures~\\cite{cheng2022masked,cheng2021mask2former,li2022panoptic}. The reason is that the queries are implicitly tied to certain spatial locations of the input and thus unrobust to transformations (\\textit{e.g.,}~scaling, cropping), which we show in our supplementary file. As a result, the same entity across different image views are likely predicted by different queries. Therefore, it is not possible to ensemble \nthe results of unmatched image views using previous query-based methods.\nHowever, with CropFormer, during inference, we can easily ensemble the batch-level prediction results. This is because the same entities between the full image and its crop are represented by the same queries, by virtue of the association module. For the final segmentation output, we ensemble the per-pixel mask predictions obtained from the full image and every crop of the 4 corners (Sec.~\\ref{subsec:cropformer}). \nThe confidence score of each entity comes from the batch-level entityness prediction score. \n\n\\vspace{-4mm}\n\\paragraph{Panoptic Segmentation} Similar to EntityIns, we select 345 categories, including 274 things and 71 stuff, with the highest entity-level frequency to construct EntityPan. In EntityPan, there are 9,968 and 1,481 images for training and testing. Table~\\ref{Tab:aba_benchmark_classaware_panop} shows the performance of two popular panoptic segmentation methods, including PanopticFPN~\\cite{kirillov2019panopticfpn} and Mask2Former~\\cite{cheng2022masked}. In this table, we see that the PQs are much lower than those of existing panoptic datasets, due to the greater variety of class labels which makes the task more challenging.\nIn addition, current panoptic methods perform worse on EntityPan compared to existing datasets, particularly on the recognition quality (RQ) side which adversely impacts PQ.\n\nTable~\\ref{Tab:aba_ivresults} shows the overall improvement of CropFromer to the Mask2Former~\\cite{cheng2022masked} under 1$\\times$ training setting. The first and second row is our baseline Mask2Former with the single-scale inference (800 and 1040 shortest side) and (1333 and 1732 longest side) for the full images, where 1040=800$\\times\\delta$ and 1732=1333$\\times\\delta$ with $\\delta$ 0.7 in default. For the results in the third and fourth rows, we use test-time bipartite-matching to associate the same entities obtained with multi-scale images and crops. We find no improvements with such inference strategies. Whereas, using the crop output from CropFormer's batch decoder achieves a significant AP$^\\text{e}$ gain as indicated by the second last row. By combining the full image and four crop outputs from CropFormer (final row), we obtain even stronger 1.5 AP$^\\text{e}$ and 1.7 AP$^\\text{e}_{75}$ gain compared to the baseline (first row).\n\n\\vspace{-4mm}\n\\paragraph{CropDataloader} Table~\\ref{abl_1}(a) and (b) show the ablation study on the usage of crop ratio and crop type. As indicated by Table~\\ref{abl_1}(a), CropFormer obtains the best performance with a crop ratio $\\delta$ of 0.7. Smaller crop ratios perform worse and it might be a problem caused by queries that are not robust against drastic image transformation, which we leave for future work. Table~\\ref{abl_1}(b) shows the impacts of the type of crop and the number of crops. 4 and 8 fixed crops during training and testing performs the best.\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Table~\\ref{abl_1}(c) shows the ablation study on the structure of the association module. There is a slight performance difference between using self-attention or a feedforward module following cross-attention.", "table_source": "\\begin{table}[t!]\n\\begin{minipage}{\\textwidth}\n\\begin{minipage}[t]{0.08\\textwidth}\n            \\centering\n            \\scriptsize\n             \\setlength{\\tabcolsep}{2pt}\n            \\begin{tabular}{c|c}\n            \\cellcolor{lightgray!30} $\\delta$ & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            0.5 & 38.5 \\\\ \n            0.6 & 40.2  \\\\ \n            0.7 & \\textbf{41.0}  \\\\ \n            0.8 & 40.9 \\\\ \\hline\n            \\end{tabular}\n        \n        (a)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.22\\textwidth}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{c|c|c}\n        \n            \\cellcolor{lightgray!30} Train & \\cellcolor{lightgray!30} Test & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            Random & Fixed (4) & 39.7\\\\ \n            Fixed (4) & Fixed (4)  & 41.0  \\\\\n            Fixed (4) & Fixed (8)  & \\textbf{41.3} \\\\ \n            Fixed (8) & Fixed (8)  & 41.0 \\\\ \\hline\n        \\end{tabular}\n        \n    (b)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.15\\textwidth}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{ccc|c}\n            \\cellcolor{lightgray!30} XAtt & \\cellcolor{lightgray!30} SAtt & \\cellcolor{lightgray!30} FFN & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            \\checkmark & $\\circ$ & $\\circ$ &  40.7 \\\\ \n            \\checkmark & $\\circ$ & \\checkmark & 40.8  \\\\ \n            \\checkmark & \\checkmark & $\\circ$ & 40.8 \\\\\n            \\checkmark & \\checkmark & \\checkmark & \\textbf{41.0} \\\\ \\hline\n        \\end{tabular}\n        \n    (c)\n    \\end{minipage}\n\\end{minipage}\n\\caption{Ablation study on the usage of crop ratio $\\delta$, crop type and association module in CropFormer. In sub-table (b), `Random' indicates random crops and `Fixed (4/8)' indicates 4 or 8 fixed corner crops. In sub-table(c), \\checkmark and $\\circ$ means whether we use the module or not.}\n\\vspace{-0.1in}\n\\label{abl_1}\n\\end{table}", "cell_list_gold": [{"value": "39.7", "char_index": [191, 195], "type": "Result", "training data/set": "EntitySeg Random", "test data/set": "EntitySeg Fixed (4)", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"pretrain": "COCO-E"}}, {"value": "41.0", "char_index": [236, 240], "type": "Result", "training data/set": "EntitySeg Fixed (4)", "test data/set": "EntitySeg Fixed (4)", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"pretrain": "COCO-E"}}, {"value": "41.3", "char_index": [290, 294], "type": "Result", "training data/set": "EntitySeg Fixed (4)", "test data/set": "EntitySeg Fixed (8)", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"pretrain": "COCO-E"}}, {"value": "41.0", "char_index": [337, 341], "type": "Result", "training data/set": "EntitySeg Fixed (8)", "test data/set": "EntitySeg Fixed (8)", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"pretrain": "COCO-E"}}]}, "2211.05776v1_table11": {"table_code": "\\begin{tabular}{ccc|c}\n            \\cellcolor{lightgray!30} XAtt & \\cellcolor{lightgray!30} SAtt & \\cellcolor{lightgray!30} FFN & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            \\checkmark & $\\circ$ & $\\circ$ &  40.7 \\\\ \n            \\checkmark & $\\circ$ & \\checkmark & 40.8  \\\\ \n            \\checkmark & \\checkmark & $\\circ$ & 40.8 \\\\\n            \\checkmark & \\checkmark & \\checkmark & \\textbf{41.0} \\\\ \\hline\n        \\end{tabular}", "table_label": "{abl_1}", "table_numeric_cells": [["40.7", "40.7", 225, 229, 225, 229], ["40.8", "40.8", 282, 286, 282, 286], ["40.8", "40.8", 340, 344, 340, 344], ["41.0", "\\textbf{41.0}", 407, 411, 399, 412]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Similar to video-level Mask2Former~\\cite{cheng2021mask2former}, the association module aims to generate batch queries $\\mathbf{Q_b}$ that are fully shared by the full image and its crop to represent the same entities consistently. Instead of treating $\\mathbf{Q_b}$ as learnable network parameters, here we generate $\\mathbf{Q_b}$ directly from $\\mathbf{E_i}=\\{\\mathbf{E}_{I^o}, \\mathbf{E}_{I^c})\\}$, considering that $\\mathbf{E_i}$ already contains strong segmentation features. \nIn particular, we use a Transformer architecture with cross attention (XAtt), self-attention (SAtt), and feedforward network (FFN) to obtain $\\mathbf{Q_b}$:\n\n\\vspace{-4mm}\n\\paragraph{Panoptic Segmentation} Similar to EntityIns, we select 345 categories, including 274 things and 71 stuff, with the highest entity-level frequency to construct EntityPan. In EntityPan, there are 9,968 and 1,481 images for training and testing. Table~\\ref{Tab:aba_benchmark_classaware_panop} shows the performance of two popular panoptic segmentation methods, including PanopticFPN~\\cite{kirillov2019panopticfpn} and Mask2Former~\\cite{cheng2022masked}. In this table, we see that the PQs are much lower than those of existing panoptic datasets, due to the greater variety of class labels which makes the task more challenging.\nIn addition, current panoptic methods perform worse on EntityPan compared to existing datasets, particularly on the recognition quality (RQ) side which adversely impacts PQ.\n\nTable~\\ref{Tab:aba_ivresults} shows the overall improvement of CropFromer to the Mask2Former~\\cite{cheng2022masked} under 1$\\times$ training setting. The first and second row is our baseline Mask2Former with the single-scale inference (800 and 1040 shortest side) and (1333 and 1732 longest side) for the full images, where 1040=800$\\times\\delta$ and 1732=1333$\\times\\delta$ with $\\delta$ 0.7 in default. For the results in the third and fourth rows, we use test-time bipartite-matching to associate the same entities obtained with multi-scale images and crops. We find no improvements with such inference strategies. Whereas, using the crop output from CropFormer's batch decoder achieves a significant AP$^\\text{e}$ gain as indicated by the second last row. By combining the full image and four crop outputs from CropFormer (final row), we obtain even stronger 1.5 AP$^\\text{e}$ and 1.7 AP$^\\text{e}_{75}$ gain compared to the baseline (first row).\n\n\\vspace{-4mm}\n\\paragraph{Other Batch Fusion Design} Table~\\ref{Tab:aba_fusion_style} shows the ablation study with different fusion designs between original images and cropped patches. Directly using video-level Mask2former or VITA fusion method merely brings marginal performance gains.\n\n\\vspace{-4mm}\n\\paragraph{CropDataloader} Table~\\ref{abl_1}(a) and (b) show the ablation study on the usage of crop ratio and crop type. As indicated by Table~\\ref{abl_1}(a), CropFormer obtains the best performance with a crop ratio $\\delta$ of 0.7. Smaller crop ratios perform worse and it might be a problem caused by queries that are not robust against drastic image transformation, which we leave for future work. Table~\\ref{abl_1}(b) shows the impacts of the type of crop and the number of crops. 4 and 8 fixed crops during training and testing performs the best.\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Table~\\ref{abl_1}(c) shows the ablation study on the structure of the association module. There is a slight performance difference between using self-attention or a feedforward module following cross-attention.", "table_source": "\\begin{table}[t!]\n\\begin{minipage}{\\textwidth}\n\\begin{minipage}[t]{0.08\\textwidth}\n            \\centering\n            \\scriptsize\n             \\setlength{\\tabcolsep}{2pt}\n            \\begin{tabular}{c|c}\n            \\cellcolor{lightgray!30} $\\delta$ & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            0.5 & 38.5 \\\\ \n            0.6 & 40.2  \\\\ \n            0.7 & \\textbf{41.0}  \\\\ \n            0.8 & 40.9 \\\\ \\hline\n            \\end{tabular}\n        \n        (a)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.22\\textwidth}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{c|c|c}\n        \n            \\cellcolor{lightgray!30} Train & \\cellcolor{lightgray!30} Test & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            Random & Fixed (4) & 39.7\\\\ \n            Fixed (4) & Fixed (4)  & 41.0  \\\\\n            Fixed (4) & Fixed (8)  & \\textbf{41.3} \\\\ \n            Fixed (8) & Fixed (8)  & 41.0 \\\\ \\hline\n        \\end{tabular}\n        \n    (b)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.15\\textwidth}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{ccc|c}\n            \\cellcolor{lightgray!30} XAtt & \\cellcolor{lightgray!30} SAtt & \\cellcolor{lightgray!30} FFN & \\cellcolor{lightgray!30} AP$^\\text{e}$ \\\\ \\hline\n            \\checkmark & $\\circ$ & $\\circ$ &  40.7 \\\\ \n            \\checkmark & $\\circ$ & \\checkmark & 40.8  \\\\ \n            \\checkmark & \\checkmark & $\\circ$ & 40.8 \\\\\n            \\checkmark & \\checkmark & \\checkmark & \\textbf{41.0} \\\\ \\hline\n        \\end{tabular}\n        \n    (c)\n    \\end{minipage}\n\\end{minipage}\n\\caption{Ablation study on the usage of crop ratio $\\delta$, crop type and association module in CropFormer. In sub-table (b), `Random' indicates random crops and `Fixed (4/8)' indicates 4 or 8 fixed corner crops. In sub-table(c), \\checkmark and $\\circ$ means whether we use the module or not.}\n\\vspace{-0.1in}\n\\label{abl_1}\n\\end{table}", "cell_list_gold": [{"value": "40.7", "char_index": [225, 229], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"XAtt": "true", "SAtt": "false", "FFN": "false", "pretrained weights": "COCO-E"}}, {"value": "40.8", "char_index": [282, 286], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"XAtt": "true", "SAtt": "false", "FFN": "true", "pretrained weights": "COCO-E"}}, {"value": "40.8", "char_index": [340, 344], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"XAtt": "true", "SAtt": "true", "FFN": "false", "pretrained weights": "COCO-E"}}, {"value": "41.0", "char_index": [407, 411], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"XAtt": "true", "SAtt": "true", "FFN": "true", "pretrained weights": "COCO-E"}}]}, "2211.05776v1_table12": {"table_code": "\\begin{tabular}{c|c|c}\n            \\cellcolor{lightgray!30} Training & \\cellcolor{lightgray!30} Baseline &  \\cellcolor{lightgray!30} Ours  \\\\  \\hline\n            1$\\times$ & 39.5 & 41.0 \\textcolor[RGB]{34,139,34}{(+1.5)}  \\\\ \n            2$\\times$ & 40.2 & 42.1 \\textcolor[RGB]{34,139,34}{(+1.9)}  \\\\ \n            3$\\times$ & 40.9 & 42.8 \\textcolor[RGB]{34,139,34}{(+1.9)}\\\\ \n            4$\\times$ & 40.9 & 42.7 \\textcolor[RGB]{34,139,34}{(+1.8)} \\\\ \\hline\n            \\end{tabular}", "table_label": "{abl_2}", "table_numeric_cells": [["39.5", "39.5", 174, 178, 174, 178], ["40.2", "40.2", 250, 254, 250, 254], ["40.9", "40.9", 326, 330, 326, 330], ["40.9", "40.9", 400, 404, 400, 404]], "text_chunk_selected": "\\subsection{Mask2Former Preliminaries}\nMask2Former~\\cite{cheng2022masked} is a universal Transformer-based framework for image or video instance segmentation. Given an input, Mask2Former uses a Transformer decoder to decode $N$ number of $K$-dimensional queries $\\mathbf{Q}\\in \\mathbb{R}^{N\\times K}$ to generate embeddings $\\mathbf{E} \\in \\mathbb{R}^{N \\times 1 \\times 1 \\times 1 \\times K}$ to predict $N$ segmentation masks, where the \\nth{2}, \\nth{3}, and \\nth{4} dimensions of $\\mathbf{E}$ correspond to temporal ($T$), output height ($H$), and output width  ($W$) dimensions respectively. When applying the image-level Mask2Former to an image input, $\\mathbf{E}$ is broadcasted along spatial dimensions to the shape of $N \\times 1 \\times H \\times W \\times K$. Alternatively, when applying video-level Mask2Former to a video input, $\\mathbf{E}$ is broadcasted to the shape of $N \\times T \\times H \\times W \\times K$. Note that in video-level Mask2Former, $\\mathbf{E}$ is further broadcasted along the both temporal and spatial dimensions, enabling Mask2Former to utilize the same embeddings to represent the same visual instances across different frames consistently.\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Image Encoder and Decoder} Based on learnable image-level queries $\\mathbf{Q}_{i} \\in \\mathbb{R}^{N\\times K}$,  we use image encoder $\\Theta$ and decoder $\\Phi_{i}$ to generate image-level embeddings $\\mathbf{E_i} \\in \\mathbb{R}^{N \\times 2 \\times 1 \\times 1 \\times K}$ for the full input image and its crop. \nFor the image encoder and decoder, we directly adopt the ones in standard image-level Mask2Former~\\cite{cheng2022masked} and do not modify their structures or designs. $\\mathbf{E_i}$ is obtained as follows:\n\nSimilar to video-level Mask2Former, the full image and its crop can be seen as 2 video frames. Thus, we broadcast $\\mathbf{E_b}$ to the shape of $N \\times 2 \\times 1 \\times 1 \\times K$ to share it between the full image and its crop. $\\mathbf{E_b}$ and batch-level pyramid features $\\mathbf{P_b^h}\\in\\mathbb{R}^{1\\times2\\times H^h\\times W^h}$ (reshaped from $\\mathbf{P_i^h}\\in\\mathbb{R}^{2\\times1\\times H^h\\times W^h}$) are used for batch-level mask classification and pixel-wise prediction:\n\nTable~\\ref{Tab:aba_ivresults} shows the overall improvement of CropFromer to the Mask2Former~\\cite{cheng2022masked} under 1$\\times$ training setting. The first and second row is our baseline Mask2Former with the single-scale inference (800 and 1040 shortest side) and (1333 and 1732 longest side) for the full images, where 1040=800$\\times\\delta$ and 1732=1333$\\times\\delta$ with $\\delta$ 0.7 in default. For the results in the third and fourth rows, we use test-time bipartite-matching to associate the same entities obtained with multi-scale images and crops. We find no improvements with such inference strategies. Whereas, using the crop output from CropFormer's batch decoder achieves a significant AP$^\\text{e}$ gain as indicated by the second last row. By combining the full image and four crop outputs from CropFormer (final row), we obtain even stronger 1.5 AP$^\\text{e}$ and 1.7 AP$^\\text{e}_{75}$ gain compared to the baseline (first row).\n\n\\vspace{-4mm}\n\\paragraph{CropDataloader} Table~\\ref{abl_1}(a) and (b) show the ablation study on the usage of crop ratio and crop type. As indicated by Table~\\ref{abl_1}(a), CropFormer obtains the best performance with a crop ratio $\\delta$ of 0.7. Smaller crop ratios perform worse and it might be a problem caused by queries that are not robust against drastic image transformation, which we leave for future work. Table~\\ref{abl_1}(b) shows the impacts of the type of crop and the number of crops. 4 and 8 fixed crops during training and testing performs the best.\n\n\\vspace{-4mm}\n\\paragraph{Longer Training Schedules} \nTable~\\ref{abl_2}(a) indicates that CropFormer consistently improves over the baseline Mask2Former when we adopt 2$\\times$/3$\\times$ \ntraining schedules. A similar trend is observed when using Swin-L~\\cite{liu2021swin} backbone, which is provided in our supplementary file.\n\n\\vspace{-4mm}\n\\paragraph{Improvement on class-aware segmentation tasks} \nTable~\\ref{abl_2}(b) shows the \nclass-aware task performance of CropFromer with Swin-T~\\cite{liu2021swin} backbone and 3$\\times$ training schedule. Compared to the class-agnostic entity segmentation tasks, CropFormer provides smaller gains on class-aware segmentation task, suggesting that CropFormer mainly benefits mask prediction rather than class prediction.", "table_source": "\\begin{table}[t!]\n\\begin{minipage}{\\textwidth}\n\\begin{minipage}[t]{0.16\\textwidth}\n            \\centering\n            \\scriptsize\n            \\setlength{\\tabcolsep}{2pt}\n            \\begin{tabular}{c|c|c}\n            \\cellcolor{lightgray!30} Training & \\cellcolor{lightgray!30} Baseline &  \\cellcolor{lightgray!30} Ours  \\\\  \\hline\n            1$\\times$ & 39.5 & 41.0 \\textcolor[RGB]{34,139,34}{(+1.5)}  \\\\ \n            2$\\times$ & 40.2 & 42.1 \\textcolor[RGB]{34,139,34}{(+1.9)}  \\\\ \n            3$\\times$ & 40.9 & 42.8 \\textcolor[RGB]{34,139,34}{(+1.9)}\\\\ \n            4$\\times$ & 40.9 & 42.7 \\textcolor[RGB]{34,139,34}{(+1.8)} \\\\ \\hline\n            \\end{tabular}\n        \n        (a)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.36\\textwidth}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{c|c|c}\n            \\cellcolor{lightgray!30} Seg-Task & \\cellcolor{lightgray!30} Baseline & \\cellcolor{lightgray!30} Ours\\\\ \\hline\n            Entity (AP$^\\text{e}$) & 40.9 & 42.8 \\textcolor[RGB]{34,139,34}{(+1.9)} \\\\ \n            Semantic (mIoU)  & 45.0 & 45.6\\textcolor[RGB]{34,139,34}{(+0.6)}  \\\\ \n            Instance (AP) & 22.7 & 23.2 \\textcolor[RGB]{34,139,34}{(+0.5)} \\\\ \n            Panoptic (PQ) & 9.8 & 10.4\\textcolor[RGB]{34,139,34}{(+0.6)} \\\\ \\hline\n        \\end{tabular}\n        \n    (b)\n        \\end{minipage}\n\\end{minipage}\n\\caption{Ablation study on the longer training time and overall improvements in our CropFormer.}\n\\vspace{-0.25in}\n\\label{abl_2}\n\\end{table}", "cell_list_gold": [{"value": "39.5", "char_index": [174, 178], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "41.0", "char_index": [181, 185], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "CropFormer", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "40.2", "char_index": [250, 254], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "2$\\times$"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "42.1", "char_index": [257, 261], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "2$\\times$"}, "model": "CropFormer", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "40.9", "char_index": [326, 330], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "3$\\times$"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "42.8", "char_index": [333, 337], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "3$\\times$"}, "model": "CropFormer", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "40.9", "char_index": [400, 404], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "4$\\times$"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "42.7", "char_index": [407, 411], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "4$\\times$"}, "model": "CropFormer", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}]}, "2211.05776v1_table13": {"table_code": "\\begin{tabular}{c|c|c}\n            \\cellcolor{lightgray!30} Seg-Task & \\cellcolor{lightgray!30} Baseline & \\cellcolor{lightgray!30} Ours\\\\ \\hline\n            Entity (AP$^\\text{e}$) & 40.9 & 42.8 \\textcolor[RGB]{34,139,34}{(+1.9)} \\\\ \n            Semantic (mIoU)  & 45.0 & 45.6\\textcolor[RGB]{34,139,34}{(+0.6)}  \\\\ \n            Instance (AP) & 22.7 & 23.2 \\textcolor[RGB]{34,139,34}{(+0.5)} \\\\ \n            Panoptic (PQ) & 9.8 & 10.4\\textcolor[RGB]{34,139,34}{(+0.6)} \\\\ \\hline\n        \\end{tabular}", "table_label": "{abl_2}", "table_numeric_cells": [["40.9", "40.9", 183, 187, 183, 187], ["45.0", "45.0", 265, 269, 265, 269], ["22.7", "22.7", 344, 348, 344, 348], ["9.8", "9.8", 423, 426, 423, 426]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\subsection{Mask2Former Preliminaries}\nMask2Former~\\cite{cheng2022masked} is a universal Transformer-based framework for image or video instance segmentation. Given an input, Mask2Former uses a Transformer decoder to decode $N$ number of $K$-dimensional queries $\\mathbf{Q}\\in \\mathbb{R}^{N\\times K}$ to generate embeddings $\\mathbf{E} \\in \\mathbb{R}^{N \\times 1 \\times 1 \\times 1 \\times K}$ to predict $N$ segmentation masks, where the \\nth{2}, \\nth{3}, and \\nth{4} dimensions of $\\mathbf{E}$ correspond to temporal ($T$), output height ($H$), and output width  ($W$) dimensions respectively. When applying the image-level Mask2Former to an image input, $\\mathbf{E}$ is broadcasted along spatial dimensions to the shape of $N \\times 1 \\times H \\times W \\times K$. Alternatively, when applying video-level Mask2Former to a video input, $\\mathbf{E}$ is broadcasted to the shape of $N \\times T \\times H \\times W \\times K$. Note that in video-level Mask2Former, $\\mathbf{E}$ is further broadcasted along the both temporal and spatial dimensions, enabling Mask2Former to utilize the same embeddings to represent the same visual instances across different frames consistently.\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\nTable~\\ref{Tab:aba_ivresults} shows the overall improvement of CropFromer to the Mask2Former~\\cite{cheng2022masked} under 1$\\times$ training setting. The first and second row is our baseline Mask2Former with the single-scale inference (800 and 1040 shortest side) and (1333 and 1732 longest side) for the full images, where 1040=800$\\times\\delta$ and 1732=1333$\\times\\delta$ with $\\delta$ 0.7 in default. For the results in the third and fourth rows, we use test-time bipartite-matching to associate the same entities obtained with multi-scale images and crops. We find no improvements with such inference strategies. Whereas, using the crop output from CropFormer's batch decoder achieves a significant AP$^\\text{e}$ gain as indicated by the second last row. By combining the full image and four crop outputs from CropFormer (final row), we obtain even stronger 1.5 AP$^\\text{e}$ and 1.7 AP$^\\text{e}_{75}$ gain compared to the baseline (first row).\n\n\\vspace{-4mm}\n\\paragraph{CropDataloader} Table~\\ref{abl_1}(a) and (b) show the ablation study on the usage of crop ratio and crop type. As indicated by Table~\\ref{abl_1}(a), CropFormer obtains the best performance with a crop ratio $\\delta$ of 0.7. Smaller crop ratios perform worse and it might be a problem caused by queries that are not robust against drastic image transformation, which we leave for future work. Table~\\ref{abl_1}(b) shows the impacts of the type of crop and the number of crops. 4 and 8 fixed crops during training and testing performs the best.\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Table~\\ref{abl_1}(c) shows the ablation study on the structure of the association module. There is a slight performance difference between using self-attention or a feedforward module following cross-attention.\n\n\\vspace{-4mm}\n\\paragraph{Longer Training Schedules} \nTable~\\ref{abl_2}(a) indicates that CropFormer consistently improves over the baseline Mask2Former when we adopt 2$\\times$/3$\\times$ \ntraining schedules. A similar trend is observed when using Swin-L~\\cite{liu2021swin} backbone, which is provided in our supplementary file.\n\n\\vspace{-4mm}\n\\paragraph{Improvement on class-aware segmentation tasks} \nTable~\\ref{abl_2}(b) shows the \nclass-aware task performance of CropFromer with Swin-T~\\cite{liu2021swin} backbone and 3$\\times$ training schedule. Compared to the class-agnostic entity segmentation tasks, CropFormer provides smaller gains on class-aware segmentation task, suggesting that CropFormer mainly benefits mask prediction rather than class prediction.", "table_source": "\\begin{table}[t!]\n\\begin{minipage}{\\textwidth}\n\\begin{minipage}[t]{0.16\\textwidth}\n            \\centering\n            \\scriptsize\n            \\setlength{\\tabcolsep}{2pt}\n            \\begin{tabular}{c|c|c}\n            \\cellcolor{lightgray!30} Training & \\cellcolor{lightgray!30} Baseline &  \\cellcolor{lightgray!30} Ours  \\\\  \\hline\n            1$\\times$ & 39.5 & 41.0 \\textcolor[RGB]{34,139,34}{(+1.5)}  \\\\ \n            2$\\times$ & 40.2 & 42.1 \\textcolor[RGB]{34,139,34}{(+1.9)}  \\\\ \n            3$\\times$ & 40.9 & 42.8 \\textcolor[RGB]{34,139,34}{(+1.9)}\\\\ \n            4$\\times$ & 40.9 & 42.7 \\textcolor[RGB]{34,139,34}{(+1.8)} \\\\ \\hline\n            \\end{tabular}\n        \n        (a)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.36\\textwidth}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{c|c|c}\n            \\cellcolor{lightgray!30} Seg-Task & \\cellcolor{lightgray!30} Baseline & \\cellcolor{lightgray!30} Ours\\\\ \\hline\n            Entity (AP$^\\text{e}$) & 40.9 & 42.8 \\textcolor[RGB]{34,139,34}{(+1.9)} \\\\ \n            Semantic (mIoU)  & 45.0 & 45.6\\textcolor[RGB]{34,139,34}{(+0.6)}  \\\\ \n            Instance (AP) & 22.7 & 23.2 \\textcolor[RGB]{34,139,34}{(+0.5)} \\\\ \n            Panoptic (PQ) & 9.8 & 10.4\\textcolor[RGB]{34,139,34}{(+0.6)} \\\\ \\hline\n        \\end{tabular}\n        \n    (b)\n        \\end{minipage}\n\\end{minipage}\n\\caption{Ablation study on the longer training time and overall improvements in our CropFormer.}\n\\vspace{-0.25in}\n\\label{abl_2}\n\\end{table}", "cell_list_gold": [{"value": "40.9", "char_index": [183, 187], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "3$\\times$"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "42.8", "char_index": [190, 194], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^\\text{e}$", "experimental settings": {"training schedule": "3$\\times$"}, "model": "CropFormer", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "45.0", "char_index": [265, 269], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"training schedule": "3$\\times$"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "45.6", "char_index": [272, 276], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"training schedule": "3$\\times$"}, "model": "CropFormer", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "22.7", "char_index": [344, 348], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "Instance Segmentation", "metric": "AP", "experimental settings": {"training schedule": "3$\\times$"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "23.2", "char_index": [351, 355], "type": "Result", "training data/set": "EntityIns", "test data/set": "EntityIns", "task": "Instance Segmentation", "metric": "AP", "experimental settings": {"training schedule": "3$\\times$"}, "model": "CropFormer", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "9.8", "char_index": [423, 426], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "Panoptic Segmentation", "metric": "PQ", "experimental settings": {"training schedule": "3$\\times$"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}, {"value": "10.4", "char_index": [429, 433], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "Panoptic Segmentation", "metric": "PQ", "experimental settings": {"training schedule": "3$\\times$"}, "model": "CropFormer", "model settings": {"backbone": "Swin-T", "pretrained weights": "COCO-E"}}]}, "2211.05776v1_table1": {"table_code": "\\begin{tabular}{c|ccc}\n             & \\cellcolor{lightgray!30} ann$_{1}$ & \\cellcolor{lightgray!30} ann$_{2}$ & \\cellcolor{lightgray!30} ann$_{3}$\\\\ \\hline\n            \\cellcolor{lightgray!30} ann$_{1}$ & - & - & -\\\\\n            \\cellcolor{lightgray!30} ann$_{2}$ & 95.4 & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{3}$ & 94.8 & 95.2 & -\\\\ \\hline\n        \\end{tabular}", "table_label": "{tab:consistency}", "table_numeric_cells": [["95.4", "95.4", 266, 270, 266, 270], ["94.8", "94.8", 331, 335, 331, 335], ["95.2", "95.2", 338, 342, 338, 342]], "text_chunk_selected": "\\author{\nLu Qi$^{1}$\\thanks{Equal contribution.},~\nJason Kuen$^{2*}$,~\nWeidong Guo$^{3*}$,~\nTiancheng Shen$^4$,~\nJiuxiang Gu$^{2}$,~\nWenbo Li$^{4}$,~\\\\\nJiaya Jia$^{4}$~\nZhe Lin$^2$,~\nMing-Hsuan Yang$^{1}$\n\\\\[0.2cm]\n$^1$The University of California, Merced~~\\\\\n$^2$Adobe Research~~\\\\\n$^3$QQ Browser Lab, Tencent~~\\\\\n$^4$The Chinese University of Hong Kong~~\n\\vspace{-0.4in}\n}\n\\maketitle\n\n\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\nMotivated by the above-discussed observations, we propose a new large-scale, high-quality dataset for entity segmentation. Our dataset has three important characteristics: \n1) we collect about 33,227 images from public datasets and the Internet. These images cover various domains such as landscapes, outdoor/indoor scenes, cartoons, graphical illustrations, and computer games; \n2) the annotations are open-world without any predefined class restrictions and densely cover\nthe entire images; \n3) we elevate the segmentation quality objective of entity segmentation~\\cite{qi2021open} to the next level by annotating high-quality masks with fine-grained details on high-resolution images. There are over 70\\% images that \nthat have high image resolutions, spanning from \n2K to 15K, with significantly more accurate boundary annotations compared to existing panoptic datasets. \nAs shown in Figure~\\ref{fig:comp_coco}, our diversified dataset provides fine-grained segmentation masks on high-res images, along with open-world annotations on low-res images from public datasets.\n\n\\section{Related Work}\n\\paragraph{Image Segmentation Dataset}\nNumerous datasets have been proposed for semantic, instance, and panoptic segmentation,~\\emph{e.g}., Microsoft COCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene}, KITTI~\\cite{geiger2012we}, LVIS~\\cite{gupta2019lvis}, PASCAL-VOC~\\cite{everingham2010pascal}, Open-Images~\\cite{kuznetsova2020open},  Cityscapes~\\cite{cordts2016cityscapes},~\\emph{etc}. \nIn addition, there are some datasets designed for specific scenarios, such as amodal segmentation (COCO-Amodal~\\cite{zhu2017semantic}, KINS~\\cite{qi2019amodal}), human segmentation (CelebAMask-HQ~\\cite{CelebAMask-HQ}, LIP~\\cite{gong2017look}, MHP~\\cite{zhao2018understanding}) and domain adaptation (Synscapes~\\cite{wrenninge2018synscapes}, GTA5~\\cite{richter2016playing}). \nDespite significant contributions from these datasets, it is of great interest to fulfill the needs of real-world applications with high-quality images with large diversity. \nFor example, a segmentation model should be robust to high-resolution images from different domains. \nMeanwhile, models should be able to segment unseen objects in the open-world setting. \nMost relevant to our work is the  ADE20K dataset~\\cite{zhou2017scene}, which has large-scale open-vocabulary categories. \nHowever, the collected images in ADE20K are of low resolution and from narrowed domains. \nIn our dataset, we collect the images from multiple image domains, including indoor, outdoor, street scenes, and even cartoon and remote image domains. \nIn addition, over 80\\% of the images resolution\nfall within the high-resolution range of 2000px and 8000 px. Compared to the ADE20K~\\cite{zhou2017places} and LVIS~\\cite{gupta2019lvis} dataset with a predefined category list, our annotation process is different.\nWe first conduct class-agnostic mask annotation on each entity. After that, the category information is then labeled in an open-vocabulary manner.\n\nThe EntitySeg dataset contains 33,227 images with high-quality mask annotations. \nCompared with existing dataets, there are three distinct properties in EntitySeg. \nFirst, 71.25\\% and 86.23\\% of the images are of high resolution with at least 2000px$\\times$2000px and 1000px$\\times$1000px which is more consistent with current digital imaging trends.\nSecond, our dataset is open-world and is not limited to predefined classes. We regard \nevery semantically-coherent region in the images as an entity, even \nif it is a blurred region or it cannot be semantically recognized easily. Third, the mask annotation along the boundaries are more accurate than existing datasets, as shown in Figure~\\ref{fig:comp_coco}. \nIn the following, we describe our EntitySeg dataset and provide  comprehensive analysis.\n\n\\subsection{Image Collection}\nInspired by the collection criterion of COCO~\\cite{lin2014microsoft}, we collect most non-iconic images with more contextual information and non-canonical viewpoints. \nIn addition, the image domains should be as diversified as possible to guarantee substantial domain diversity. Therefore, the image sources of our dataset include several public datasets and the Internet where the images permitted for academic research use. For public dataset sources, we select part of the images from\nCOCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene}, \nPascal VOC~\\cite{everingham2010pascal}, Cityscapes~\\cite{cordts2016cityscapes}, \nMapillary Vistas~\\cite{neuhold2017mapillary}, ImageNet~\\cite{krizhevsky2017imagenet}, \nOpen Images~\\cite{kuznetsova2020open}, DIV2K~\\cite{agustsson2017ntire}, Flick2K~\\cite{wang2019flickr1024}, Clipart~\\cite{inoue2018cross},\nComic~\\cite{inoue2018cross}, DOTA~\\cite{xia2018dota} and some computer game screenshots from Synscapes~\\cite{wrenninge2018synscapes} and GTA5~\\cite{richter2016playing}. From the Internet, we crawled mainly high-resolution images from Pixabay~\\cite{pixabay}, Unsplash~\\cite{Unsplash}, and LAION-400M~\\cite{schuhmann2021laion}. In Figure~\\ref{fig:statistics_01}(c), we show the distribution of image sources in our dataset and see that most of the images are from high-resolution sources like Pixabay, Unsplash, and LAION-400M. In Figure~\\ref{fig:statistics_01}(a), our dataset has more high-resolution images whose resolutions are normally distributed in the resolution range between 0px to 15,000px, whereas all the images of ADE20K~\\cite{zhou2017scene} and COCO~\\cite{lin2014microsoft} are under 1000px.\n\n\\subsection{Image Annotation}\nThe annotation process for our dataset mainly consists of three steps. For an image, we firstly annotate all entities with class-agnostic pixel-level masks where each mask has no overlaps with others. After that, each mask is annotated with a class label based on a large vocabulary class set, which is not fixed but updated continuously over time. There are two special considerations here.\nOn one hand, we annotate the entity as `unknown' or `blurred' if it cannot be name easily or severely blurred. On the other hand, the entity is annotated as a  `\\textit{supercategory}\\_other' if we merely know it is a \\textit{supercategory} but unable to identify its fine-grained class. Our annotation process is the direct opposite of those of the popular segmentation datasets like COCO~\\cite{lin2014microsoft} and ADE20K~\\cite{zhou2017places}, where class sets are predefined first before annotating masks based on the predefined classes. In Table~\\ref{Tab:comp_entity_other}, it shows that our dataset has \na greater proportion of covered image areas and number of masks on average than COCO- and ADE20K-Panoptic. This is by virtue of our novel annotation process that allows us to take into account all the semantically-coherent regions, and then assign class labels to them. \nIn addition, our annotation process is more similar to the the human vision system. This is consistent with human vision system that is inherently class-agnostic and is able to recognize entities without understanding its use and purpose, as explained by Marr~\\cite{Marr1982Vision}.\n\nAnnotation consistency is a crucial aspect of any human-labeled dataset since it tells whether the annotation task is well-defined and reasonable. We randomly selected 500 images from our dataset ($1.5\\%$ of the entire dataset) and asked another two annotators to \nseparately annotate them again after four months. Table~\\ref{tab:consistency}(a) shows the class-agnostic mask mAP in the first step of the annotation process compared to those of the other two annotators. We use one as ground truth, and another one as prediction results. \nIn addition, we evaluate the category consistency by accuracy (ACC) under the same mask annotations in Table~\\ref{tab:consistency}(b). \nThese two tables indicate our dataset has a high degree of annotation consistency both in the mask and category labeling stages.", "table_source": "\\begin{table}[t!]\n\\begin{minipage}{\\textwidth}\n\\begin{minipage}[t]{0.21\\textwidth}\n            \\centering\n            \\footnotesize\n            \\setlength{\\tabcolsep}{2pt}\n            \\begin{tabular}{c|ccc}\n             & \\cellcolor{lightgray!30} ann$_{1}$ & \\cellcolor{lightgray!30} ann$_{2}$ & \\cellcolor{lightgray!30} ann$_{3}$\\\\ \\hline\n            \\cellcolor{lightgray!30} ann$_{1}$ & - & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{2}$ & 90.6 & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{3}$ & 91.2 & 92.1 & -\\\\ \\hline\n            \\end{tabular}\n        \n        (a)\n        \\end{minipage}\n        \\begin{minipage}[t]{0.30\\textwidth}\n        \\centering\n        \\footnotesize\n        \\setlength{\\tabcolsep}{2pt}\n        \\begin{tabular}{c|ccc}\n             & \\cellcolor{lightgray!30} ann$_{1}$ & \\cellcolor{lightgray!30} ann$_{2}$ & \\cellcolor{lightgray!30} ann$_{3}$\\\\ \\hline\n            \\cellcolor{lightgray!30} ann$_{1}$ & - & - & -\\\\\n            \\cellcolor{lightgray!30} ann$_{2}$ & 95.4 & - & -\\\\ \n            \\cellcolor{lightgray!30} ann$_{3}$ & 94.8 & 95.2 & -\\\\ \\hline\n        \\end{tabular}\n        \n    (b)\n        \\end{minipage}\n\\end{minipage}\n\\caption{Annotation consistency of class-agnostic localization and class-aware categories among annotators.}\n\\vspace{-0.1in}\n\\label{tab:consistency}\n\\end{table}", "cell_list_gold": [{"value": "95.4", "char_index": [266, 270], "type": "Other"}, {"value": "94.8", "char_index": [331, 335], "type": "Other"}, {"value": "95.2", "char_index": [338, 342], "type": "Other"}]}, "2211.05776v1_table3": {"table_code": "\\begin{table}[t!]\n\\addtolength{\\tabcolsep}{-2pt}\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|c|ccc}\n\\cellcolor{lightgray!30} Model & \\cellcolor{lightgray!30} Backbone & \\cellcolor{lightgray!30} Iteration & \\cellcolor{lightgray!30} Pretrain  & \\cellcolor{lightgray!30} AP$^{e}$ & \\cellcolor{lightgray!30} AP$^{e}_{50}$ & \\cellcolor{lightgray!30} AP$^{e}_{75}$ \\\\ \\hline\n\\multirow{2}*{Mask-RCNN~\\cite{he2017mask}} & \\multirow{2}*{Swin-T} & \\multirow{2}*{1$\\times$} & ImageNet & 24.9 & 45.8 & 24.1 \\\\\n& & & COCO-E & 28.4 & 49.2 & 28.1 \\\\ \\hline\n\\multirow{2}*{EntityFramework~\\cite{qi2021open}} & \\multirow{2}*{Swin-T} &\\multirow{2}*{1$\\times$} & ImageNet & 26.0 & 42.8 & 25.7 \\\\\n& & & COCO-E & 29.9 & 47.6 & 30.1 \\\\ \\hline\n\\multirow{6}*{Mask2Former~\\cite{cheng2022masked}} & \\multirow{5}*{Swin-T} & \\multirow{2}*{1$\\times$} & ImageNet & 33.2 & 50.2 & 33.1 \\\\ \\cline{4-4}\n& & & \\multirow{6}*{COCO-E} & 39.5 & 56.9 & 40.2 \\\\ \\cline{3-3}\n& & 2$\\times$ & & 40.2 & 57.6 & 41.1 \\\\\n& & 3$\\times$ & & 40.9 & 58.1 & 41.6 \\\\\n& & 4$\\times$ & & 40.9 & 57.9  & 41.9 \\\\ \\cline{2-3}\n& Swin-L & 3$\\times$ & & 46.2 & 63.7 & 47.5 \\\\ \\hline\n\\end{tabular}\n\\caption{Entity segmentation benchmark in Entity Dataset. The column of `Pretrain' indicates the pretraining weights we used where the `ImageNet' is ImageNet pretraining and `COCO-E' refers to pretraining on COCO dataset that has been converted to class-agnostic entity segmentation format \\cite{qi2021open}.}\n\\vspace{-0.2in}\n\\label{Tab:aba_benchmark_entity}\n\\end{table}", "table_label": "{Tab:aba_benchmark_entity}", "table_numeric_cells": [["24.9", "24.9", 477, 481, 477, 481], ["45.8", "45.8", 484, 488, 484, 488], ["24.1", "24.1", 491, 495, 491, 495], ["28.4", "28.4", 514, 518, 514, 518], ["49.2", "49.2", 521, 525, 521, 525], ["28.1", "28.1", 528, 532, 528, 532], ["26.0", "26.0", 655, 659, 655, 659], ["42.8", "42.8", 662, 666, 662, 666], ["25.7", "25.7", 669, 673, 669, 673], ["29.9", "29.9", 692, 696, 692, 696], ["47.6", "47.6", 699, 703, 699, 703], ["30.1", "30.1", 706, 710, 706, 710], ["33.2", "33.2", 835, 839, 835, 839], ["50.2", "50.2", 842, 846, 842, 846], ["33.1", "33.1", 849, 853, 849, 853], ["39.5", "39.5", 899, 903, 899, 903], ["56.9", "56.9", 906, 910, 906, 910], ["40.2", "40.2", 913, 917, 913, 917], ["40.2", "40.2", 951, 955, 951, 955], ["57.6", "57.6", 958, 962, 958, 962], ["41.1", "41.1", 965, 969, 965, 969], ["40.9", "40.9", 991, 995, 991, 995], ["58.1", "58.1", 998, 1002, 998, 1002], ["41.6", "41.6", 1005, 1009, 1005, 1009], ["40.9", "40.9", 1031, 1035, 1031, 1035], ["57.9", "57.9", 1038, 1042, 1038, 1042], ["41.9", "41.9", 1046, 1050, 1046, 1050], ["46.2", "46.2", 1091, 1095, 1091, 1095], ["63.7", "63.7", 1098, 1102, 1098, 1102], ["47.5", "47.5", 1105, 1109, 1105, 1109]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\vspace{-4mm}\n\\paragraph{Scene Parsing Methods}\nScene parsing methods are mainly developed by convolution-based dense prediction or transformer-based query prediction. \nThe method based on convolution-based dense prediction often uses explicit localization information like bounding box proposals or pixel position for pixel-level mask prediction.\nThese approaches include\nFCN~\\cite{long2015fully}, DeepLab~\\cite{chen2017deeplab}, PSPNet~\\cite{zhao2017pyramid}, Mask R-CNN~\\cite{he2017mask}, PANet~\\cite{liu2018path}, SOLO~\\cite{wang2019solo}, CondInst~\\cite{tian2020conditional}, PanopticFPN~\\cite{kirillov2019panopticfpn}, and PanopticFCN~\\cite{li2022fully}. \nMotivated by the success of transformers DETR~\\cite{carion2020end} for detection tasks~\\cite{lin2017feature,lin2017focal,dai2017deformable,qi2021multi}, numerous methods, e.g., Max-Deeplab~\\cite{wang2021max} and Mask2Former~\\cite{cheng2022masked}, directly predict segmentation masks without the guidance of bounding boxes. \nThe key to these methods is to use learnable queries to model the implicit location of the potential instance area.\nAlthough query-based methods are effective, they cannot benefit from TTA which is commonly used to boost the accuracy of convolution-based methods.\nSpecifically, it is challenging for a query-based method to ensemble multiple results from augmented image views, due to the lack of exact location for queries.\nMotivated by video-level Mask2Former, we endow the queries with ability to connect the same entities among different image views. \nAs such, our approach can enhance high-resolution image segmentation by combining results from full image and local crops.\n\n\\subsection{Mask2Former Preliminaries}\nMask2Former~\\cite{cheng2022masked} is a universal Transformer-based framework for image or video instance segmentation. Given an input, Mask2Former uses a Transformer decoder to decode $N$ number of $K$-dimensional queries $\\mathbf{Q}\\in \\mathbb{R}^{N\\times K}$ to generate embeddings $\\mathbf{E} \\in \\mathbb{R}^{N \\times 1 \\times 1 \\times 1 \\times K}$ to predict $N$ segmentation masks, where the \\nth{2}, \\nth{3}, and \\nth{4} dimensions of $\\mathbf{E}$ correspond to temporal ($T$), output height ($H$), and output width  ($W$) dimensions respectively. When applying the image-level Mask2Former to an image input, $\\mathbf{E}$ is broadcasted along spatial dimensions to the shape of $N \\times 1 \\times H \\times W \\times K$. Alternatively, when applying video-level Mask2Former to a video input, $\\mathbf{E}$ is broadcasted to the shape of $N \\times T \\times H \\times W \\times K$. Note that in video-level Mask2Former, $\\mathbf{E}$ is further broadcasted along the both temporal and spatial dimensions, enabling Mask2Former to utilize the same embeddings to represent the same visual instances across different frames consistently.\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Image Encoder and Decoder} Based on learnable image-level queries $\\mathbf{Q}_{i} \\in \\mathbb{R}^{N\\times K}$,  we use image encoder $\\Theta$ and decoder $\\Phi_{i}$ to generate image-level embeddings $\\mathbf{E_i} \\in \\mathbb{R}^{N \\times 2 \\times 1 \\times 1 \\times K}$ for the full input image and its crop. \nFor the image encoder and decoder, we directly adopt the ones in standard image-level Mask2Former~\\cite{cheng2022masked} and do not modify their structures or designs. $\\mathbf{E_i}$ is obtained as follows:\n\nSimilar to video-level Mask2Former, the full image and its crop can be seen as 2 video frames. Thus, we broadcast $\\mathbf{E_b}$ to the shape of $N \\times 2 \\times 1 \\times 1 \\times K$ to share it between the full image and its crop. $\\mathbf{E_b}$ and batch-level pyramid features $\\mathbf{P_b^h}\\in\\mathbb{R}^{1\\times2\\times H^h\\times W^h}$ (reshaped from $\\mathbf{P_i^h}\\in\\mathbb{R}^{2\\times1\\times H^h\\times W^h}$) are used for batch-level mask classification and pixel-wise prediction:\n\n\\subsection{Entity Dataset Benchmark}\n\\label{subsec:entity_dataset_benchmark}\n\\paragraph{Entity Segmentation} We divide the EntitySeg dataset into train and test sets, which have 31,913 and 1,314 images. To obtain a less-biased dataset split, we constructed 20 random dataset splits. We selected the dataset split that is statistically closest to the \nall 20 splits in terms of entity segmentation accuracy, as detailed in the supplementary file.Furthermore, we use class-agnostic metric AP$^\\text{e}$~\\cite{qi2021open} with a strict non-overlapping mask constraint to evaluate the entity segmentation task.\n\nTable~\\ref{Tab:aba_benchmark_entity} shows the benchmark of our dataset with Mask-RCNN~\\cite{he2017mask}, EntityFramework~\\cite{qi2021open}, Mask2Former~\\cite{cheng2022masked}. The first two methods are convolution-based dense prediction methods, whereas the last is a transformer-based query prediction method. In this table, the transformer-based Mask2Former is better than the other two convolution-based methods, demonstrating the advantage of transform-based methods on high-quality mask generation, Moreover, the COCO-E pretrained weights can further boost entity segmentation performance. We also explored the optimal training iterations needed by Mask2Former and found that it performs the best with 3$\\times$ training schedule.", "table_source": "\\begin{table}[t!]\n\\addtolength{\\tabcolsep}{-2pt}\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|c|ccc}\n\\cellcolor{lightgray!30} Model & \\cellcolor{lightgray!30} Backbone & \\cellcolor{lightgray!30} Iteration & \\cellcolor{lightgray!30} Pretrain  & \\cellcolor{lightgray!30} AP$^{e}$ & \\cellcolor{lightgray!30} AP$^{e}_{50}$ & \\cellcolor{lightgray!30} AP$^{e}_{75}$ \\\\ \\hline\n\\multirow{2}*{Mask-RCNN~\\cite{he2017mask}} & \\multirow{2}*{Swin-T} & \\multirow{2}*{1$\\times$} & ImageNet & 24.9 & 45.8 & 24.1 \\\\\n& & & COCO-E & 28.4 & 49.2 & 28.1 \\\\ \\hline\n\\multirow{2}*{EntityFramework~\\cite{qi2021open}} & \\multirow{2}*{Swin-T} &\\multirow{2}*{1$\\times$} & ImageNet & 26.0 & 42.8 & 25.7 \\\\\n& & & COCO-E & 29.9 & 47.6 & 30.1 \\\\ \\hline\n\\multirow{6}*{Mask2Former~\\cite{cheng2022masked}} & \\multirow{5}*{Swin-T} & \\multirow{2}*{1$\\times$} & ImageNet & 33.2 & 50.2 & 33.1 \\\\ \\cline{4-4}\n& & & \\multirow{6}*{COCO-E} & 39.5 & 56.9 & 40.2 \\\\ \\cline{3-3}\n& & 2$\\times$ & & 40.2 & 57.6 & 41.1 \\\\\n& & 3$\\times$ & & 40.9 & 58.1 & 41.6 \\\\\n& & 4$\\times$ & & 40.9 & 57.9  & 41.9 \\\\ \\cline{2-3}\n& Swin-L & 3$\\times$ & & 46.2 & 63.7 & 47.5 \\\\ \\hline\n\\end{tabular}\n\\caption{Entity segmentation benchmark in Entity Dataset. The column of `Pretrain' indicates the pretraining weights we used where the `ImageNet' is ImageNet pretraining and `COCO-E' refers to pretraining on COCO dataset that has been converted to class-agnostic entity segmentation format \\cite{qi2021open}.}\n\\vspace{-0.2in}\n\\label{Tab:aba_benchmark_entity}\n\\end{table}", "cell_list_gold": [{"value": "24.9", "char_index": [477, 481], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "45.8", "char_index": [484, 488], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "24.1", "char_index": [491, 495], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "28.4", "char_index": [514, 518], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "49.2", "char_index": [521, 525], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "28.1", "char_index": [528, 532], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask-RCNN", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "26.0", "char_index": [655, 659], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "EntityFramework", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "42.8", "char_index": [662, 666], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "EntityFramework", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "25.7", "char_index": [669, 673], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "EntityFramework", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "29.9", "char_index": [692, 696], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "EntityFramework", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "47.6", "char_index": [699, 703], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "EntityFramework", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "30.1", "char_index": [706, 710], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "EntityFramework", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "33.2", "char_index": [835, 839], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "50.2", "char_index": [842, 846], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "33.1", "char_index": [849, 853], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "ImageNet"}}, {"value": "39.5", "char_index": [899, 903], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "56.9", "char_index": [906, 910], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "40.2", "char_index": [913, 917], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "1$\\times$", "pretrain": "COCO-E"}}, {"value": "40.2", "char_index": [951, 955], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "2$\\times$", "pretrain": "COCO-E"}}, {"value": "57.6", "char_index": [958, 962], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "2$\\times$", "pretrain": "COCO-E"}}, {"value": "41.1", "char_index": [965, 969], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "2$\\times$", "pretrain": "COCO-E"}}, {"value": "40.9", "char_index": [991, 995], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "3$\\times$", "pretrain": "COCO-E"}}, {"value": "58.1", "char_index": [998, 1002], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "3$\\times$", "pretrain": "COCO-E"}}, {"value": "41.6", "char_index": [1005, 1009], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "3$\\times$", "pretrain": "COCO-E"}}, {"value": "40.9", "char_index": [1031, 1035], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "4$\\times$", "pretrain": "COCO-E"}}, {"value": "57.9", "char_index": [1038, 1042], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "4$\\times$", "pretrain": "COCO-E"}}, {"value": "41.9", "char_index": [1046, 1050], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "iteration": "4$\\times$", "pretrain": "COCO-E"}}, {"value": "46.2", "char_index": [1091, 1095], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "iteration": "3$\\times$", "pretrain": "COCO-E"}}, {"value": "63.7", "char_index": [1098, 1102], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "iteration": "3$\\times$", "pretrain": "COCO-E"}}, {"value": "47.5", "char_index": [1105, 1109], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "iteration": "3$\\times$", "pretrain": "COCO-E"}}]}, "2211.05776v1_table4": {"table_code": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|c}\n\\cellcolor{lightgray!30} Model & \\cellcolor{lightgray!30} Backbone & \\cellcolor{lightgray!30} Pretrain & \\cellcolor{lightgray!30} mIoU \\\\ \\hline\nDeeplabV3~\\cite{chen2017rethinking} & R-50 & ImageNet & 27.9  \\\\ \\hline\n\\multirow{6}*{Mask2Former~\\cite{cheng2022masked}} & \\multirow{2}*{R-50} & ImageNet & 37.8 \\\\ \n& & COCO-P & 43.3  \\\\ \\cline{2-4}\n& \\multirow{2}*{Swin-T} & COCO-E & 43.0  \\\\ \n& & COCO-P & 45.0 \\\\ \\cline{2-4}\n& \\multirow{2}*{Swin-L} & COCO-E & 50.7  \\\\ \n& & COCO-P & 50.5 \\\\ \\hline\n\\end{tabular}\n\\caption{Benchmark on class-aware semantic segmentation in EntitySem Dataset. The `COCO-P' and `COCO-E' indicate weights trained in\nthe COCO datasets with panoptic and entity\nsegmentation tasks. }\n\\vspace{-0.1in}\n\\label{Tab:aba_benchmark_classaware_sem}\n\\end{table}", "table_label": "{Tab:aba_benchmark_classaware_sem}", "table_numeric_cells": [["27.9", "27.9", 267, 271, 267, 271], ["37.8", "37.8", 368, 372, 368, 372], ["43.3", "43.3", 390, 394, 390, 394], ["43.0", "43.0", 446, 450, 446, 450], ["45.0", "45.0", 469, 473, 469, 473], ["50.7", "50.7", 524, 528, 524, 528], ["50.5", "50.5", 547, 551, 547, 551]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\section{Introduction}\nDense image segmentation is an important topic in computer vision with a wide range of applications such as autonomous driving~\\cite{qi2019amodal}, robotics~\\cite{shu2014human}, video surveillance~\\cite{morrison2018cartman}, and image editing tools. \nRecently, entity segmentation~\\cite{qi2021open} has been introduced as a promising dense instance-aware approach for tackling generalization issues caused by narrow image domains and/or predefined class sets in existing panoptic~\\cite{kirillov2019panoptic,kirillov2019panopticfpn,xiong2019upsnet,li2020fully,carion2020end} \nand semantic~\\cite{long2015fully,zhao2017pyramid,chen2017rethinking,chen2018encoder,zhao2018psanet,huang2019ccnet,liu2019auto} segmentation tasks.\nAlthough the early method~\\cite{qi2021open} shows some promising results, it merely borrows existing panoptic segmentation datasets to demonstrate the feasibility and benefits of the entity segmentation. \nHowever, existing panoptic segmentation datasets (\\textit{e.g,} COCO~\\cite{lin2014microsoft}, Cityscapes~\\cite{cordts2016cityscapes}, ADE20K~\\cite{zhou2017scene}) \nsuffer from the same issues that entity segmentation~\\cite{qi2021open} aims to avoid, since those datasets are collected and annotated with \npanoptic segmentation task requirements in mind.\n\n\\subsection{Image Collection}\nInspired by the collection criterion of COCO~\\cite{lin2014microsoft}, we collect most non-iconic images with more contextual information and non-canonical viewpoints. \nIn addition, the image domains should be as diversified as possible to guarantee substantial domain diversity. Therefore, the image sources of our dataset include several public datasets and the Internet where the images permitted for academic research use. For public dataset sources, we select part of the images from\nCOCO~\\cite{lin2014microsoft}, ADE20K~\\cite{zhou2017scene}, \nPascal VOC~\\cite{everingham2010pascal}, Cityscapes~\\cite{cordts2016cityscapes}, \nMapillary Vistas~\\cite{neuhold2017mapillary}, ImageNet~\\cite{krizhevsky2017imagenet}, \nOpen Images~\\cite{kuznetsova2020open}, DIV2K~\\cite{agustsson2017ntire}, Flick2K~\\cite{wang2019flickr1024}, Clipart~\\cite{inoue2018cross},\nComic~\\cite{inoue2018cross}, DOTA~\\cite{xia2018dota} and some computer game screenshots from Synscapes~\\cite{wrenninge2018synscapes} and GTA5~\\cite{richter2016playing}. From the Internet, we crawled mainly high-resolution images from Pixabay~\\cite{pixabay}, Unsplash~\\cite{Unsplash}, and LAION-400M~\\cite{schuhmann2021laion}. In Figure~\\ref{fig:statistics_01}(c), we show the distribution of image sources in our dataset and see that most of the images are from high-resolution sources like Pixabay, Unsplash, and LAION-400M. In Figure~\\ref{fig:statistics_01}(a), our dataset has more high-resolution images whose resolutions are normally distributed in the resolution range between 0px to 15,000px, whereas all the images of ADE20K~\\cite{zhou2017scene} and COCO~\\cite{lin2014microsoft} are under 1000px.\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Image Encoder and Decoder} Based on learnable image-level queries $\\mathbf{Q}_{i} \\in \\mathbb{R}^{N\\times K}$,  we use image encoder $\\Theta$ and decoder $\\Phi_{i}$ to generate image-level embeddings $\\mathbf{E_i} \\in \\mathbb{R}^{N \\times 2 \\times 1 \\times 1 \\times K}$ for the full input image and its crop. \nFor the image encoder and decoder, we directly adopt the ones in standard image-level Mask2Former~\\cite{cheng2022masked} and do not modify their structures or designs. $\\mathbf{E_i}$ is obtained as follows:\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Similar to video-level Mask2Former~\\cite{cheng2021mask2former}, the association module aims to generate batch queries $\\mathbf{Q_b}$ that are fully shared by the full image and its crop to represent the same entities consistently. Instead of treating $\\mathbf{Q_b}$ as learnable network parameters, here we generate $\\mathbf{Q_b}$ directly from $\\mathbf{E_i}=\\{\\mathbf{E}_{I^o}, \\mathbf{E}_{I^c})\\}$, considering that $\\mathbf{E_i}$ already contains strong segmentation features. \nIn particular, we use a Transformer architecture with cross attention (XAtt), self-attention (SAtt), and feedforward network (FFN) to obtain $\\mathbf{Q_b}$:\n\nTable~\\ref{Tab:aba_benchmark_entity} shows the benchmark of our dataset with Mask-RCNN~\\cite{he2017mask}, EntityFramework~\\cite{qi2021open}, Mask2Former~\\cite{cheng2022masked}. The first two methods are convolution-based dense prediction methods, whereas the last is a transformer-based query prediction method. In this table, the transformer-based Mask2Former is better than the other two convolution-based methods, demonstrating the advantage of transform-based methods on high-quality mask generation, Moreover, the COCO-E pretrained weights can further boost entity segmentation performance. We also explored the optimal training iterations needed by Mask2Former and found that it performs the best with 3$\\times$ training schedule.\n\n\\vspace{-4mm}\n\\paragraph{Semantic Segmentation} We choose 150 categories with the highest pixel-level frequency as EntitySem for semantic segmentation. EntitySem has 9,729 and 1,444 images for training and testing, respectively. In Table~\\ref{Tab:aba_benchmark_classaware_sem}, we evaluate the two popular semantic segmentation methods DeeplabV3~\\cite{chen2017deeplab} and Mask2Former~\\cite{cheng2022masked} on EntitySem. Semantic segmentation performance is still related to the pretraining weights and network structure. Mask2Former with Swin-L backbone and COCO-E pretrained weights obtains the best mIoU of 50.5 on EntitySem. ", "table_source": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|c}\n\\cellcolor{lightgray!30} Model & \\cellcolor{lightgray!30} Backbone & \\cellcolor{lightgray!30} Pretrain & \\cellcolor{lightgray!30} mIoU \\\\ \\hline\nDeeplabV3~\\cite{chen2017rethinking} & R-50 & ImageNet & 27.9  \\\\ \\hline\n\\multirow{6}*{Mask2Former~\\cite{cheng2022masked}} & \\multirow{2}*{R-50} & ImageNet & 37.8 \\\\ \n& & COCO-P & 43.3  \\\\ \\cline{2-4}\n& \\multirow{2}*{Swin-T} & COCO-E & 43.0  \\\\ \n& & COCO-P & 45.0 \\\\ \\cline{2-4}\n& \\multirow{2}*{Swin-L} & COCO-E & 50.7  \\\\ \n& & COCO-P & 50.5 \\\\ \\hline\n\\end{tabular}\n\\caption{Benchmark on class-aware semantic segmentation in EntitySem Dataset. The `COCO-P' and `COCO-E' indicate weights trained in\nthe COCO datasets with panoptic and entity\nsegmentation tasks. }\n\\vspace{-0.1in}\n\\label{Tab:aba_benchmark_classaware_sem}\n\\end{table}", "cell_list_gold": [{"value": "27.9", "char_index": [267, 271], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"xx": "yy"}, "model": "DeeplabV3", "model settings": {"Backbone": "R-50", "Pretrain": "ImageNet"}}, {"value": "37.8", "char_index": [368, 372], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"Backbone": "R-50", "Pretrain": "ImageNet"}}, {"value": "43.3", "char_index": [390, 394], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"Backbone": "R-50", "Pretrain": "COCO-P"}}, {"value": "43.0", "char_index": [446, 450], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"Backbone": "Swin-T", "Pretrain": "COCO-E"}}, {"value": "45.0", "char_index": [469, 473], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"Backbone": "Swin-T", "Pretrain": "COCO-P"}}, {"value": "50.7", "char_index": [524, 528], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"Backbone": "Swin-L", "Pretrain": "COCO-E"}}, {"value": "50.5", "char_index": [547, 551], "type": "Result", "training data/set": "EntitySem", "test data/set": "EntitySem", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"Backbone": "Swin-L", "Pretrain": "COCO-P"}}]}, "2211.05776v1_table6": {"table_code": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|ccc}\n\\cellcolor{lightgray!30} Model & \\cellcolor{lightgray!30} Backbone & \\cellcolor{lightgray!30} Pretrain & \\cellcolor{lightgray!30} PQ & \\cellcolor{lightgray!30} SQ & \\cellcolor{lightgray!30} RQ \\\\ \\hline\n\\multirow{2}*{PanopticFPN~\\cite{kirillov2019panopticfpn}} &\n\\multirow{2}*{R-50} & ImageNet & 3.6 & 25.8 & 5.5   \\\\\n& & COCO-P & 6.7 & 36.4 & 10.1  \\\\ \\hline\n\\multirow{7}*{Mask2Former~\\cite{cheng2022masked}} & \\multirow{2}*{R-50} & ImageNet & 5.5 & 26.3 & 8.2 \\\\ \n& & COCO-I & 9.6 & 39.0 & 14.1 \\\\ \\cline{2-6}\n& \\multirow{2}*{Swin-T} & COCO-E & 7.4 & 32.6 & 11.0  \\\\ \n& & COCO-P & 9.8 & 38.5 & 14.6 \\\\ \\cline{2-6}\n& \\multirow{2}*{Swin-L} & COCO-E & 11.7 & 43.2 & 17.3 \\\\ \n& & COCO-P & 13.4 & 48.7 & 19.9 \\\\ \\hline\n\\end{tabular}\n\\caption{Benchmark on class-aware panoptic segmentation in Entity Dataset.}\n\\vspace{-0.2in}\n\\label{Tab:aba_benchmark_classaware_panop}\n\\end{table}", "table_label": "{Tab:aba_benchmark_classaware_panop}", "table_numeric_cells": [["3.6", "3.6", 364, 367, 364, 367], ["25.8", "25.8", 370, 374, 370, 374], ["5.5", "5.5", 377, 380, 377, 380], ["6.7", "6.7", 399, 402, 399, 402], ["36.4", "36.4", 405, 409, 405, 409], ["10.1", "10.1", 412, 416, 412, 416], ["5.5", "5.5", 513, 516, 513, 516], ["26.3", "26.3", 519, 523, 519, 523], ["8.2", "8.2", 526, 529, 526, 529], ["9.6", "9.6", 547, 550, 547, 550], ["39.0", "39.0", 553, 557, 553, 557], ["14.1", "14.1", 560, 564, 560, 564], ["7.4", "7.4", 615, 618, 615, 618], ["32.6", "32.6", 621, 625, 621, 625], ["11.0", "11.0", 628, 632, 628, 632], ["9.8", "9.8", 651, 654, 651, 654], ["38.5", "38.5", 657, 661, 657, 661], ["14.6", "14.6", 664, 668, 664, 668], ["11.7", "11.7", 719, 723, 719, 723], ["43.2", "43.2", 726, 730, 726, 730], ["17.3", "17.3", 733, 737, 733, 737], ["13.4", "13.4", 755, 759, 755, 759], ["48.7", "48.7", 762, 766, 762, 766], ["19.9", "19.9", 769, 773, 769, 773]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\section{Introduction}\nDense image segmentation is an important topic in computer vision with a wide range of applications such as autonomous driving~\\cite{qi2019amodal}, robotics~\\cite{shu2014human}, video surveillance~\\cite{morrison2018cartman}, and image editing tools. \nRecently, entity segmentation~\\cite{qi2021open} has been introduced as a promising dense instance-aware approach for tackling generalization issues caused by narrow image domains and/or predefined class sets in existing panoptic~\\cite{kirillov2019panoptic,kirillov2019panopticfpn,xiong2019upsnet,li2020fully,carion2020end} \nand semantic~\\cite{long2015fully,zhao2017pyramid,chen2017rethinking,chen2018encoder,zhao2018psanet,huang2019ccnet,liu2019auto} segmentation tasks.\nAlthough the early method~\\cite{qi2021open} shows some promising results, it merely borrows existing panoptic segmentation datasets to demonstrate the feasibility and benefits of the entity segmentation. \nHowever, existing panoptic segmentation datasets (\\textit{e.g,} COCO~\\cite{lin2014microsoft}, Cityscapes~\\cite{cordts2016cityscapes}, ADE20K~\\cite{zhou2017scene}) \nsuffer from the same issues that entity segmentation~\\cite{qi2021open} aims to avoid, since those datasets are collected and annotated with \npanoptic segmentation task requirements in mind.\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Image Encoder and Decoder} Based on learnable image-level queries $\\mathbf{Q}_{i} \\in \\mathbb{R}^{N\\times K}$,  we use image encoder $\\Theta$ and decoder $\\Phi_{i}$ to generate image-level embeddings $\\mathbf{E_i} \\in \\mathbb{R}^{N \\times 2 \\times 1 \\times 1 \\times K}$ for the full input image and its crop. \nFor the image encoder and decoder, we directly adopt the ones in standard image-level Mask2Former~\\cite{cheng2022masked} and do not modify their structures or designs. $\\mathbf{E_i}$ is obtained as follows:\n\nTable~\\ref{Tab:aba_benchmark_entity} shows the benchmark of our dataset with Mask-RCNN~\\cite{he2017mask}, EntityFramework~\\cite{qi2021open}, Mask2Former~\\cite{cheng2022masked}. The first two methods are convolution-based dense prediction methods, whereas the last is a transformer-based query prediction method. In this table, the transformer-based Mask2Former is better than the other two convolution-based methods, demonstrating the advantage of transform-based methods on high-quality mask generation, Moreover, the COCO-E pretrained weights can further boost entity segmentation performance. We also explored the optimal training iterations needed by Mask2Former and found that it performs the best with 3$\\times$ training schedule.\n\n\\vspace{-4mm}\n\\paragraph{Semantic Segmentation} We choose 150 categories with the highest pixel-level frequency as EntitySem for semantic segmentation. EntitySem has 9,729 and 1,444 images for training and testing, respectively. In Table~\\ref{Tab:aba_benchmark_classaware_sem}, we evaluate the two popular semantic segmentation methods DeeplabV3~\\cite{chen2017deeplab} and Mask2Former~\\cite{cheng2022masked} on EntitySem. Semantic segmentation performance is still related to the pretraining weights and network structure. Mask2Former with Swin-L backbone and COCO-E pretrained weights obtains the best mIoU of 50.5 on EntitySem. \n\n\\vspace{-4mm}\n\\paragraph{Instance Segmentation}\nWe select 206 thing categories with the highest object-level frequency in the EntityClass dataset to benchmark instance segmentation. In the EntityIns, 8,993 and 1,498 images for training and testing, respectively. In Table~\\ref{Tab:aba_benchmark_classaware_ins}, we ablate two popular instance segmentation methods including Mask-RCNN~\\cite{he2017mask} and Mask2Former~\\cite{cheng2022masked} on EntityIns. Mask2Former with Swin-L backbone and COCO-P pretrained weights the best AP of 30.3 on EntityIns. \n\n\\vspace{-4mm}\n\\paragraph{Panoptic Segmentation} Similar to EntityIns, we select 345 categories, including 274 things and 71 stuff, with the highest entity-level frequency to construct EntityPan. In EntityPan, there are 9,968 and 1,481 images for training and testing. Table~\\ref{Tab:aba_benchmark_classaware_panop} shows the performance of two popular panoptic segmentation methods, including PanopticFPN~\\cite{kirillov2019panopticfpn} and Mask2Former~\\cite{cheng2022masked}. In this table, we see that the PQs are much lower than those of existing panoptic datasets, due to the greater variety of class labels which makes the task more challenging.\nIn addition, current panoptic methods perform worse on EntityPan compared to existing datasets, particularly on the recognition quality (RQ) side which adversely impacts PQ.", "table_source": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|ccc}\n\\cellcolor{lightgray!30} Model & \\cellcolor{lightgray!30} Backbone & \\cellcolor{lightgray!30} Pretrain & \\cellcolor{lightgray!30} PQ & \\cellcolor{lightgray!30} SQ & \\cellcolor{lightgray!30} RQ \\\\ \\hline\n\\multirow{2}*{PanopticFPN~\\cite{kirillov2019panopticfpn}} &\n\\multirow{2}*{R-50} & ImageNet & 3.6 & 25.8 & 5.5   \\\\\n& & COCO-P & 6.7 & 36.4 & 10.1  \\\\ \\hline\n\\multirow{7}*{Mask2Former~\\cite{cheng2022masked}} & \\multirow{2}*{R-50} & ImageNet & 5.5 & 26.3 & 8.2 \\\\ \n& & COCO-I & 9.6 & 39.0 & 14.1 \\\\ \\cline{2-6}\n& \\multirow{2}*{Swin-T} & COCO-E & 7.4 & 32.6 & 11.0  \\\\ \n& & COCO-P & 9.8 & 38.5 & 14.6 \\\\ \\cline{2-6}\n& \\multirow{2}*{Swin-L} & COCO-E & 11.7 & 43.2 & 17.3 \\\\ \n& & COCO-P & 13.4 & 48.7 & 19.9 \\\\ \\hline\n\\end{tabular}\n\\caption{Benchmark on class-aware panoptic segmentation in Entity Dataset.}\n\\vspace{-0.2in}\n\\label{Tab:aba_benchmark_classaware_panop}\n\\end{table}", "cell_list_gold": [{"value": "3.6", "char_index": [364, 367], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "PQ", "experimental settings": {"xx": "yy"}, "model": "PanopticFPN", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "25.8", "char_index": [370, 374], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "SQ", "experimental settings": {"xx": "yy"}, "model": "PanopticFPN", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "5.5", "char_index": [377, 380], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "RQ", "experimental settings": {"xx": "yy"}, "model": "PanopticFPN", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "6.7", "char_index": [399, 402], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "PQ", "experimental settings": {"xx": "yy"}, "model": "PanopticFPN", "model settings": {"backbone": "R-50", "pretrain": "COCO-P"}}, {"value": "36.4", "char_index": [405, 409], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "SQ", "experimental settings": {"xx": "yy"}, "model": "PanopticFPN", "model settings": {"backbone": "R-50", "pretrain": "COCO-P"}}, {"value": "10.1", "char_index": [412, 416], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "RQ", "experimental settings": {"xx": "yy"}, "model": "PanopticFPN", "model settings": {"backbone": "R-50", "pretrain": "COCO-P"}}, {"value": "5.5", "char_index": [513, 516], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "PQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "26.3", "char_index": [519, 523], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "SQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "8.2", "char_index": [526, 529], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "RQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "ImageNet"}}, {"value": "9.6", "char_index": [547, 550], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "PQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "39.0", "char_index": [553, 557], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "SQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "14.1", "char_index": [560, 564], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "RQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "R-50", "pretrain": "COCO-I"}}, {"value": "7.4", "char_index": [615, 618], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "PQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-E"}}, {"value": "32.6", "char_index": [621, 625], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "SQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-E"}}, {"value": "11.0", "char_index": [628, 632], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "RQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-E"}}, {"value": "9.8", "char_index": [651, 654], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "PQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-P"}}, {"value": "38.5", "char_index": [657, 661], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "SQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-P"}}, {"value": "14.6", "char_index": [664, 668], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "RQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-T", "pretrain": "COCO-P"}}, {"value": "11.7", "char_index": [719, 723], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "PQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-E"}}, {"value": "43.2", "char_index": [726, 730], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "SQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-E"}}, {"value": "17.3", "char_index": [733, 737], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "RQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-E"}}, {"value": "13.4", "char_index": [755, 759], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "PQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-P"}}, {"value": "48.7", "char_index": [762, 766], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "SQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-P"}}, {"value": "19.9", "char_index": [769, 773], "type": "Result", "training data/set": "EntityPan", "test data/set": "EntityPan", "task": "panoptic segmentation", "metric": "RQ", "experimental settings": {"xx": "yy"}, "model": "Mask2Former", "model settings": {"backbone": "Swin-L", "pretrain": "COCO-P"}}]}, "2211.05776v1_table7": {"table_code": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|ccc|c}\n\\cellcolor{lightgray!30} Method & \\cellcolor{lightgray!30} Decoder & \\cellcolor{lightgray!30} AP$^{e}$ & \\cellcolor{lightgray!30} AP$^{e}_{50}$ & A\\cellcolor{lightgray!30} P$^{e}_{75}$ & \\cellcolor{lightgray!30} RT (ms) \\\\ \\hline\nSS-Mask2Former & Image-O & 39.5 & 56.9 & 40.2 & 637 \\\\ \nSS-Mask2Former($\\times\\delta$) & Image-O & 39.9 & 57.4 & 40.3 & 876 \\\\ \nMS-Mask2Former & Image-O & 39.2 & 56.3 & 39.5 & 1324 \\\\ \nMS-Mask2Former & Batch-OC & 39.3 & 56.4 & 39.7 & 2783 \\\\ \\hline\n\\multirow{4}*{CropFormer} & Image-O & 39.3 & 56.7 & 39.8 & 637 \\\\ \n & Batch-O & 39.1 & 56.6 & 39.7 & 1514 \\\\ \n & Batch-C & 40.2 & 57.5 & 40.8 & 1507 \\\\\n & Batch-OC & \\textbf{41.0} & \\textbf{58.4} & \\textbf{41.9} & 1545 \\\\ \\hline\n\\end{tabular}\n\\caption{Ablation study on the ensemble strategy on full image and four crops. The `Decoder' column indicates whether we use the inference result of the full image (`O'), four cropped patches (`C'), or both of them (`OC') from the `Image' or `Batch' decoder. Here, the run-time (RT) is the time of network forward except the data processing and calculated on A100 GPU.}\n\\vspace{-0.1in}\n\\label{Tab:aba_ivresults}\n\\end{table}", "table_label": "{Tab:aba_ivresults}", "table_numeric_cells": [["39.5", "39.5", 325, 329, 325, 329], ["56.9", "56.9", 332, 336, 332, 336], ["40.2", "40.2", 339, 343, 339, 343], ["637", "637", 346, 349, 346, 349], ["39.9", "39.9", 397, 401, 397, 401], ["57.4", "57.4", 404, 408, 404, 408], ["40.3", "40.3", 411, 415, 411, 415], ["876", "876", 418, 421, 418, 421], ["39.2", "39.2", 453, 457, 453, 457], ["56.3", "56.3", 460, 464, 460, 464], ["39.5", "39.5", 467, 471, 467, 471], ["1324", "1324", 474, 478, 474, 478], ["39.3", "39.3", 511, 515, 511, 515], ["56.4", "56.4", 518, 522, 518, 522], ["39.7", "39.7", 525, 529, 525, 529], ["2783", "2783", 532, 536, 532, 536], ["39.3", "39.3", 585, 589, 585, 589], ["56.7", "56.7", 592, 596, 592, 596], ["39.8", "39.8", 599, 603, 599, 603], ["637", "637", 606, 609, 606, 609], ["39.1", "39.1", 627, 631, 627, 631], ["56.6", "56.6", 634, 638, 634, 638], ["39.7", "39.7", 641, 645, 641, 645], ["1514", "1514", 648, 652, 648, 652], ["40.2", "40.2", 670, 674, 670, 674], ["57.5", "57.5", 677, 681, 677, 681], ["40.8", "40.8", 684, 688, 684, 688], ["1507", "1507", 691, 695, 691, 695], ["41.0", "\\textbf{41.0}", 721, 725, 713, 726], ["58.4", "\\textbf{58.4}", 737, 741, 729, 742], ["41.9", "\\textbf{41.9}", 753, 757, 745, 758], ["1545", "1545", 761, 765, 761, 765]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\subsection{Mask2Former Preliminaries}\nMask2Former~\\cite{cheng2022masked} is a universal Transformer-based framework for image or video instance segmentation. Given an input, Mask2Former uses a Transformer decoder to decode $N$ number of $K$-dimensional queries $\\mathbf{Q}\\in \\mathbb{R}^{N\\times K}$ to generate embeddings $\\mathbf{E} \\in \\mathbb{R}^{N \\times 1 \\times 1 \\times 1 \\times K}$ to predict $N$ segmentation masks, where the \\nth{2}, \\nth{3}, and \\nth{4} dimensions of $\\mathbf{E}$ correspond to temporal ($T$), output height ($H$), and output width  ($W$) dimensions respectively. When applying the image-level Mask2Former to an image input, $\\mathbf{E}$ is broadcasted along spatial dimensions to the shape of $N \\times 1 \\times H \\times W \\times K$. Alternatively, when applying video-level Mask2Former to a video input, $\\mathbf{E}$ is broadcasted to the shape of $N \\times T \\times H \\times W \\times K$. Note that in video-level Mask2Former, $\\mathbf{E}$ is further broadcasted along the both temporal and spatial dimensions, enabling Mask2Former to utilize the same embeddings to represent the same visual instances across different frames consistently.\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Image Encoder and Decoder} Based on learnable image-level queries $\\mathbf{Q}_{i} \\in \\mathbb{R}^{N\\times K}$,  we use image encoder $\\Theta$ and decoder $\\Phi_{i}$ to generate image-level embeddings $\\mathbf{E_i} \\in \\mathbb{R}^{N \\times 2 \\times 1 \\times 1 \\times K}$ for the full input image and its crop. \nFor the image encoder and decoder, we directly adopt the ones in standard image-level Mask2Former~\\cite{cheng2022masked} and do not modify their structures or designs. $\\mathbf{E_i}$ is obtained as follows:\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Similar to video-level Mask2Former~\\cite{cheng2021mask2former}, the association module aims to generate batch queries $\\mathbf{Q_b}$ that are fully shared by the full image and its crop to represent the same entities consistently. Instead of treating $\\mathbf{Q_b}$ as learnable network parameters, here we generate $\\mathbf{Q_b}$ directly from $\\mathbf{E_i}=\\{\\mathbf{E}_{I^o}, \\mathbf{E}_{I^c})\\}$, considering that $\\mathbf{E_i}$ already contains strong segmentation features. \nIn particular, we use a Transformer architecture with cross attention (XAtt), self-attention (SAtt), and feedforward network (FFN) to obtain $\\mathbf{Q_b}$:\n\nwhere $f_{\\{\\text{q},\\text{k},\\text{v}\\}}$ are linear transformations. Since the full image is more crucial to entity segmentation than its crop, we take the image-level embeddings of the full image $\\mathbf{E}_{I^o}$ as the \\textit{query}, while treating all the image-level embeddings $\\mathbf{E_i}$ as and \\textit{key} and \\textit{value}. This design is similar to the cross-attention \nof original \nTransformer \\cite{vaswani2017attention} and SelfDoc \\cite{li2021selfdoc}.\n\nSimilar to video-level Mask2Former, the full image and its crop can be seen as 2 video frames. Thus, we broadcast $\\mathbf{E_b}$ to the shape of $N \\times 2 \\times 1 \\times 1 \\times K$ to share it between the full image and its crop. $\\mathbf{E_b}$ and batch-level pyramid features $\\mathbf{P_b^h}\\in\\mathbb{R}^{1\\times2\\times H^h\\times W^h}$ (reshaped from $\\mathbf{P_i^h}\\in\\mathbb{R}^{2\\times1\\times H^h\\times W^h}$) are used for batch-level mask classification and pixel-wise prediction:\n\nTable~\\ref{Tab:aba_ivresults} shows the overall improvement of CropFromer to the Mask2Former~\\cite{cheng2022masked} under 1$\\times$ training setting. The first and second row is our baseline Mask2Former with the single-scale inference (800 and 1040 shortest side) and (1333 and 1732 longest side) for the full images, where 1040=800$\\times\\delta$ and 1732=1333$\\times\\delta$ with $\\delta$ 0.7 in default. For the results in the third and fourth rows, we use test-time bipartite-matching to associate the same entities obtained with multi-scale images and crops. We find no improvements with such inference strategies. Whereas, using the crop output from CropFormer's batch decoder achieves a significant AP$^\\text{e}$ gain as indicated by the second last row. By combining the full image and four crop outputs from CropFormer (final row), we obtain even stronger 1.5 AP$^\\text{e}$ and 1.7 AP$^\\text{e}_{75}$ gain compared to the baseline (first row).", "table_source": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|ccc|c}\n\\cellcolor{lightgray!30} Method & \\cellcolor{lightgray!30} Decoder & \\cellcolor{lightgray!30} AP$^{e}$ & \\cellcolor{lightgray!30} AP$^{e}_{50}$ & A\\cellcolor{lightgray!30} P$^{e}_{75}$ & \\cellcolor{lightgray!30} RT (ms) \\\\ \\hline\nSS-Mask2Former & Image-O & 39.5 & 56.9 & 40.2 & 637 \\\\ \nSS-Mask2Former($\\times\\delta$) & Image-O & 39.9 & 57.4 & 40.3 & 876 \\\\ \nMS-Mask2Former & Image-O & 39.2 & 56.3 & 39.5 & 1324 \\\\ \nMS-Mask2Former & Batch-OC & 39.3 & 56.4 & 39.7 & 2783 \\\\ \\hline\n\\multirow{4}*{CropFormer} & Image-O & 39.3 & 56.7 & 39.8 & 637 \\\\ \n & Batch-O & 39.1 & 56.6 & 39.7 & 1514 \\\\ \n & Batch-C & 40.2 & 57.5 & 40.8 & 1507 \\\\\n & Batch-OC & \\textbf{41.0} & \\textbf{58.4} & \\textbf{41.9} & 1545 \\\\ \\hline\n\\end{tabular}\n\\caption{Ablation study on the ensemble strategy on full image and four crops. The `Decoder' column indicates whether we use the inference result of the full image (`O'), four cropped patches (`C'), or both of them (`OC') from the `Image' or `Batch' decoder. Here, the run-time (RT) is the time of network forward except the data processing and calculated on A100 GPU.}\n\\vspace{-0.1in}\n\\label{Tab:aba_ivresults}\n\\end{table}", "cell_list_gold": [{"value": "39.5", "char_index": [325, 329], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}", "experimental settings": {"iteration": "1$\\times$"}, "model": "SS-Mask2Former", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "56.9", "char_index": [332, 336], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}", "experimental settings": {"iteration": "1$\\times$"}, "model": "SS-Mask2Former", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "40.2", "char_index": [339, 343], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}", "experimental settings": {"iteration": "1$\\times$"}, "model": "SS-Mask2Former", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "637", "char_index": [346, 349], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "RT (ms)", "experimental settings": {"iteration": "1$\\times$"}, "model": "SS-Mask2Former", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.9", "char_index": [397, 401], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}", "experimental settings": {"iteration": "1$\\times$"}, "model": "SS-Mask2Former($\\times\\delta$)", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "57.4", "char_index": [404, 408], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}", "experimental settings": {"iteration": "1$\\times$"}, "model": "SS-Mask2Former($\\times\\delta$)", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "40.3", "char_index": [411, 415], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}", "experimental settings": {"iteration": "1$\\times$"}, "model": "SS-Mask2Former($\\times\\delta$)", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "876", "char_index": [418, 421], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "RT (ms)", "experimental settings": {"iteration": "1$\\times$"}, "model": "SS-Mask2Former($\\times\\delta$)", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.2", "char_index": [453, 457], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}", "experimental settings": {"iteration": "1$\\times$"}, "model": "MS-Mask2Former", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "56.3", "char_index": [460, 464], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}", "experimental settings": {"iteration": "1$\\times$"}, "model": "MS-Mask2Former", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.5", "char_index": [467, 471], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}", "experimental settings": {"iteration": "1$\\times$"}, "model": "MS-Mask2Former", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "1324", "char_index": [474, 478], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "RT (ms)", "experimental settings": {"iteration": "1$\\times$"}, "model": "MS-Mask2Former", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.3", "char_index": [511, 515], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}", "experimental settings": {"iteration": "1$\\times$"}, "model": "MS-Mask2Former", "model settings": {"Decoder": "Batch-OC", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "56.4", "char_index": [518, 522], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}", "experimental settings": {"iteration": "1$\\times$"}, "model": "MS-Mask2Former", "model settings": {"Decoder": "Batch-OC", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.7", "char_index": [525, 529], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}", "experimental settings": {"iteration": "1$\\times$"}, "model": "MS-Mask2Former", "model settings": {"Decoder": "Batch-OC", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "2783", "char_index": [532, 536], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "RT (ms)", "experimental settings": {"iteration": "1$\\times$"}, "model": "MS-Mask2Former", "model settings": {"Decoder": "Batch-OC", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.3", "char_index": [585, 589], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "56.7", "char_index": [592, 596], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.8", "char_index": [599, 603], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "637", "char_index": [606, 609], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "RT (ms)", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Image-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.1", "char_index": [627, 631], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "56.6", "char_index": [634, 638], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.7", "char_index": [641, 645], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "1514", "char_index": [648, 652], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "RT (ms)", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-O", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "40.2", "char_index": [670, 674], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-C", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "57.5", "char_index": [677, 681], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-C", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "40.8", "char_index": [684, 688], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-C", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "1507", "char_index": [691, 695], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "RT (ms)", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-C", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "41.0", "char_index": [721, 725], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-OC", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "58.4", "char_index": [737, 741], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-OC", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "41.9", "char_index": [753, 757], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-OC", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "1545", "char_index": [761, 765], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "RT (ms)", "experimental settings": {"iteration": "1$\\times$"}, "model": "CropFormer", "model settings": {"Decoder": "Batch-OC", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}]}, "2211.05776v1_table8": {"table_code": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|ccc}\n\\cellcolor{lightgray!30} Fusion-Style & \\cellcolor{lightgray!30} Type & \\cellcolor{lightgray!30} AP$^{e}$ & \\cellcolor{lightgray!30} AP$^{e}_{50}$ & \\cellcolor{lightgray!30} AP$^{e}_{75}$ \\\\ \\hline\nVI-Mask2Former~\\cite{cheng2022masked,cheng2021mask2former} & Parallel & 39.7 & 57.3 & 40.6 \\\\ \nVITA~\\cite{heo2022vita} & Cascade & 39.9 & 57.8 & 40.7  \\\\\nOurs & Cascade & \\textbf{41.0} & \\textbf{58.4} & \\textbf{41.9} \\\\ \\hline\n\\end{tabular}\n\\caption{Ablation study on the fusion style between image decoder and batch decoder. `Type' indicates the structure relationship between two decoders.}\n\\vspace{-0.2in}\n\\label{Tab:aba_fusion_style}\n\\end{table}", "table_label": "{Tab:aba_fusion_style}", "table_numeric_cells": [["39.7", "39.7", 336, 340, 336, 340], ["57.3", "57.3", 343, 347, 343, 347], ["40.6", "40.6", 350, 354, 350, 354], ["39.9", "39.9", 395, 399, 395, 399], ["57.8", "57.8", 402, 406, 402, 406], ["40.7", "40.7", 409, 413, 409, 413], ["41.0", "\\textbf{41.0}", 443, 447, 435, 448], ["58.4", "\\textbf{58.4}", 459, 463, 451, 464], ["41.9", "\\textbf{41.9}", 475, 479, 467, 480]], "text_chunk_selected": "\\begin{abstract}\nIn dense image segmentation tasks (\\textit{e.g.,} semantic, panoptic), existing methods can hardly generalize well to unseen image domains, predefined classes, and image resolution \\& quality variations.\nMotivated by these observations, we construct a large-scale entity segmentation dataset to explore fine-grained entity segmentation, with a strong focus on open-world and high-quality dense segmentation. \nThe dataset contains images spanning diverse image domains and resolutions, along with high-quality mask annotations for training and testing. \nGiven the high-quality and -resolution nature of the dataset, we propose CropFormer for high-quality segmentation, which can improve mask prediction using high-res image crops that provide more fine-grained image details than the full image. CropFormer is the first query-based Transformer architecture that can effectively ensemble mask predictions from multiple image crops, by learning queries that can associate the same entities across the full image and its crop. With CropFormer, we achieve a significant AP gain of $1.9$ on the challenging fine-grained entity segmentation task. The dataset and code will be released at \\href{http://luqi.info/entityv2.github.io/}{http://luqi.info/entityv2.github.io/}.\n\\end{abstract}\n\n\\vspace{-4mm}\n\\paragraph{Crop Dataloader} In CropFormer, the crop dataloader is designed to generate a batch of images that simultaneously include the full images and their corresponding crops. There are two steps in our dataloader: \\textit{crop} and \\textit{resize}. At first, we augment the full image $I^o$ with a crop that is part of the full image. The \ncrop is randomly extracted from one of the fixed image corners: upper-left, upper-right, bottom-left, bottom-right. The crop size is controlled by a fixed ratio hyperparameter $\\delta \\in \\mathbb{R}$ relative to the full image size. Second, we resize both the full image and crop to the same size. In this way, we construct an input batch $\\mathbf{I} \\in \\mathbb{R}^{N \\times 2 \\times H_\\mathbf{I} \\times W_\\mathbf{I} \\times 3}$ with 2 elements (full image $\\&$ 1 crop) along the temporal dimension. Compared to the full image, corner crops preserve more image details and local information useful for fine-grained segmentation.\nGiven dataloader $\\Gamma$, we define the data preprocessing as\n\n\\vspace{-4mm}\n\\paragraph{Image Encoder and Decoder} Based on learnable image-level queries $\\mathbf{Q}_{i} \\in \\mathbb{R}^{N\\times K}$,  we use image encoder $\\Theta$ and decoder $\\Phi_{i}$ to generate image-level embeddings $\\mathbf{E_i} \\in \\mathbb{R}^{N \\times 2 \\times 1 \\times 1 \\times K}$ for the full input image and its crop. \nFor the image encoder and decoder, we directly adopt the ones in standard image-level Mask2Former~\\cite{cheng2022masked} and do not modify their structures or designs. $\\mathbf{E_i}$ is obtained as follows:\n\n\\vspace{-4mm}\n\\paragraph{Association Module} Similar to video-level Mask2Former~\\cite{cheng2021mask2former}, the association module aims to generate batch queries $\\mathbf{Q_b}$ that are fully shared by the full image and its crop to represent the same entities consistently. Instead of treating $\\mathbf{Q_b}$ as learnable network parameters, here we generate $\\mathbf{Q_b}$ directly from $\\mathbf{E_i}=\\{\\mathbf{E}_{I^o}, \\mathbf{E}_{I^c})\\}$, considering that $\\mathbf{E_i}$ already contains strong segmentation features. \nIn particular, we use a Transformer architecture with cross attention (XAtt), self-attention (SAtt), and feedforward network (FFN) to obtain $\\mathbf{Q_b}$:\n\n\\vspace{-4mm}\n\\paragraph{Batch Decoder}\nGiven $\\mathbf{Q_b} \\in \\mathbb{R}^{N \\times 1 \\times 1 \\times 1 \\times K}$, we obtain batch embeddings $\\mathbf{E_b} \\in \\mathbb{R}^{N \\times 1 \\times 1 \\times 1 \\times K}$:\n\n\\vspace{-4mm}\n\\paragraph{Instance Segmentation}\nWe select 206 thing categories with the highest object-level frequency in the EntityClass dataset to benchmark instance segmentation. In the EntityIns, 8,993 and 1,498 images for training and testing, respectively. In Table~\\ref{Tab:aba_benchmark_classaware_ins}, we ablate two popular instance segmentation methods including Mask-RCNN~\\cite{he2017mask} and Mask2Former~\\cite{cheng2022masked} on EntityIns. Mask2Former with Swin-L backbone and COCO-P pretrained weights the best AP of 30.3 on EntityIns. \n\nTable~\\ref{Tab:aba_ivresults} shows the overall improvement of CropFromer to the Mask2Former~\\cite{cheng2022masked} under 1$\\times$ training setting. The first and second row is our baseline Mask2Former with the single-scale inference (800 and 1040 shortest side) and (1333 and 1732 longest side) for the full images, where 1040=800$\\times\\delta$ and 1732=1333$\\times\\delta$ with $\\delta$ 0.7 in default. For the results in the third and fourth rows, we use test-time bipartite-matching to associate the same entities obtained with multi-scale images and crops. We find no improvements with such inference strategies. Whereas, using the crop output from CropFormer's batch decoder achieves a significant AP$^\\text{e}$ gain as indicated by the second last row. By combining the full image and four crop outputs from CropFormer (final row), we obtain even stronger 1.5 AP$^\\text{e}$ and 1.7 AP$^\\text{e}_{75}$ gain compared to the baseline (first row).\n\n\\vspace{-4mm}\n\\paragraph{Other Batch Fusion Design} Table~\\ref{Tab:aba_fusion_style} shows the ablation study with different fusion designs between original images and cropped patches. Directly using video-level Mask2former or VITA fusion method merely brings marginal performance gains.", "table_source": "\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|ccc}\n\\cellcolor{lightgray!30} Fusion-Style & \\cellcolor{lightgray!30} Type & \\cellcolor{lightgray!30} AP$^{e}$ & \\cellcolor{lightgray!30} AP$^{e}_{50}$ & \\cellcolor{lightgray!30} AP$^{e}_{75}$ \\\\ \\hline\nVI-Mask2Former~\\cite{cheng2022masked,cheng2021mask2former} & Parallel & 39.7 & 57.3 & 40.6 \\\\ \nVITA~\\cite{heo2022vita} & Cascade & 39.9 & 57.8 & 40.7  \\\\\nOurs & Cascade & \\textbf{41.0} & \\textbf{58.4} & \\textbf{41.9} \\\\ \\hline\n\\end{tabular}\n\\caption{Ablation study on the fusion style between image decoder and batch decoder. `Type' indicates the structure relationship between two decoders.}\n\\vspace{-0.2in}\n\\label{Tab:aba_fusion_style}\n\\end{table}", "cell_list_gold": [{"value": "39.7", "char_index": [336, 340], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "VI-Mask2Former", "model settings": {"Type": "Parallel", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "57.3", "char_index": [343, 347], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "VI-Mask2Former", "model settings": {"Type": "Parallel", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "40.6", "char_index": [350, 354], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "VI-Mask2Former", "model settings": {"Type": "Parallel", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "39.9", "char_index": [395, 399], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "VITA", "model settings": {"Type": "Cascade", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "57.8", "char_index": [402, 406], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "VITA", "model settings": {"Type": "Cascade", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "40.7", "char_index": [409, 413], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "VITA", "model settings": {"Type": "Cascade", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "41.0", "char_index": [443, 447], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "Ours", "model settings": {"Type": "Cascade", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "58.4", "char_index": [459, 463], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{50}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "Ours", "model settings": {"Type": "Cascade", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}, {"value": "41.9", "char_index": [475, 479], "type": "Result", "training data/set": "EntitySeg", "test data/set": "EntitySeg", "task": "Entity Segmentation", "metric": "AP$^{e}_{75}$", "experimental settings": {"training schedule": "1$\\times$"}, "model": "Ours", "model settings": {"Type": "Cascade", "backbone": "Swin-L", "pretrained weights": "COCO-E"}}]}, "2211.05778v1_table0": {"table_code": "\\begin{table}[t]\n    \\centering\n    \\renewcommand\\arraystretch{1.0}\n    \\setlength{\\tabcolsep}{2.8mm}\n    \\footnotesize\n    \n\\begin{tabular}{l|c|c|c|c}\n\t\\renewcommand{\\arraystretch}{0.1}\n\t\\setlength\\tabcolsep{0.1mm}\n\tModel Name & $C_1$ & $C'$  & $L_{1, 2, 3, 4}$ & \\#Params \\\\\n\t\\hline\n\tInternImage-T (origin) &64 & 16 & 4, 4, 18, 4 & 30M \\\\\n\tInternImage-S &80 & 16 & 4, 4, 21, 4 & 50M \\\\\n\tInternImage-B &112 & 16 & 4, 4, 21, 4 & 97M \\\\\n\tInternImage-L &160 & 16 & 5, 5, 22, 5 & 223M \\\\\n\tInternImage-XL &192 & 16 & 5, 5, 24, 5 & 335M \\\\\n\tInternImage-H &320 & 16 & 6, 6, 32, 6 & 1.08B \\\\\n\\end{tabular}\n\n    \\caption{\\textbf{Hyper-parameters for models of different scales}. InternImage-T is the origin model, and -S/B/L/XL/H are scaled up from -T. ``\\#Params'' denotes the number of parameters.}\n    \\label{tab:model_size}\n\\end{table}", "table_label": "{tab:model_size}", "table_numeric_cells": [["64", "64", 310, 312, 310, 312], ["16", "16", 315, 317, 315, 317], ["30M", "30M", 334, 337, 334, 337], ["80", "80", 357, 359, 357, 359], ["16", "16", 362, 364, 362, 364], ["50M", "50M", 381, 384, 381, 384], ["112", "112", 404, 407, 404, 407], ["16", "16", 410, 412, 410, 412], ["97M", "97M", 429, 432, 429, 432], ["160", "160", 452, 455, 452, 455], ["16", "16", 458, 460, 458, 460], ["223M", "223M", 477, 481, 477, 481], ["192", "192", 502, 505, 502, 505], ["16", "16", 508, 510, 508, 510], ["335M", "335M", 527, 531, 527, 531], ["320", "320", 551, 554, 551, 554], ["16", "16", 557, 559, 557, 559], ["1.08B", "1.08B", 576, 581, 576, 581]], "text_chunk_selected": "(3) We evaluate the proposed model on representative vision tasks including image classification, object detection, instance and semantic segmentation,\nand compared it with state-of-the-art CNNs and large-scale ViTs by scaling the model size ranging from 30 million to 1 billion, the data ranging from 1 million to 400 million.\nSpecifically, our model with different parameter sizes can consistently outperform prior arts on ImageNet~\\cite{deng2009imagenet}. \nInternImage-B achieves 84.9\\% top-1 accuracy trained only on the ImageNet-1K dataset, outperforming CNN-based counterparts~\\cite{ding2022replknet,liu2022convnet} by at least 1.1 points.\nWith large-scale parameters (\\emph{i.e.}, 1 billion) and training data (\\emph{i.e.}, 427 million), the top-1 accuracy of InternImage-H is further boosted to 89.2\\%, which is close to well-engineering ViTs~\\cite{liu2021swin,zhai2022scaling} and hybrid-ViTs~\\cite{dai2021coatnet}.\nIn addition, on COCO~\\cite{caesar2018coco}, a challenging downstream benchmark,\nour best model InternImage-H achieves state-of-the-art 65.4\\% box mAP with 2.18 billion parameters, 2.3 points higher than SwinV2-G~\\cite{liu2021swinv2} (65.4 \\vs 63.1) with 27\\% fewer parameters.\n\nwhere $K$ represents the total number of sampling points, and $k$ enumerates the sampling point. \n$\\mathbf{w}_k\\!\\in\\!\\mathbb{R}^{C\\times C}$ denotes the projection weights of the $k$-th sampling point,\nand $\\mathbf{m}_k\\!\\in\\!\\mathbb{R}$ represents the modulation scalar of the $k$-th sampling point, which is normalized by sigmoid function.\n$p_k$ denotes the $k$-th location of the pre-defined grid sampling $\\{(-1, -1), (-1, 0), ..., (0, +1), ..., (+1, +1)\\}$ as in regular convolutions,\nand $\\Delta p_k$ is the offset corresponding to the $k$-th grid sampling location.\nWe see from the equation that (1) for long-range dependencies, the sampling offset $\\Delta p_k$ is flexible and able to interact with short- or long-range features; and (2) for adaptive spatial aggregation, both the sampling offset $\\Delta p_k$ and modulation scalar $\\mathbf{m}_k$ are learnable and conditioned by input $\\mathbf{x}$. \nSo it can be found that \\emph{DCNv2 shares similar favorable properties with MHSA}, which motivated us to develop large-scale CNN-based foundation models on the basis of this operator.\n\nwhere $G$ denotes the total number of aggregation groups.\nFor the $g$-th group,\n$\\mathbf{w}_g\\!\\in\\!\\mathbb{R}^{C\\times C'}$, $\\mathbf{m}_{gk}\\!\\in\\!\\mathbb{R}$ denote the location-irrelevant projection weights of the group, where $C'\\!=\\!C/G$ represents the group dimension.\n$\\mathbf{m}_{gk}\\!\\in\\!\\mathbb{R}$ denotes the modulation scalar of the $k$-th sampling point in the $g$-th group, normalized by the softmax function along the dimension $K$.\n$\\mathbf{x}_g\\!\\in\\!\\mathbb{R}^{C'\\times H \\times W}$ represents the sliced input feature map.\n$\\Delta p_{gk}$ is the offset corresponding to the grid sampling location $p_k$ in the $g$-th group.\n\n\\textbf{Stem \\& downsampling layers.}\nTo obtain hierarchical feature maps, we use \nconvolutional stem and downsampling layers to resize the feature maps to different scales.\nAs shown in Fig. \\ref{fig:arch}, the stem layer is placed before the first stage to reduce the input resolution by 4 times.\nIt consists of two convolutions, two LN layers, and one GELU layer,\nwhere the kernel size of the two convolutions is 3, the stride is 2, the padding is 1,\nand the output channel of the first convolution is half of the second one.\nSimilarly, the downsampling layer is made up of a 3$\\times$3 convolution with a stride of 2 and a padding of 1, followed by one LN layer.\nIt sits between the two stages and is used to downsample the input feature map by 2 times.\n\n\\textbf{Stacking rules.}\nTo clarify the block-stacking process, we first list the integral hyper-parameters of the InternImage\\ as follows:\\\\\n\\indent $C_i$: the channel number of the $i$-th stage;\\\\\n\\indent $G_i$: the group number of the DCNv3 in the $i$-th stage;\\\\\n\\indent$L_i$: the number of basic blocks in the $i$-th stage.\\\\\nSince our model has 4 stages, a variant is decided by 12 hyper-parameters,\nwhose search space is too large to exhaustively enumerate and find the best variant.\nTo reduce the search space, we summarize the design experiences of prior arts~\\cite{he2016deep,liu2021swin,liu2022convnet} into 4 rules as shown in Fig.~\\ref{fig:arch},\nwhere the first rule makes the channel numbers of the last three stages determined by the channel number $C_1$ of the first stage,\nand the second rule lets the group number corresponding to the channel number of stages.\nFor the number of stacked blocks in different stages, we simplify the stacking pattern to ``AABA'', which means the block number of stage 1, 2, and 4 are the same, and are not greater than that of the stage 3 as illustrated in the last two rules.\nWith these rules, a InternImage\\ variant can be defined by using only 4 hyper-parameters $(C_1, C', L_1, L_3)$.\n\nLet us choose a model with 30 million parameters as the origin and discretize $C_1$ to $\\{16, 32, 64\\}$, $L_1$ to $\\{1, 2, 3, 4, 5\\}$, and $C'$ to $\\{16, 32\\}$.\nIn this way, the original huge search space is reduced to 30,\nand we can find the best model from the 30 variants by training and evaluating them in ImageNet~\\cite{deng2009imagenet}.\nIn practice, we use the best hyper-parameter setting $(64, 16, 4, 18)$ to define the base model and scale it to different scales.\n\n\\textbf{Scaling rules.} Based on the optimal origin model under the aforementioned constraints, we further explore the parameter scaling rules inspired by \\cite{tan2019efficientnet}.\nSpecifically, we consider two scaling dimensions: depth $D$ (\\emph{i.e.}, $3L_1\\!+\\!L_3$) and width $C_1$,\nand scale the two dimensions using $\\alpha$, $\\beta$ and a composite factor $\\phi$. The scaling rules can be written as:\n\nwhere $\\alpha\\!\\geq\\!1$, $\\beta\\!\\geq\\!1$, and $\\alpha\\beta^{1.99}\\!\\approx\\!2$. Here, 1.99 is specific for InternImage and calculated by doubling the model width and keeping the depth constant.\nWe experimentally find out that the best scaling setting is $\\alpha\\!=\\!1.09$ and $\\beta\\!=\\!1.36$, and then we base on it to construct InternImage\\ variants with different parameter scales, namely InternImage-T/S/B/L/XL, whose complexity is similar to those of ConvNeXt~\\cite{liu2022convnet}. \nTo further test the capability, we built a larger InternImage-H with 1 billion parameters.\nThe detailed configurations are summarized in Table~\\ref{tab:model_size}.", "table_source": "\\begin{table}[t]\n    \\centering\n    \\renewcommand\\arraystretch{1.0}\n    \\setlength{\\tabcolsep}{2.8mm}\n    \\footnotesize\n    \n\\begin{tabular}{l|c|c|c|c}\n\t\\renewcommand{\\arraystretch}{0.1}\n\t\\setlength\\tabcolsep{0.1mm}\n\tModel Name & $C_1$ & $C'$  & $L_{1, 2, 3, 4}$ & \\#Params \\\\\n\t\\hline\n\tInternImage-T (origin) &64 & 16 & 4, 4, 18, 4 & 30M \\\\\n\tInternImage-S &80 & 16 & 4, 4, 21, 4 & 50M \\\\\n\tInternImage-B &112 & 16 & 4, 4, 21, 4 & 97M \\\\\n\tInternImage-L &160 & 16 & 5, 5, 22, 5 & 223M \\\\\n\tInternImage-XL &192 & 16 & 5, 5, 24, 5 & 335M \\\\\n\tInternImage-H &320 & 16 & 6, 6, 32, 6 & 1.08B \\\\\n\\end{tabular}\n\n    \\caption{\\textbf{Hyper-parameters for models of different scales}. InternImage-T is the origin model, and -S/B/L/XL/H are scaled up from -T. ``\\#Params'' denotes the number of parameters.}\n    \\label{tab:model_size}\n\\end{table}", "cell_list_gold": [{"value": "64", "char_index": [310, 312], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["C_1", "channel number of the first stage"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "16", "char_index": [315, 317], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["C'", "group dimension"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "30M", "char_index": [334, 337], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["\\#Params", "number of parameters"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "80", "char_index": [357, 359], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["C_1", "channel number of the first stage"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "16", "char_index": [362, 364], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["C'", "group dimension"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "50M", "char_index": [381, 384], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["\\#Params", "number of parameters"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "112", "char_index": [404, 407], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["C_1", "channel number of the first stage"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "16", "char_index": [410, 412], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["C'", "group dimension"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "97M", "char_index": [429, 432], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["\\#Params", "number of parameters"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "160", "char_index": [452, 455], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["C_1", "channel number of the first stage"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "16", "char_index": [458, 460], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["C'", "group dimension"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "223M", "char_index": [477, 481], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["\\#Params", "number of parameters"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "192", "char_index": [502, 505], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["C_1", "channel number of the first stage"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "16", "char_index": [508, 510], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["C'", "group dimension"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "335M", "char_index": [527, 531], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["\\#Params", "number of parameters"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "320", "char_index": [551, 554], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["C_1", "channel number of the first stage"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "16", "char_index": [557, 559], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["C'", "group dimension"], "dataset": ["ImageNet", "COCO", "ADE20K"]}, {"value": "1.08B", "char_index": [576, 581], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["\\#Params", "number of parameters"], "dataset": ["ImageNet", "COCO", "ADE20K"]}]}, "2211.05778v1_table1": {"table_code": "\\begin{table}[t]\n    \\centering\n    \\setlength{\\tabcolsep}{0.9mm}\n    \\footnotesize\n    \n\\begin{tabular}{l|c|c|c|c|c}\n\t\\renewcommand{\\arraystretch}{0.1}\n\tMethod & Type & Res & \\#Params & \\#FLOPs & Acc (\\%)  \\\\\n\t\\hline\n\tPVT-S~\\cite{wang2021pyramid} & T &$224^2$  & 25M & 4G & 79.8 \\\\\n\tSwin-T~\\cite{liu2021swin} & T &$224^2$ & 29M & 5G & 81.3\\\\\n\tCoAtNet-0~\\cite{dai2021coatnet} & T &$224^2$ & 25M & 4G & 81.6 \\\\\n\tPVTv2-B2~\\cite{wang2021pvtv2} & T & $224^2$  & 25M & 4G & 82.0 \\\\\n\tDeiT III-S~\\cite{touvron2022deit3} & T &$224^2$  & 22M & 5G & 81.4\\\\\n\tSwinV2-T/8~\\cite{liu2021swinv2} & T &$256^2$ & 28M & 6G & 81.8\\\\\n\tConvNeXt-T~\\cite{liu2022convnet} & C & $224^2$ & 29M & 5G & 82.1\\\\\n\t\\rowcolor{gray!20}\n\tInternImage-T (ours) & C &$224^2$  & 30M & 5G & 83.5 \\\\\n\t\\hline\n\tPVT-L~\\cite{wang2021pyramid} & T &$224^2$  & 61M & 10G & 81.7 \\\\\n\tSwin-S~\\cite{liu2021swin} & T &$224^2$  & 50M & 9G & 83.0\\\\\n\tCoAtNet-1~\\cite{dai2021coatnet} & T &$224^2$ & 42M & 8G & 83.3 \\\\\n\tPVTv2-B4~\\cite{wang2021pvtv2} & T & $224^2$  & 63M & 10G & 83.6 \\\\\n\tSwinV2-S/8~\\cite{liu2021swinv2} & T &$256^2$ & 50M & 12G & 83.7\\\\\n\tConvNeXt-S~\\cite{liu2022convnet} & C &$224^2$  & 50M & 9G & 83.1\\\\\n\t\\rowcolor{gray!20}\n\tInternImage-S (ours) & C &$224^2$  & 50M & 8G & 84.2 \\\\\n\t\\hline\n\tSwin-B~\\cite{liu2021swin} & T &$224^2$  & 88M & 15G & 83.5\\\\\n\tCoAtNet-2~\\cite{dai2021coatnet} & T &$224^2$ & 75M & 16G & 84.1 \\\\\n\tPVTv2-B5~\\cite{wang2021pvtv2} & T & $224^2$  & 82M & 12G & 83.8 \\\\\n\tDeiT III-B~\\cite{touvron2022deit3} & T &$224^2$ & 87M & 18G & 83.8 \\\\\n\tSwinV2-B/8~\\cite{liu2021swinv2} & T &$256^2$  & 88M & 20G & 84.2\\\\\n\tRepLKNet-31B~\\cite{ding2022replknet} & C &$224^2$  & 79M & 15G & 83.5\\\\\n\tConvNeXt-B~\\cite{liu2022convnet} & C &$224^2$  & 88M & 15G & 83.8\\\\\n\t\\rowcolor{gray!20}\n\tInternImage-B (ours) & C &$224^2$ & 97M & 16G & 84.9 \\\\\n\t\\hline\n\tSwin-L$^\\ddagger$~\\cite{liu2021swin} & T &$384^2$ & 197M & 104G & 87.3\\\\\n\tCoAtNet-3$^\\ddagger$~\\cite{dai2021coatnet} & T &$384^2$ & 168M & 107G & 87.6 \\\\\n\tCoAtNet-4$^\\ddagger$~\\cite{dai2021coatnet} & T &$384^2$ & 275M & 190G & 87.9 \\\\\n\tDeiT III-L$^\\ddagger$~\\cite{touvron2022deit3} & T &$384^2$ & 304M & 191G & 87.7 \\\\\n\tSwinV2-L/24$^\\ddagger$~\\cite{liu2021swinv2} & T &$384^2$ & 197M & 115G & 87.6\\\\\n\tRepLKNet-31L$^\\ddagger$~\\cite{ding2022replknet} & C &$384^2$ & 172M & 96G & 86.6\\\\\n\tConvNeXt-L$^\\ddagger$~\\cite{liu2022convnet} & C &$384^2$ & 198M & 101G & 87.5\\\\\n\tConvNeXt-XL$^\\ddagger$~\\cite{liu2022convnet} & C &$384^2$ & 350M & 179G & 87.8\\\\\n\t\\rowcolor{gray!10}\n\tInternImage-L$^\\ddagger$ (ours) & C &$384^2$ & 223M & 108G & 87.7 \\\\\n\t\\rowcolor{gray!20}\n\tInternImage-XL$^\\ddagger$ (ours) & C &$384^2$ & 335M & 163G & 88.0 \\\\\n\t\\hline\n\tViT-G/14$^\\#$~\\cite{zhai2022scaling} & T & $518^2$  & 1.84B & 5160G & 90.5\\\\\n\tCoAtNet-6$^\\#$~\\cite{dai2021coatnet} & T & $512^2$ & 1.47B & 1521G & 90.5\\\\\n\tCoAtNet-7$^\\#$~\\cite{dai2021coatnet} & T & $512^2$ & 2.44B & 2586G & 90.9\\\\\n\tFlorence-CoSwin-H$^\\#$~\\cite{yuan2021florence} & T & $-$ & 893M & $-$ & 90.0 \\\\\n\tSwinV2-G$^\\#$~\\cite{liu2021swinv2} & T & $640^2$ & 3.00B & $-$ & 90.2 \\\\\n\tRepLKNet-XL$^\\#$~\\cite{ding2022replknet} & C & $384^2$ & 335M & 129G & 87.8\\\\\n\tBiT-L-ResNet152x4$^\\#$~\\cite{kolesnikov2020big} & C & $480^2$ & 928M & $-$ & 87.5 \\\\\n\t\\rowcolor{gray!10}\n\tInternImage-H$^\\#$ (ours) & C & $224^2$ & 1.08B & 188G & 88.5  \\\\\n\t\\rowcolor{gray!20}\n\tInternImage-H$^\\#$ (ours) & C & $640^2$ & 1.08B & 1478G & 89.2  \\\\\n\\end{tabular}\n\n    \\caption{\\textbf{Image classification performance on the ImageNet validation set}. ``Type'' refers to model type, where ``T'' and ``C'' denote transformer and CNN, respectively. ``Res'' means the input resolution.\n    \t``Acc'' is the top-1 accuracy.\n    \t``$^\\ddagger$\" indicates the model is pre-trained on ImageNet-22K~\\cite{deng2009imagenet}. ``$^\\#$\" indicates pre-training on extra large-scale private dataset such as JFT-300M~\\cite{xie2020self}, FLD-900M~\\cite{yuan2021florence}, or the joint public dataset in this work.}\n    \\label{tab:cls_imagenet}\n\\end{table}", "table_label": "{tab:cls_imagenet}", "table_numeric_cells": [["25M", "25M", 264, 267, 264, 267], ["4G", "4G", 270, 272, 270, 272], ["79.8", "79.8", 275, 279, 275, 279], ["29M", "29M", 325, 328, 325, 328], ["5G", "5G", 331, 333, 331, 333], ["81.3", "81.3", 336, 340, 336, 340], ["25M", "25M", 391, 394, 391, 394], ["4G", "4G", 397, 399, 397, 399], ["81.6", "81.6", 402, 406, 402, 406], ["25M", "25M", 458, 461, 458, 461], ["4G", "4G", 464, 466, 464, 466], ["82.0", "82.0", 469, 473, 469, 473], ["22M", "22M", 529, 532, 529, 532], ["5G", "5G", 535, 537, 535, 537], ["81.4", "81.4", 540, 544, 540, 544], ["28M", "28M", 595, 598, 595, 598], ["6G", "6G", 601, 603, 601, 603], ["81.8", "81.8", 606, 610, 606, 610], ["29M", "29M", 663, 666, 663, 666], ["5G", "5G", 669, 671, 669, 671], ["82.1", "82.1", 674, 678, 674, 678], ["30M", "30M", 739, 742, 739, 742], ["5G", "5G", 745, 747, 745, 747], ["83.5", "83.5", 750, 754, 750, 754], ["61M", "61M", 812, 815, 812, 815], ["10G", "10G", 818, 821, 818, 821], ["81.7", "81.7", 824, 828, 824, 828], ["50M", "50M", 875, 878, 875, 878], ["9G", "9G", 881, 883, 881, 883], ["83.0", "83.0", 886, 890, 886, 890], ["42M", "42M", 941, 944, 941, 944], ["8G", "8G", 947, 949, 947, 949], ["83.3", "83.3", 952, 956, 952, 956], ["63M", "63M", 1008, 1011, 1008, 1011], ["10G", "10G", 1014, 1017, 1014, 1017], ["83.6", "83.6", 1020, 1024, 1020, 1024], ["50M", "50M", 1076, 1079, 1076, 1079], ["12G", "12G", 1082, 1085, 1082, 1085], ["83.7", "83.7", 1088, 1092, 1088, 1092], ["50M", "50M", 1145, 1148, 1145, 1148], ["9G", "9G", 1151, 1153, 1151, 1153], ["83.1", "83.1", 1156, 1160, 1156, 1160], ["50M", "50M", 1221, 1224, 1221, 1224], ["8G", "8G", 1227, 1229, 1227, 1229], ["84.2", "84.2", 1232, 1236, 1232, 1236], ["88M", "88M", 1291, 1294, 1291, 1294], ["15G", "15G", 1297, 1300, 1297, 1300], ["83.5", "83.5", 1303, 1307, 1303, 1307], ["75M", "75M", 1358, 1361, 1358, 1361], ["16G", "16G", 1364, 1367, 1364, 1367], ["84.1", "84.1", 1370, 1374, 1370, 1374], ["82M", "82M", 1426, 1429, 1426, 1429], ["12G", "12G", 1432, 1435, 1432, 1435], ["83.8", "83.8", 1438, 1442, 1438, 1442], ["87M", "87M", 1497, 1500, 1497, 1500], ["18G", "18G", 1503, 1506, 1503, 1506], ["83.8", "83.8", 1509, 1513, 1509, 1513], ["88M", "88M", 1566, 1569, 1566, 1569], ["20G", "20G", 1572, 1575, 1572, 1575], ["84.2", "84.2", 1578, 1582, 1578, 1582], ["79M", "79M", 1639, 1642, 1639, 1642], ["15G", "15G", 1645, 1648, 1645, 1648], ["83.5", "83.5", 1651, 1655, 1651, 1655], ["88M", "88M", 1708, 1711, 1708, 1711], ["15G", "15G", 1714, 1717, 1714, 1717], ["83.8", "83.8", 1720, 1724, 1720, 1724], ["97M", "97M", 1784, 1787, 1784, 1787], ["16G", "16G", 1790, 1793, 1790, 1793], ["84.9", "84.9", 1796, 1800, 1796, 1800], ["197M", "197M", 1865, 1869, 1865, 1869], ["104G", "104G", 1872, 1876, 1872, 1876], ["87.3", "87.3", 1879, 1883, 1879, 1883], ["168M", "168M", 1945, 1949, 1945, 1949], ["107G", "107G", 1952, 1956, 1952, 1956], ["87.6", "87.6", 1959, 1963, 1959, 1963], ["275M", "275M", 2026, 2030, 2026, 2030], ["190G", "190G", 2033, 2037, 2033, 2037], ["87.9", "87.9", 2040, 2044, 2040, 2044], ["304M", "304M", 2110, 2114, 2110, 2114], ["191G", "191G", 2117, 2121, 2117, 2121], ["87.7", "87.7", 2124, 2128, 2124, 2128], ["197M", "197M", 2192, 2196, 2192, 2196], ["115G", "115G", 2199, 2203, 2199, 2203], ["87.6", "87.6", 2206, 2210, 2206, 2210], ["172M", "172M", 2277, 2281, 2277, 2281], ["96G", "96G", 2284, 2287, 2284, 2287], ["86.6", "86.6", 2290, 2294, 2290, 2294], ["198M", "198M", 2357, 2361, 2357, 2361], ["101G", "101G", 2364, 2368, 2364, 2368], ["87.5", "87.5", 2371, 2375, 2371, 2375], ["350M", "350M", 2439, 2443, 2439, 2443], ["179G", "179G", 2446, 2450, 2446, 2450], ["87.8", "87.8", 2453, 2457, 2453, 2457], ["223M", "223M", 2528, 2532, 2528, 2532], ["108G", "108G", 2535, 2539, 2535, 2539], ["87.7", "87.7", 2542, 2546, 2542, 2546], ["335M", "335M", 2619, 2623, 2619, 2623], ["163G", "163G", 2626, 2630, 2626, 2630], ["88.0", "88.0", 2633, 2637, 2633, 2637], ["1.84B", "1.84B", 2704, 2709, 2704, 2709], ["5160G", "5160G", 2712, 2717, 2712, 2717], ["90.5", "90.5", 2720, 2724, 2720, 2724], ["1.47B", "1.47B", 2781, 2786, 2781, 2786], ["1521G", "1521G", 2789, 2794, 2789, 2794], ["90.5", "90.5", 2797, 2801, 2797, 2801], ["2.44B", "2.44B", 2858, 2863, 2858, 2863], ["2586G", "2586G", 2866, 2871, 2866, 2871], ["90.9", "90.9", 2874, 2878, 2874, 2878], ["893M", "893M", 2941, 2945, 2941, 2945], ["90.0", "90.0", 2954, 2958, 2954, 2958], ["3.00B", "3.00B", 3014, 3019, 3014, 3019], ["90.2", "90.2", 3028, 3032, 3028, 3032], ["335M", "335M", 3094, 3098, 3094, 3098], ["129G", "129G", 3101, 3105, 3101, 3105], ["87.8", "87.8", 3108, 3112, 3108, 3112], ["928M", "928M", 3180, 3184, 3180, 3184], ["87.5", "87.5", 3193, 3197, 3193, 3197], ["1.08B", "1.08B", 3264, 3269, 3264, 3269], ["188G", "188G", 3272, 3276, 3272, 3276], ["88.5", "88.5", 3279, 3283, 3279, 3283], ["1.08B", "1.08B", 3351, 3356, 3351, 3356], ["1478G", "1478G", 3359, 3364, 3359, 3364], ["89.2", "89.2", 3367, 3371, 3367, 3371]], "text_chunk_selected": "\\author{\n    Wenhai Wang$^{1*}$, \n    Jifeng Dai$^{2,1*}$,\n    Zhe Chen$^{3,1*}$,\n    Zhenhang Huang$^{1*}$,\n    Zhiqi Li$^{3,1*}$,\n    Xizhou Zhu$^{4*}$,\\\\\n    Xiaowei Hu$^{1}$,\n    Tong Lu$^{3}$,\n    Lewei Lu$^{4}$,\n    Hongsheng Li$^{5}$,\n    Xiaogang Wang$^{3,5}$,\n    Yu Qiao$^{1}$\\textsuperscript{\\Letter}\\\\\n    $^1$ Shanghai AI Laboratory~~~\n    $^2$Tsinghua University~~~\\\\\n    $^3$Nanjing University~~~\n    $^4$SenseTime Research~~~\n    $^5$The Chinese University of Hong Kong\\\\\n    {\\small \\url{https://github.com/OpenGVLab/InternImage}}\n}\n\n\\textbf{Revisiting DCNv2.} A straightforward way to bridge the gap between convolution and MHSA is to introduce long-range dependencies and adaptive spatial aggregation into regular convolutions.\nLet us start with DCNv2~\\cite{zhu2019dcnv2}, which is a general variant of regular convolution.\nGiven an input $\\mathbf{x}\\!\\in\\!\\mathbb{R}^{C\\times H\\times W}$ and current pixel $p_0$, DCNv2 can be formulated as:\n\nwhere $K$ represents the total number of sampling points, and $k$ enumerates the sampling point. \n$\\mathbf{w}_k\\!\\in\\!\\mathbb{R}^{C\\times C}$ denotes the projection weights of the $k$-th sampling point,\nand $\\mathbf{m}_k\\!\\in\\!\\mathbb{R}$ represents the modulation scalar of the $k$-th sampling point, which is normalized by sigmoid function.\n$p_k$ denotes the $k$-th location of the pre-defined grid sampling $\\{(-1, -1), (-1, 0), ..., (0, +1), ..., (+1, +1)\\}$ as in regular convolutions,\nand $\\Delta p_k$ is the offset corresponding to the $k$-th grid sampling location.\nWe see from the equation that (1) for long-range dependencies, the sampling offset $\\Delta p_k$ is flexible and able to interact with short- or long-range features; and (2) for adaptive spatial aggregation, both the sampling offset $\\Delta p_k$ and modulation scalar $\\mathbf{m}_k$ are learnable and conditioned by input $\\mathbf{x}$. \nSo it can be found that \\emph{DCNv2 shares similar favorable properties with MHSA}, which motivated us to develop large-scale CNN-based foundation models on the basis of this operator.\n\nwhere $G$ denotes the total number of aggregation groups.\nFor the $g$-th group,\n$\\mathbf{w}_g\\!\\in\\!\\mathbb{R}^{C\\times C'}$, $\\mathbf{m}_{gk}\\!\\in\\!\\mathbb{R}$ denote the location-irrelevant projection weights of the group, where $C'\\!=\\!C/G$ represents the group dimension.\n$\\mathbf{m}_{gk}\\!\\in\\!\\mathbb{R}$ denotes the modulation scalar of the $k$-th sampling point in the $g$-th group, normalized by the softmax function along the dimension $K$.\n$\\mathbf{x}_g\\!\\in\\!\\mathbb{R}^{C'\\times H \\times W}$ represents the sliced input feature map.\n$\\Delta p_{gk}$ is the offset corresponding to the grid sampling location $p_k$ in the $g$-th group.\n\n\\textbf{Stem \\& downsampling layers.}\nTo obtain hierarchical feature maps, we use \nconvolutional stem and downsampling layers to resize the feature maps to different scales.\nAs shown in Fig. \\ref{fig:arch}, the stem layer is placed before the first stage to reduce the input resolution by 4 times.\nIt consists of two convolutions, two LN layers, and one GELU layer,\nwhere the kernel size of the two convolutions is 3, the stride is 2, the padding is 1,\nand the output channel of the first convolution is half of the second one.\nSimilarly, the downsampling layer is made up of a 3$\\times$3 convolution with a stride of 2 and a padding of 1, followed by one LN layer.\nIt sits between the two stages and is used to downsample the input feature map by 2 times.\n\nwhere $\\alpha\\!\\geq\\!1$, $\\beta\\!\\geq\\!1$, and $\\alpha\\beta^{1.99}\\!\\approx\\!2$. Here, 1.99 is specific for InternImage and calculated by doubling the model width and keeping the depth constant.\nWe experimentally find out that the best scaling setting is $\\alpha\\!=\\!1.09$ and $\\beta\\!=\\!1.36$, and then we base on it to construct InternImage\\ variants with different parameter scales, namely InternImage-T/S/B/L/XL, whose complexity is similar to those of ConvNeXt~\\cite{liu2022convnet}. \nTo further test the capability, we built a larger InternImage-H with 1 billion parameters.\nThe detailed configurations are summarized in Table~\\ref{tab:model_size}.\n\n\\subsection{Image Classification}\n\\textbf{Settings.}\nWe evaluate the classification performance of InternImage\\ on ImageNet~\\cite{deng2009imagenet}. \nFor fair comparisons, following common practices~\\cite{touvron2021training,wang2021pyramid,liu2021swin,liu2022convnet}, InternImage-T/S/B are trained on ImageNet-1K ($\\sim$1.3 million) for 300 epochs, \nand InternImage-L/XL are first trained on ImageNet-22K ($\\sim$14.2 million) for 90 epochs and then fine-tuned on ImageNet-1K for 30 epochs.\nTo further explore the capability of our model and match the large-scale private data used in previous methods~\\cite{dai2021coatnet,liu2021swinv2,yuan2021florence}, we adopt M3I Pre-training~\\cite{anonymous2022m2i}, an effective pre-training approach, to pre-train InternImage-H on a 427 million joint dataset of public Laion-400M~\\cite{schuhmann2021laion}, YFCC-15M~\\cite{thomee2016yfcc100m}, and CC12M~\\cite{changpinyo2021conceptual} for 30 epochs,\nand then we fine-tune the model on ImageNet-22K and -1K for 30 epochs, respectively.\n\n\\textbf{Results.} Table~\\ref{tab:cls_imagenet} shows the classification results of models with different scales.\nWith comparable parameters and computational costs, our models are comparable or even superior to the state-of-the-art transformer-based and CNN-based models. \nFor example, InternImage-T achieves 83.5\\% top-1 accuracy, outperforming ConvNext-T~\\cite{liu2022convnet} with a clear margin of 1.4 points.\nInternImage-S/B keeps the leading position and surpasses the second-best method by at least 0.4 points.\nWhen pre-trained on ImageNet-22K and the large-scale joint dataset, the top-1 accuracy of InternImage-XL and -H are boosted to 88.0\\% and 89.2\\%, respectively,\nwhich is better than previous CNNs which are also trained with large-scale data, and closes the gap with state-of-the-art large transformer-based models to around 1 point.\nThis gap may be caused by the discrepancy between large-scale inaccessible private data and the aforementioned joint public data.\n\\emph{These results show that our InternImage\\ not only has good performance on the common parameter scale and the public training data, but also can effectively extend to large-scale parameters and data.}", "table_source": "\\begin{table}[t]\n    \\centering\n    \\setlength{\\tabcolsep}{0.9mm}\n    \\footnotesize\n    \n\\begin{tabular}{l|c|c|c|c|c}\n\t\\renewcommand{\\arraystretch}{0.1}\n\tMethod & Type & Res & \\#Params & \\#FLOPs & Acc (\\%)  \\\\\n\t\\hline\n\tPVT-S~\\cite{wang2021pyramid} & T &$224^2$  & 25M & 4G & 79.8 \\\\\n\tSwin-T~\\cite{liu2021swin} & T &$224^2$ & 29M & 5G & 81.3\\\\\n\tCoAtNet-0~\\cite{dai2021coatnet} & T &$224^2$ & 25M & 4G & 81.6 \\\\\n\tPVTv2-B2~\\cite{wang2021pvtv2} & T & $224^2$  & 25M & 4G & 82.0 \\\\\n\tDeiT III-S~\\cite{touvron2022deit3} & T &$224^2$  & 22M & 5G & 81.4\\\\\n\tSwinV2-T/8~\\cite{liu2021swinv2} & T &$256^2$ & 28M & 6G & 81.8\\\\\n\tConvNeXt-T~\\cite{liu2022convnet} & C & $224^2$ & 29M & 5G & 82.1\\\\\n\t\\rowcolor{gray!20}\n\tInternImage-T (ours) & C &$224^2$  & 30M & 5G & 83.5 \\\\\n\t\\hline\n\tPVT-L~\\cite{wang2021pyramid} & T &$224^2$  & 61M & 10G & 81.7 \\\\\n\tSwin-S~\\cite{liu2021swin} & T &$224^2$  & 50M & 9G & 83.0\\\\\n\tCoAtNet-1~\\cite{dai2021coatnet} & T &$224^2$ & 42M & 8G & 83.3 \\\\\n\tPVTv2-B4~\\cite{wang2021pvtv2} & T & $224^2$  & 63M & 10G & 83.6 \\\\\n\tSwinV2-S/8~\\cite{liu2021swinv2} & T &$256^2$ & 50M & 12G & 83.7\\\\\n\tConvNeXt-S~\\cite{liu2022convnet} & C &$224^2$  & 50M & 9G & 83.1\\\\\n\t\\rowcolor{gray!20}\n\tInternImage-S (ours) & C &$224^2$  & 50M & 8G & 84.2 \\\\\n\t\\hline\n\tSwin-B~\\cite{liu2021swin} & T &$224^2$  & 88M & 15G & 83.5\\\\\n\tCoAtNet-2~\\cite{dai2021coatnet} & T &$224^2$ & 75M & 16G & 84.1 \\\\\n\tPVTv2-B5~\\cite{wang2021pvtv2} & T & $224^2$  & 82M & 12G & 83.8 \\\\\n\tDeiT III-B~\\cite{touvron2022deit3} & T &$224^2$ & 87M & 18G & 83.8 \\\\\n\tSwinV2-B/8~\\cite{liu2021swinv2} & T &$256^2$  & 88M & 20G & 84.2\\\\\n\tRepLKNet-31B~\\cite{ding2022replknet} & C &$224^2$  & 79M & 15G & 83.5\\\\\n\tConvNeXt-B~\\cite{liu2022convnet} & C &$224^2$  & 88M & 15G & 83.8\\\\\n\t\\rowcolor{gray!20}\n\tInternImage-B (ours) & C &$224^2$ & 97M & 16G & 84.9 \\\\\n\t\\hline\n\tSwin-L$^\\ddagger$~\\cite{liu2021swin} & T &$384^2$ & 197M & 104G & 87.3\\\\\n\tCoAtNet-3$^\\ddagger$~\\cite{dai2021coatnet} & T &$384^2$ & 168M & 107G & 87.6 \\\\\n\tCoAtNet-4$^\\ddagger$~\\cite{dai2021coatnet} & T &$384^2$ & 275M & 190G & 87.9 \\\\\n\tDeiT III-L$^\\ddagger$~\\cite{touvron2022deit3} & T &$384^2$ & 304M & 191G & 87.7 \\\\\n\tSwinV2-L/24$^\\ddagger$~\\cite{liu2021swinv2} & T &$384^2$ & 197M & 115G & 87.6\\\\\n\tRepLKNet-31L$^\\ddagger$~\\cite{ding2022replknet} & C &$384^2$ & 172M & 96G & 86.6\\\\\n\tConvNeXt-L$^\\ddagger$~\\cite{liu2022convnet} & C &$384^2$ & 198M & 101G & 87.5\\\\\n\tConvNeXt-XL$^\\ddagger$~\\cite{liu2022convnet} & C &$384^2$ & 350M & 179G & 87.8\\\\\n\t\\rowcolor{gray!10}\n\tInternImage-L$^\\ddagger$ (ours) & C &$384^2$ & 223M & 108G & 87.7 \\\\\n\t\\rowcolor{gray!20}\n\tInternImage-XL$^\\ddagger$ (ours) & C &$384^2$ & 335M & 163G & 88.0 \\\\\n\t\\hline\n\tViT-G/14$^\\#$~\\cite{zhai2022scaling} & T & $518^2$  & 1.84B & 5160G & 90.5\\\\\n\tCoAtNet-6$^\\#$~\\cite{dai2021coatnet} & T & $512^2$ & 1.47B & 1521G & 90.5\\\\\n\tCoAtNet-7$^\\#$~\\cite{dai2021coatnet} & T & $512^2$ & 2.44B & 2586G & 90.9\\\\\n\tFlorence-CoSwin-H$^\\#$~\\cite{yuan2021florence} & T & $-$ & 893M & $-$ & 90.0 \\\\\n\tSwinV2-G$^\\#$~\\cite{liu2021swinv2} & T & $640^2$ & 3.00B & $-$ & 90.2 \\\\\n\tRepLKNet-XL$^\\#$~\\cite{ding2022replknet} & C & $384^2$ & 335M & 129G & 87.8\\\\\n\tBiT-L-ResNet152x4$^\\#$~\\cite{kolesnikov2020big} & C & $480^2$ & 928M & $-$ & 87.5 \\\\\n\t\\rowcolor{gray!10}\n\tInternImage-H$^\\#$ (ours) & C & $224^2$ & 1.08B & 188G & 88.5  \\\\\n\t\\rowcolor{gray!20}\n\tInternImage-H$^\\#$ (ours) & C & $640^2$ & 1.08B & 1478G & 89.2  \\\\\n\\end{tabular}\n\n    \\caption{\\textbf{Image classification performance on the ImageNet validation set}. ``Type'' refers to model type, where ``T'' and ``C'' denote transformer and CNN, respectively. ``Res'' means the input resolution.\n    \t``Acc'' is the top-1 accuracy.\n    \t``$^\\ddagger$\" indicates the model is pre-trained on ImageNet-22K~\\cite{deng2009imagenet}. ``$^\\#$\" indicates pre-training on extra large-scale private dataset such as JFT-300M~\\cite{xie2020self}, FLD-900M~\\cite{yuan2021florence}, or the joint public dataset in this work.}\n    \\label{tab:cls_imagenet}\n\\end{table}", "cell_list_gold": [{"value": "25M", "char_index": [264, 267], "type": "Hyper-parameter/Architecture", "model": "PVT-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "4G", "char_index": [270, 272], "type": "Hyper-parameter/Architecture", "model": "PVT-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "79.8", "char_index": [275, 279], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "PVT-S", "model settings": {"Type": "T"}}, {"value": "29M", "char_index": [325, 328], "type": "Hyper-parameter/Architecture", "model": "Swin-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "5G", "char_index": [331, 333], "type": "Hyper-parameter/Architecture", "model": "Swin-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "81.3", "char_index": [336, 340], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "Swin-T", "model settings": {"Type": "T"}}, {"value": "25M", "char_index": [391, 394], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-0", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "4G", "char_index": [397, 399], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-0", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "81.6", "char_index": [402, 406], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "CoAtNet-0", "model settings": {"Type": "T"}}, {"value": "25M", "char_index": [458, 461], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B2", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "4G", "char_index": [464, 466], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B2", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "82.0", "char_index": [469, 473], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "PVTv2-B2", "model settings": {"Type": "T"}}, {"value": "22M", "char_index": [529, 532], "type": "Hyper-parameter/Architecture", "model": "DeiT III-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "5G", "char_index": [535, 537], "type": "Hyper-parameter/Architecture", "model": "DeiT III-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "81.4", "char_index": [540, 544], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "DeiT III-S", "model settings": {"Type": "T"}}, {"value": "28M", "char_index": [595, 598], "type": "Hyper-parameter/Architecture", "model": "SwinV2-T/8", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "6G", "char_index": [601, 603], "type": "Hyper-parameter/Architecture", "model": "SwinV2-T/8", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "81.8", "char_index": [606, 610], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "SwinV2-T/8", "model settings": {"Type": "T"}}, {"value": "29M", "char_index": [663, 666], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "5G", "char_index": [669, 671], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "82.1", "char_index": [674, 678], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "ConvNeXt-T", "model settings": {"Type": "C"}}, {"value": "30M", "char_index": [739, 742], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "5G", "char_index": [745, 747], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.5", "char_index": [750, 754], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "InternImage-T", "model settings": {"Type": "C"}}, {"value": "61M", "char_index": [812, 815], "type": "Hyper-parameter/Architecture", "model": "PVT-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "10G", "char_index": [818, 821], "type": "Hyper-parameter/Architecture", "model": "PVT-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "81.7", "char_index": [824, 828], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "PVT-L", "model settings": {"Type": "T"}}, {"value": "50M", "char_index": [875, 878], "type": "Hyper-parameter/Architecture", "model": "Swin-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "9G", "char_index": [881, 883], "type": "Hyper-parameter/Architecture", "model": "Swin-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.0", "char_index": [886, 890], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "Swin-S", "model settings": {"Type": "T"}}, {"value": "42M", "char_index": [941, 944], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-1", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "8G", "char_index": [947, 949], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-1", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.3", "char_index": [952, 956], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "CoAtNet-1", "model settings": {"Type": "T"}}, {"value": "63M", "char_index": [1008, 1011], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B4", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "10G", "char_index": [1014, 1017], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B4", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.6", "char_index": [1020, 1024], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "PVTv2-B4", "model settings": {"Type": "T"}}, {"value": "50M", "char_index": [1076, 1079], "type": "Hyper-parameter/Architecture", "model": "SwinV2-S/8", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "12G", "char_index": [1082, 1085], "type": "Hyper-parameter/Architecture", "model": "SwinV2-S/8", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.7", "char_index": [1088, 1092], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "SwinV2-S/8", "model settings": {"Type": "T"}}, {"value": "50M", "char_index": [1145, 1148], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "9G", "char_index": [1151, 1153], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.1", "char_index": [1156, 1160], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "ConvNeXt-S", "model settings": {"Type": "C"}}, {"value": "50M", "char_index": [1221, 1224], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "8G", "char_index": [1227, 1229], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "84.2", "char_index": [1232, 1236], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "InternImage-S", "model settings": {"Type": "C"}}, {"value": "88M", "char_index": [1291, 1294], "type": "Hyper-parameter/Architecture", "model": "Swin-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "15G", "char_index": [1297, 1300], "type": "Hyper-parameter/Architecture", "model": "Swin-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.5", "char_index": [1303, 1307], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "Swin-B", "model settings": {"Type": "T"}}, {"value": "75M", "char_index": [1358, 1361], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-2", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "16G", "char_index": [1364, 1367], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-2", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "84.1", "char_index": [1370, 1374], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "CoAtNet-2", "model settings": {"Type": "T"}}, {"value": "82M", "char_index": [1426, 1429], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B5", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "12G", "char_index": [1432, 1435], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B5", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.8", "char_index": [1438, 1442], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "PVTv2-B5", "model settings": {"Type": "T"}}, {"value": "87M", "char_index": [1497, 1500], "type": "Hyper-parameter/Architecture", "model": "DeiT III-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "18G", "char_index": [1503, 1506], "type": "Hyper-parameter/Architecture", "model": "DeiT III-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.8", "char_index": [1509, 1513], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "DeiT III-B", "model settings": {"Type": "T"}}, {"value": "88M", "char_index": [1566, 1569], "type": "Hyper-parameter/Architecture", "model": "SwinV2-B/8", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "20G", "char_index": [1572, 1575], "type": "Hyper-parameter/Architecture", "model": "SwinV2-B/8", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "84.2", "char_index": [1578, 1582], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "SwinV2-B/8", "model settings": {"Type": "T"}}, {"value": "79M", "char_index": [1639, 1642], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-31B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "15G", "char_index": [1645, 1648], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-31B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.5", "char_index": [1651, 1655], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "RepLKNet-31B", "model settings": {"Type": "C"}}, {"value": "88M", "char_index": [1708, 1711], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "15G", "char_index": [1714, 1717], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "83.8", "char_index": [1720, 1724], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "ConvNeXt-B", "model settings": {"Type": "C"}}, {"value": "97M", "char_index": [1784, 1787], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "16G", "char_index": [1790, 1793], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "84.9", "char_index": [1796, 1800], "type": "Result", "training data/set": "ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "300"}, "model": "InternImage-B", "model settings": {"Type": "C"}}, {"value": "197M", "char_index": [1865, 1869], "type": "Hyper-parameter/Architecture", "model": "Swin-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "104G", "char_index": [1872, 1876], "type": "Hyper-parameter/Architecture", "model": "Swin-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.3", "char_index": [1879, 1883], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "Swin-L", "model settings": {"Type": "T"}}, {"value": "168M", "char_index": [1945, 1949], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-3", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "107G", "char_index": [1952, 1956], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-3", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.6", "char_index": [1959, 1963], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "CoAtNet-3", "model settings": {"Type": "T"}}, {"value": "275M", "char_index": [2026, 2030], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-4", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "190G", "char_index": [2033, 2037], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-4", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.9", "char_index": [2040, 2044], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "CoAtNet-4", "model settings": {"Type": "T"}}, {"value": "304M", "char_index": [2110, 2114], "type": "Hyper-parameter/Architecture", "model": "DeiT III-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "191G", "char_index": [2117, 2121], "type": "Hyper-parameter/Architecture", "model": "DeiT III-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.7", "char_index": [2124, 2128], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "DeiT III-L", "model settings": {"Type": "T"}}, {"value": "197M", "char_index": [2192, 2196], "type": "Hyper-parameter/Architecture", "model": "SwinV2-L/24", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "115G", "char_index": [2199, 2203], "type": "Hyper-parameter/Architecture", "model": "SwinV2-L/24", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.6", "char_index": [2206, 2210], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "SwinV2-L/24", "model settings": {"Type": "T"}}, {"value": "172M", "char_index": [2277, 2281], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-31L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "96G", "char_index": [2284, 2287], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-31L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "86.6", "char_index": [2290, 2294], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "RepLKNet-31L", "model settings": {"Type": "C"}}, {"value": "198M", "char_index": [2357, 2361], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "101G", "char_index": [2364, 2368], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.5", "char_index": [2371, 2375], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "ConvNeXt-L", "model settings": {"Type": "C"}}, {"value": "350M", "char_index": [2439, 2443], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-XL", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "179G", "char_index": [2446, 2450], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-XL", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.8", "char_index": [2453, 2457], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "ConvNeXt-XL", "model settings": {"Type": "C"}}, {"value": "223M", "char_index": [2528, 2532], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "108G", "char_index": [2535, 2539], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.7", "char_index": [2542, 2546], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "InternImage-L", "model settings": {"Type": "C"}}, {"value": "335M", "char_index": [2619, 2623], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "163G", "char_index": [2626, 2630], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "88.0", "char_index": [2633, 2637], "type": "Result", "training data/set": "ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "90 30"}, "model": "InternImage-XL", "model settings": {"Type": "C"}}, {"value": "1.84B", "char_index": [2704, 2709], "type": "Hyper-parameter/Architecture", "model": "ViT-G/14", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "5160G", "char_index": [2712, 2717], "type": "Hyper-parameter/Architecture", "model": "ViT-G/14", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "90.5", "char_index": [2720, 2724], "type": "Result", "training data/set": "JFT-300M ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "ViT-G/14", "model settings": {"Type": "T"}}, {"value": "1.47B", "char_index": [2781, 2786], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-6", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "1521G", "char_index": [2789, 2794], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-6", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "90.5", "char_index": [2797, 2801], "type": "Result", "training data/set": "JFT-300M ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "CoAtNet-6", "model settings": {"Type": "T"}}, {"value": "2.44B", "char_index": [2858, 2863], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-7", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "2586G", "char_index": [2866, 2871], "type": "Hyper-parameter/Architecture", "model": "CoAtNet-7", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "90.9", "char_index": [2874, 2878], "type": "Result", "training data/set": "JFT-300M ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "CoAtNet-7", "model settings": {"Type": "T"}}, {"value": "893M", "char_index": [2941, 2945], "type": "Hyper-parameter/Architecture", "model": "Florence-CoSwin-H", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "90.0", "char_index": [2954, 2958], "type": "Result", "training data/set": "FLD-900M ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "Florence-CoSwin-H", "model settings": {"Type": "T"}}, {"value": "3.00B", "char_index": [3014, 3019], "type": "Hyper-parameter/Architecture", "model": "SwinV2-G", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "90.2", "char_index": [3028, 3032], "type": "Result", "training data/set": "JFT-300M ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "SwinV2-G", "model settings": {"Type": "T"}}, {"value": "335M", "char_index": [3094, 3098], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-XL", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "129G", "char_index": [3101, 3105], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-XL", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "87.8", "char_index": [3108, 3112], "type": "Result", "training data/set": "JFT-300M ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "RepLKNet-XL", "model settings": {"Type": "C"}}, {"value": "928M", "char_index": [3180, 3184], "type": "Hyper-parameter/Architecture", "model": "BiT-L-ResNet152x4", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "87.5", "char_index": [3193, 3197], "type": "Result", "training data/set": "JFT-300M ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "BiT-L-ResNet152x4", "model settings": {"Type": "C"}}, {"value": "1.08B", "char_index": [3264, 3269], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "188G", "char_index": [3272, 3276], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "88.5", "char_index": [3279, 3283], "type": "Result", "training data/set": "M3I Pre-training ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "InternImage-H", "model settings": {"Type": "C"}}, {"value": "1.08B", "char_index": [3351, 3356], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ImageNet"}, {"value": "1478G", "char_index": [3359, 3364], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ImageNet"}, {"value": "89.2", "char_index": [3367, 3371], "type": "Result", "training data/set": "M3I Pre-training ImageNet-22K ImageNet-1K", "test data/set": "ImageNet validation", "task": "image classification", "metric": ["Acc", "top-1 accuracy"], "experimental settings": {"number of epochs": "30 30"}, "model": "InternImage-H", "model settings": {"Type": "C"}}]}, "2211.05778v1_table2": {"table_code": "\\begin{table*}[t]\\small\n\t\\centering\n\t\\renewcommand{\\arraystretch}{0.93}\n\t\\footnotesize\n    \\setlength\\tabcolsep{1.65mm}{\n    \\begin{tabular}{l|cc|cccccc|cccccc}\n        \\multirow{2}{*}{Method} & \\multirow{2}{*}{\\#Params} & \\multirow{2}{*}{\\#FLOPs} &\n        \\multicolumn{6}{c|}{Mask R-CNN 1$\\times$ schedule} & \\multicolumn{6}{c}{Mask R-CNN 3$\\times$+MS schedule}\\\\\n        ~ &  &\n        & $\\rm AP^b$ & $\\rm AP^b_{50}$ & $\\rm AP^b_{75}$ & $\\rm AP^m$ & $\\rm AP^m_{50}$ & $\\rm AP^m_{75}$ \n        & $\\rm AP^b$ & $\\rm AP^b_{50}$ & $\\rm AP^b_{75}$ & $\\rm AP^m$ & $\\rm AP^m_{50}$ & $\\rm AP^m_{75}$   \\\\\n\t    \\hline\n        Swin-T~\\cite{liu2021swin} & 48M & 267G\n        & 42.7 & 65.2 & 46.8 & 39.3 & 62.2 & 42.2 & 46.0 & 68.1 & 50.3 & 41.6 & 65.1 & 44.9  \\\\\n        ConvNeXt-T~\\cite{liu2022convnet} & 48M & 262G\n        & 44.2 & 66.6 & 48.3 & 40.1 & 63.3 & 42.8 & 46.2 & 67.9 & 50.8 & 41.7 & 65.0 & 44.9  \\\\\n        PVTv2-B2~\\cite{wang2021pvtv2} & 45M & 309G\n        & 45.3 & 67.1 & 49.6 & 41.2 & 64.2 & 44.4 & 47.8 & 69.7 & 52.6 & 43.1 & 66.8 & 46.7  \\\\\n        ViT-S~\\cite{chen2022vitadapter} & 48M & 353G & 44.7 & 65.8 & 48.3 & 39.9 & 62.5 & 42.8 & 48.2 & 69.7 & 52.5 & 42.8 & 66.4 & 45.9   \\\\\n        \\rowcolor{gray!20}\n        InternImage-T (ours)  & 49M & 270G\n        & 47.2 & 69.0 & 52.1 & 42.5 & 66.1 & 45.8 & 49.1 & 70.3 & 54.0 & 43.7 & 67.3 & 47.1  \\\\\n        \\hline\n        Swin-S~\\cite{liu2021swin} & 69M & 354G\n        & 44.8 & 66.6 & 48.9 & 40.9 & 63.4 & 44.2 & 48.2 & 69.8 & 52.8 & 43.2 & 67.0 & 46.1 \\\\\n        ConvNeXt-S~\\cite{liu2022convnet} & 70M & 348G\n        & 45.4 & 67.9 & 50.0 & 41.8 & 65.2 & 45.1 & 47.9 & 70.0 & 52.7 & 42.9 & 66.9 & 46.2 \\\\\n        PVTv2-B3~\\cite{wang2021pvtv2} & 65M & 397G\n        & 47.0 & 68.1 & 51.7 & 42.5 & 65.7 & 45.7 & 48.4 & 69.8 & 53.3 & 43.2 & 66.9 & 46.7 \\\\\n        \\rowcolor{gray!20}\n        InternImage-S (ours)  & 69M & 340G\n        & 47.8 & 69.9 & 52.8 & 43.3 & 67.1 & 46.7 & 49.7 & 71.1 & 54.5 & 44.4 & 68.5 & 47.8 \\\\\n        \\hline\n        Swin-B~\\cite{liu2021swin} & 107M & 496G\n        & 46.9 & $-$ & $-$ & 42.3 & $-$ & $-$ & 48.6 & 70.0 & 53.4 & 43.3 & 67.1 & 46.7  \\\\\n        ConvNeXt-B~\\cite{liu2022convnet} & 108M & 486G & 47.0 & 69.4 & 51.7 & 42.7 & 66.3 & 46.0 & 48.5 & 70.1 & 53.3 & 43.5 & 67.1 & 46.7 \\\\\n        PVTv2-B5~\\cite{wang2021pvtv2} & 102M & 557G\n        & 47.4 & 68.6 & 51.9 & 42.5 & 65.7 & 46.0 & 48.4 & 69.2 & 52.9 & 42.9 & 66.6 & 46.2  \\\\\n        ViT-B~\\cite{chen2022vitadapter}  & 120M & 781G\n        & 47.0 & 68.2 & 51.4 & 41.8 & 65.1 & 44.9 & 49.6 & 70.6 & 54.0 & 43.6 & 67.7 & 46.9  \\\\\n        \\rowcolor{gray!20}\n        InternImage-B (ours) & 115M & 501G\n        & 48.8 & 71.0 & 53.9 & 44.0 & 67.8 & 47.5 & 50.3 & 71.4 & 55.3 & 44.8 & 68.7 & 48.0 \\\\\n    \\end{tabular}}\n    \n    \\caption{\n    \\textbf{Object detection and instance segmentation with Mask R-CNN on COCO \\texttt{val2017}.}\n    The FLOPs are measured with 1280$\\times$800 inputs.\n    AP$^\\text{b}$ and AP$^\\text{m}$ represent box AP and mask AP, respectively.\n    ``MS\" means multi-scale training. \n\t}\n    \\label{tab:results_detection_mask}\n    \\label{tab:mask}\n\\end{table*}", "table_label": "{tab:results_detection_mask}", "table_numeric_cells": [["48M", "48M", 647, 650, 647, 650], ["267G", "267G", 653, 657, 653, 657], ["42.7", "42.7", 668, 672, 668, 672], ["65.2", "65.2", 675, 679, 675, 679], ["46.8", "46.8", 682, 686, 682, 686], ["39.3", "39.3", 689, 693, 689, 693], ["62.2", "62.2", 696, 700, 696, 700], ["42.2", "42.2", 703, 707, 703, 707], ["46.0", "46.0", 710, 714, 710, 714], ["68.1", "68.1", 717, 721, 717, 721], ["50.3", "50.3", 724, 728, 724, 728], ["41.6", "41.6", 731, 735, 731, 735], ["65.1", "65.1", 738, 742, 738, 742], ["44.9", "44.9", 745, 749, 745, 749], ["48M", "48M", 797, 800, 797, 800], ["262G", "262G", 803, 807, 803, 807], ["44.2", "44.2", 818, 822, 818, 822], ["66.6", "66.6", 825, 829, 825, 829], ["48.3", "48.3", 832, 836, 832, 836], ["40.1", "40.1", 839, 843, 839, 843], ["63.3", "63.3", 846, 850, 846, 850], ["42.8", "42.8", 853, 857, 853, 857], ["46.2", "46.2", 860, 864, 860, 864], ["67.9", "67.9", 867, 871, 867, 871], ["50.8", "50.8", 874, 878, 874, 878], ["41.7", "41.7", 881, 885, 881, 885], ["65.0", "65.0", 888, 892, 888, 892], ["44.9", "44.9", 895, 899, 895, 899], ["45M", "45M", 944, 947, 944, 947], ["309G", "309G", 950, 954, 950, 954], ["45.3", "45.3", 965, 969, 965, 969], ["67.1", "67.1", 972, 976, 972, 976], ["49.6", "49.6", 979, 983, 979, 983], ["41.2", "41.2", 986, 990, 986, 990], ["64.2", "64.2", 993, 997, 993, 997], ["44.4", "44.4", 1000, 1004, 1000, 1004], ["47.8", "47.8", 1007, 1011, 1007, 1011], ["69.7", "69.7", 1014, 1018, 1014, 1018], ["52.6", "52.6", 1021, 1025, 1021, 1025], ["43.1", "43.1", 1028, 1032, 1028, 1032], ["66.8", "66.8", 1035, 1039, 1035, 1039], ["46.7", "46.7", 1042, 1046, 1042, 1046], ["48M", "48M", 1093, 1096, 1093, 1096], ["353G", "353G", 1099, 1103, 1099, 1103], ["44.7", "44.7", 1106, 1110, 1106, 1110], ["65.8", "65.8", 1113, 1117, 1113, 1117], ["48.3", "48.3", 1120, 1124, 1120, 1124], ["39.9", "39.9", 1127, 1131, 1127, 1131], ["62.5", "62.5", 1134, 1138, 1134, 1138], ["42.8", "42.8", 1141, 1145, 1141, 1145], ["48.2", "48.2", 1148, 1152, 1148, 1152], ["69.7", "69.7", 1155, 1159, 1155, 1159], ["52.5", "52.5", 1162, 1166, 1162, 1166], ["42.8", "42.8", 1169, 1173, 1169, 1173], ["66.4", "66.4", 1176, 1180, 1176, 1180], ["45.9", "45.9", 1183, 1187, 1183, 1187], ["49M", "49M", 1252, 1255, 1252, 1255], ["270G", "270G", 1258, 1262, 1258, 1262], ["47.2", "47.2", 1273, 1277, 1273, 1277], ["69.0", "69.0", 1280, 1284, 1280, 1284], ["52.1", "52.1", 1287, 1291, 1287, 1291], ["42.5", "42.5", 1294, 1298, 1294, 1298], ["66.1", "66.1", 1301, 1305, 1301, 1305], ["45.8", "45.8", 1308, 1312, 1308, 1312], ["49.1", "49.1", 1315, 1319, 1315, 1319], ["70.3", "70.3", 1322, 1326, 1322, 1326], ["54.0", "54.0", 1329, 1333, 1329, 1333], ["43.7", "43.7", 1336, 1340, 1336, 1340], ["67.3", "67.3", 1343, 1347, 1343, 1347], ["47.1", "47.1", 1350, 1354, 1350, 1354], ["69M", "69M", 1410, 1413, 1410, 1413], ["354G", "354G", 1416, 1420, 1416, 1420], ["44.8", "44.8", 1431, 1435, 1431, 1435], ["66.6", "66.6", 1438, 1442, 1438, 1442], ["48.9", "48.9", 1445, 1449, 1445, 1449], ["40.9", "40.9", 1452, 1456, 1452, 1456], ["63.4", "63.4", 1459, 1463, 1459, 1463], ["44.2", "44.2", 1466, 1470, 1466, 1470], ["48.2", "48.2", 1473, 1477, 1473, 1477], ["69.8", "69.8", 1480, 1484, 1480, 1484], ["52.8", "52.8", 1487, 1491, 1487, 1491], ["43.2", "43.2", 1494, 1498, 1494, 1498], ["67.0", "67.0", 1501, 1505, 1501, 1505], ["46.1", "46.1", 1508, 1512, 1508, 1512], ["70M", "70M", 1559, 1562, 1559, 1562], ["348G", "348G", 1565, 1569, 1565, 1569], ["45.4", "45.4", 1580, 1584, 1580, 1584], ["67.9", "67.9", 1587, 1591, 1587, 1591], ["50.0", "50.0", 1594, 1598, 1594, 1598], ["41.8", "41.8", 1601, 1605, 1601, 1605], ["65.2", "65.2", 1608, 1612, 1608, 1612], ["45.1", "45.1", 1615, 1619, 1615, 1619], ["47.9", "47.9", 1622, 1626, 1622, 1626], ["70.0", "70.0", 1629, 1633, 1629, 1633], ["52.7", "52.7", 1636, 1640, 1636, 1640], ["42.9", "42.9", 1643, 1647, 1643, 1647], ["66.9", "66.9", 1650, 1654, 1650, 1654], ["46.2", "46.2", 1657, 1661, 1657, 1661], ["65M", "65M", 1705, 1708, 1705, 1708], ["397G", "397G", 1711, 1715, 1711, 1715], ["47.0", "47.0", 1726, 1730, 1726, 1730], ["68.1", "68.1", 1733, 1737, 1733, 1737], ["51.7", "51.7", 1740, 1744, 1740, 1744], ["42.5", "42.5", 1747, 1751, 1747, 1751], ["65.7", "65.7", 1754, 1758, 1754, 1758], ["45.7", "45.7", 1761, 1765, 1761, 1765], ["48.4", "48.4", 1768, 1772, 1768, 1772], ["69.8", "69.8", 1775, 1779, 1775, 1779], ["53.3", "53.3", 1782, 1786, 1782, 1786], ["43.2", "43.2", 1789, 1793, 1789, 1793], ["66.9", "66.9", 1796, 1800, 1796, 1800], ["46.7", "46.7", 1803, 1807, 1803, 1807], ["69M", "69M", 1870, 1873, 1870, 1873], ["340G", "340G", 1876, 1880, 1876, 1880], ["47.8", "47.8", 1891, 1895, 1891, 1895], ["69.9", "69.9", 1898, 1902, 1898, 1902], ["52.8", "52.8", 1905, 1909, 1905, 1909], ["43.3", "43.3", 1912, 1916, 1912, 1916], ["67.1", "67.1", 1919, 1923, 1919, 1923], ["46.7", "46.7", 1926, 1930, 1926, 1930], ["49.7", "49.7", 1933, 1937, 1933, 1937], ["71.1", "71.1", 1940, 1944, 1940, 1944], ["54.5", "54.5", 1947, 1951, 1947, 1951], ["44.4", "44.4", 1954, 1958, 1954, 1958], ["68.5", "68.5", 1961, 1965, 1961, 1965], ["47.8", "47.8", 1968, 1972, 1968, 1972], ["107M", "107M", 2027, 2031, 2027, 2031], ["496G", "496G", 2034, 2038, 2034, 2038], ["46.9", "46.9", 2049, 2053, 2049, 2053], ["42.3", "42.3", 2068, 2072, 2068, 2072], ["48.6", "48.6", 2087, 2091, 2087, 2091], ["70.0", "70.0", 2094, 2098, 2094, 2098], ["53.4", "53.4", 2101, 2105, 2101, 2105], ["43.3", "43.3", 2108, 2112, 2108, 2112], ["67.1", "67.1", 2115, 2119, 2115, 2119], ["46.7", "46.7", 2122, 2126, 2122, 2126], ["108M", "108M", 2174, 2178, 2174, 2178], ["486G", "486G", 2181, 2185, 2181, 2185], ["47.0", "47.0", 2188, 2192, 2188, 2192], ["69.4", "69.4", 2195, 2199, 2195, 2199], ["51.7", "51.7", 2202, 2206, 2202, 2206], ["42.7", "42.7", 2209, 2213, 2209, 2213], ["66.3", "66.3", 2216, 2220, 2216, 2220], ["46.0", "46.0", 2223, 2227, 2223, 2227], ["48.5", "48.5", 2230, 2234, 2230, 2234], ["70.1", "70.1", 2237, 2241, 2237, 2241], ["53.3", "53.3", 2244, 2248, 2244, 2248], ["43.5", "43.5", 2251, 2255, 2251, 2255], ["67.1", "67.1", 2258, 2262, 2258, 2262], ["46.7", "46.7", 2265, 2269, 2265, 2269], ["102M", "102M", 2313, 2317, 2313, 2317], ["557G", "557G", 2320, 2324, 2320, 2324], ["47.4", "47.4", 2335, 2339, 2335, 2339], ["68.6", "68.6", 2342, 2346, 2342, 2346], ["51.9", "51.9", 2349, 2353, 2349, 2353], ["42.5", "42.5", 2356, 2360, 2356, 2360], ["65.7", "65.7", 2363, 2367, 2363, 2367], ["46.0", "46.0", 2370, 2374, 2370, 2374], ["48.4", "48.4", 2377, 2381, 2377, 2381], ["69.2", "69.2", 2384, 2388, 2384, 2388], ["52.9", "52.9", 2391, 2395, 2391, 2395], ["42.9", "42.9", 2398, 2402, 2398, 2402], ["66.6", "66.6", 2405, 2409, 2405, 2409], ["46.2", "46.2", 2412, 2416, 2412, 2416], ["120M", "120M", 2464, 2468, 2464, 2468], ["781G", "781G", 2471, 2475, 2471, 2475], ["47.0", "47.0", 2486, 2490, 2486, 2490], ["68.2", "68.2", 2493, 2497, 2493, 2497], ["51.4", "51.4", 2500, 2504, 2500, 2504], ["41.8", "41.8", 2507, 2511, 2507, 2511], ["65.1", "65.1", 2514, 2518, 2514, 2518], ["44.9", "44.9", 2521, 2525, 2521, 2525], ["49.6", "49.6", 2528, 2532, 2528, 2532], ["70.6", "70.6", 2535, 2539, 2535, 2539], ["54.0", "54.0", 2542, 2546, 2542, 2546], ["43.6", "43.6", 2549, 2553, 2549, 2553], ["67.7", "67.7", 2556, 2560, 2556, 2560], ["46.9", "46.9", 2563, 2567, 2563, 2567], ["115M", "115M", 2630, 2634, 2630, 2634], ["501G", "501G", 2637, 2641, 2637, 2641], ["48.8", "48.8", 2652, 2656, 2652, 2656], ["71.0", "71.0", 2659, 2663, 2659, 2663], ["53.9", "53.9", 2666, 2670, 2666, 2670], ["44.0", "44.0", 2673, 2677, 2673, 2677], ["67.8", "67.8", 2680, 2684, 2680, 2684], ["47.5", "47.5", 2687, 2691, 2687, 2691], ["50.3", "50.3", 2694, 2698, 2694, 2698], ["71.4", "71.4", 2701, 2705, 2701, 2705], ["55.3", "55.3", 2708, 2712, 2708, 2712], ["44.8", "44.8", 2715, 2719, 2715, 2719], ["68.7", "68.7", 2722, 2726, 2722, 2726], ["48.0", "48.0", 2729, 2733, 2729, 2733]], "text_chunk_selected": "\\author{\n    Wenhai Wang$^{1*}$, \n    Jifeng Dai$^{2,1*}$,\n    Zhe Chen$^{3,1*}$,\n    Zhenhang Huang$^{1*}$,\n    Zhiqi Li$^{3,1*}$,\n    Xizhou Zhu$^{4*}$,\\\\\n    Xiaowei Hu$^{1}$,\n    Tong Lu$^{3}$,\n    Lewei Lu$^{4}$,\n    Hongsheng Li$^{5}$,\n    Xiaogang Wang$^{3,5}$,\n    Yu Qiao$^{1}$\\textsuperscript{\\Letter}\\\\\n    $^1$ Shanghai AI Laboratory~~~\n    $^2$Tsinghua University~~~\\\\\n    $^3$Nanjing University~~~\n    $^4$SenseTime Research~~~\n    $^5$The Chinese University of Hong Kong\\\\\n    {\\small \\url{https://github.com/OpenGVLab/InternImage}}\n}\n\nwhere $K$ represents the total number of sampling points, and $k$ enumerates the sampling point. \n$\\mathbf{w}_k\\!\\in\\!\\mathbb{R}^{C\\times C}$ denotes the projection weights of the $k$-th sampling point,\nand $\\mathbf{m}_k\\!\\in\\!\\mathbb{R}$ represents the modulation scalar of the $k$-th sampling point, which is normalized by sigmoid function.\n$p_k$ denotes the $k$-th location of the pre-defined grid sampling $\\{(-1, -1), (-1, 0), ..., (0, +1), ..., (+1, +1)\\}$ as in regular convolutions,\nand $\\Delta p_k$ is the offset corresponding to the $k$-th grid sampling location.\nWe see from the equation that (1) for long-range dependencies, the sampling offset $\\Delta p_k$ is flexible and able to interact with short- or long-range features; and (2) for adaptive spatial aggregation, both the sampling offset $\\Delta p_k$ and modulation scalar $\\mathbf{m}_k$ are learnable and conditioned by input $\\mathbf{x}$. \nSo it can be found that \\emph{DCNv2 shares similar favorable properties with MHSA}, which motivated us to develop large-scale CNN-based foundation models on the basis of this operator.\n\nwhere $G$ denotes the total number of aggregation groups.\nFor the $g$-th group,\n$\\mathbf{w}_g\\!\\in\\!\\mathbb{R}^{C\\times C'}$, $\\mathbf{m}_{gk}\\!\\in\\!\\mathbb{R}$ denote the location-irrelevant projection weights of the group, where $C'\\!=\\!C/G$ represents the group dimension.\n$\\mathbf{m}_{gk}\\!\\in\\!\\mathbb{R}$ denotes the modulation scalar of the $k$-th sampling point in the $g$-th group, normalized by the softmax function along the dimension $K$.\n$\\mathbf{x}_g\\!\\in\\!\\mathbb{R}^{C'\\times H \\times W}$ represents the sliced input feature map.\n$\\Delta p_{gk}$ is the offset corresponding to the grid sampling location $p_k$ in the $g$-th group.\n\n\\textbf{Stem \\& downsampling layers.}\nTo obtain hierarchical feature maps, we use \nconvolutional stem and downsampling layers to resize the feature maps to different scales.\nAs shown in Fig. \\ref{fig:arch}, the stem layer is placed before the first stage to reduce the input resolution by 4 times.\nIt consists of two convolutions, two LN layers, and one GELU layer,\nwhere the kernel size of the two convolutions is 3, the stride is 2, the padding is 1,\nand the output channel of the first convolution is half of the second one.\nSimilarly, the downsampling layer is made up of a 3$\\times$3 convolution with a stride of 2 and a padding of 1, followed by one LN layer.\nIt sits between the two stages and is used to downsample the input feature map by 2 times.\n\nwhere $\\alpha\\!\\geq\\!1$, $\\beta\\!\\geq\\!1$, and $\\alpha\\beta^{1.99}\\!\\approx\\!2$. Here, 1.99 is specific for InternImage and calculated by doubling the model width and keeping the depth constant.\nWe experimentally find out that the best scaling setting is $\\alpha\\!=\\!1.09$ and $\\beta\\!=\\!1.36$, and then we base on it to construct InternImage\\ variants with different parameter scales, namely InternImage-T/S/B/L/XL, whose complexity is similar to those of ConvNeXt~\\cite{liu2022convnet}. \nTo further test the capability, we built a larger InternImage-H with 1 billion parameters.\nThe detailed configurations are summarized in Table~\\ref{tab:model_size}.\n\n\\subsection{Image Classification}\n\\textbf{Settings.}\nWe evaluate the classification performance of InternImage\\ on ImageNet~\\cite{deng2009imagenet}. \nFor fair comparisons, following common practices~\\cite{touvron2021training,wang2021pyramid,liu2021swin,liu2022convnet}, InternImage-T/S/B are trained on ImageNet-1K ($\\sim$1.3 million) for 300 epochs, \nand InternImage-L/XL are first trained on ImageNet-22K ($\\sim$14.2 million) for 90 epochs and then fine-tuned on ImageNet-1K for 30 epochs.\nTo further explore the capability of our model and match the large-scale private data used in previous methods~\\cite{dai2021coatnet,liu2021swinv2,yuan2021florence}, we adopt M3I Pre-training~\\cite{anonymous2022m2i}, an effective pre-training approach, to pre-train InternImage-H on a 427 million joint dataset of public Laion-400M~\\cite{schuhmann2021laion}, YFCC-15M~\\cite{thomee2016yfcc100m}, and CC12M~\\cite{changpinyo2021conceptual} for 30 epochs,\nand then we fine-tune the model on ImageNet-22K and -1K for 30 epochs, respectively.\n\n\\textbf{Settings.}\nWe verify the effectiveness of our InternImage\\ on the COCO benchmark~\\cite{lin2014microsoft}, on top of two representative object detection frameworks: Mask R-CNN~\\cite{he2017mask}, and Cascade Mask R-CNN~\\cite{cai2018cascade}.\nWe follow common practices~\\cite{liu2021swin,wang2021pvtv2} to initialize the backbone with pre-trained classification weights, and by default use a 1$\\times$ (12 epochs) or 3$\\times$ (36 epochs) training schedule to train detection models.\n\n\\textbf{Results.}\nAs shown in Table~\\ref{tab:results_detection_mask}, when using Mask R-CNN for object detection, \nwe find that under a comparable number of parameters, our models significantly surpass their counterparts. \nFor example, with the $1\\times$ training schedule, the box AP (AP$^{\\rm b}$) of InternImage-T is 4.5 points better than Swin-T~\\cite{liu2021swin} (47.2 \\vs 42.7),\nand 3.0 points higher than ConvNeXt-T~\\cite{liu2022convnet} (47.2 \\vs 44.2).\nAs reported in Table~\\ref{tab:results_detection_cascade}, with more parameters and more advanced Cascade Mask R-CNN, InternImage-XL achieves AP$^{\\rm b}$ of 55.3, surpassing ConvNeXt-XL by 2.7 points (55.3 \\vs 53.6).", "table_source": "\\begin{table*}[t]\\small\n\t\\centering\n\t\\renewcommand{\\arraystretch}{0.93}\n\t\\footnotesize\n    \\setlength\\tabcolsep{1.65mm}{\n    \\begin{tabular}{l|cc|cccccc|cccccc}\n        \\multirow{2}{*}{Method} & \\multirow{2}{*}{\\#Params} & \\multirow{2}{*}{\\#FLOPs} &\n        \\multicolumn{6}{c|}{Mask R-CNN 1$\\times$ schedule} & \\multicolumn{6}{c}{Mask R-CNN 3$\\times$+MS schedule}\\\\\n        ~ &  &\n        & $\\rm AP^b$ & $\\rm AP^b_{50}$ & $\\rm AP^b_{75}$ & $\\rm AP^m$ & $\\rm AP^m_{50}$ & $\\rm AP^m_{75}$ \n        & $\\rm AP^b$ & $\\rm AP^b_{50}$ & $\\rm AP^b_{75}$ & $\\rm AP^m$ & $\\rm AP^m_{50}$ & $\\rm AP^m_{75}$   \\\\\n\t    \\hline\n        Swin-T~\\cite{liu2021swin} & 48M & 267G\n        & 42.7 & 65.2 & 46.8 & 39.3 & 62.2 & 42.2 & 46.0 & 68.1 & 50.3 & 41.6 & 65.1 & 44.9  \\\\\n        ConvNeXt-T~\\cite{liu2022convnet} & 48M & 262G\n        & 44.2 & 66.6 & 48.3 & 40.1 & 63.3 & 42.8 & 46.2 & 67.9 & 50.8 & 41.7 & 65.0 & 44.9  \\\\\n        PVTv2-B2~\\cite{wang2021pvtv2} & 45M & 309G\n        & 45.3 & 67.1 & 49.6 & 41.2 & 64.2 & 44.4 & 47.8 & 69.7 & 52.6 & 43.1 & 66.8 & 46.7  \\\\\n        ViT-S~\\cite{chen2022vitadapter} & 48M & 353G & 44.7 & 65.8 & 48.3 & 39.9 & 62.5 & 42.8 & 48.2 & 69.7 & 52.5 & 42.8 & 66.4 & 45.9   \\\\\n        \\rowcolor{gray!20}\n        InternImage-T (ours)  & 49M & 270G\n        & 47.2 & 69.0 & 52.1 & 42.5 & 66.1 & 45.8 & 49.1 & 70.3 & 54.0 & 43.7 & 67.3 & 47.1  \\\\\n        \\hline\n        Swin-S~\\cite{liu2021swin} & 69M & 354G\n        & 44.8 & 66.6 & 48.9 & 40.9 & 63.4 & 44.2 & 48.2 & 69.8 & 52.8 & 43.2 & 67.0 & 46.1 \\\\\n        ConvNeXt-S~\\cite{liu2022convnet} & 70M & 348G\n        & 45.4 & 67.9 & 50.0 & 41.8 & 65.2 & 45.1 & 47.9 & 70.0 & 52.7 & 42.9 & 66.9 & 46.2 \\\\\n        PVTv2-B3~\\cite{wang2021pvtv2} & 65M & 397G\n        & 47.0 & 68.1 & 51.7 & 42.5 & 65.7 & 45.7 & 48.4 & 69.8 & 53.3 & 43.2 & 66.9 & 46.7 \\\\\n        \\rowcolor{gray!20}\n        InternImage-S (ours)  & 69M & 340G\n        & 47.8 & 69.9 & 52.8 & 43.3 & 67.1 & 46.7 & 49.7 & 71.1 & 54.5 & 44.4 & 68.5 & 47.8 \\\\\n        \\hline\n        Swin-B~\\cite{liu2021swin} & 107M & 496G\n        & 46.9 & $-$ & $-$ & 42.3 & $-$ & $-$ & 48.6 & 70.0 & 53.4 & 43.3 & 67.1 & 46.7  \\\\\n        ConvNeXt-B~\\cite{liu2022convnet} & 108M & 486G & 47.0 & 69.4 & 51.7 & 42.7 & 66.3 & 46.0 & 48.5 & 70.1 & 53.3 & 43.5 & 67.1 & 46.7 \\\\\n        PVTv2-B5~\\cite{wang2021pvtv2} & 102M & 557G\n        & 47.4 & 68.6 & 51.9 & 42.5 & 65.7 & 46.0 & 48.4 & 69.2 & 52.9 & 42.9 & 66.6 & 46.2  \\\\\n        ViT-B~\\cite{chen2022vitadapter}  & 120M & 781G\n        & 47.0 & 68.2 & 51.4 & 41.8 & 65.1 & 44.9 & 49.6 & 70.6 & 54.0 & 43.6 & 67.7 & 46.9  \\\\\n        \\rowcolor{gray!20}\n        InternImage-B (ours) & 115M & 501G\n        & 48.8 & 71.0 & 53.9 & 44.0 & 67.8 & 47.5 & 50.3 & 71.4 & 55.3 & 44.8 & 68.7 & 48.0 \\\\\n    \\end{tabular}}\n    \n    \\caption{\n    \\textbf{Object detection and instance segmentation with Mask R-CNN on COCO \\texttt{val2017}.}\n    The FLOPs are measured with 1280$\\times$800 inputs.\n    AP$^\\text{b}$ and AP$^\\text{m}$ represent box AP and mask AP, respectively.\n    ``MS\" means multi-scale training. \n\t}\n    \\label{tab:results_detection_mask}\n    \\label{tab:mask}\n\\end{table*}", "cell_list_gold": [{"value": "48M", "char_index": [647, 650], "type": "Hyper-parameter/Architecture", "model": "Swin-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "267G", "char_index": [653, 657], "type": "Hyper-parameter/Architecture", "model": "Swin-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "42.7", "char_index": [668, 672], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65.2", "char_index": [675, 679], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.8", "char_index": [682, 686], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "39.3", "char_index": [689, 693], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "62.2", "char_index": [696, 700], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.2", "char_index": [703, 707], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.0", "char_index": [710, 714], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "68.1", "char_index": [717, 721], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "50.3", "char_index": [724, 728], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "41.6", "char_index": [731, 735], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65.1", "char_index": [738, 742], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.9", "char_index": [745, 749], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48M", "char_index": [797, 800], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "262G", "char_index": [803, 807], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "44.2", "char_index": [818, 822], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.6", "char_index": [825, 829], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.3", "char_index": [832, 836], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "40.1", "char_index": [839, 843], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "63.3", "char_index": [846, 850], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.8", "char_index": [853, 857], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.2", "char_index": [860, 864], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.9", "char_index": [867, 871], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "50.8", "char_index": [874, 878], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "41.7", "char_index": [881, 885], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65.0", "char_index": [888, 892], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.9", "char_index": [895, 899], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "45M", "char_index": [944, 947], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B2", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "309G", "char_index": [950, 954], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B2", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "45.3", "char_index": [965, 969], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.1", "char_index": [972, 976], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "49.6", "char_index": [979, 983], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "41.2", "char_index": [986, 990], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "64.2", "char_index": [993, 997], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.4", "char_index": [1000, 1004], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "47.8", "char_index": [1007, 1011], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69.7", "char_index": [1014, 1018], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "52.6", "char_index": [1021, 1025], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "43.1", "char_index": [1028, 1032], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.8", "char_index": [1035, 1039], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.7", "char_index": [1042, 1046], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B2", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48M", "char_index": [1093, 1096], "type": "Hyper-parameter/Architecture", "model": "ViT-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "353G", "char_index": [1099, 1103], "type": "Hyper-parameter/Architecture", "model": "ViT-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "44.7", "char_index": [1106, 1110], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65.8", "char_index": [1113, 1117], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.3", "char_index": [1120, 1124], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "39.9", "char_index": [1127, 1131], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "62.5", "char_index": [1134, 1138], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.8", "char_index": [1141, 1145], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.2", "char_index": [1148, 1152], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69.7", "char_index": [1155, 1159], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "52.5", "char_index": [1162, 1166], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.8", "char_index": [1169, 1173], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.4", "char_index": [1176, 1180], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "45.9", "char_index": [1183, 1187], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "49M", "char_index": [1252, 1255], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "270G", "char_index": [1258, 1262], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "47.2", "char_index": [1273, 1277], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69.0", "char_index": [1280, 1284], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "52.1", "char_index": [1287, 1291], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.5", "char_index": [1294, 1298], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.1", "char_index": [1301, 1305], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "45.8", "char_index": [1308, 1312], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "49.1", "char_index": [1315, 1319], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "70.3", "char_index": [1322, 1326], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "54.0", "char_index": [1329, 1333], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "43.7", "char_index": [1336, 1340], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.3", "char_index": [1343, 1347], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "47.1", "char_index": [1350, 1354], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-T", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69M", "char_index": [1410, 1413], "type": "Hyper-parameter/Architecture", "model": "Swin-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "354G", "char_index": [1416, 1420], "type": "Hyper-parameter/Architecture", "model": "Swin-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "44.8", "char_index": [1431, 1435], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.6", "char_index": [1438, 1442], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.9", "char_index": [1445, 1449], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "40.9", "char_index": [1452, 1456], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "63.4", "char_index": [1459, 1463], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.2", "char_index": [1466, 1470], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.2", "char_index": [1473, 1477], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69.8", "char_index": [1480, 1484], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "52.8", "char_index": [1487, 1491], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "43.2", "char_index": [1494, 1498], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.0", "char_index": [1501, 1505], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.1", "char_index": [1508, 1512], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "70M", "char_index": [1559, 1562], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "348G", "char_index": [1565, 1569], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "45.4", "char_index": [1580, 1584], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.9", "char_index": [1587, 1591], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "50.0", "char_index": [1594, 1598], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "41.8", "char_index": [1601, 1605], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65.2", "char_index": [1608, 1612], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "45.1", "char_index": [1615, 1619], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "47.9", "char_index": [1622, 1626], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "70.0", "char_index": [1629, 1633], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "52.7", "char_index": [1636, 1640], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.9", "char_index": [1643, 1647], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.9", "char_index": [1650, 1654], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.2", "char_index": [1657, 1661], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65M", "char_index": [1705, 1708], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B3", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "397G", "char_index": [1711, 1715], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B3", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "47.0", "char_index": [1726, 1730], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "68.1", "char_index": [1733, 1737], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "51.7", "char_index": [1740, 1744], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.5", "char_index": [1747, 1751], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65.7", "char_index": [1754, 1758], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "45.7", "char_index": [1761, 1765], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.4", "char_index": [1768, 1772], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69.8", "char_index": [1775, 1779], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "53.3", "char_index": [1782, 1786], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "43.2", "char_index": [1789, 1793], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.9", "char_index": [1796, 1800], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.7", "char_index": [1803, 1807], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B3", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69M", "char_index": [1870, 1873], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "340G", "char_index": [1876, 1880], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "47.8", "char_index": [1891, 1895], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69.9", "char_index": [1898, 1902], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "52.8", "char_index": [1905, 1909], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "43.3", "char_index": [1912, 1916], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.1", "char_index": [1919, 1923], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.7", "char_index": [1926, 1930], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "49.7", "char_index": [1933, 1937], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "71.1", "char_index": [1940, 1944], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "54.5", "char_index": [1947, 1951], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.4", "char_index": [1954, 1958], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "68.5", "char_index": [1961, 1965], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "47.8", "char_index": [1968, 1972], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-S", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "107M", "char_index": [2027, 2031], "type": "Hyper-parameter/Architecture", "model": "Swin-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "496G", "char_index": [2034, 2038], "type": "Hyper-parameter/Architecture", "model": "Swin-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "46.9", "char_index": [2049, 2053], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.3", "char_index": [2068, 2072], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.6", "char_index": [2087, 2091], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "70.0", "char_index": [2094, 2098], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "53.4", "char_index": [2101, 2105], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "43.3", "char_index": [2108, 2112], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.1", "char_index": [2115, 2119], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.7", "char_index": [2122, 2126], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "Swin-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "108M", "char_index": [2174, 2178], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "486G", "char_index": [2181, 2185], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "47.0", "char_index": [2188, 2192], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69.4", "char_index": [2195, 2199], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "51.7", "char_index": [2202, 2206], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.7", "char_index": [2209, 2213], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.3", "char_index": [2216, 2220], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.0", "char_index": [2223, 2227], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.5", "char_index": [2230, 2234], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "70.1", "char_index": [2237, 2241], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "53.3", "char_index": [2244, 2248], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "43.5", "char_index": [2251, 2255], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.1", "char_index": [2258, 2262], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.7", "char_index": [2265, 2269], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ConvNeXt-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "102M", "char_index": [2313, 2317], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B5", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "557G", "char_index": [2320, 2324], "type": "Hyper-parameter/Architecture", "model": "PVTv2-B5", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "47.4", "char_index": [2335, 2339], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "68.6", "char_index": [2342, 2346], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "51.9", "char_index": [2349, 2353], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.5", "char_index": [2356, 2360], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65.7", "char_index": [2363, 2367], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.0", "char_index": [2370, 2374], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.4", "char_index": [2377, 2381], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "69.2", "char_index": [2384, 2388], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "52.9", "char_index": [2391, 2395], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "42.9", "char_index": [2398, 2402], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "66.6", "char_index": [2405, 2409], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.2", "char_index": [2412, 2416], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "PVTv2-B5", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "120M", "char_index": [2464, 2468], "type": "Hyper-parameter/Architecture", "model": "ViT-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "781G", "char_index": [2471, 2475], "type": "Hyper-parameter/Architecture", "model": "ViT-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "47.0", "char_index": [2486, 2490], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "68.2", "char_index": [2493, 2497], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "51.4", "char_index": [2500, 2504], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "41.8", "char_index": [2507, 2511], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "65.1", "char_index": [2514, 2518], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.9", "char_index": [2521, 2525], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "49.6", "char_index": [2528, 2532], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "70.6", "char_index": [2535, 2539], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "54.0", "char_index": [2542, 2546], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "43.6", "char_index": [2549, 2553], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.7", "char_index": [2556, 2560], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.9", "char_index": [2563, 2567], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "ViT-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "115M", "char_index": [2630, 2634], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "501G", "char_index": [2637, 2641], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "48.8", "char_index": [2652, 2656], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "71.0", "char_index": [2659, 2663], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "53.9", "char_index": [2666, 2670], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.0", "char_index": [2673, 2677], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "67.8", "char_index": [2680, 2684], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "47.5", "char_index": [2687, 2691], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "50.3", "char_index": [2694, 2698], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "71.4", "char_index": [2701, 2705], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "55.3", "char_index": [2708, 2712], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.8", "char_index": [2715, 2719], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "68.7", "char_index": [2722, 2726], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{50}", "AP_{50}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.0", "char_index": [2729, 2733], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m_{75}", "AP_{75}"], "experimental settings": {"schedule": "3$\\times$+MS"}, "model": "InternImage-B", "model settings": {"backbone": "Mask R-CNN"}}]}, "2211.05778v1_table3": {"table_code": "\\begin{table}[t]\\small\n\t\\centering\n\t\\renewcommand{\\arraystretch}{0.93}\n\t\\footnotesize\n    \\setlength\\tabcolsep{2.4mm}{\n    \\begin{tabular}{l|cc|cccccc|cc}\n        Method & \\#Params & \\#FLOPs & $\\rm AP^b$ & $\\rm AP^m$ \\\\\n        \\hline\n        Swin-L$^\\ddagger$~\\cite{liu2021swin} & 253M & 1382G & 51.8 & 44.9  \\\\\n        ConvNeXt-L$^\\ddagger$~\\cite{liu2022convnet} & 255M & 1354G & 53.5 & 46.4 \\\\\n        \\rowcolor{gray!20}\n        InternImage-L$^\\ddagger$ (ours) & 277M & 1399G & 54.9 & 47.7  \\\\\n        \\hline\n        ConvNeXt-XL$^\\ddagger$~\\cite{liu2022convnet} & 407M & 1898G & 53.6 & 46.5 \\\\\n        \\rowcolor{gray!20}\n        InternImage-XL$^\\ddagger$ (ours) & 387M & 1782G & 55.3 & 48.0  \\\\\n    \\end{tabular}}\n    \n    \\caption{\n    \\textbf{Object detection and instance segmentation with Cascade Mask R-CNN 1$\\times$ schedule on COCO \\texttt{val2017}.}\n    The FLOPs are measured with 1280$\\times$800 inputs.\n\t}\n    \\label{tab:results_detection_cascade}\n    \\label{tab:mask}\n\\end{table}", "table_label": "{tab:results_detection_cascade}", "table_numeric_cells": [["253M", "253M", 282, 286, 282, 286], ["1382G", "1382G", 289, 294, 289, 294], ["51.8", "51.8", 297, 301, 297, 301], ["44.9", "44.9", 304, 308, 304, 308], ["255M", "255M", 367, 371, 367, 371], ["1354G", "1354G", 374, 379, 374, 379], ["53.5", "53.5", 382, 386, 382, 386], ["46.4", "46.4", 389, 393, 389, 393], ["277M", "277M", 466, 470, 466, 470], ["1399G", "1399G", 473, 478, 473, 478], ["54.9", "54.9", 481, 485, 481, 485], ["47.7", "47.7", 488, 492, 488, 492], ["407M", "407M", 567, 571, 567, 571], ["1898G", "1898G", 574, 579, 574, 579], ["53.6", "53.6", 582, 586, 582, 586], ["46.5", "46.5", 589, 593, 589, 593], ["387M", "387M", 667, 671, 667, 671], ["1782G", "1782G", 674, 679, 674, 679], ["55.3", "55.3", 682, 686, 682, 686], ["48.0", "48.0", 689, 693, 689, 693]], "text_chunk_selected": "\\author{\n    Wenhai Wang$^{1*}$, \n    Jifeng Dai$^{2,1*}$,\n    Zhe Chen$^{3,1*}$,\n    Zhenhang Huang$^{1*}$,\n    Zhiqi Li$^{3,1*}$,\n    Xizhou Zhu$^{4*}$,\\\\\n    Xiaowei Hu$^{1}$,\n    Tong Lu$^{3}$,\n    Lewei Lu$^{4}$,\n    Hongsheng Li$^{5}$,\n    Xiaogang Wang$^{3,5}$,\n    Yu Qiao$^{1}$\\textsuperscript{\\Letter}\\\\\n    $^1$ Shanghai AI Laboratory~~~\n    $^2$Tsinghua University~~~\\\\\n    $^3$Nanjing University~~~\n    $^4$SenseTime Research~~~\n    $^5$The Chinese University of Hong Kong\\\\\n    {\\small \\url{https://github.com/OpenGVLab/InternImage}}\n}\n\nTo bridge the gap between CNNs and ViTs, we first summarize their differences from two aspects:\n(1) From the operator level~\\cite{dosovitskiy2020image,liu2022convnet,ding2022replknet}, the multi-head self-attention (MHSA) of ViTs has long-range dependencies and adaptive spatial aggregation (see Fig. \\ref{fig:coreop}(a)).\nBenefiting from the flexible MHSA,\nViTs can learn more powerful and robust representations than CNNs from massive data.\n(2) From the architecture view~\\cite{dosovitskiy2020image,ding2022replknet,yu2022metaformer}, besides MHSA, ViTs contain a series of advanced components that are not included in standard CNNs, such as Layer Normalization (LN)~\\cite{ba2016layernorm}, feed-forward network (FFN)~\\cite{vaswani2017attention}, GELU~\\cite{hendrycks2016gelu}, etc.\nAlthough recent works~\\cite{liu2022convnet,ding2022replknet} have made meaningful attempts to introduce long-range dependencies into CNNs by using dense convolutions with very large kernels (\\emph{e.g.}, 31$\\times$31) as shown in Fig. \\ref{fig:coreop} (c),\nthere is still a considerable gap with the state-of-the-art large-scale ViTs~\\cite{zhai2022scalingvit,dai2021coatnet,liu2021swinv2,riquelme2021vmoe,wei2022fdswin} in terms of performance and model scale.\n\nwhere $K$ represents the total number of sampling points, and $k$ enumerates the sampling point. \n$\\mathbf{w}_k\\!\\in\\!\\mathbb{R}^{C\\times C}$ denotes the projection weights of the $k$-th sampling point,\nand $\\mathbf{m}_k\\!\\in\\!\\mathbb{R}$ represents the modulation scalar of the $k$-th sampling point, which is normalized by sigmoid function.\n$p_k$ denotes the $k$-th location of the pre-defined grid sampling $\\{(-1, -1), (-1, 0), ..., (0, +1), ..., (+1, +1)\\}$ as in regular convolutions,\nand $\\Delta p_k$ is the offset corresponding to the $k$-th grid sampling location.\nWe see from the equation that (1) for long-range dependencies, the sampling offset $\\Delta p_k$ is flexible and able to interact with short- or long-range features; and (2) for adaptive spatial aggregation, both the sampling offset $\\Delta p_k$ and modulation scalar $\\mathbf{m}_k$ are learnable and conditioned by input $\\mathbf{x}$. \nSo it can be found that \\emph{DCNv2 shares similar favorable properties with MHSA}, which motivated us to develop large-scale CNN-based foundation models on the basis of this operator.\n\nwhere $G$ denotes the total number of aggregation groups.\nFor the $g$-th group,\n$\\mathbf{w}_g\\!\\in\\!\\mathbb{R}^{C\\times C'}$, $\\mathbf{m}_{gk}\\!\\in\\!\\mathbb{R}$ denote the location-irrelevant projection weights of the group, where $C'\\!=\\!C/G$ represents the group dimension.\n$\\mathbf{m}_{gk}\\!\\in\\!\\mathbb{R}$ denotes the modulation scalar of the $k$-th sampling point in the $g$-th group, normalized by the softmax function along the dimension $K$.\n$\\mathbf{x}_g\\!\\in\\!\\mathbb{R}^{C'\\times H \\times W}$ represents the sliced input feature map.\n$\\Delta p_{gk}$ is the offset corresponding to the grid sampling location $p_k$ in the $g$-th group.\n\n\\textbf{Basic block.}\nUnlike the widely used bottlenecks in traditional CNNs~\\cite{he2016deep}, the design of our basic block is closer to ViTs, which is equipped with more advanced components including LN~\\cite{ba2016layernorm}, feed-forward networks (FFN)~\\cite{vaswani2017attention}, and GELU~\\cite{hendrycks2016gelu}.\nThis design is proved to be efficient~\\cite{wang2021pyramid,wang2021pvtv2,liu2021swin,liu2022convnet,ding2022replknet} in various vision tasks.\nThe details of our basic block are illustrated in Fig. \\ref{fig:arch}, where the core operator is DCNv3,\nand the sampling offsets and modulation scales are predicted by passing input feature $\\mathbf{x}$ through a separable convolution (a 3$\\times$3 depth-wise convolution followed by a linear projection).\nFor other components, we use the post-normalization setting~\\cite{xiong2020layer} by default and follow the same design as that of the plain transformer~\\cite{vaswani2017attention,dosovitskiy2020image}.\n\nwhere $\\alpha\\!\\geq\\!1$, $\\beta\\!\\geq\\!1$, and $\\alpha\\beta^{1.99}\\!\\approx\\!2$. Here, 1.99 is specific for InternImage and calculated by doubling the model width and keeping the depth constant.\nWe experimentally find out that the best scaling setting is $\\alpha\\!=\\!1.09$ and $\\beta\\!=\\!1.36$, and then we base on it to construct InternImage\\ variants with different parameter scales, namely InternImage-T/S/B/L/XL, whose complexity is similar to those of ConvNeXt~\\cite{liu2022convnet}. \nTo further test the capability, we built a larger InternImage-H with 1 billion parameters.\nThe detailed configurations are summarized in Table~\\ref{tab:model_size}.\n\n\\textbf{Settings.}\nWe verify the effectiveness of our InternImage\\ on the COCO benchmark~\\cite{lin2014microsoft}, on top of two representative object detection frameworks: Mask R-CNN~\\cite{he2017mask}, and Cascade Mask R-CNN~\\cite{cai2018cascade}.\nWe follow common practices~\\cite{liu2021swin,wang2021pvtv2} to initialize the backbone with pre-trained classification weights, and by default use a 1$\\times$ (12 epochs) or 3$\\times$ (36 epochs) training schedule to train detection models.\n\n\\textbf{Results.}\nAs shown in Table~\\ref{tab:results_detection_mask}, when using Mask R-CNN for object detection, \nwe find that under a comparable number of parameters, our models significantly surpass their counterparts. \nFor example, with the $1\\times$ training schedule, the box AP (AP$^{\\rm b}$) of InternImage-T is 4.5 points better than Swin-T~\\cite{liu2021swin} (47.2 \\vs 42.7),\nand 3.0 points higher than ConvNeXt-T~\\cite{liu2022convnet} (47.2 \\vs 44.2).\nAs reported in Table~\\ref{tab:results_detection_cascade}, with more parameters and more advanced Cascade Mask R-CNN, InternImage-XL achieves AP$^{\\rm b}$ of 55.3, surpassing ConvNeXt-XL by 2.7 points (55.3 \\vs 53.6).", "table_source": "\\begin{table}[t]\\small\n\t\\centering\n\t\\renewcommand{\\arraystretch}{0.93}\n\t\\footnotesize\n    \\setlength\\tabcolsep{2.4mm}{\n    \\begin{tabular}{l|cc|cccccc|cc}\n        Method & \\#Params & \\#FLOPs & $\\rm AP^b$ & $\\rm AP^m$ \\\\\n        \\hline\n        Swin-L$^\\ddagger$~\\cite{liu2021swin} & 253M & 1382G & 51.8 & 44.9  \\\\\n        ConvNeXt-L$^\\ddagger$~\\cite{liu2022convnet} & 255M & 1354G & 53.5 & 46.4 \\\\\n        \\rowcolor{gray!20}\n        InternImage-L$^\\ddagger$ (ours) & 277M & 1399G & 54.9 & 47.7  \\\\\n        \\hline\n        ConvNeXt-XL$^\\ddagger$~\\cite{liu2022convnet} & 407M & 1898G & 53.6 & 46.5 \\\\\n        \\rowcolor{gray!20}\n        InternImage-XL$^\\ddagger$ (ours) & 387M & 1782G & 55.3 & 48.0  \\\\\n    \\end{tabular}}\n    \n    \\caption{\n    \\textbf{Object detection and instance segmentation with Cascade Mask R-CNN 1$\\times$ schedule on COCO \\texttt{val2017}.}\n    The FLOPs are measured with 1280$\\times$800 inputs.\n\t}\n    \\label{tab:results_detection_cascade}\n    \\label{tab:mask}\n\\end{table}", "cell_list_gold": [{"value": "253M", "char_index": [282, 286], "type": "Hyper-parameter/Architecture", "model": "Swin-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "1382G", "char_index": [289, 294], "type": "Hyper-parameter/Architecture", "model": "Swin-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "51.8", "char_index": [297, 301], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-L", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "44.9", "char_index": [304, 308], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "Swin-L", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "255M", "char_index": [367, 371], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "1354G", "char_index": [374, 379], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "53.5", "char_index": [382, 386], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-L", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.4", "char_index": [389, 393], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-L", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "277M", "char_index": [466, 470], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "1399G", "char_index": [473, 478], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "54.9", "char_index": [481, 485], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-L", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "47.7", "char_index": [488, 492], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-L", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "407M", "char_index": [567, 571], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-XL", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "1898G", "char_index": [574, 579], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-XL", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "53.6", "char_index": [582, 586], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-XL", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "46.5", "char_index": [589, 593], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "ConvNeXt-XL", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "387M", "char_index": [667, 671], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "1782G", "char_index": [674, 679], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "COCO"}, {"value": "55.3", "char_index": [682, 686], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-XL", "model settings": {"backbone": "Mask R-CNN"}}, {"value": "48.0", "char_index": [689, 693], "type": "Result", "training data/set": "COCO", "test data/set": "COCO val2017", "task": "instance segmentation", "metric": ["AP^m", "AP"], "experimental settings": {"schedule": "1$\\times$"}, "model": "InternImage-XL", "model settings": {"backbone": "Mask R-CNN"}}]}, "2211.05778v1_table4": {"table_code": "\\begin{table}[t]\\small\n\t\\centering\n\t\\renewcommand{\\arraystretch}{0.95}\n\t\\footnotesize\n    \\setlength\\tabcolsep{0.1mm}{\n    \\begin{tabular}{l|l|c|cc}\n        \\multirow{2}{*}{Method} & \\multirow{2}{*}{Detector} & \\multirow{2}{*}{\\#Params} & \\multicolumn{2}{c}{$\\rm AP^\\text{b}$} \\\\\n        &  & & val2017 & test-dev \\\\\n\t    \\hline\n\t\tSwin-L$^\\ddagger$~\\cite{liu2021swin} \n        & DINO~\\cite{zhang2022dino} & 218M & 63.2 & 63.3 \\\\\n\n        \n        Florence-CoSwin-H$^\\#$~\\cite{yuan2021florence} \n        & DyHead~\\cite{dai2021dynamic} & \n        637M\n        & 62.0 & 62.4 \\\\\n        \n        SwinV2-G$^\\#$\\cite{liu2021swinv2} \n        & HTC++~\\cite{liu2021swin} & 3.00B & 62.5 & 63.1 \\\\\n        \n        BEiT-3$^\\#$~\\cite{wang2022beit3} \n        & ViTDet~\\cite{li2022exploring} & 1.90B &$-$ & 63.7 \\\\\n        \n        FD-SwinV2-G$^\\#$~\\cite{wei2022fdswin}\n        & HTC++~\\cite{liu2021swin} & 3.00B & $-$ & 64.2 \\\\\n        \n        FocalNet-H$^\\ddagger$~\\cite{yang2022focal} \n        & DINO~\\cite{zhang2022dino} & 746M & 64.2 & 64.3 \\\\\n        \n        ViT-Huge~\\cite{chen2022group} \n        & Group-DETRv2~\\cite{chen2022group} & 629M & $-$ & 64.5 \\\\\n        \\hline\n        \\rowcolor{gray!10}\n        InternImage-XL$^\\ddagger$ (ours) \n        & DINO~\\cite{zhang2022dino} & 602M & 64.2 & 64.3 \\\\ \n        \\rowcolor{gray!20}\n        InternImage-H$^\\#$ (ours) \n        & DINO~\\cite{zhang2022dino} & 2.18B & 65.0 & 65.4\\\\\n    \\end{tabular}\n    }\n    \\vspace{0.05cm}\n    \\caption{\\textbf{Comparison of the state-of-the-art detectors on COCO \\texttt{val2017} and test-dev.}\n    }\n    \\label{tab:sota}\n\\end{table}", "table_label": "{tab:sota}", "table_numeric_cells": [["218M", "218M", 407, 411, 407, 411], ["63.2", "63.2", 414, 418, 414, 418], ["63.3", "63.3", 421, 425, 421, 425], ["637M", "637M", 545, 549, 545, 549], ["62.0", "62.0", 560, 564, 560, 564], ["62.4", "62.4", 567, 571, 567, 571], ["3.00B", "3.00B", 664, 669, 664, 669], ["62.5", "62.5", 672, 676, 672, 676], ["63.1", "63.1", 679, 683, 679, 683], ["1.90B", "1.90B", 780, 785, 780, 785], ["63.7", "63.7", 793, 797, 793, 797], ["3.00B", "3.00B", 893, 898, 893, 898], ["64.2", "64.2", 907, 911, 907, 911], ["746M", "746M", 1014, 1018, 1014, 1018], ["64.2", "64.2", 1021, 1025, 1021, 1025], ["64.3", "64.3", 1028, 1032, 1028, 1032], ["629M", "629M", 1130, 1134, 1130, 1134], ["64.5", "64.5", 1143, 1147, 1143, 1147], ["602M", "602M", 1273, 1277, 1273, 1277], ["64.2", "64.2", 1280, 1284, 1280, 1284], ["64.3", "64.3", 1287, 1291, 1287, 1291], ["2.18B", "2.18B", 1396, 1401, 1396, 1401], ["65.0", "65.0", 1404, 1408, 1404, 1408], ["65.4", "65.4", 1411, 1415, 1411, 1415]], "text_chunk_selected": "\\author{\n    Wenhai Wang$^{1*}$, \n    Jifeng Dai$^{2,1*}$,\n    Zhe Chen$^{3,1*}$,\n    Zhenhang Huang$^{1*}$,\n    Zhiqi Li$^{3,1*}$,\n    Xizhou Zhu$^{4*}$,\\\\\n    Xiaowei Hu$^{1}$,\n    Tong Lu$^{3}$,\n    Lewei Lu$^{4}$,\n    Hongsheng Li$^{5}$,\n    Xiaogang Wang$^{3,5}$,\n    Yu Qiao$^{1}$\\textsuperscript{\\Letter}\\\\\n    $^1$ Shanghai AI Laboratory~~~\n    $^2$Tsinghua University~~~\\\\\n    $^3$Nanjing University~~~\n    $^4$SenseTime Research~~~\n    $^5$The Chinese University of Hong Kong\\\\\n    {\\small \\url{https://github.com/OpenGVLab/InternImage}}\n}\n\n(3) We evaluate the proposed model on representative vision tasks including image classification, object detection, instance and semantic segmentation,\nand compared it with state-of-the-art CNNs and large-scale ViTs by scaling the model size ranging from 30 million to 1 billion, the data ranging from 1 million to 400 million.\nSpecifically, our model with different parameter sizes can consistently outperform prior arts on ImageNet~\\cite{deng2009imagenet}. \nInternImage-B achieves 84.9\\% top-1 accuracy trained only on the ImageNet-1K dataset, outperforming CNN-based counterparts~\\cite{ding2022replknet,liu2022convnet} by at least 1.1 points.\nWith large-scale parameters (\\emph{i.e.}, 1 billion) and training data (\\emph{i.e.}, 427 million), the top-1 accuracy of InternImage-H is further boosted to 89.2\\%, which is close to well-engineering ViTs~\\cite{liu2021swin,zhai2022scaling} and hybrid-ViTs~\\cite{dai2021coatnet}.\nIn addition, on COCO~\\cite{caesar2018coco}, a challenging downstream benchmark,\nour best model InternImage-H achieves state-of-the-art 65.4\\% box mAP with 2.18 billion parameters, 2.3 points higher than SwinV2-G~\\cite{liu2021swinv2} (65.4 \\vs 63.1) with 27\\% fewer parameters.\n\n\\textbf{Basic block.}\nUnlike the widely used bottlenecks in traditional CNNs~\\cite{he2016deep}, the design of our basic block is closer to ViTs, which is equipped with more advanced components including LN~\\cite{ba2016layernorm}, feed-forward networks (FFN)~\\cite{vaswani2017attention}, and GELU~\\cite{hendrycks2016gelu}.\nThis design is proved to be efficient~\\cite{wang2021pyramid,wang2021pvtv2,liu2021swin,liu2022convnet,ding2022replknet} in various vision tasks.\nThe details of our basic block are illustrated in Fig. \\ref{fig:arch}, where the core operator is DCNv3,\nand the sampling offsets and modulation scales are predicted by passing input feature $\\mathbf{x}$ through a separable convolution (a 3$\\times$3 depth-wise convolution followed by a linear projection).\nFor other components, we use the post-normalization setting~\\cite{xiong2020layer} by default and follow the same design as that of the plain transformer~\\cite{vaswani2017attention,dosovitskiy2020image}.\n\n\\textbf{Stem \\& downsampling layers.}\nTo obtain hierarchical feature maps, we use \nconvolutional stem and downsampling layers to resize the feature maps to different scales.\nAs shown in Fig. \\ref{fig:arch}, the stem layer is placed before the first stage to reduce the input resolution by 4 times.\nIt consists of two convolutions, two LN layers, and one GELU layer,\nwhere the kernel size of the two convolutions is 3, the stride is 2, the padding is 1,\nand the output channel of the first convolution is half of the second one.\nSimilarly, the downsampling layer is made up of a 3$\\times$3 convolution with a stride of 2 and a padding of 1, followed by one LN layer.\nIt sits between the two stages and is used to downsample the input feature map by 2 times.\n\n\\subsection{Image Classification}\n\\textbf{Settings.}\nWe evaluate the classification performance of InternImage\\ on ImageNet~\\cite{deng2009imagenet}. \nFor fair comparisons, following common practices~\\cite{touvron2021training,wang2021pyramid,liu2021swin,liu2022convnet}, InternImage-T/S/B are trained on ImageNet-1K ($\\sim$1.3 million) for 300 epochs, \nand InternImage-L/XL are first trained on ImageNet-22K ($\\sim$14.2 million) for 90 epochs and then fine-tuned on ImageNet-1K for 30 epochs.\nTo further explore the capability of our model and match the large-scale private data used in previous methods~\\cite{dai2021coatnet,liu2021swinv2,yuan2021florence}, we adopt M3I Pre-training~\\cite{anonymous2022m2i}, an effective pre-training approach, to pre-train InternImage-H on a 427 million joint dataset of public Laion-400M~\\cite{schuhmann2021laion}, YFCC-15M~\\cite{thomee2016yfcc100m}, and CC12M~\\cite{changpinyo2021conceptual} for 30 epochs,\nand then we fine-tune the model on ImageNet-22K and -1K for 30 epochs, respectively.\n\n\\textbf{Settings.}\nWe verify the effectiveness of our InternImage\\ on the COCO benchmark~\\cite{lin2014microsoft}, on top of two representative object detection frameworks: Mask R-CNN~\\cite{he2017mask}, and Cascade Mask R-CNN~\\cite{cai2018cascade}.\nWe follow common practices~\\cite{liu2021swin,wang2021pvtv2} to initialize the backbone with pre-trained classification weights, and by default use a 1$\\times$ (12 epochs) or 3$\\times$ (36 epochs) training schedule to train detection models.\n\n\\textbf{Results.}\nAs shown in Table~\\ref{tab:results_detection_mask}, when using Mask R-CNN for object detection, \nwe find that under a comparable number of parameters, our models significantly surpass their counterparts. \nFor example, with the $1\\times$ training schedule, the box AP (AP$^{\\rm b}$) of InternImage-T is 4.5 points better than Swin-T~\\cite{liu2021swin} (47.2 \\vs 42.7),\nand 3.0 points higher than ConvNeXt-T~\\cite{liu2022convnet} (47.2 \\vs 44.2).\nAs reported in Table~\\ref{tab:results_detection_cascade}, with more parameters and more advanced Cascade Mask R-CNN, InternImage-XL achieves AP$^{\\rm b}$ of 55.3, surpassing ConvNeXt-XL by 2.7 points (55.3 \\vs 53.6).\n\nTo further push the performance bound of object detection, we follow the advanced setting used in leading methods~\\cite{wang2022beit3, liu2021swinv2, zhang2022dino} to initialize the model with the weights pre-trained on ImageNet-22K or the large-scale joint dataset,\nand fine-tune it along with the DINO~\\cite{zhang2022dino} detector on the Objects365~\\cite{shao2019objects365} and COCO datasets one after another for 26 epochs and 12 epochs, respectively.\nAs shown in Table~\\ref{tab:sota}, our method establishes a new record of 65.0 AP$^\\text{b}$ and 65.4 AP$^\\text{b}$ on COCO val2017 and test-dev.\nCompared to previous state-of-the-art models, we surpass FD-SwinV2-G~\\cite{wei2022fdswin} by 1.2 points (65.4 \\vs 64.2), using 27\\% fewer parameters, \nwhich shows the effectiveness of our models on the object detection task.", "table_source": "\\begin{table}[t]\\small\n\t\\centering\n\t\\renewcommand{\\arraystretch}{0.95}\n\t\\footnotesize\n    \\setlength\\tabcolsep{0.1mm}{\n    \\begin{tabular}{l|l|c|cc}\n        \\multirow{2}{*}{Method} & \\multirow{2}{*}{Detector} & \\multirow{2}{*}{\\#Params} & \\multicolumn{2}{c}{$\\rm AP^\\text{b}$} \\\\\n        &  & & val2017 & test-dev \\\\\n\t    \\hline\n\t\tSwin-L$^\\ddagger$~\\cite{liu2021swin} \n        & DINO~\\cite{zhang2022dino} & 218M & 63.2 & 63.3 \\\\\n\n        \n        Florence-CoSwin-H$^\\#$~\\cite{yuan2021florence} \n        & DyHead~\\cite{dai2021dynamic} & \n        637M\n        & 62.0 & 62.4 \\\\\n        \n        SwinV2-G$^\\#$\\cite{liu2021swinv2} \n        & HTC++~\\cite{liu2021swin} & 3.00B & 62.5 & 63.1 \\\\\n        \n        BEiT-3$^\\#$~\\cite{wang2022beit3} \n        & ViTDet~\\cite{li2022exploring} & 1.90B &$-$ & 63.7 \\\\\n        \n        FD-SwinV2-G$^\\#$~\\cite{wei2022fdswin}\n        & HTC++~\\cite{liu2021swin} & 3.00B & $-$ & 64.2 \\\\\n        \n        FocalNet-H$^\\ddagger$~\\cite{yang2022focal} \n        & DINO~\\cite{zhang2022dino} & 746M & 64.2 & 64.3 \\\\\n        \n        ViT-Huge~\\cite{chen2022group} \n        & Group-DETRv2~\\cite{chen2022group} & 629M & $-$ & 64.5 \\\\\n        \\hline\n        \\rowcolor{gray!10}\n        InternImage-XL$^\\ddagger$ (ours) \n        & DINO~\\cite{zhang2022dino} & 602M & 64.2 & 64.3 \\\\ \n        \\rowcolor{gray!20}\n        InternImage-H$^\\#$ (ours) \n        & DINO~\\cite{zhang2022dino} & 2.18B & 65.0 & 65.4\\\\\n    \\end{tabular}\n    }\n    \\vspace{0.05cm}\n    \\caption{\\textbf{Comparison of the state-of-the-art detectors on COCO \\texttt{val2017} and test-dev.}\n    }\n    \\label{tab:sota}\n\\end{table}", "cell_list_gold": [{"value": "218M", "char_index": [407, 411], "type": "Hyper-parameter/Architecture", "model": "Swin-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "63.2", "char_index": [414, 418], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "Swin-L", "model settings": {"Detector": "DINO"}}, {"value": "63.3", "char_index": [421, 425], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "Swin-L", "model settings": {"Detector": "DINO"}}, {"value": "637M", "char_index": [545, 549], "type": "Hyper-parameter/Architecture", "model": "Florence-CoSwin-H", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "62.0", "char_index": [560, 564], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "Florence-CoSwin-H", "model settings": {"Detector": "DyHead"}}, {"value": "62.4", "char_index": [567, 571], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "Florence-CoSwin-H", "model settings": {"Detector": "DyHead"}}, {"value": "3.00B", "char_index": [664, 669], "type": "Hyper-parameter/Architecture", "model": "SwinV2-G", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "62.5", "char_index": [672, 676], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "SwinV2-G", "model settings": {"Detector": "HTC++"}}, {"value": "63.1", "char_index": [679, 683], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "SwinV2-G", "model settings": {"Detector": "HTC++"}}, {"value": "1.90B", "char_index": [780, 785], "type": "Hyper-parameter/Architecture", "model": "BEiT-3", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "63.7", "char_index": [793, 797], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "BEiT-3", "model settings": {"Detector": "ViTDet"}}, {"value": "3.00B", "char_index": [893, 898], "type": "Hyper-parameter/Architecture", "model": "FD-SwinV2-G", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "64.2", "char_index": [907, 911], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "FD-SwinV2-G", "model settings": {"Detector": "HTC++"}}, {"value": "746M", "char_index": [1014, 1018], "type": "Hyper-parameter/Architecture", "model": "FocalNet-H", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "64.2", "char_index": [1021, 1025], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "FocalNet-H", "model settings": {"Detector": "DINO"}}, {"value": "64.3", "char_index": [1028, 1032], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "FocalNet-H", "model settings": {"Detector": "DINO"}}, {"value": "629M", "char_index": [1130, 1134], "type": "Hyper-parameter/Architecture", "model": "ViT-Huge", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "64.5", "char_index": [1143, 1147], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "ViT-Huge", "model settings": {"Detector": "Group-DETRv2"}}, {"value": "602M", "char_index": [1273, 1277], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "64.2", "char_index": [1280, 1284], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "InternImage-XL", "model settings": {"Detector": "DINO"}}, {"value": "64.3", "char_index": [1287, 1291], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "InternImage-XL", "model settings": {"Detector": "DINO"}}, {"value": "2.18B", "char_index": [1396, 1401], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "COCO"}, {"value": "65.0", "char_index": [1404, 1408], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO val2017", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "InternImage-H", "model settings": {"Detector": "DINO"}}, {"value": "65.4", "char_index": [1411, 1415], "type": "Result", "training data/set": "Objects365 COCO", "test data/set": "COCO test-dev", "task": "object detection", "metric": ["AP^b", "AP"], "experimental settings": {"number of epochs": "26 12"}, "model": "InternImage-H", "model settings": {"Detector": "DINO"}}]}, "2211.05778v1_table5": {"table_code": "\\begin{table}[t]\n    \\centering\n    \\renewcommand{\\arraystretch}{1.0}\n    \\setlength{\\tabcolsep}{1.3mm}\n    \\footnotesize\n    \\begin{tabular}{l|c|c|c|cc}\n    \t\\multirow{2}{*}{Method} & Crop & \\multirow{2}{*}{\\#Params} & \\multirow{2}{*}{\\#FLOPs} & mIoU & mIoU\\\\\n    \t& Size & & & (ss) & (ms)   \\\\\n    \t\\whline\n    \tSwin-T~\\cite{liu2021swin} & 512$^2$ & 60M  & 945G & 44.5 & 45.8 \\\\\n    \tConvNeXt-T~\\cite{liu2022convnet} & 512$^2$ & 60M & 939G & 46.0 & 46.7 \\\\\n    \tSLaK-T~\\cite{liu2022slak} & 512$^2$ & 65M & 936G & 47.6 & $-$ \\\\\n    \t\\rowcolor{gray!20}\n    \tInternImage-T (ours) & 512$^2$ & 59M & 944G & 47.9 & 48.1  \\\\\n    \t\\hline\n        Swin-S~\\cite{liu2021swin} & 512$^2$ & 81M &  1038G &  47.6 &  49.5 \\\\\n        ConvNeXt-S~\\cite{liu2022convnet}  & 512$^2$ &82M & 1027G  & 48.7 & 49.6  \\\\\n        SLaK-S~\\cite{liu2022slak} & 512$^2$ &91M & 1028G & 49.4 & $-$ \\\\\n        \\rowcolor{gray!20}\n        InternImage-S (ours) & 512$^2$ & 80M & 1017G & 50.1 & 50.9 \\\\\n        \\hline\n        Swin-B~\\cite{liu2021swin} \n        & 512$^2$ & 121M & 1188G & 48.1 & 49.7  \\\\\n        ConvNeXt-B~\\cite{liu2022convnet}  \n        & 512$^2$ & 122M & 1170G & 49.1 & 49.9  \\\\\n        RepLKNet-31B~\\cite{ding2022replknet} \n        & 512$^2$ & 112M & 1170G & 49.9 & 50.6  \\\\\n        SLaK-B~\\cite{liu2022slak} & 512$^2$ & 135M & 1172G & 50.2 & $-$    \\\\\n        \\rowcolor{gray!20}\n        InternImage-B (ours) \n        & 512$^2$ & 128M & 1185G & 50.8 & 51.3 \\\\\n        \\hline\n        Swin-L$^\\ddagger$~\\cite{liu2021swin} \n        & 640$^2$ & 234M & 2468G & 52.1 & 53.5 \\\\\n        RepLKNet-31L$^\\ddagger$~\\cite{ding2022replknet} \n        & 640$^2$ & 207M & 2404G & 52.4 & 52.7 \\\\\n        ConvNeXt-L$^\\ddagger$~\\cite{liu2022convnet} \n        & 640$^2$ & 235M & 2458G & 53.2 & 53.7 \\\\\n        ConvNeXt-XL$^\\ddagger$~\\cite{liu2022convnet} \n        & 640$^2$ & 391M & 3335G & 53.6 & 54.0 \\\\\n        \\rowcolor{gray!10}\n        InternImage-L$^\\ddagger$ (ours) \n        & 640$^2$ & 256M  & 2526G  & 53.9 & 54.1 \\\\\n        \\rowcolor{gray!20}\n        InternImage-XL$^\\ddagger$ (ours)\n        & 640$^2$ & 368M & 3142G & 55.0 & 55.3 \\\\\n        \\hline\n        SwinV2-G$^\\#$~\\cite{liu2021swinv2}\n        & 896$^2$ & 3.00B & $-$ & $-$ & 59.9 \\\\\n        \\rowcolor{gray!10}\n        InternImage-H$^\\#$ (ours)\n        & 896$^2$ & 1.12B & 3566G & 59.9 & 60.3 \\\\\n        \\hline\n        BEiT-3$^\\#$~\\cite{wang2022beit3} & 896$^2$ & 1.90B & $-$ & $-$ & 62.8 \\\\ \n        \\rowcolor{gray!20}\n        InternImage-H$^\\#$ (ours) + &  &  &  &  &  \\\\\n        \\rowcolor{gray!20}\n        Mask2Former~\\cite{cheng2021masked}\n        & \\multirow{-2}{*}{896$^2$}& \\multirow{-2}{*}{1.31B}& \\multirow{-2}{*}{4635G}& \\multirow{-2}{*}{62.5}& \\multirow{-2}{*}{62.9}\\\\\n    \\end{tabular}\n    \\caption{\\textbf{Semantic segmentation performance on the ADE20K validation set.}\n    \t``SS'' and ``MS\" means single-scale and multi-scale testing, respectively. \n    }\n    \\label{tab:seg}\n\\end{table}", "table_label": "{tab:seg}", "table_numeric_cells": [["60M", "60M", 352, 355, 352, 355], ["945G", "945G", 359, 363, 359, 363], ["44.5", "44.5", 366, 370, 366, 370], ["45.8", "45.8", 373, 377, 373, 377], ["60M", "60M", 431, 434, 431, 434], ["939G", "939G", 437, 441, 437, 441], ["46.0", "46.0", 444, 448, 444, 448], ["46.7", "46.7", 451, 455, 451, 455], ["65M", "65M", 502, 505, 502, 505], ["936G", "936G", 508, 512, 508, 512], ["47.6", "47.6", 515, 519, 515, 519], ["59M", "59M", 591, 594, 591, 594], ["944G", "944G", 597, 601, 597, 601], ["47.9", "47.9", 604, 608, 604, 608], ["48.1", "48.1", 611, 615, 611, 615], ["81M", "81M", 678, 681, 678, 681], ["1038G", "1038G", 685, 690, 685, 690], ["47.6", "47.6", 694, 698, 694, 698], ["49.5", "49.5", 702, 706, 702, 706], ["82M", "82M", 763, 766, 763, 766], ["1027G", "1027G", 769, 774, 769, 774], ["48.7", "48.7", 778, 782, 778, 782], ["49.6", "49.6", 785, 789, 785, 789], ["91M", "91M", 839, 842, 839, 842], ["1028G", "1028G", 845, 850, 845, 850], ["49.4", "49.4", 853, 857, 853, 857], ["80M", "80M", 935, 938, 935, 938], ["1017G", "1017G", 941, 946, 941, 946], ["50.1", "50.1", 949, 953, 949, 953], ["50.9", "50.9", 956, 960, 956, 960], ["121M", "121M", 1034, 1038, 1034, 1038], ["1188G", "1188G", 1041, 1046, 1041, 1046], ["48.1", "48.1", 1049, 1053, 1049, 1053], ["49.7", "49.7", 1056, 1060, 1056, 1060], ["122M", "122M", 1128, 1132, 1128, 1132], ["1170G", "1170G", 1135, 1140, 1135, 1140], ["49.1", "49.1", 1143, 1147, 1143, 1147], ["49.9", "49.9", 1150, 1154, 1150, 1154], ["112M", "112M", 1225, 1229, 1225, 1229], ["1170G", "1170G", 1232, 1237, 1232, 1237], ["49.9", "49.9", 1240, 1244, 1240, 1244], ["50.6", "50.6", 1247, 1251, 1247, 1251], ["135M", "135M", 1302, 1306, 1302, 1306], ["1172G", "1172G", 1309, 1314, 1309, 1314], ["50.2", "50.2", 1317, 1321, 1317, 1321], ["128M", "128M", 1411, 1415, 1411, 1415], ["1185G", "1185G", 1418, 1423, 1418, 1423], ["50.8", "50.8", 1426, 1430, 1426, 1430], ["51.3", "51.3", 1433, 1437, 1433, 1437], ["234M", "234M", 1522, 1526, 1522, 1526], ["2468G", "2468G", 1529, 1534, 1529, 1534], ["52.1", "52.1", 1537, 1541, 1537, 1541], ["53.5", "53.5", 1544, 1548, 1544, 1548], ["207M", "207M", 1629, 1633, 1629, 1633], ["2404G", "2404G", 1636, 1641, 1636, 1641], ["52.4", "52.4", 1644, 1648, 1644, 1648], ["52.7", "52.7", 1651, 1655, 1651, 1655], ["235M", "235M", 1732, 1736, 1732, 1736], ["2458G", "2458G", 1739, 1744, 1739, 1744], ["53.2", "53.2", 1747, 1751, 1747, 1751], ["53.7", "53.7", 1754, 1758, 1754, 1758], ["391M", "391M", 1836, 1840, 1836, 1840], ["3335G", "3335G", 1843, 1848, 1843, 1848], ["53.6", "53.6", 1851, 1855, 1851, 1855], ["54.0", "54.0", 1858, 1862, 1858, 1862], ["256M", "256M", 1954, 1958, 1954, 1958], ["2526G", "2526G", 1962, 1967, 1962, 1967], ["53.9", "53.9", 1971, 1975, 1971, 1975], ["54.1", "54.1", 1978, 1982, 1978, 1982], ["368M", "368M", 2074, 2078, 2074, 2078], ["3142G", "3142G", 2081, 2086, 2081, 2086], ["55.0", "55.0", 2089, 2093, 2089, 2093], ["55.3", "55.3", 2096, 2100, 2096, 2100], ["3.00B", "3.00B", 2182, 2187, 2182, 2187], ["59.9", "59.9", 2202, 2206, 2202, 2206], ["1.12B", "1.12B", 2291, 2296, 2291, 2296], ["3566G", "3566G", 2299, 2304, 2299, 2304], ["59.9", "59.9", 2307, 2311, 2307, 2311], ["60.3", "60.3", 2314, 2318, 2314, 2318], ["1.90B", "1.90B", 2390, 2395, 2390, 2395], ["62.8", "62.8", 2410, 2414, 2410, 2414], ["1.31B", "\\multirow{-2}{*}{1.31B}", 2624, 2629, 2607, 2630], ["4635G", "\\multirow{-2}{*}{4635G}", 2649, 2654, 2632, 2655], ["62.5", "\\multirow{-2}{*}{62.5}", 2674, 2678, 2657, 2679], ["62.9", "\\multirow{-2}{*}{62.9}", 2698, 2702, 2681, 2703]], "text_chunk_selected": "\\author{\n    Wenhai Wang$^{1*}$, \n    Jifeng Dai$^{2,1*}$,\n    Zhe Chen$^{3,1*}$,\n    Zhenhang Huang$^{1*}$,\n    Zhiqi Li$^{3,1*}$,\n    Xizhou Zhu$^{4*}$,\\\\\n    Xiaowei Hu$^{1}$,\n    Tong Lu$^{3}$,\n    Lewei Lu$^{4}$,\n    Hongsheng Li$^{5}$,\n    Xiaogang Wang$^{3,5}$,\n    Yu Qiao$^{1}$\\textsuperscript{\\Letter}\\\\\n    $^1$ Shanghai AI Laboratory~~~\n    $^2$Tsinghua University~~~\\\\\n    $^3$Nanjing University~~~\n    $^4$SenseTime Research~~~\n    $^5$The Chinese University of Hong Kong\\\\\n    {\\small \\url{https://github.com/OpenGVLab/InternImage}}\n}\n\nwhere $K$ represents the total number of sampling points, and $k$ enumerates the sampling point. \n$\\mathbf{w}_k\\!\\in\\!\\mathbb{R}^{C\\times C}$ denotes the projection weights of the $k$-th sampling point,\nand $\\mathbf{m}_k\\!\\in\\!\\mathbb{R}$ represents the modulation scalar of the $k$-th sampling point, which is normalized by sigmoid function.\n$p_k$ denotes the $k$-th location of the pre-defined grid sampling $\\{(-1, -1), (-1, 0), ..., (0, +1), ..., (+1, +1)\\}$ as in regular convolutions,\nand $\\Delta p_k$ is the offset corresponding to the $k$-th grid sampling location.\nWe see from the equation that (1) for long-range dependencies, the sampling offset $\\Delta p_k$ is flexible and able to interact with short- or long-range features; and (2) for adaptive spatial aggregation, both the sampling offset $\\Delta p_k$ and modulation scalar $\\mathbf{m}_k$ are learnable and conditioned by input $\\mathbf{x}$. \nSo it can be found that \\emph{DCNv2 shares similar favorable properties with MHSA}, which motivated us to develop large-scale CNN-based foundation models on the basis of this operator.\n\n\\textbf{Stem \\& downsampling layers.}\nTo obtain hierarchical feature maps, we use \nconvolutional stem and downsampling layers to resize the feature maps to different scales.\nAs shown in Fig. \\ref{fig:arch}, the stem layer is placed before the first stage to reduce the input resolution by 4 times.\nIt consists of two convolutions, two LN layers, and one GELU layer,\nwhere the kernel size of the two convolutions is 3, the stride is 2, the padding is 1,\nand the output channel of the first convolution is half of the second one.\nSimilarly, the downsampling layer is made up of a 3$\\times$3 convolution with a stride of 2 and a padding of 1, followed by one LN layer.\nIt sits between the two stages and is used to downsample the input feature map by 2 times.\n\nwhere $\\alpha\\!\\geq\\!1$, $\\beta\\!\\geq\\!1$, and $\\alpha\\beta^{1.99}\\!\\approx\\!2$. Here, 1.99 is specific for InternImage and calculated by doubling the model width and keeping the depth constant.\nWe experimentally find out that the best scaling setting is $\\alpha\\!=\\!1.09$ and $\\beta\\!=\\!1.36$, and then we base on it to construct InternImage\\ variants with different parameter scales, namely InternImage-T/S/B/L/XL, whose complexity is similar to those of ConvNeXt~\\cite{liu2022convnet}. \nTo further test the capability, we built a larger InternImage-H with 1 billion parameters.\nThe detailed configurations are summarized in Table~\\ref{tab:model_size}.\n\n\\subsection{Image Classification}\n\\textbf{Settings.}\nWe evaluate the classification performance of InternImage\\ on ImageNet~\\cite{deng2009imagenet}. \nFor fair comparisons, following common practices~\\cite{touvron2021training,wang2021pyramid,liu2021swin,liu2022convnet}, InternImage-T/S/B are trained on ImageNet-1K ($\\sim$1.3 million) for 300 epochs, \nand InternImage-L/XL are first trained on ImageNet-22K ($\\sim$14.2 million) for 90 epochs and then fine-tuned on ImageNet-1K for 30 epochs.\nTo further explore the capability of our model and match the large-scale private data used in previous methods~\\cite{dai2021coatnet,liu2021swinv2,yuan2021florence}, we adopt M3I Pre-training~\\cite{anonymous2022m2i}, an effective pre-training approach, to pre-train InternImage-H on a 427 million joint dataset of public Laion-400M~\\cite{schuhmann2021laion}, YFCC-15M~\\cite{thomee2016yfcc100m}, and CC12M~\\cite{changpinyo2021conceptual} for 30 epochs,\nand then we fine-tune the model on ImageNet-22K and -1K for 30 epochs, respectively.\n\n\\textbf{Results.}\nAs shown in Table~\\ref{tab:results_detection_mask}, when using Mask R-CNN for object detection, \nwe find that under a comparable number of parameters, our models significantly surpass their counterparts. \nFor example, with the $1\\times$ training schedule, the box AP (AP$^{\\rm b}$) of InternImage-T is 4.5 points better than Swin-T~\\cite{liu2021swin} (47.2 \\vs 42.7),\nand 3.0 points higher than ConvNeXt-T~\\cite{liu2022convnet} (47.2 \\vs 44.2).\nAs reported in Table~\\ref{tab:results_detection_cascade}, with more parameters and more advanced Cascade Mask R-CNN, InternImage-XL achieves AP$^{\\rm b}$ of 55.3, surpassing ConvNeXt-XL by 2.7 points (55.3 \\vs 53.6).\n\nTo further push the performance bound of object detection, we follow the advanced setting used in leading methods~\\cite{wang2022beit3, liu2021swinv2, zhang2022dino} to initialize the model with the weights pre-trained on ImageNet-22K or the large-scale joint dataset,\nand fine-tune it along with the DINO~\\cite{zhang2022dino} detector on the Objects365~\\cite{shao2019objects365} and COCO datasets one after another for 26 epochs and 12 epochs, respectively.\nAs shown in Table~\\ref{tab:sota}, our method establishes a new record of 65.0 AP$^\\text{b}$ and 65.4 AP$^\\text{b}$ on COCO val2017 and test-dev.\nCompared to previous state-of-the-art models, we surpass FD-SwinV2-G~\\cite{wei2022fdswin} by 1.2 points (65.4 \\vs 64.2), using 27\\% fewer parameters, \nwhich shows the effectiveness of our models on the object detection task.\n\n\\textbf{Results.}\nAs shown in Table~\\ref{tab:seg}, when using UperNet~\\cite{xiao2018unified} for semantic segmentation, our InternImage~consistently outperforms prior arts~\\cite{liu2021swin, liu2022convnet, liu2022slak, ding2022replknet}.\nFor example, with almost the same parameter numbers and FLOPs, our InternImage-B reports 50.8 mIoU on the ADE20K val, which is outstanding from the strong counterparts such as ConvNeXt-B (50.8 \\emph{vs.}~49.1) and RepLKNet-31B (50.8 \\emph{vs.}~49.9).\nFurthermore, our InternImage-H yields 60.3 MS mIoU, which is slightly better than SwinV2-G~\\cite{liu2021swinv2}, while the parameter number is much smaller (1.12B \\emph{vs.}~3.00B). ", "table_source": "\\begin{table}[t]\n    \\centering\n    \\renewcommand{\\arraystretch}{1.0}\n    \\setlength{\\tabcolsep}{1.3mm}\n    \\footnotesize\n    \\begin{tabular}{l|c|c|c|cc}\n    \t\\multirow{2}{*}{Method} & Crop & \\multirow{2}{*}{\\#Params} & \\multirow{2}{*}{\\#FLOPs} & mIoU & mIoU\\\\\n    \t& Size & & & (ss) & (ms)   \\\\\n    \t\\whline\n    \tSwin-T~\\cite{liu2021swin} & 512$^2$ & 60M  & 945G & 44.5 & 45.8 \\\\\n    \tConvNeXt-T~\\cite{liu2022convnet} & 512$^2$ & 60M & 939G & 46.0 & 46.7 \\\\\n    \tSLaK-T~\\cite{liu2022slak} & 512$^2$ & 65M & 936G & 47.6 & $-$ \\\\\n    \t\\rowcolor{gray!20}\n    \tInternImage-T (ours) & 512$^2$ & 59M & 944G & 47.9 & 48.1  \\\\\n    \t\\hline\n        Swin-S~\\cite{liu2021swin} & 512$^2$ & 81M &  1038G &  47.6 &  49.5 \\\\\n        ConvNeXt-S~\\cite{liu2022convnet}  & 512$^2$ &82M & 1027G  & 48.7 & 49.6  \\\\\n        SLaK-S~\\cite{liu2022slak} & 512$^2$ &91M & 1028G & 49.4 & $-$ \\\\\n        \\rowcolor{gray!20}\n        InternImage-S (ours) & 512$^2$ & 80M & 1017G & 50.1 & 50.9 \\\\\n        \\hline\n        Swin-B~\\cite{liu2021swin} \n        & 512$^2$ & 121M & 1188G & 48.1 & 49.7  \\\\\n        ConvNeXt-B~\\cite{liu2022convnet}  \n        & 512$^2$ & 122M & 1170G & 49.1 & 49.9  \\\\\n        RepLKNet-31B~\\cite{ding2022replknet} \n        & 512$^2$ & 112M & 1170G & 49.9 & 50.6  \\\\\n        SLaK-B~\\cite{liu2022slak} & 512$^2$ & 135M & 1172G & 50.2 & $-$    \\\\\n        \\rowcolor{gray!20}\n        InternImage-B (ours) \n        & 512$^2$ & 128M & 1185G & 50.8 & 51.3 \\\\\n        \\hline\n        Swin-L$^\\ddagger$~\\cite{liu2021swin} \n        & 640$^2$ & 234M & 2468G & 52.1 & 53.5 \\\\\n        RepLKNet-31L$^\\ddagger$~\\cite{ding2022replknet} \n        & 640$^2$ & 207M & 2404G & 52.4 & 52.7 \\\\\n        ConvNeXt-L$^\\ddagger$~\\cite{liu2022convnet} \n        & 640$^2$ & 235M & 2458G & 53.2 & 53.7 \\\\\n        ConvNeXt-XL$^\\ddagger$~\\cite{liu2022convnet} \n        & 640$^2$ & 391M & 3335G & 53.6 & 54.0 \\\\\n        \\rowcolor{gray!10}\n        InternImage-L$^\\ddagger$ (ours) \n        & 640$^2$ & 256M  & 2526G  & 53.9 & 54.1 \\\\\n        \\rowcolor{gray!20}\n        InternImage-XL$^\\ddagger$ (ours)\n        & 640$^2$ & 368M & 3142G & 55.0 & 55.3 \\\\\n        \\hline\n        SwinV2-G$^\\#$~\\cite{liu2021swinv2}\n        & 896$^2$ & 3.00B & $-$ & $-$ & 59.9 \\\\\n        \\rowcolor{gray!10}\n        InternImage-H$^\\#$ (ours)\n        & 896$^2$ & 1.12B & 3566G & 59.9 & 60.3 \\\\\n        \\hline\n        BEiT-3$^\\#$~\\cite{wang2022beit3} & 896$^2$ & 1.90B & $-$ & $-$ & 62.8 \\\\ \n        \\rowcolor{gray!20}\n        InternImage-H$^\\#$ (ours) + &  &  &  &  &  \\\\\n        \\rowcolor{gray!20}\n        Mask2Former~\\cite{cheng2021masked}\n        & \\multirow{-2}{*}{896$^2$}& \\multirow{-2}{*}{1.31B}& \\multirow{-2}{*}{4635G}& \\multirow{-2}{*}{62.5}& \\multirow{-2}{*}{62.9}\\\\\n    \\end{tabular}\n    \\caption{\\textbf{Semantic segmentation performance on the ADE20K validation set.}\n    \t``SS'' and ``MS\" means single-scale and multi-scale testing, respectively. \n    }\n    \\label{tab:seg}\n\\end{table}", "cell_list_gold": [{"value": "60M", "char_index": [352, 355], "type": "Hyper-parameter/Architecture", "model": "Swin-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "945G", "char_index": [359, 363], "type": "Hyper-parameter/Architecture", "model": "Swin-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "44.5", "char_index": [366, 370], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "Swin-T", "model settings": {"number of parameters": "60M", "number of FLOPs": "945G"}}, {"value": "45.8", "char_index": [373, 377], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "Swin-T", "model settings": {"number of parameters": "60M", "number of FLOPs": "945G"}}, {"value": "60M", "char_index": [431, 434], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "939G", "char_index": [437, 441], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "46.0", "char_index": [444, 448], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "ConvNeXt-T", "model settings": {"number of parameters": "60M", "number of FLOPs": "939G"}}, {"value": "46.7", "char_index": [451, 455], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "ConvNeXt-T", "model settings": {"number of parameters": "60M", "number of FLOPs": "939G"}}, {"value": "65M", "char_index": [502, 505], "type": "Hyper-parameter/Architecture", "model": "SLaK-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "936G", "char_index": [508, 512], "type": "Hyper-parameter/Architecture", "model": "SLaK-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "47.6", "char_index": [515, 519], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "SLaK-T", "model settings": {"number of parameters": "65M", "number of FLOPs": "936G"}}, {"value": "59M", "char_index": [591, 594], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "944G", "char_index": [597, 601], "type": "Hyper-parameter/Architecture", "model": "InternImage-T", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "47.9", "char_index": [604, 608], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "InternImage-T", "model settings": {"number of parameters": "59M", "number of FLOPs": "944G"}}, {"value": "48.1", "char_index": [611, 615], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "InternImage-T", "model settings": {"number of parameters": "59M", "number of FLOPs": "944G"}}, {"value": "81M", "char_index": [678, 681], "type": "Hyper-parameter/Architecture", "model": "Swin-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1038G", "char_index": [685, 690], "type": "Hyper-parameter/Architecture", "model": "Swin-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "47.6", "char_index": [694, 698], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "Swin-S", "model settings": {"number of parameters": "81M", "number of FLOPs": "1038G"}}, {"value": "49.5", "char_index": [702, 706], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "Swin-S", "model settings": {"number of parameters": "81M", "number of FLOPs": "1038G"}}, {"value": "82M", "char_index": [763, 766], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1027G", "char_index": [769, 774], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "48.7", "char_index": [778, 782], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "ConvNeXt-S", "model settings": {"number of parameters": "82M", "number of FLOPs": "1027G"}}, {"value": "49.6", "char_index": [785, 789], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "ConvNeXt-S", "model settings": {"number of parameters": "82M", "number of FLOPs": "1027G"}}, {"value": "91M", "char_index": [839, 842], "type": "Hyper-parameter/Architecture", "model": "SLaK-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1028G", "char_index": [845, 850], "type": "Hyper-parameter/Architecture", "model": "SLaK-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "49.4", "char_index": [853, 857], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "SLaK-S", "model settings": {"number of parameters": "91M", "number of FLOPs": "1028G"}}, {"value": "80M", "char_index": [935, 938], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1017G", "char_index": [941, 946], "type": "Hyper-parameter/Architecture", "model": "InternImage-S", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "50.1", "char_index": [949, 953], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "InternImage-S", "model settings": {"number of parameters": "80M", "number of FLOPs": "1017G"}}, {"value": "50.9", "char_index": [956, 960], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "InternImage-S", "model settings": {"number of parameters": "80M", "number of FLOPs": "1017G"}}, {"value": "121M", "char_index": [1034, 1038], "type": "Hyper-parameter/Architecture", "model": "Swin-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1188G", "char_index": [1041, 1046], "type": "Hyper-parameter/Architecture", "model": "Swin-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "48.1", "char_index": [1049, 1053], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "Swin-B", "model settings": {"number of parameters": "121M", "number of FLOPs": "1188G"}}, {"value": "49.7", "char_index": [1056, 1060], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "Swin-B", "model settings": {"number of parameters": "121M", "number of FLOPs": "1188G"}}, {"value": "122M", "char_index": [1128, 1132], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1170G", "char_index": [1135, 1140], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "49.1", "char_index": [1143, 1147], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "ConvNeXt-B", "model settings": {"number of parameters": "122M", "number of FLOPs": "1170G"}}, {"value": "49.9", "char_index": [1150, 1154], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "ConvNeXt-B", "model settings": {"number of parameters": "122M", "number of FLOPs": "1170G"}}, {"value": "112M", "char_index": [1225, 1229], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-31B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1170G", "char_index": [1232, 1237], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-31B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "49.9", "char_index": [1240, 1244], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "RepLKNet-31B", "model settings": {"number of parameters": "112M", "number of FLOPs": "1170G"}}, {"value": "50.6", "char_index": [1247, 1251], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "RepLKNet-31B", "model settings": {"number of parameters": "112M", "number of FLOPs": "1170G"}}, {"value": "135M", "char_index": [1302, 1306], "type": "Hyper-parameter/Architecture", "model": "SLaK-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1172G", "char_index": [1309, 1314], "type": "Hyper-parameter/Architecture", "model": "SLaK-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "50.2", "char_index": [1317, 1321], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "SLaK-B", "model settings": {"number of parameters": "135M", "number of FLOPs": "1172G"}}, {"value": "128M", "char_index": [1411, 1415], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "1185G", "char_index": [1418, 1423], "type": "Hyper-parameter/Architecture", "model": "InternImage-B", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "50.8", "char_index": [1426, 1430], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "single-scale testing": "true", "ss": "true"}, "model": "InternImage-B", "model settings": {"number of parameters": "128M", "number of FLOPs": "1185G"}}, {"value": "51.3", "char_index": [1433, 1437], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "512$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "InternImage-B", "model settings": {"number of parameters": "128M", "number of FLOPs": "1185G"}}, {"value": "234M", "char_index": [1522, 1526], "type": "Hyper-parameter/Architecture", "model": "Swin-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "2468G", "char_index": [1529, 1534], "type": "Hyper-parameter/Architecture", "model": "Swin-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "52.1", "char_index": [1537, 1541], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "single-scale testing": "true", "ss": "true"}, "model": "Swin-L", "model settings": {"number of parameters": "234M", "number of FLOPs": "2468G"}}, {"value": "53.5", "char_index": [1544, 1548], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "Swin-L", "model settings": {"number of parameters": "234M", "number of FLOPs": "2468G"}}, {"value": "207M", "char_index": [1629, 1633], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-31L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "2404G", "char_index": [1636, 1641], "type": "Hyper-parameter/Architecture", "model": "RepLKNet-31L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "52.4", "char_index": [1644, 1648], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "single-scale testing": "true", "ss": "true"}, "model": "RepLKNet-31L", "model settings": {"number of parameters": "207M", "number of FLOPs": "2404G"}}, {"value": "52.7", "char_index": [1651, 1655], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "RepLKNet-31L", "model settings": {"number of parameters": "207M", "number of FLOPs": "2404G"}}, {"value": "235M", "char_index": [1732, 1736], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "2458G", "char_index": [1739, 1744], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "53.2", "char_index": [1747, 1751], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "single-scale testing": "true", "ss": "true"}, "model": "ConvNeXt-L", "model settings": {"number of parameters": "235M", "number of FLOPs": "2458G"}}, {"value": "53.7", "char_index": [1754, 1758], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "ConvNeXt-L", "model settings": {"number of parameters": "235M", "number of FLOPs": "2458G"}}, {"value": "391M", "char_index": [1836, 1840], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-XL", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "3335G", "char_index": [1843, 1848], "type": "Hyper-parameter/Architecture", "model": "ConvNeXt-XL", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "53.6", "char_index": [1851, 1855], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "single-scale testing": "true", "ss": "true"}, "model": "ConvNeXt-XL", "model settings": {"number of parameters": "391M", "number of FLOPs": "3335G"}}, {"value": "54.0", "char_index": [1858, 1862], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "ConvNeXt-XL", "model settings": {"number of parameters": "391M", "number of FLOPs": "3335G"}}, {"value": "256M", "char_index": [1954, 1958], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "2526G", "char_index": [1962, 1967], "type": "Hyper-parameter/Architecture", "model": "InternImage-L", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "53.9", "char_index": [1971, 1975], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "single-scale testing": "true", "ss": "true"}, "model": "InternImage-L", "model settings": {"number of parameters": "256M", "number of FLOPs": "2526G"}}, {"value": "54.1", "char_index": [1978, 1982], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "InternImage-L", "model settings": {"number of parameters": "256M", "number of FLOPs": "2526G"}}, {"value": "368M", "char_index": [2074, 2078], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "3142G", "char_index": [2081, 2086], "type": "Hyper-parameter/Architecture", "model": "InternImage-XL", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "55.0", "char_index": [2089, 2093], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "single-scale testing": "true", "ss": "true"}, "model": "InternImage-XL", "model settings": {"number of parameters": "368M", "number of FLOPs": "3142G"}}, {"value": "55.3", "char_index": [2096, 2100], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "640$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "InternImage-XL", "model settings": {"number of parameters": "368M", "number of FLOPs": "3142G"}}, {"value": "3.00B", "char_index": [2182, 2187], "type": "Hyper-parameter/Architecture", "model": "SwinV2-G", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "59.9", "char_index": [2202, 2206], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "896$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "SwinV2-G", "model settings": {"number of parameters": "3.00B"}}, {"value": "1.12B", "char_index": [2291, 2296], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "3566G", "char_index": [2299, 2304], "type": "Hyper-parameter/Architecture", "model": "InternImage-H", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "59.9", "char_index": [2307, 2311], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "896$^2$", "single-scale testing": "true", "ss": "true"}, "model": "InternImage-H", "model settings": {"number of parameters": "1.12B", "number of FLOPs": "3566G"}}, {"value": "60.3", "char_index": [2314, 2318], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "896$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "InternImage-H", "model settings": {"number of parameters": "1.12B", "number of FLOPs": "3566G"}}, {"value": "1.90B", "char_index": [2390, 2395], "type": "Hyper-parameter/Architecture", "model": "BEiT-3", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "62.8", "char_index": [2410, 2414], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "896$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "BEiT-3", "model settings": {"number of parameters": "1.90B"}}, {"value": "1.31B", "char_index": [2624, 2629], "type": "Hyper-parameter/Architecture", "model": "InternImage-H + Mask2Former", "parameter/architecture name": ["# Params", "number of parameters"], "dataset": "ADE20K"}, {"value": "4635G", "char_index": [2649, 2654], "type": "Hyper-parameter/Architecture", "model": "InternImage-H + Mask2Former", "parameter/architecture name": ["# FLOPs", "number of FLOPs"], "dataset": "ADE20K"}, {"value": "62.5", "char_index": [2674, 2678], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "896$^2$", "single-scale testing": "true", "ss": "true"}, "model": "InternImage-H + Mask2Former", "model settings": {"number of parameters": "1.31B", "number of FLOPs": "4635G"}}, {"value": "62.9", "char_index": [2698, 2702], "type": "Result", "training data/set": "ADE20K", "test data/set": "ADE20K", "task": "Semantic Segmentation", "metric": "mIoU", "experimental settings": {"crop size": "896$^2$", "multi-scale testing": "true", "ms": "true"}, "model": "InternImage-H + Mask2Former", "model settings": {"number of parameters": "1.31B", "number of FLOPs": "4635G"}}]}}